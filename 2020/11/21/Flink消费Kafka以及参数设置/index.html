<!-- build time:Tue Jan 12 2021 23:56:36 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta name="referrer" content="no-referrer"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Flink消费Kafka以及参数设置 | 布兰特 | 不忘初心</title><meta name="description" content="在实时计算的场景下，绝大多数的数据源都是消息系统，而 Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；同时也讲了 Flink 作为生产者像 Kafka 写入数据的方式和代码实现。将从以下几个方面介绍 Flink 消费 Kafka 中的数据方式和源码实现。Kafka 连接 FlinkFlink 中支持了比较丰富的用来连接第三方的连接器，Kafka Connector 是 F"><meta property="og:type" content="article"><meta property="og:title" content="Flink消费Kafka以及参数设置"><meta property="og:url" content="cpeixin.cn/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/index.html"><meta property="og:site_name" content="布兰特 | 不忘初心"><meta property="og:description" content="在实时计算的场景下，绝大多数的数据源都是消息系统，而 Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；同时也讲了 Flink 作为生产者像 Kafka 写入数据的方式和代码实现。将从以下几个方面介绍 Flink 消费 Kafka 中的数据方式和源码实现。Kafka 连接 FlinkFlink 中支持了比较丰富的用来连接第三方的连接器，Kafka Connector 是 F"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1605948621977-67935dc7-b002-4650-9256-b6298b68c5ce.png#align=left&display=inline&height=696&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1392&originWidth=1514&size=276244&status=done&style=none&width=757"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1602237870349-32c86f4f-8115-4ce1-a4d6-8b197b0e513d.png#align=left&display=inline&height=1000&margin=%5Bobject%20Object%5D&originHeight=1000&originWidth=1928&size=0&status=done&style=none&width=1928"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1602237870409-f14c04fa-493b-4c04-ad56-39a6b896b8f7.png#align=left&display=inline&height=1022&margin=%5Bobject%20Object%5D&originHeight=1022&originWidth=2798&size=0&status=done&style=none&width=2798"><meta property="article:published_time" content="2020-11-21T10:25:43.000Z"><meta property="article:modified_time" content="2020-11-21T10:39:46.895Z"><meta property="article:author" content="Brent"><meta property="article:tag" content="Flink"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1605948621977-67935dc7-b002-4650-9256-b6298b68c5ce.png#align=left&display=inline&height=696&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1392&originWidth=1514&size=276244&status=done&style=none&width=757"><link rel="canonical" href="cpeixin.cn/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/index.html"><link rel="alternate" href="/atom.xml" title="布兰特 | 不忘初心" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.0"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Brent</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">大数据工程师 &amp; 机器学习</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Malaysia</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBases/">DataBases</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">31</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%B6%E6%9E%84/">架构</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97/">源码系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/">计算机组成原理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Apriori/" rel="tag">Apriori</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/COS/" rel="tag">COS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CPU/" rel="tag">CPU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DMP/" rel="tag">DMP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/" rel="tag">EM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ETL/" rel="tag">ETL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="tag-list-count">33</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/" rel="tag">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HashMap/" rel="tag">HashMap</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/" rel="tag">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-Means/" rel="tag">K-Means</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/" rel="tag">KNN</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" rel="tag">LRU淘汰算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OLAP/" rel="tag">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/" rel="tag">PageRank</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parquet/" rel="tag">Parquet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Random-Forest/" rel="tag">Random Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flask/" rel="tag">flask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kali/" rel="tag">kali</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/" rel="tag">mapreduce</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paxos/" rel="tag">paxos</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shadowsock/" rel="tag">shadowsock</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skipList/" rel="tag">skipList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/" rel="tag">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" rel="tag">单例模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A0%86/" rel="tag">堆</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/" rel="tag">布隆过滤器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%A3%E5%88%97%E8%A1%A8/" rel="tag">散列表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" rel="tag">数据仓库</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" rel="tag">数据清洗</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" rel="tag">数据采集</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" rel="tag">时间序列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" rel="tag">服务器安全</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%88/" rel="tag">栈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" rel="tag">用户画像</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" rel="tag">红黑树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" rel="tag">线性表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag">词向量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8/" rel="tag">链表</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%9F%E5%88%97/" rel="tag">队列</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/Apriori/" style="font-size:13.11px">Apriori</a> <a href="/tags/COS/" style="font-size:13px">COS</a> <a href="/tags/CPU/" style="font-size:13px">CPU</a> <a href="/tags/DMP/" style="font-size:13px">DMP</a> <a href="/tags/Decision-Tree/" style="font-size:13.44px">Decision Tree</a> <a href="/tags/EM/" style="font-size:13.11px">EM</a> <a href="/tags/ETL/" style="font-size:13px">ETL</a> <a href="/tags/Flink/" style="font-size:14px">Flink</a> <a href="/tags/GPT-2/" style="font-size:13px">GPT-2</a> <a href="/tags/HBase/" style="font-size:13.56px">HBase</a> <a href="/tags/HashMap/" style="font-size:13px">HashMap</a> <a href="/tags/Hive/" style="font-size:13px">Hive</a> <a href="/tags/IDEA/" style="font-size:13px">IDEA</a> <a href="/tags/K-Means/" style="font-size:13px">K-Means</a> <a href="/tags/KNN/" style="font-size:13.11px">KNN</a> <a href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" style="font-size:13px">LRU淘汰算法</a> <a href="/tags/Naive-Bayes/" style="font-size:13.11px">Naive Bayes</a> <a href="/tags/OLAP/" style="font-size:13px">OLAP</a> <a href="/tags/PageRank/" style="font-size:13.11px">PageRank</a> <a href="/tags/Parquet/" style="font-size:13px">Parquet</a> <a href="/tags/Random-Forest/" style="font-size:13px">Random Forest</a> <a href="/tags/SVM/" style="font-size:13.11px">SVM</a> <a href="/tags/docker/" style="font-size:13.11px">docker</a> <a href="/tags/flask/" style="font-size:13.11px">flask</a> <a href="/tags/flink/" style="font-size:13px">flink</a> <a href="/tags/hdfs/" style="font-size:13.33px">hdfs</a> <a href="/tags/hive/" style="font-size:13.44px">hive</a> <a href="/tags/java/" style="font-size:13px">java</a> <a href="/tags/kafka/" style="font-size:13.67px">kafka</a> <a href="/tags/kali/" style="font-size:13px">kali</a> <a href="/tags/mapreduce/" style="font-size:13.11px">mapreduce</a> <a href="/tags/mysql/" style="font-size:13.22px">mysql</a> <a href="/tags/paxos/" style="font-size:13.11px">paxos</a> <a href="/tags/python/" style="font-size:13.56px">python</a> <a href="/tags/redis/" style="font-size:13.33px">redis</a> <a href="/tags/scala/" style="font-size:13.33px">scala</a> <a href="/tags/shadowsock/" style="font-size:13px">shadowsock</a> <a href="/tags/skipList/" style="font-size:13px">skipList</a> <a href="/tags/sklearn/" style="font-size:13px">sklearn</a> <a href="/tags/spark/" style="font-size:13.89px">spark</a> <a href="/tags/yarn/" style="font-size:13px">yarn</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size:13.11px">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size:13.22px">二叉树</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size:13px">动态规划</a> <a href="/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" style="font-size:13px">单例模式</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size:13px">回溯</a> <a href="/tags/%E5%A0%86/" style="font-size:13px">堆</a> <a href="/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/" style="font-size:13px">布隆过滤器</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size:13.78px">排序</a> <a href="/tags/%E6%95%A3%E5%88%97%E8%A1%A8/" style="font-size:13px">散列表</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" style="font-size:13.78px">数据仓库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" style="font-size:13px">数据清洗</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" style="font-size:13px">数据采集</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size:13px">数组</a> <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" style="font-size:13px">时间序列</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" style="font-size:13.11px">服务器安全</a> <a href="/tags/%E6%A0%88/" style="font-size:13px">栈</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:13px">深度学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:13px">爬虫</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size:13.33px">特征工程</a> <a href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" style="font-size:13.11px">用户画像</a> <a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" style="font-size:13px">红黑树</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" style="font-size:13px">线性表</a> <a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" style="font-size:13px">词向量</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size:13px">递归</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size:13px">逻辑回归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size:13.11px">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size:13px">队列</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">二月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">九月 2015</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2021/01/12/Hive-Cli-%E5%90%AF%E5%8A%A8%E5%8D%A1%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98/" class="title">Hive Cli 启动卡死的问题</a></p><p class="item-date"><time datetime="2021-01-12T15:54:22.000Z" itemprop="datePublished">2021-01-12</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97/">源码系列</a></p><p class="item-title"><a href="/2020/12/06/MapReduce%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80/" class="title">MapReduce源码解析(一)</a></p><p class="item-date"><time datetime="2020-12-06T10:15:23.000Z" itemprop="datePublished">2020-12-06</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E6%96%B0%E6%A8%A1%E5%BC%8F-%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB/" class="title">大数据集群新模式-存算分离</a></p><p class="item-date"><time datetime="2020-11-29T15:02:43.000Z" itemprop="datePublished">2020-11-29</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/" class="title">Flink消费Kafka以及参数设置</a></p><p class="item-date"><time datetime="2020-11-21T10:25:43.000Z" itemprop="datePublished">2020-11-21</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/11/19/%E5%85%B3%E4%BA%8EHive%E4%B8%AD%E7%9A%84NULL/" class="title">关于Hive中的NULL</a></p><p class="item-date"><time datetime="2020-11-18T16:35:14.000Z" itemprop="datePublished">2020-11-19</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-Flink消费Kafka以及参数设置" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Flink消费Kafka以及参数设置</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/" class="article-date"><time datetime="2020-11-21T10:25:43.000Z" itemprop="datePublished">2020-11-21</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/Flink/" rel="tag">Flink</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/" class="leancloud_visitors" data-flag-title="Flink消费Kafka以及参数设置">0</span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 4k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 18(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p>在实时计算的场景下，绝大多数的数据源都是消息系统，而 Kafka 从众多的消息中间件中脱颖而出，主要是因为<strong>高吞吐</strong>、<strong>低延迟</strong>的特点；同时也讲了 Flink 作为生产者像 Kafka 写入数据的方式和代码实现。将从以下几个方面介绍 Flink 消费 Kafka 中的数据方式和源码实现。</p><h3 id="Kafka-连接-Flink"><a href="#Kafka-连接-Flink" class="headerlink" title="Kafka 连接 Flink"></a>Kafka 连接 Flink</h3><p>Flink 中支持了比较丰富的用来连接第三方的连接器，Kafka Connector 是 Flink 支持的各种各样的连接器中比较完善的之一。</p><p>Flink 提供了专门的 Kafka 连接器，向 Kafka Topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量。</p><p>同时也提过，我们在使用 Kafka 连接器时需要引用相对应的 Jar 包依赖。对于某些连接器比如 Kafka 是有版本要求的，一定要去官方网站找到对应的依赖版本。我在下表中给出了不同版本的 Kafka，以及对应的 Connector 关系：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1605948621977-67935dc7-b002-4650-9256-b6298b68c5ce.png#align=left&display=inline&height=696&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1392&originWidth=1514&size=276244&status=done&style=none&width=757" alt="image.png"></p><h4 id="Kafka-本地环境搭建"><a href="#Kafka-本地环境搭建" class="headerlink" title="Kafka 本地环境搭建"></a>Kafka 本地环境搭建</h4><p>我们在本地环境搭建一个 Kafka_2.11-2.1.0 版本的 Kafka 单机环境，然后模拟一些数据写入到队列中。</p><p>我们可以在这里下载对应版本的 Kafka，把压缩包进行解压，然后使用下面的命令启动单机版本的 Kafka。</p><p>解压：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; tar -xzf kafka_2.11-2.1.0.tgz</span><br><span class="line">&gt; cd kafka_2.11-2.1.0</span><br></pre></td></tr></table></figure><p>启动 ZooKeeper 和 Kafka Server：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动ZK：nohup bin&#x2F;zookeeper-server-start.sh config&#x2F;zookeeper.properties  &amp;</span><br><span class="line">启动Server: </span><br><span class="line">nohup bin&#x2F;kafka-server-start.sh config&#x2F;server.properties &amp;</span><br></pre></td></tr></table></figure><p>创建一个名为 test 的 Topic：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure><h4><a href="#" class="headerlink"></a></h4><h4 id="Kafka-Producer"><a href="#Kafka-Producer" class="headerlink" title="Kafka Producer"></a>Kafka Producer</h4><p>首先我们需要新增一个依赖，然后向名为 test 的 Topic 中写入数据。新增 Maven 依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;flink-connector-kafka_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">   &lt;version&gt;1.10.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><p>向Topic中写入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public class KafkaProducer &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; text &#x3D; env.addSource(new MyNoParalleSource()).setParallelism(1);</span><br><span class="line"></span><br><span class="line">        Properties properties &#x3D; new Properties();</span><br><span class="line"></span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 2.0 配置 KafkaProducer</span><br><span class="line"></span><br><span class="line">        FlinkKafkaProducer&lt;String&gt; producer &#x3D; new FlinkKafkaProducer&lt;String&gt;(</span><br><span class="line"></span><br><span class="line">                &quot;127.0.0.1:9092&quot;, &#x2F;&#x2F;broker 列表</span><br><span class="line"></span><br><span class="line">                &quot;test&quot;,           &#x2F;&#x2F;topic</span><br><span class="line"></span><br><span class="line">                new SimpleStringSchema()); &#x2F;&#x2F; 消息序列化</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;写入 Kafka 时附加记录的事件时间戳</span><br><span class="line"></span><br><span class="line">        producer.setWriteTimestampToKafka(true);</span><br><span class="line"></span><br><span class="line">        text.addSink(producer);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是，我们这里使用了一个自定义的 MyNoParalleSource 类，该类使用了 Flink 提供的自定义 Source 方法，该方法会源源不断地产生一些测试数据，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">public class MyNoParalleSource implements SourceFunction&lt;String&gt; &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;private long count &#x3D; 1L;</span><br><span class="line"></span><br><span class="line">    private boolean isRunning &#x3D; true;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line"></span><br><span class="line">     * 主要的方法</span><br><span class="line"></span><br><span class="line">     * 启动一个source</span><br><span class="line"></span><br><span class="line">     * 大部分情况下，都需要在这个run方法中实现一个循环，这样就可以循环产生数据了</span><br><span class="line"></span><br><span class="line">     *</span><br><span class="line"></span><br><span class="line">     * @param ctx</span><br><span class="line"></span><br><span class="line">     * @throws Exception</span><br><span class="line"></span><br><span class="line">     *&#x2F;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line"></span><br><span class="line">    public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        while(isRunning)&#123;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;图书的排行榜</span><br><span class="line"></span><br><span class="line">            List&lt;String&gt; books &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            books.add(&quot;Pyhton从入门到放弃&quot;);&#x2F;&#x2F;10</span><br><span class="line"></span><br><span class="line">            books.add(&quot;Java从入门到放弃&quot;);&#x2F;&#x2F;8</span><br><span class="line"></span><br><span class="line">            books.add(&quot;Php从入门到放弃&quot;);&#x2F;&#x2F;5</span><br><span class="line"></span><br><span class="line">            books.add(&quot;C++从入门到放弃&quot;);&#x2F;&#x2F;3</span><br><span class="line"></span><br><span class="line">            books.add(&quot;Scala从入门到放弃&quot;);</span><br><span class="line"></span><br><span class="line">            int i &#x3D; new Random().nextInt(5);</span><br><span class="line"></span><br><span class="line">            ctx.collect(books.get(i));</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;每2秒产生一条数据</span><br><span class="line"></span><br><span class="line">            Thread.sleep(2000);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;取消一个cancel的时候会调用的方法</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line"></span><br><span class="line">    public void cancel() &#123;</span><br><span class="line"></span><br><span class="line">        isRunning &#x3D; false;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-如何消费-Kafka"><a href="#Flink-如何消费-Kafka" class="headerlink" title="Flink 如何消费 Kafka"></a>Flink 如何消费 Kafka</h3><p>Flink 在和 Kafka 对接的过程中，跟 Kafka 的版本是强相关的。我们在使用 Kafka 连接器时需要引用相对应的 Jar 包依赖，对于某些连接器比如 Kafka 是有版本要求的，一定要去<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html" target="_blank" rel="external nofollow noopener noreferrer">官方网站</a>找到对应的依赖版本。<br>我们本地的 Kafka 版本是 2.1.0，所以需要对应的类是 FlinkKafkaConsumer。首先需要在 pom.xml 中引入 jar 包依赖：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-connector-kafka_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><p>下面将对 Flink 消费 Kafka 数据的方式进行分类讲解。</p><h4 id="消费单个-Topic"><a href="#消费单个-Topic" class="headerlink" title="消费单个 Topic"></a>消费单个 Topic</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>);</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"127.0.0.1:9092"</span>);</span><br><span class="line">    <span class="comment">// 如果你是0.8版本的Kafka，需要配置</span></span><br><span class="line">    <span class="comment">//properties.setProperty("zookeeper.connect", "localhost:2181");</span></span><br><span class="line">    <span class="comment">//设置消费组</span></span><br><span class="line">    properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"group_test"</span>);</span><br><span class="line">    FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(<span class="string">"test"</span>, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">    <span class="comment">//设置从最早的ffset消费</span></span><br><span class="line">    consumer.setStartFromEarliest();</span><br><span class="line">    <span class="comment">//还可以手动指定相应的 topic, partition，offset,然后从指定好的位置开始消费</span></span><br><span class="line">    <span class="comment">//HashMap&lt;KafkaTopicPartition, Long&gt; map = new HashMap&lt;&gt;();</span></span><br><span class="line">    <span class="comment">//map.put(new KafkaTopicPartition("test", 1), 10240L);</span></span><br><span class="line">    <span class="comment">//假如partition有多个，可以指定每个partition的消费位置</span></span><br><span class="line">    <span class="comment">//map.put(new KafkaTopicPartition("test", 2), 10560L);</span></span><br><span class="line">    <span class="comment">//然后各个partition从指定位置消费</span></span><br><span class="line">    <span class="comment">//consumer.setStartFromSpecificOffsets(map);</span></span><br><span class="line">    env.addSource(consumer).flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            System.out.println(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    env.execute(<span class="string">"start consumer..."</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在设置消费 Kafka 中的数据时，可以显示地指定从某个 Topic 的每一个 Partition 中进行消费。</p><h4 id="消费多个-Topic"><a href="#消费多个-Topic" class="headerlink" title="消费多个 Topic"></a>消费多个 Topic</h4><p>我们的业务中会有这样的情况，同样的数据根据类型不同发送到了不同的 Topic 中，比如线上的订单数据根据来源不同分别发往移动端和 PC 端两个 Topic 中。但是我们不想把同样的代码复制一份，需重新指定一个 Topic 进行消费，这时候应该怎么办呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Properties properties &#x3D; new Properties();</span><br><span class="line">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);</span><br><span class="line">&#x2F;&#x2F; 如果你是0.8版本的Kafka，需要配置</span><br><span class="line">&#x2F;&#x2F;properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);</span><br><span class="line">&#x2F;&#x2F;设置消费组</span><br><span class="line">properties.setProperty(&quot;group.id&quot;, &quot;group_test&quot;);</span><br><span class="line">FlinkKafkaConsumer&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">ArrayList&lt;String&gt; topics &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">        topics.add(&quot;test_A&quot;);</span><br><span class="line">        topics.add(&quot;test_B&quot;);</span><br><span class="line">&#x2F;&#x2F; 传入一个 list，完美解决了这个问题</span><br><span class="line">FlinkKafkaConsumer&lt;Tuple2&lt;String, String&gt;&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(topics, new SimpleStringSchema(), properties);</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>我们可以传入一个 list 来解决消费多个 Topic 的问题，如果用户需要区分两个 Topic 中的数据，那么需要在发往 Kafka 中数据新增一个字段，用来区分来源。</p><h4 id="消息序列化"><a href="#消息序列化" class="headerlink" title="消息序列化"></a>消息序列化</h4><p>我们在上述消费 Kafka 消息时，都默认指定了消息的序列化方式，即 SimpleStringSchema。这里需要注意的是，在我们使用 SimpleStringSchema 的时候，返回的结果中只有原数据，没有 topic、parition 等信息，这时候可以自定义序列化的方式来实现自定义返回数据的结构。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public class CustomDeSerializationSchema implements KafkaDeserializationSchema&lt;ConsumerRecord&lt;String, String&gt;&gt; &#123;</span><br><span class="line">    &#x2F;&#x2F;是否表示流的最后一条元素,设置为false，表示数据会源源不断地到来</span><br><span class="line">    @Override</span><br><span class="line">    public boolean isEndOfStream(ConsumerRecord&lt;String, String&gt; nextElement) &#123;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;这里返回一个ConsumerRecord&lt;String,String&gt;类型的数据，除了原数据还包括topic，offset，partition等信息</span><br><span class="line">    @Override</span><br><span class="line">    public ConsumerRecord&lt;String, String&gt; deserialize(ConsumerRecord&lt;byte[], byte[]&gt; record) throws Exception &#123;</span><br><span class="line">        return new ConsumerRecord&lt;String, String&gt;(</span><br><span class="line">                record.topic(),</span><br><span class="line">                record.partition(),</span><br><span class="line">                record.offset(),</span><br><span class="line">                new String(record.key()),</span><br><span class="line">                new String(record.value())</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;指定数据的输入类型</span><br><span class="line">    @Override</span><br><span class="line">    public TypeInformation&lt;ConsumerRecord&lt;String, String&gt;&gt; getProducedType() &#123;</span><br><span class="line">        return TypeInformation.of(new TypeHint&lt;ConsumerRecord&lt;String, String&gt;&gt;()&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里自定义了 CustomDeSerializationSchema 信息，就可以直接使用了。</p><h4 id="-1"><a href="#-1" class="headerlink"></a></h4><h4 id="Parition-和-Topic-动态发现"><a href="#Parition-和-Topic-动态发现" class="headerlink" title="Parition 和 Topic 动态发现"></a>Parition 和 Topic 动态发现</h4><p>在很多场景下，随着业务的扩展，我们需要对 Kafka 的分区进行扩展，为了防止新增的分区没有被及时发现导致数据丢失，消费者必须要感知 Partition 的动态变化，可以使用 FlinkKafkaConsumer 的动态分区发现实现。</p><p>我们只需要指定下面的配置，即可打开动态分区发现功能：每隔 10ms 会动态获取 Topic 的元数据，对于新增的 Partition 会自动从最早的位点开始消费数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, &quot;10&quot;);</span><br></pre></td></tr></table></figure><p>如果业务场景需要我们动态地发现 Topic，可以指定 Topic 的正则表达式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(Pattern.compile(&quot;^test_([A-Za-z0-9]*)$&quot;), new SimpleStringSchema(), properties);</span><br></pre></td></tr></table></figure><h4 id="-2"><a href="#-2" class="headerlink"></a></h4><h4 id="Flink-消费-Kafka-设置-offset-的方法"><a href="#Flink-消费-Kafka-设置-offset-的方法" class="headerlink" title="Flink 消费 Kafka 设置 offset 的方法"></a>Flink 消费 Kafka 设置 offset 的方法</h4><p>Flink 消费 Kafka 需要指定消费的 offset，也就是<strong>偏移量</strong>。Flink 读取 Kafka 的消息有五种消费方式：</p><ul><li><p>指定 Topic 和 Partition</p></li><li><p>从最早位点开始消费</p></li><li><p>从指定时间点开始消费</p></li><li><p>从最新的数据开始消费</p></li><li><p>从上次消费位点开始消费</p></li></ul><p>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line">* Flink从指定的topic和parition中指定的offset开始</span><br><span class="line">*&#x2F;</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; offsets &#x3D; new HashedMap();</span><br><span class="line">offsets.put(new KafkaTopicPartition(&quot;test&quot;, 0), 10000L);</span><br><span class="line">offsets.put(new KafkaTopicPartition(&quot;test&quot;, 1), 20000L);</span><br><span class="line">offsets.put(new KafkaTopicPartition(&quot;test&quot;, 2), 30000L);</span><br><span class="line">consumer.setStartFromSpecificOffsets(offsets);</span><br><span class="line">&#x2F;**</span><br><span class="line">* Flink从topic中最早的offset消费</span><br><span class="line">*&#x2F;</span><br><span class="line">consumer.setStartFromEarliest();</span><br><span class="line">&#x2F;**</span><br><span class="line">* Flink从topic中指定的时间点开始消费</span><br><span class="line">*&#x2F;</span><br><span class="line">consumer.setStartFromTimestamp(1559801580000l);</span><br><span class="line">&#x2F;**</span><br><span class="line">* Flink从topic中最新的数据开始消费</span><br><span class="line">*&#x2F;</span><br><span class="line">consumer.setStartFromLatest();</span><br><span class="line">&#x2F;**</span><br><span class="line">* Flink从topic中指定的group上次消费的位置开始消费，所以必须配置group.id参数</span><br><span class="line">*&#x2F;</span><br><span class="line">consumer.setStartFromGroupOffsets();</span><br></pre></td></tr></table></figure><h4 id="Offset提交"><a href="#Offset提交" class="headerlink" title="Offset提交"></a>Offset提交</h4><p>Flink Kafka Consumer允许配置offset提交回Kafka brokers(Kafka 0.8是写回Zookeeper)的行为，注意Flink Kafka Consumer 并不依赖于这个提交的offset来进行容错性保证，这个提交的offset仅仅作为监控consumer处理进度的一种手段。</p><p>配置offset提交行为的方式有多种，主要取决于Job的checkpoint机制是否启动。</p><p>1、<strong>checkpoint禁用</strong>:如果checkpoint禁用，Flink Kafka Consumer依赖于Kafka 客户端内部的自动周期性offset提交能力。因此，为了启用或者禁用offset提交，仅需在给定的Properties配置中设置enable.auto.commit / auto.commit.interval.ms，就会按固定的时间间隔定期 auto commit offset 到 kafka。</p><p>2、<strong>checkpoint启用</strong>:如果checkpoint启用，当checkpoint完成之后，Flink Kafka Consumer将会提交offset保存到checkpoint State中，这个时候作业消费的 offset 是 Flink 在 state 中自己管理和容错，保证了kafka broker中的committed offset与 checkpoint stata中的offset相一致。用户可以在Consumer中调用<code>setCommitOffsetsOnCheckpoints(boolean)</code> 方法来选择启用或者禁用offset committing，默认的情况下是setCommitOffsetsOnCheckpoints(true)，checkpoint成功后，将offset同步给kafka。注意，在这种情况下，配置在Properties中的自动周期性offset提交将会被完全忽略。</p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1602237870349-32c86f4f-8115-4ce1-a4d6-8b197b0e513d.png#align=left&display=inline&height=1000&margin=%5Bobject%20Object%5D&originHeight=1000&originWidth=1928&size=0&status=done&style=none&width=1928" alt><br>从上面的类图可以看出，FlinkKafkaConsumer 继承了 FlinkKafkaConsumerBase，而 FlinkKafkaConsumerBase 最终是对 SourceFunction 进行了实现。</p><p>整体的流程：FlinkKafkaConsumer 首先创建了 KafkaFetcher 对象，然后 KafkaFetcher 创建了 KafkaConsumerThread 和 Handover，KafkaConsumerThread 负责直接从 Kafka 中读取 msg，并交给 Handover，然后 Handover 将 msg 传递给 KafkaFetcher.emitRecord 将消息发出。</p><p>因为 FlinkKafkaConsumerBase 实现了 RichFunction 接口，所以当程序启动的时候，会首先调用 FlinkKafkaConsumerBase.open 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">public void open(Configuration configuration) throws Exception &#123;</span><br><span class="line">   &#x2F;&#x2F; 指定offset的提交方式</span><br><span class="line">   this.offsetCommitMode &#x3D; OffsetCommitModes.fromConfiguration(</span><br><span class="line">         getIsAutoCommitEnabled(),</span><br><span class="line">         enableCommitOnCheckpoints,</span><br><span class="line">         ((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled());</span><br><span class="line">   &#x2F;&#x2F; 创建分区发现器</span><br><span class="line">   this.partitionDiscoverer &#x3D; createPartitionDiscoverer(</span><br><span class="line">         topicsDescriptor,</span><br><span class="line">         getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">         getRuntimeContext().getNumberOfParallelSubtasks());</span><br><span class="line">   this.partitionDiscoverer.open();</span><br><span class="line">   subscribedPartitionsToStartOffsets &#x3D; new HashMap&lt;&gt;();</span><br><span class="line">   final List&lt;KafkaTopicPartition&gt; allPartitions &#x3D; partitionDiscoverer.discoverPartitions();</span><br><span class="line">   if (restoredState !&#x3D; null) &#123;</span><br><span class="line">      for (KafkaTopicPartition partition : allPartitions) &#123;</span><br><span class="line">         if (!restoredState.containsKey(partition)) &#123;</span><br><span class="line">            restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET);</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      for (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123;</span><br><span class="line">         if (!restoredFromOldState) &#123;</span><br><span class="line">           </span><br><span class="line">            if (KafkaTopicPartitionAssigner.assign(</span><br><span class="line">               restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks())</span><br><span class="line">                  &#x3D;&#x3D; getRuntimeContext().getIndexOfThisSubtask())&#123;</span><br><span class="line">               subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">            &#125;</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">           subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      if (filterRestoredPartitionsWithCurrentTopicsDescriptor) &#123;</span><br><span class="line">         subscribedPartitionsToStartOffsets.entrySet().removeIf(entry -&gt; &#123;</span><br><span class="line">            if (!topicsDescriptor.isMatchingTopic(entry.getKey().getTopic())) &#123;</span><br><span class="line">               LOG.warn(</span><br><span class="line">                  &quot;&#123;&#125; is removed from subscribed partitions since it is no longer associated with topics descriptor of current execution.&quot;,</span><br><span class="line">                  entry.getKey());</span><br><span class="line">               return true;</span><br><span class="line">            &#125;</span><br><span class="line">            return false;</span><br><span class="line">         &#125;);</span><br><span class="line">      &#125;</span><br><span class="line">      LOG.info(&quot;Consumer subtask &#123;&#125; will start reading &#123;&#125; partitions with offsets in restored state: &#123;&#125;&quot;,</span><br><span class="line">         getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">    </span><br><span class="line">      switch (startupMode) &#123;</span><br><span class="line">         case SPECIFIC_OFFSETS:</span><br><span class="line">            if (specificStartupOffsets &#x3D;&#x3D; null) &#123;</span><br><span class="line">               throw new IllegalStateException(</span><br><span class="line">                  &quot;Startup mode for the consumer set to &quot; + StartupMode.SPECIFIC_OFFSETS +</span><br><span class="line">                     &quot;, but no specific offsets were specified.&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">            for (KafkaTopicPartition seedPartition : allPartitions) &#123;</span><br><span class="line">               Long specificOffset &#x3D; specificStartupOffsets.get(seedPartition);</span><br><span class="line">               if (specificOffset !&#x3D; null) &#123;</span><br><span class="line">                                 subscribedPartitionsToStartOffsets.put(seedPartition, specificOffset - 1);</span><br><span class="line">               &#125; else &#123;</span><br><span class="line">               subscribedPartitionsToStartOffsets.put(seedPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</span><br><span class="line">               &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            break;</span><br><span class="line">         case TIMESTAMP:</span><br><span class="line">            if (startupOffsetsTimestamp &#x3D;&#x3D; null) &#123;</span><br><span class="line">               throw new IllegalStateException(</span><br><span class="line">                  &quot;Startup mode for the consumer set to &quot; + StartupMode.TIMESTAMP +</span><br><span class="line">                     &quot;, but no startup timestamp was specified.&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">            for (Map.Entry&lt;KafkaTopicPartition, Long&gt; partitionToOffset</span><br><span class="line">                  : fetchOffsetsWithTimestamp(allPartitions, startupOffsetsTimestamp).entrySet()) &#123;</span><br><span class="line">               subscribedPartitionsToStartOffsets.put(</span><br><span class="line">                  partitionToOffset.getKey(),</span><br><span class="line">                  (partitionToOffset.getValue() &#x3D;&#x3D; null)</span><br><span class="line">                      KafkaTopicPartitionStateSentinel.LATEST_OFFSET</span><br><span class="line">                        : partitionToOffset.getValue() - 1);</span><br><span class="line">            &#125;</span><br><span class="line">            break;</span><br><span class="line">         default:</span><br><span class="line">            for (KafkaTopicPartition seedPartition : allPartitions) &#123;</span><br><span class="line">               subscribedPartitionsToStartOffsets.put(seedPartition, startupMode.getStateSentinel());</span><br><span class="line">            &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      if (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line">         switch (startupMode) &#123;</span><br><span class="line">            case EARLIEST:</span><br><span class="line">               LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the earliest offsets: &#123;&#125;&quot;,</span><br><span class="line">                  getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line">               break;</span><br><span class="line">            case LATEST:</span><br><span class="line">               LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the latest offsets: &#123;&#125;&quot;,</span><br><span class="line">                  getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line">               break;</span><br><span class="line">            case TIMESTAMP:</span><br><span class="line">               LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from timestamp &#123;&#125;: &#123;&#125;&quot;,</span><br><span class="line">                  getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">                  startupOffsetsTimestamp,</span><br><span class="line">                  subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line">               break;</span><br><span class="line">            case SPECIFIC_OFFSETS:</span><br><span class="line">               LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the specified startup offsets &#123;&#125;: &#123;&#125;&quot;,</span><br><span class="line">                  getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">                  specificStartupOffsets,</span><br><span class="line">                  subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line">               List&lt;KafkaTopicPartition&gt; partitionsDefaultedToGroupOffsets &#x3D; new ArrayList&lt;&gt;(subscribedPartitionsToStartOffsets.size());</span><br><span class="line">               for (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">                  if (subscribedPartition.getValue() &#x3D;&#x3D; KafkaTopicPartitionStateSentinel.GROUP_OFFSET) &#123;</span><br><span class="line">                     partitionsDefaultedToGroupOffsets.add(subscribedPartition.getKey());</span><br><span class="line">                  &#125;</span><br><span class="line">               &#125;</span><br><span class="line">               if (partitionsDefaultedToGroupOffsets.size() &gt; 0) &#123;</span><br><span class="line">                  LOG.warn(&quot;Consumer subtask &#123;&#125; cannot find offsets for the following &#123;&#125; partitions in the specified startup offsets: &#123;&#125;&quot; +</span><br><span class="line">                        &quot;; their startup offsets will be defaulted to their committed group offsets in Kafka.&quot;,</span><br><span class="line">                     getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                     partitionsDefaultedToGroupOffsets.size(),</span><br><span class="line">                     partitionsDefaultedToGroupOffsets);</span><br><span class="line">               &#125;</span><br><span class="line">               break;</span><br><span class="line">            case GROUP_OFFSETS:</span><br><span class="line">               LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the committed group offsets in Kafka: &#123;&#125;&quot;,</span><br><span class="line">                  getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">                  subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line">         &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">         LOG.info(&quot;Consumer subtask &#123;&#125; initially has no partitions to read from.&quot;,</span><br><span class="line">            getRuntimeContext().getIndexOfThisSubtask());</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对 Kafka 中的 Topic 和 Partition 的数据进行读取的核心逻辑都在 run 方法中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">public void run(SourceContext&lt;T&gt; sourceContext) throws Exception &#123;</span><br><span class="line">   if (subscribedPartitionsToStartOffsets &#x3D;&#x3D; null) &#123;</span><br><span class="line">      throw new Exception(&quot;The partitions were not set for the consumer&quot;);</span><br><span class="line">   &#125;</span><br><span class="line">   this.successfulCommits &#x3D; this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);</span><br><span class="line">   this.failedCommits &#x3D;  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);</span><br><span class="line">   final int subtaskIndex &#x3D; this.getRuntimeContext().getIndexOfThisSubtask();</span><br><span class="line">   this.offsetCommitCallback &#x3D; new KafkaCommitCallback() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public void onSuccess() &#123;</span><br><span class="line">         successfulCommits.inc();</span><br><span class="line">      &#125;</span><br><span class="line">      @Override</span><br><span class="line">      public void onException(Throwable cause) &#123;</span><br><span class="line">         LOG.warn(String.format(&quot;Consumer subtask %d failed async Kafka commit.&quot;, subtaskIndex), cause);</span><br><span class="line">         failedCommits.inc();</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;;</span><br><span class="line">   if (subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line">      sourceContext.markAsTemporarilyIdle();</span><br><span class="line">   &#125;</span><br><span class="line">   LOG.info(&quot;Consumer subtask &#123;&#125; creating fetcher with offsets &#123;&#125;.&quot;,</span><br><span class="line">      getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets);</span><br><span class="line">  </span><br><span class="line">   this.kafkaFetcher &#x3D; createFetcher(</span><br><span class="line">         sourceContext,</span><br><span class="line">         subscribedPartitionsToStartOffsets,</span><br><span class="line">         periodicWatermarkAssigner,</span><br><span class="line">         punctuatedWatermarkAssigner,</span><br><span class="line">         (StreamingRuntimeContext) getRuntimeContext(),</span><br><span class="line">         offsetCommitMode,</span><br><span class="line">         getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),</span><br><span class="line">         useMetrics);</span><br><span class="line">   if (!running) &#123;</span><br><span class="line">      return;</span><br><span class="line">   &#125;</span><br><span class="line">   if (discoveryIntervalMillis &#x3D;&#x3D; PARTITION_DISCOVERY_DISABLED) &#123;</span><br><span class="line">      kafkaFetcher.runFetchLoop();</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">      runWithPartitionDiscovery();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="-3"><a href="#-3" class="headerlink"></a></h3><h3 id="Flink-消费-Kafka-数据代码"><a href="#Flink-消费-Kafka-数据代码" class="headerlink" title="Flink 消费 Kafka 数据代码"></a>Flink 消费 Kafka 数据代码</h3><p>上面介绍了 Flink 消费 Kafka 的方式，以及消息序列化的方式，同时介绍了分区和 Topic 的动态发现方法，那么回到我们的项目中来，消费 Kafka 数据的完整代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class KafkaConsumer &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties &#x3D; new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);</span><br><span class="line">        &#x2F;&#x2F;设置消费组</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;group_test&quot;);</span><br><span class="line">        properties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, &quot;10&quot;);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        &#x2F;&#x2F;设置从最早的ffset消费</span><br><span class="line">        consumer.setStartFromEarliest();</span><br><span class="line">        env.addSource(consumer).flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123;</span><br><span class="line">                System.out.println(value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        env.execute(&quot;start consumer...&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以直接右键运行代码，在控制台中可以看到数据的正常打印，如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1602237870409-f14c04fa-493b-4c04-ad56-39a6b896b8f7.png#align=left&display=inline&height=1022&margin=%5Bobject%20Object%5D&originHeight=1022&originWidth=2798&size=0&status=done&style=none&width=2798" alt><br>通过代码可知，我们之前发往 Kafka 的消息被完整地打印出来了。</p><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>如果checkpoint时间过长，offset未提交到kafka，此时节点宕机了，重启之后的重复消费如何保证呢？</p><p><strong>首先开启checkpoint时offset是flink通过状态state管理和恢复的</strong>，并不是从kafka的offset位置恢复。在checkpoint机制下，作业从最近一次checkpoint恢复，本身是会回放部分历史数据，导致部分数据重复消费，Flink引擎仅保证计算状态的精准一次，<strong>要想做到端到端精准一次需要依赖一些幂等的存储系统或者事务操作。</strong></p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="cpeixin.cn/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/" title="Flink消费Kafka以及参数设置" target="_blank" rel="external">cpeixin.cn/2020/11/21/Flink%E6%B6%88%E8%B4%B9Kafka%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external nofollow noopener noreferrer">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/cpeixin" target="_blank" class="img-burn thumb-sm visible-lg" rel="external nofollow noopener noreferrer"><img src="/images/avatar.jpg" class="img-rounded w-full" alt></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><span class="text-dark">Brent</span><small class="ml-1x">大数据工程师 &amp; 机器学习</small></a></h3><div>一心九用的工程师</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2020/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E6%96%B0%E6%A8%A1%E5%BC%8F-%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB/" title="大数据集群新模式-存算分离"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2020/11/19/%E5%85%B3%E4%BA%8EHive%E4%B8%AD%E7%9A%84NULL/" title="关于Hive中的NULL"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank" rel="external nofollow noopener noreferrer">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank" rel="external nofollow noopener noreferrer">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"SsxmBzBQ3R2S2zWTv0FrONel-gzGzoHsz",appKey:"w0K528Ye7NhOr07RHrzVzHbW",placeholder:"说点什么呢？",avatar:"mm",meta:meta,pageSize:"10",visitor:!0})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html><script type="text/javascript" src="//cdn.jsdelivr.net/gh/ygbhf/clicklove/clicklove.js"></script><!-- rebuild by neat -->