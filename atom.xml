<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布兰特 | 不忘初心</title>
  
  <subtitle>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="cpeixin.cn/"/>
  <updated>2020-04-05T15:10:24.541Z</updated>
  <id>cpeixin.cn/</id>
  
  <author>
    <name>Brent</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>def neverGrowUp()</title>
    <link href="cpeixin.cn/2020/04/06/def-neverGrowUp/"/>
    <id>cpeixin.cn/2020/04/06/def-neverGrowUp/</id>
    <published>2020-04-05T16:00:00.000Z</published>
    <updated>2020-04-05T15:10:24.541Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neverGrowUp</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="title">while</span> <span class="title">true</span>:</span></span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗疫英雄</title>
    <link href="cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/"/>
    <id>cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/</id>
    <published>2020-04-04T14:45:15.000Z</published>
    <updated>2020-04-05T14:46:33.308Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>致敬缅怀每一位抗疫英雄<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586098032229-b4c6c795-bf87-4105-8f82-87a86e48a89a.jpeg#align=left&display=inline&height=1796&name=WechatIMG86.jpeg&originHeight=1796&originWidth=1072&size=175464&status=done&style=none&width=1072" alt="WechatIMG86.jpeg"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;致敬缅怀每一位抗疫英雄&lt;br&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/jpeg/1072113
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python Flask &amp; Ajax 数据传输</title>
    <link href="cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/"/>
    <id>cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/</id>
    <published>2020-03-11T14:43:01.000Z</published>
    <updated>2020-04-04T17:13:00.080Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂</p><p>忙活三个小时，终于把前端和后端打通了～～</p><p>前端demo：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"&#123;&#123; url_for('send_message') &#125;&#125;"</span> <span class="attr">method</span>=<span class="string">"post"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span> =<span class="string">"domain"</span> <span class="attr">rows</span>=<span class="string">"30"</span> <span class="attr">cols</span>=<span class="string">"100"</span> <span class="attr">placeholder</span>=<span class="string">"请输入需要查询的域名,如cq5999.com"</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;input id="submit" type="submit" value="发送"&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">id</span>=<span class="string">"btn-bq"</span> <span class="attr">data-toggle</span>=<span class="string">"modal"</span> <span class="attr">data-target</span>=<span class="string">"#myModal"</span>&gt;</span>查询<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"send_content"</span>&gt;</span>向后台发送消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"send_content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send"</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"发送"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"recv_content"</span>&gt;</span>从后台接收消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"recv_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"recv_content"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 发送 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> message = $(<span class="string">"#send_content"</span>).val()</span></span><br><span class="line">        alert(message)</span><br><span class="line"><span class="javascript">        $.ajax(&#123;</span></span><br><span class="line"><span class="actionscript">            url:<span class="string">"/send_message"</span>,</span></span><br><span class="line"><span class="actionscript">            type:<span class="string">"POST"</span>,</span></span><br><span class="line">            data:&#123;</span><br><span class="line">                message:message</span><br><span class="line">            &#125;,</span><br><span class="line"><span class="actionscript">            dataType: <span class="string">'json'</span>,</span></span><br><span class="line"><span class="actionscript">            success:<span class="function"><span class="keyword">function</span> <span class="params">(data)</span> </span>&#123;</span></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 接收 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        $.getJSON(<span class="string">"/change_to_json"</span>,<span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#recv_content"</span>).val(data.message) <span class="comment">//将后端数据显示在前端</span></span></span><br><span class="line"><span class="javascript">            <span class="built_in">console</span>.log(<span class="string">"传到前端的数据的类型："</span> + <span class="keyword">typeof</span> (data.message))</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#send_content"</span>).val(<span class="string">""</span>)<span class="comment">//发送的输入框清空</span></span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>后端demo:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template, request, jsonify</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"index_v6.html"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/send_message', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_message</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_get = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    message_get = request.form[<span class="string">"domain"</span>].split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># message_get = request.form['message'] #input提交</span></span><br><span class="line">    print(<span class="string">"收到前端发过来的信息：%s"</span> % message_get)</span><br><span class="line">    print(<span class="string">"收到数据的类型为："</span> + str(type(message_get)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"收到消息"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/change_to_json', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_to_json</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_json = &#123;</span><br><span class="line">        <span class="string">"message"</span>: message_get</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jsonify(message_json)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>,debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂&lt;/p&gt;&lt;p&gt;忙活三个小时，终于把前端和后端打通了～～&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Python Flask接口设计-示例</title>
    <link href="cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/"/>
    <id>cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/</id>
    <published>2020-03-10T15:08:35.000Z</published>
    <updated>2020-04-04T17:12:52.356Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p><a name="LHF1q"></a></p><h3 id="Get-请求"><a href="#Get-请求" class="headerlink" title="Get 请求"></a>Get 请求</h3><p><strong><strong>开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。</strong></strong><br><strong><br>**</strong>方法 1:****</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["GET"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><p>此种方式对应的request请求方式：</p><ol><li>拼接请求链接, 直接请求：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0/test_1.0?name=ccc&amp;age=18</a></li><li>request 请求中带有参数，如下图</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583826674613-bc99538a-988e-4386-b8e6-9eb9fce1862f.png#align=left&display=inline&height=610&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%883.47.43.png&originHeight=610&originWidth=1424&size=98593&status=done&style=none&width=1424" alt="屏幕快照 2020-03-10 下午3.47.43.png"></p><p>方法 2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">banWordSingleStart</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> getWordStatus(word)</span><br></pre></td></tr></table></figure><p>此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字</p><p>请求链接：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0</a>/api/banWordSingle/输入词</p><p><a name="vJdOc"></a></p><h3 id="Post-请求"><a href="#Post-请求" class="headerlink" title="Post 请求"></a>Post 请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["POST"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">8080</span>)</span><br></pre></td></tr></table></figure><p>请求方式：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583831085097-3a858ae4-259d-408d-a162-6a4ed8c5e291.png#align=left&display=inline&height=692&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%885.00.28.png&originHeight=692&originWidth=1438&size=99272&status=done&style=none&width=1438" alt="屏幕快照 2020-03-10 下午5.00.28.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;LHF1q&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;Get-请求&quot;&gt;&lt;a href=&quot;#Get-请求&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>IDEA install TabNine</title>
    <link href="cpeixin.cn/2020/01/22/IDEA-install-TabNine/"/>
    <id>cpeixin.cn/2020/01/22/IDEA-install-TabNine/</id>
    <published>2020-01-22T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:48.223Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>TabNine是我目前遇到过最好的智能补全工具</p><p>TabNine基于GPT-2的插件</p><p>安装<br>IDEA编译器，找到plugins</p><p>Windows pycharm：File&gt;settings&gt;plugins;<br>Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins</p><p>查找 TabNine, 点击 install, 随后 restart</p><p>重启后：Help&gt;Edit Custom Properties…&gt;Create;</p><p>在跳出来的idea.properties中输入（注：英文字符） TabNine::config</p><p>随即会自动弹出TabNine激活页面；</p><p>激活<br>点击Activation Key下面的here；</p><p>输入你的邮箱号；</p><p>复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）</p><p>等待自动安装，观察页面（最下面有log可以看当前进度）；</p><p>激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；</p><p>确认激活完成，重启pycharm即可；</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;TabNine是我目前遇到过最好的智能补全工具&lt;/p&gt;&lt;p&gt;TabNine基于GPT-2的插件&lt;/p&gt;&lt;p&gt;安装&lt;br&gt;IDEA编译器，找到pl
      
    
    </summary>
    
    
      <category term="开发工具" scheme="cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="IDEA" scheme="cpeixin.cn/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 Chinese 自动生成文章 - 环境准备</title>
    <link href="cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <id>cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</id>
    <published>2020-01-01T14:28:43.000Z</published>
    <updated>2020-04-13T09:28:23.224Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p><a name="R14AA"></a></p><h2 id="Google-Colab"><a href="#Google-Colab" class="headerlink" title="Google Colab"></a>Google Colab</h2><p><br>Colaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。<br><br><br>Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。<br><br><br>缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。<br><br><br><strong>优点 免费！ 免费！免费！</strong><br>**<br><a name="dpofS"></a></p><h3 id="谷歌云盘"><a href="#谷歌云盘" class="headerlink" title="谷歌云盘"></a>谷歌云盘</h3><p><br>当登录账号进入<a href="https://drive.google.com/drive/my-drive" target="_blank" rel="external nofollow noopener noreferrer">谷歌云盘</a>时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723531238-28bbbd81-69e8-472d-b048-1ac67166a201.png#align=left&display=inline&height=612&name=image.png&originHeight=612&originWidth=1268&size=104029&status=done&style=none&width=1268" alt="image.png"><br>可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：</p><p><a name="EHdj9"></a></p><h3 id="引入Colab"><a href="#引入Colab" class="headerlink" title="引入Colab"></a>引入Colab</h3><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723706098-527d9fff-e46e-4dd1-b92a-0640b0d61555.png#align=left&display=inline&height=674&name=image.png&originHeight=674&originWidth=1125&size=104056&status=done&style=none&width=1125" alt="image.png"><br><br><br><br><br></p><p><a name="kykCO"></a></p><h3 id="设置GPU环境"><a href="#设置GPU环境" class="headerlink" title="设置GPU环境"></a>设置GPU环境</h3><p><br>打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723911273-f07371f9-e982-44b2-af34-b3781f294879.png#align=left&display=inline&height=739&name=image.png&originHeight=739&originWidth=1191&size=94677&status=done&style=none&width=1191" alt="image.png"><br><br><br></p><p><a name="f4U2h"></a></p><h3 id="挂载和切换工作目录"><a href="#挂载和切换工作目录" class="headerlink" title="挂载和切换工作目录"></a>挂载和切换工作目录</h3><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录</span></span><br><span class="line"></span><br><span class="line">os.chdir(<span class="string">'/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese'</span>) <span class="comment">## 共享云盘的目录</span></span><br></pre></td></tr></table></figure><p>其中： My Drive 代表你的google网盘根目录</p><pre><code>code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录</code></pre><p><a name="MyewB"></a></p><h3 id="在Colab中运行任务"><a href="#在Colab中运行任务" class="headerlink" title="在Colab中运行任务"></a>在Colab中运行任务</h3><p>下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769633567-4e4118a0-5c52-4517-9233-71d897e7fd68.png#align=left&display=inline&height=1748&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.15.46.png&originHeight=1748&originWidth=3096&size=378104&status=done&style=none&width=3096" alt="屏幕快照 2020-04-13 下午5.15.46.png"></p><p><a name="GZDbL"></a></p><h3 id="ipynb文件内容"><a href="#ipynb文件内容" class="headerlink" title=".ipynb文件内容"></a>.ipynb文件内容</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769997876-4536842e-6bb3-4d6f-8df9-e220a66026a0.png#align=left&display=inline&height=1702&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.23.22.png&originHeight=1702&originWidth=3154&size=396138&status=done&style=none&width=3154" alt="屏幕快照 2020-04-13 下午5.23.22.png"><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;R14AA&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;Google-Colab&quot;&gt;&lt;a href=&quot;#Google-Colab&quot; cl
      
    
    </summary>
    
    
      <category term="NLP" scheme="cpeixin.cn/categories/NLP/"/>
    
    
      <category term="GPT-2" scheme="cpeixin.cn/tags/GPT-2/"/>
    
  </entry>
  
  <entry>
    <title>架构思想</title>
    <link href="cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/"/>
    <id>cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/</id>
    <published>2019-12-20T02:26:15.000Z</published>
    <updated>2020-04-04T11:23:45.206Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h2><a href="#" class="headerlink"></a></h2><p>关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。<br><br><br>从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。<br><br><br>具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。<br><br><br>架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。<br><a name="-3"></a></p><h4 id="架构思想"><a href="#架构思想" class="headerlink" title="架构思想"></a>架构思想</h4><p>CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度</p><ul><li><p>满足业务发展需求是最高准则</p></li><li><p>业务建模，抽象和枚举是两种方式，需要平衡，不能走极端</p></li><li><p>模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计</p></li><li><p>基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。</p></li><li><p>分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程</p></li><li><p>分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中</p></li><li><p>不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术</p></li><li><p>好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难</p></li><li><p>业务架构、系统架构、数据模型<br><a name="-4"></a></p><h4 id="面对一块新业务，如何系统架构？"><a href="#面对一块新业务，如何系统架构？" class="headerlink" title="面对一块新业务，如何系统架构？"></a>面对一块新业务，如何系统架构？</h4></li><li><p>业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。</p></li><li><p>系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？</p></li><li><p>系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？</p></li><li><p>模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。<br><a name="-5"></a></p><h4 id="如何模块化"><a href="#如何模块化" class="headerlink" title="如何模块化"></a>如何模块化</h4></li><li><p>基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。</p></li><li><p>基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。</p></li><li><p>基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。</p></li><li><p>基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。<br><a name="-6"></a></p><h4 id="提升系统的稳定性"><a href="#提升系统的稳定性" class="headerlink" title="提升系统的稳定性"></a>提升系统的稳定性</h4></li><li><p>流控</p></li></ul><p>双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。<br>另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。</p><ul><li>容灾</li></ul><p>最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。<br>另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。</p><ul><li>稳定性</li></ul><p>第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。</p><ul><li>容量规划</li></ul><p>做好容量规划、系统间强弱依赖关系梳理。<br>如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。<br><a name="-7"></a></p><h4 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h4><ul><li><p>分布式系统</p></li><li><p>分布式缓存</p></li><li><p>分布式数据<br><a name="api"></a></p><h4 id="API-和乐高积木有什么相似之处？"><a href="#API-和乐高积木有什么相似之处？" class="headerlink" title="API 和乐高积木有什么相似之处？"></a>API 和乐高积木有什么相似之处？</h4><p>相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。<br>对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。<br>你可以发现很多 API 和乐高积木的相似之处：</p></li><li><p>标准化：通用、标准化的组件，作为基本的构建块（building blocks）；<br></p></li><li><p>可用性：强调可用性，附有文档或使用说明；<br></p></li><li><p>可定制：为不同功能使用不同的API；<br></p></li><li><p>创造性：能够组合不同的 API 来创造混搭的结果；</p></li></ul><p><br>乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。<br>虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。<br><br><br>让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;关于什么
      
    
    </summary>
    
    
      <category term="架构" scheme="cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>kali中文设置</title>
    <link href="cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/"/>
    <id>cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/</id>
    <published>2019-12-01T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:21.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>更新源</p><p><a href="https://blog.csdn.net/qq_38333291/article/details/89764967" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_38333291/article/details/89764967</a></p><p>设置编码和中文字体安装</p><p><a href="http://www.linuxdiyf.com/linux/20701.html" target="_blank" rel="external nofollow noopener noreferrer">http://www.linuxdiyf.com/linux/20701.html</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;更新源&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_38333291/article/details/897
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="kali" scheme="cpeixin.cn/tags/kali/"/>
    
  </entry>
  
  <entry>
    <title>分布式下的数据hash分布</title>
    <link href="cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/"/>
    <id>cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/</id>
    <published>2019-11-19T15:05:08.000Z</published>
    <updated>2020-04-04T11:24:04.737Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动在Spark SQL上的核心优化实践</title>
    <link href="cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</id>
    <published>2019-11-12T10:57:27.000Z</published>
    <updated>2020-05-16T10:59:03.835Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --><p><br>本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践<br>原文链接：<a href="https://juejin.im/post/5dc3ed336fb9a04a7847f25c" target="_blank" rel="external nofollow noopener noreferrer">https://juejin.im/post/5dc3ed336fb9a04a7847f25c</a><br><a name="KLy4v"></a></p><h2 id="Spark-SQL-架构简介"><a href="#Spark-SQL-架构简介" class="headerlink" title="Spark SQL 架构简介"></a>Spark SQL 架构简介</h2><p>我们先简单聊一下Spark SQL 的架构。下面这张图描述了一条 SQL 提交之后需要经历的几个阶段，结合这些阶段就可以看到在哪些环节可以做优化。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589617132571-55a23e83-cc96-437b-93a9-871d41c6724e.webp#align=left&display=inline&height=448&margin=%5Bobject%20Object%5D&originHeight=448&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>很多时候，做数据仓库建模的同学更倾向于直接写 SQL 而非使用 Spark 的 DSL。一条 SQL 提交之后会被 Parser 解析并转化为 Unresolved Logical Plan。它的重点是 Logical Plan 也即逻辑计划，它描述了希望做什么样的查询。Unresolved 是指该查询相关的一些信息未知，比如不知道查询的目标表的 Schema 以及数据位置。<br>上述信息存于 Catalog 内。在生产环境中，一般由 Hive Metastore 提供 Catalog 服务。Analyzer 会结合 Catalog 将 Unresolved Logical Plan 转换为 Resolved Logical Plan。</p><p>到这里还不够。不同的人写出来的 SQL 不一样，生成的 Resolved Logical Plan 也就不一样，执行效率也不一样。为了保证无论用户如何写 SQL 都可以高效的执行，Spark SQL 需要对 Resolved Logical Plan 进行优化，这个优化由 Optimizer 完成。Optimizer 包含了一系列规则，对 Resolved Logical Plan 进行等价转换，最终生成 Optimized Logical Plan。<strong>该 Optimized Logical Plan 不能保证是全局最优的，但至少是接近最优的。</strong><br>上述过程只与 SQL 有关，与查询有关，但是与 Spark 无关，因此无法直接提交给 Spark 执行。Query Planner 负责将 Optimized Logical Plan 转换为 Physical Plan，进而可以直接由 Spark 执行。<br><br><br>由于同一种逻辑算子可以有多种物理实现。如 Join 有多种实现，ShuffledHashJoin、BroadcastHashJoin、BroadcastNestedLoopJoin、SortMergeJoin 等。因此 Optimized Logical Plan 可被 Query Planner 转换为多个 Physical Plan。如何选择最优的 Physical Plan 成为一件非常影响最终执行性能的事情。一种比较好的方式是，<strong>构建一个 Cost Model，并对所有候选的 Physical Plan 应用该 Model 并挑选 Cost 最小的 Physical Plan 作为最终的 Selected Physical Plan。</strong></p><p>Physical Plan 可直接转换成 RDD 由 Spark 执行。我们经常说“计划赶不上变化”，在执行过程中，可能发现原计划不是最优的，后续执行计划如果能根据运行时的统计信息进行调整可能提升整体执行效率。这部分<strong>动态调整由 Adaptive Execution</strong> 完成。<br><br><br>后面介绍字节跳动在 Spark SQL 上做的一些优化，主要围绕这一节介绍的逻辑计划优化与物理计划优化展开。<br><a name="Z68Tc"></a></p><h2 id="Spark-SQL引擎优化"><a href="#Spark-SQL引擎优化" class="headerlink" title="Spark SQL引擎优化"></a>Spark SQL引擎优化</h2><p><a name="L4CUN"></a></p><h3 id="Bucket-Join改进"><a href="#Bucket-Join改进" class="headerlink" title="Bucket Join改进"></a>Bucket Join改进</h3><p>在 Spark 里，实际并没有 Bucket Join 算子。这里说的 Bucket Join 泛指不需要 Shuffle 的 Sort Merge Join。<br>下图展示了 Sort Merge Join 的基本原理。用虚线框代表的 Table 1 和 Table 2 是两张需要按某字段进行 Join 的表。虚线框内的 partition 0 到 partition m 是该表转换成 RDD 后的 Partition，而非表的分区。</p><p>假设 Table 1 与 Table 2 转换为 RDD 后分别包含 m 和 k 个 Partition。为了进行 Join，需要通过 Shuffle 保证相同 Join Key 的数据在同一个 Partition 内且 Partition 内按 Key 排序，同时保证 Table 1 与 Table 2 经过 Shuffle 后的 RDD 的 Partition 数相同。<br><br><br>如下图所示，经过 Shuffle 后只需要启动 n 个 Task，每个 Task 处理 Table 1 与 Table 2 中对应 Partition 的数据进行 Join 即可。如 Task 0 只需要顺序扫描 Shuffle 后的左右两边的 partition 0 即可完成 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443202-f22b73ca-17f1-450c-86ca-e47c9b934bca.webp#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&originHeight=629&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>该方法的优势是适用场景广，几乎可用于任意大小的数据集。劣势是每次 Join 都需要对全量数据进行 Shuffle，而 Shuffle 是最影响 Spark SQL 性能的环节。如果能避免 Shuffle 往往能大幅提升 Spark SQL 性能。</p><p>对于大数据的场景来讲，数据一般是一次写入多次查询。如果经常对两张表按相同或类似的方式进行 Join，每次都需要付出 Shuffle 的代价。与其这样，<strong>不如让数据在写的时候，就让数据按照利于 Join 的方式分布，从而使得 Join 时无需进行 Shuffle。</strong>如下图所示，Table 1 与 Table 2 内的数据按照相同的 Key 进行分桶且桶数都为 n，同时桶内按该 Key 排序。对这两张表进行 Join 时，可以避免 Shuffle，直接启动 n 个 Task 进行 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443345-00aac217-cf9d-4ca2-beea-3a27868e9709.webp#align=left&display=inline&height=697&margin=%5Bobject%20Object%5D&originHeight=697&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>字节跳动对 Spark SQL 的 BucketJoin 做了四项比较大的改进。<br><strong><br></strong>改进一：支持与 Hive 兼容**<br>在过去一段时间，字节跳动把大量的 Hive 作业迁移到了 SparkSQL。而 Hive 与 Spark SQL 的 Bucket 表不兼容。对于使用 Bucket 表的场景，如果直接更新计算引擎，会造成 Spark SQL 写入 Hive Bucket 表的数据无法被下游的 Hive 作业当成 Bucket 表进行 Bucket Join，从而造成作业执行时间变长，可能影响 SLA。</p><p>为了解决这个问题，我们让 Spark SQL 支持 Hive 兼容模式，从而保证 Spark SQL 写入的 Bucket 表与 Hive 写入的 Bucket 表效果一致，并且这种表可以被 Hive 和 Spark SQL 当成 Bucket 表进行 Bucket Join 而不需要 Shuffle。通过这种方式保证 Hive 向 Spark SQL 的透明迁移。</p><p>第一个需要解决的问题是，Hive 的一个 Bucket 一般只包含一个文件，而 Spark SQL 的一个 Bucket 可能包含多个文件。<strong>解决办法是动态增加一次以 Bucket Key 为 Key 并且并行度与 Bucket 个数相同的 Shuffle。</strong></p><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443237-9db92d7d-db57-44a7-8c38-d02bece98cc8.webp#align=left&display=inline&height=609&margin=%5Bobject%20Object%5D&originHeight=609&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br><br><br>第二个需要解决的问题是，Hive 1.x 的哈希方式与 Spark SQL 2.x 的哈希方式（Murmur3Hash）不同，使得相同的数据在 Hive 中的 Bucket ID 与 Spark SQL 中的 Bucket ID 不同而无法直接 Join。在 Hive 兼容模式下，我们让上述动态增加的 Shuffle 使用 Hive 相同的哈希方式，从而解决该问题。<br><strong><br></strong>改进二：支持倍数关系Bucket Join**<br>Spark SQL 要求只有 Bucket 相同的表才能（必要非充分条件）进行 Bucket Join。对于两张大小相差很大的表，比如几百 GB 的维度表与几十 TB （单分区）的事实表，它们的 Bucket 个数往往不同，并且个数相差很多，默认无法进行 Bucket Join。因此我们通过两种方式支持了倍数关系的 Bucket Join，即当两张 Bucket 表的 Bucket 数是倍数关系时支持 Bucket Join。</p><p>第一种方式，Task 个数与小表 Bucket 个数相同。如下图所示，Table A 包含 3 个 Bucket，Table B 包含 6 个 Bucket。此时 Table B 的 bucket 0 与 bucket 3 的数据合集应该与 Table A 的 bucket 0 进行 Join。这种情况下，可以启动 3 个 Task。其中 Task 0 对 Table A 的 bucket 0 与 Table B 的 bucket 0 + bucket 3 进行 Join。在这里，需要对 Table B 的 bucket 0 与 bucket 3 的数据再做一次 merge sort 从而保证合集有序。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443244-bfe94284-63b6-4abe-8e6f-1df98c09fd08.webp#align=left&display=inline&height=616&margin=%5Bobject%20Object%5D&originHeight=616&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444353-c4e9ece6-e014-4583-ba9d-28bda6a5fe62.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><p>如果 Table A 与 Table B 的 Bucket 个数相差不大，可以使用上述方式。如果 Table B 的 Bucket 个数是 Bucket A Bucket 个数的 10 倍，那上述方式虽然避免了 Shuffle，但可能因为并行度不够反而比包含 Shuffle 的 SortMergeJoin 速度慢。此时可以使用另外一种方式，即 Task 个数与大表 Bucket 个数相等，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443251-d514de79-4fd1-4960-9e27-2a4e408c88ba.webp#align=left&display=inline&height=555&margin=%5Bobject%20Object%5D&originHeight=555&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>在该方案下，可将 Table A 的 3 个 Bucket 读多次。在上图中，直接将 Table A 与 Table A 进行 Bucket Union （新的算子，与 Union 类似，但保留了 Bucket 特性），结果相当于 6 个 Bucket，与 Table B 的 Bucket 个数相同，从而可以进行 Bucket Join。<br><strong><br></strong>改进三：支持BucketJoin 降级**<br>公司内部过去使用 Bucket 的表较少，在我们对 Bucket 做了一系列改进后，大量用户希望将表转换为 Bucket 表。转换后，表的元信息显示该表为 Bucket 表，而历史分区内的数据并未按 Bucket 表要求分布，在查询历史数据时会出现无法识别 Bucket 的问题。</p><p>同时，由于数据量上涨快，平均 Bucket 大小也快速增长。这会造成单 Task 需要处理的数据量过大进而引起使用 Bucket 后的效果可能不如直接使用基于 Shuffle 的 Join。<br><br><br>为了解决上述问题，我们实现了支持降级的 Bucket 表。基本原理是，每次修改 Bucket 信息（包含上述两种情况——将非 Bucket 表转为 Bucket 表，以及修改 Bucket 个数）时，记录修改日期。并且在决定使用哪种 Join 方式时，对于 Bucket 表先检查所查询的数据是否只包含该日期之后的分区。如果是，则当成 Bucket 表处理，支持 Bucket Join；否则当成普通无 Bucket 的表。<br><strong><br></strong>改进四：支持超集<strong><br>对于一张常用表，可能会与另外一张表按 User 字段做 Join，也可能会与另外一张表按 User 和 App 字段做 Join，与其它表按 User 与 Item 字段进行 Join。而 Spark SQL 原生的 Bucket Join 要求 Join Key Set 与表的 Bucket Key Set 完全相同才能进行 Bucket Join。在该场景中，不同 Join 的 Key Set 不同，因此无法同时使用 Bucket Join。这极大的限制了 Bucket Join 的适用场景。<br><br><br>针对此问题，我们支持了超集场景下的 Bucket Join。只要 Join Key Set 包含了 Bucket Key Set，即可进行 Bucket Join。<br><br><br>如下图所示，Table X 与 Table Y，都按字段 A 分 Bucket。而查询需要对 Table X 与 Table Y 进行 Join，且 Join Key Set 为 A 与 B。此时，由于 A 相等的数据，在两表中的 Bucket ID 相同，那 A 与 B 各自相等的数据在两表中的 Bucket ID 肯定也相同，所以数据分布是满足 Join 要求的，不需要 Shuffle。同时，Bucket Join 还需要保证两表按 Join Key Set 即 A 和 B 排序，此时只需要对 Table X 与 Table Y 进行分区内排序即可。由于两边已经按字段 A 排序了，此时再按 A 与 B 排序，代价相对较低。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443338-9f06a217-9078-4734-84ce-860707b4612e.webp#align=left&display=inline&height=661&margin=%5Bobject%20Object%5D&originHeight=661&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444719-a5289c2c-0dea-4316-8924-1e4377bce1ea.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br></strong>物化列**<br>Spark SQL 处理嵌套类型数据时，存在以下问题：</p><ul><li><strong>读取大量不必要的数据</strong>：对于 Parquet / ORC 等列式存储格式，可只读取需要的字段，而直接跳过其它字段，从而极大节省 IO。而对于嵌套数据类型的字段，如下图中的 Map 类型的 people 字段，往往只需要读取其中的子字段，如 people.age。却需要将整个 Map 类型的 people 字段全部读取出来然后抽取出 people.age 字段。这会引入大量的无意义的 IO 开销。在我们的场景中，存在不少 Map 类型的字段，而且很多包含几十至几百个 Key，这也就意味着 IO 被放大了几十至几百倍。<br></li><li><strong>无法进行向量化读取</strong>：而向量化读能极大的提升性能。但截止到目前（2019年10月26日），Spark 不支持包含嵌套数据类型的向量化读取。这极大的影响了包含嵌套数据类型的查询性能<br></li><li><strong>不支持 Filter 下推</strong>：目前（2019年10月26日）的 Spark 不支持嵌套类型字段上的 Filter 的下推<br></li><li><strong>重复计算</strong>：JSON 字段，在 Spark SQL 中以 String 类型存在，严格来说不算嵌套数据类型。不过实践中也常用于保存不固定的多个字段，在查询时通过 JSON Path 抽取目标子字段，而大型 JSON 字符串的字段抽取非常消耗 CPU。对于热点表，频繁重复抽取相同子字段非常浪费资源。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443241-a0b575d3-4b99-451f-802f-991fd41107f6.webp#align=left&display=inline&height=668&margin=%5Bobject%20Object%5D&originHeight=668&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444769-6635f36d-d873-430c-8223-adcb532766d6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>对于这个问题，做数仓的同学也想了一些解决方案。如下图所示，<strong>在名为 base_table 的表之外创建了一张名为 sub_table 的表，并且将高频使用的子字段 people.age 设置为一个额外的 Integer 类型的字段</strong>。下游不再通过 base_table 查询 people.age，而是使用 sub_table 上的 age 字段代替。通过这种方式，将嵌套类型字段上的查询转为了 Primitive 类型字段的查询，同时解决了上述问题。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444716-a4b11d0a-899e-430c-af62-8d0151c63af6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443482-11bc4e9f-a7bb-4d7d-9026-746ea16d0d27.webp#align=left&display=inline&height=611&margin=%5Bobject%20Object%5D&originHeight=611&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>这种方案存在明显缺陷：</p><ul><li><strong>额外维护了一张表</strong>，引入了大量的额外存储/计算开销。<br></li><li>无法在新表上查询新增字段的历史数据（如要支持对历史数据的查询，需要重跑历史作业，开销过大，无法接受）。<br></li><li>表的维护方需要在修改表结构后修改插入数据的作业。<br></li><li>需要下游查询方修改查询语句，推广成本较大。<br></li><li><strong>运营成本高</strong>：如果高频子字段变化，需要删除不再需要的独立子字段，并添加新子字段为独立字段。删除前，需要确保下游无业务使用该字段。而新增字段需要通知并推进下游业务方使用新字段。<br></li></ul><p>为解决上述所有问题，我们设计并实现了物化列。它的原理是：</p><ul><li>新增一个 Primitive 类型字段，比如 Integer 类型的 age 字段，并且指定它是 people.age 的物化字段。<br></li><li>插入数据时，为物化字段自动生成数据，并在 Partition Parameter 内保存物化关系。因此对插入数据的作业完全透明，表的维护方不需要修改已有作业。<br></li><li>查询时，检查所需查询的所有 Partition，如果都包含物化信息（people.age 到 age 的映射），直接将 select people.age 自动重写为 select age，从而实现对下游查询方的完全透明优化。同时兼容历史数据。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443240-98b45cc3-1d0f-4093-980d-265abb445a8a.webp#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&originHeight=683&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444777-84f75218-68e0-4929-909d-4c828ab64f33.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>下图展示了在某张核心表上使用物化列的收益：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443260-d3424ecd-67fe-41ed-a902-edf0d1ee45ab.webp#align=left&display=inline&height=423&margin=%5Bobject%20Object%5D&originHeight=423&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444729-1b39919b-8ea5-459e-bc2b-db24d8391b39.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>物化视图</strong><br>在 OLAP 领域，经常会对相同表的某些固定字段进行 Group By 和 Aggregate / Join 等耗时操作，造成大量重复性计算，浪费资源，且影响查询性能，不利于提升用户体验。<br>我们实现了基于物化视图的优化功能：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443261-8e8f3de2-937d-475b-aca0-d39fed39babb.webp#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>如上图所示，查询历史显示大量查询根据 user 进行 group by，然后对 num 进行 sum 或 count 计算。此时可创建一张物化视图，且对 user 进行 gorup by，对 num 进行 avg（avg 会自动转换为 count 和 sum）。用户对原始表进行 select user, sum(num) 查询时，Spark SQL 自动将查询重写为对物化视图的 select user, sum_num 查询。<br><strong><br></strong>Spark SQL 引擎上的其它优化<strong><br>下图展示了我们在 Spark SQL 上进行的其它部分优化工作：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443207-45af67c8-8989-4026-81f3-d646d8123845.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br></strong><br><a name="5UxSb"></a></p><h2 id="Spark-Shuffle稳定性提升与性能优化"><a href="#Spark-Shuffle稳定性提升与性能优化" class="headerlink" title="Spark Shuffle稳定性提升与性能优化"></a>Spark Shuffle稳定性提升与性能优化</h2><p><a name="5rTgp"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Ajtpe"></a></p><h3 id="Spark-Shuffle-存在的问题"><a href="#Spark-Shuffle-存在的问题" class="headerlink" title="Spark Shuffle 存在的问题"></a>Spark Shuffle 存在的问题</h3><p>Shuffle的原理，很多同学应该已经很熟悉了。鉴于时间关系，这里不介绍过多细节，只简单介绍下基本模型。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445126-5aa3b134-c904-42d9-ac04-09618bb32553.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443213-c2f4bcdb-24e6-4fdc-afc2-3397f6608fec.webp#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&originHeight=679&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，我们将 Shuffle 上游 Stage 称为 Mapper Stage，其中的 Task 称为 Mapper。Shuffle 下游 Stage 称为 Reducer Stage，其中的 Task 称为 Reducer。<br>每个 Mapper 会将自己的数据分为最多 N 个部分，N 为 Reducer 个数。每个 Reducer 需要去最多 M （Mapper 个数）个 Mapper 获取属于自己的那部分数据。<br><br><br>这个架构存在两个问题：</p><ul><li><strong>稳定性问题</strong>：Mapper 的 Shuffle Write 数据存于 Mapper 本地磁盘，只有一个副本。当该机器出现磁盘故障，或者 IO 满载，CPU 满载时，Reducer 无法读取该数据，从而引起 FetchFailedException，进而导致 Stage Retry。Stage Retry 会造成作业执行时间增长，直接影响 SLA。同时，执行时间越长，出现 Shuffle 数据无法读取的可能性越大，反过来又会造成更多 Stage Retry。如此循环，可能导致大型作业无法成功执行。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444392-8549057c-506b-4459-856d-0ce9cddf9910.webp#align=left&display=inline&height=628&margin=%5Bobject%20Object%5D&originHeight=628&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445074-9921e4da-02ca-438c-9b15-403f41fba339.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><ul><li><strong>性能问题</strong>：每个 Mapper 的数据会被大量 Reducer 读取，并且是随机读取不同部分。假设 Mapper 的 Shuffle 输出为 512MB，Reducer 有 10 万个，那平均每个 Reducer 读取数据 512MB / 100000 = 5.24KB。并且，不同 Reducer 并行读取数据。对于 Mapper 输出文件而言，存在大量的随机读取。而 HDD 的随机 IO 性能远低于顺序 IO。最终的现象是，Reducer 读取 Shuffle 数据非常慢，反映到 Metrics 上就是 Reducer Shuffle Read Blocked Time 较长，甚至占整个 Reducer 执行时间的一大半，如下图所示。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444444-a5a5eeea-5d00-473f-892c-bef8ff9e032f.webp#align=left&display=inline&height=545&margin=%5Bobject%20Object%5D&originHeight=545&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445093-1dba238c-18eb-4bc2-a637-4a5f0f069847.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>基于HDFS的Shuffle稳定性提升</strong><br>经观察，引起 Shuffle 失败的最大因素不是磁盘故障等硬件问题，而是 CPU 满载和磁盘 IO 满载。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445175-a4b309ba-f45b-4141-ac27-8771c622c313.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444483-bc3847a8-0664-4c0d-a4ed-ae5f1781331b.webp#align=left&display=inline&height=684&margin=%5Bobject%20Object%5D&originHeight=684&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，机器的 CPU 使用率接近 100%，使得 Mapper 侧的 Node Manager 内的 Spark External Shuffle Service 无法及时提供 Shuffle 服务。<br>下图中 Data Node 占用了整台机器 IO 资源的 84%，部分磁盘 IO 完全打满，这使得读取 Shuffle 数据非常慢，进而使得 Reducer 侧无法在超时时间内读取数据，造成 FetchFailedException。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444481-5779db89-9d8b-4b17-82e3-7b60b08eed02.webp#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&originHeight=627&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>无论是何种原因，问题的症结都是 Mapper 侧的 Shuffle Write 数据只保存在本地，一旦该节点出现问题，会造成该节点上所有 Shuffle Write 数据无法被 Reducer 读取。解决这个问题的一个通用方法是，通过多副本保证可用性。<br>最初始的一个简单方案是，Mapper 侧最终数据文件与索引文件不写在本地磁盘，而是直接写到 HDFS。Reducer 不再通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据，而是直接从 HDFS 上获取数据，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444436-0efdf8b6-0082-47c1-9218-36b6e6bbee4f.webp#align=left&display=inline&height=541&margin=%5Bobject%20Object%5D&originHeight=541&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>快速实现这个方案后，我们做了几组简单的测试。结果表明：</p><ul><li>Mapper 与 Reducer 不多时，Shuffle 读写性能与原始方案相比无差异。<br></li><li>Mapper 与 Reducer 较多时，Shuffle 读变得非常慢。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444384-9bb761c9-fbe1-43c8-9a82-a51e97c97f8e.webp#align=left&display=inline&height=540&margin=%5Bobject%20Object%5D&originHeight=540&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>在上面的实验过程中，HDFS 发出了报警信息。如下图所示，HDFS Name Node Proxy 的 QPS 峰值达到 60 万。（注：字节跳动自研了 Node Name Proxy，并在 Proxy 层实现了缓存，因此读 QPS 可以支撑到这个量级）。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444590-4c232e53-8507-472c-844c-18249a03f85f.webp#align=left&display=inline&height=698&margin=%5Bobject%20Object%5D&originHeight=698&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>原因在于，总共 10000 Reducer，需要从 10000 个 Mapper 处读取数据文件和索引文件，总共需要读取 HDFS 10000 * 1000 * 2 = 2 亿次。</p><p>如果只是 Name Node 的单点性能问题，还可以通过一些简单的方法解决。例如在 Spark Driver 侧保存所有 Mapper 的 Block Location，然后 Driver 将该信息广播至所有 Executor，每个 Reducer 可以直接从 Executor 处获取 Block Location，然后无须连接 Name Node，而是直接从 Data Node 读取数据。但鉴于 Data Node 的线程模型，这种方案会对 Data Node 造成较大冲击。<br>最后我们选择了一种比较简单可行的方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444471-aaff74d3-ec02-4014-8d71-1b201eafdbf1.webp#align=left&display=inline&height=582&margin=%5Bobject%20Object%5D&originHeight=582&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>Mapper 的 Shuffle 输出数据仍然按原方案写本地磁盘，写完后上传到 HDFS。Reducer 仍然按原始方案通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据。如果失败了，则从 HDFS 读取。这种方案极大减少了对 HDFS 的访问频率。<br>该方案上线近一年：</p><ul><li>覆盖 57% 以上的 Spark Shuffle 数据。<br></li><li>使得 Spark 作业整体性能提升 14%。<br></li><li>天级大作业性能提升 18%。<br></li><li>小时级作业性能提升 12%。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444424-472f0478-d4b8-47de-a9ea-557b4c201c05.webp#align=left&display=inline&height=670&margin=%5Bobject%20Object%5D&originHeight=670&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445065-2ed3ba00-5462-4f5f-9752-f2491fc7ea4d.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>该方案旨在提升 Spark Shuffle 稳定性从而提升作业稳定性，但最终没有使用方差等指标来衡量稳定性的提升。原因在于每天集群负载不一样，整体方差较大。Shuffle 稳定性提升后，Stage Retry 大幅减少，整体作业执行时间减少，也即性能提升。最终通过对比使用该方案前后的总的作业执行时间来对比性能的提升，用于衡量该方案的效果。<br><strong>Shuffle性能优化实践与探索</strong><br>如上文所分析，Shuffle 性能问题的原因在于，Shuffle Write 由 Mapper 完成，然后 Reducer 需要从所有 Mapper 处读取数据。这种模型，我们称之为以 Mapper 为中心的 Shuffle。它的问题在于：</p><ul><li>Mapper 侧会有 M 次顺序写 IO。<br></li><li>Mapper 侧会有 M * N * 2 次随机读 IO（这是最大的性能瓶颈）。<br></li><li>Mapper 侧的 External Shuffle Service 必须与 Mapper 位于同一台机器，无法做到有效的存储计算分离，Shuffle 服务无法独立扩展。<br></li></ul><p>针对上述问题，我们提出了以 Reducer 为中心的，存储计算分离的 Shuffle 方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444445-e51ea3d0-904b-48ff-af72-22657a86b3ba.webp#align=left&display=inline&height=730&margin=%5Bobject%20Object%5D&originHeight=730&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>该方案的原理是，Mapper 直接将属于不同 Reducer 的数据写到不同的 Shuffle Service。在上图中，总共 2 个 Mapper，5 个 Reducer，5 个 Shuffle Service。所有 Mapper 都将属于 Reducer 0 的数据远程流式发送给 Shuffle Service 0，并由它顺序写入磁盘。Reducer 0 只需要从 Shuffle Service 0 顺序读取所有数据即可，无需再从 M 个 Mapper 取数据。该方案的优势在于：</p><ul><li>将 M * N * 2 次随机 IO 变为 N 次顺序 IO。<br></li><li>Shuffle Service 可以独立于 Mapper 或者 Reducer 部署，从而做到独立扩展，做到存储计算分离。<br></li><li>Shuffle Service 可将数据直接存于 HDFS 等高可用存储，因此可同时解决 Shuffle 稳定性问题。<br></li></ul><p>我的分享就到这里，谢谢大家。<br><a name="23kL1"></a></p><h2 id="QA集锦"><a href="#QA集锦" class="headerlink" title="QA集锦"></a>QA集锦</h2><p><strong>- 提问：物化列新增一列，是否需要修改历史数据？</strong><br>回答：历史数据太多，不适合修改历史数据。</p><p><strong>- 提问：如果用户的请求同时包含新数据和历史数据，如何处理？</strong><br>回答：一般而言，用户修改数据都是以 Partition 为单位。所以我们在 Partition Parameter 上保存了物化列相关信息。如果用户的查询同时包含了新 Partition 与历史 Partition，我们会在新 Partition 上针对物化列进行 SQL Rewrite，历史 Partition 不 Rewrite，然后将新老 Partition 进行 Union，从而在保证数据正确性的前提下尽可能充分利用物化列的优势。</p><p><strong>- 提问：你好，你们针对用户的场景，做了很多挺有价值的优化。像物化列、物化视图，都需要根据用户的查询 Pattern 进行设置。目前你们是人工分析这些查询，还是有某种机制自动去分析并优化？</strong><br>回答：目前我们主要是通过一些审计信息辅助人工分析。同时我们也正在做物化列与物化视图的推荐服务，最终做到智能建设物化列与物化视图。</p><p><strong>- 提问：刚刚介绍的基于 HDFS 的 Spark Shuffle 稳定性提升方案，是否可以异步上传 Shuffle 数据至 HDFS？</strong><br>回答：这个想法挺好，我们之前也考虑过，但基于几点考虑，最终没有这样做。第一，单 Mapper 的 Shuffle 输出数据量一般很小，上传到 HDFS 耗时在 2 秒以内，这个时间开销可以忽略；第二，我们广泛使用 External Shuffle Service 和 Dynamic Allocation，Mapper 执行完成后可能 Executor 就回收了，如果要异步上传，就必须依赖其它组件，这会提升复杂度，ROI 较低。</p><p><a name="BtHKy"></a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>字节的这篇分享，我真的太喜欢了，所有的优化点，都是拿真实的业务场景进行举例，虽然上文有的技术点在我们的场景中还没必要去做到这种优化程度，但是如此实在的来源于线上的方案，非常容易理解。我们公司目前的数据量要比字节的量小很多，最近在新闻上看到抖音的日活已经达到了4亿，所以我们在数仓中的数据，还没有用到更细粒度的Bucket表，分区表就已经完全可以满足我们的需求。上文中的物化列方案，我觉得很新颖，但是从工程师的角度来讲，在物化列方案中，多维护一张表，添加了复杂度和运营成本，我们在数据的存储中，尽可量的回去避免复杂结构的数据类型，这样会降低存储端和计算端代码的复杂度。这篇文章是针对Spark SQL的优化方面，可以说基本上每个大数据公司都会用到Spark SQL，上述的优化方案肯定会帮助到更多的大数据团队 💪</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践&lt;br&gt;原文链接：&lt;a href
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了（二）</title>
    <link href="cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2019-09-09T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:14.168Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。<br><br><br>由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。<br>这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><br>映入眼帘的是满屏的扫描和ssh尝试登陆<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">lines <span class="number">1105</span><span class="number">-1127</span>/<span class="number">1127</span> (END)</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">49</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><p><br>看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护<br>对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@*** ~]<span class="comment"># useradd one             #创建用户</span></span><br><span class="line">[root@*** ~]<span class="comment"># passwd one              #设置密码</span></span><br></pre></td></tr></table></figure><p><br>输入新用户密码<br>首先确保文件 /etc/sudoers 中<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%wheel    ALL=(ALL)    ALL</span><br><span class="line">```  </span><br><span class="line">没有被注释</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```linux</span><br><span class="line">usermod -g wheel onerocket</span><br></pre></td></tr></table></figure><p><br>设置只有指定用户组才能使用su命令切换到root用户<br><br><br>在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"><span class="comment"># Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限</span></span><br><span class="line">sed -i <span class="string">'/pam_wheel.so use_uid/c\auth            required        pam_wheel.so use_uid '</span> /etc/pam.d/su</span><br><span class="line">n=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`</span><br><span class="line"><span class="keyword">if</span> [ $n -eq <span class="number">0</span> ];then</span><br><span class="line">echo SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defs</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><br>打开SSHD的配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。<br><br><br>修改sshd默认端口<br>虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。<br>打开sshd配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>找到#Port 22 删掉注释<br><br><br><em>服务器端口最大可以开到65536</em><br><br><br>同时再添加一个Port 61024 （随意设置）<br><br><br>Port 22<br>Port 61024<br><br><br>重启sshd服务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service sshd restart      <span class="comment">#centos6系列</span></span><br><span class="line">systemctl restart sshd  <span class="comment">#centos7系列</span></span><br><span class="line">firewall-cmd --add-port=<span class="number">61024</span>/tcp</span><br></pre></td></tr></table></figure><p><br>测试，使用新用户，新端口进行登录<br><br><br>如果登陆成功后，再将Port22注释掉，重启sshd服务。<br>到这里，关于远程登录的防护工作，就做好了。<br>最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了</title>
    <link href="cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/"/>
    <id>cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/</id>
    <published>2019-08-24T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:09.871Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h1 id="服务器自述"><a href="#服务器自述" class="headerlink" title="服务器自述"></a>服务器自述</h1><p>我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。<br><br><br>而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢<br><a name="-3"></a></p><h1 id="为什么黑我的服务器"><a href="#为什么黑我的服务器" class="headerlink" title="为什么黑我的服务器"></a>为什么黑我的服务器</h1><p>这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。<br><br><br>但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。<br><br><br>那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…<br><br><br>那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404954445-f30a25a0-5939-4773-b5d9-8bb6a7c53b02.png#align=left&display=inline&height=892&name=1.png&originHeight=892&originWidth=1244&size=1778564&status=done&style=none&width=1244" alt="1.png"><br><br><br>曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器<br><a name="-4"></a></p><h1 id="如何发现被黑"><a href="#如何发现被黑" class="headerlink" title="如何发现被黑"></a>如何发现被黑</h1><p>回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？<br><br><br>最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作<br><br><br>1.关闭了Linux防火墙<br><br><br>2.没有设置redis访问密码<br><br><br>3.没有更改redis默认端口<br><br><br>4.开放了任意IP可以远程连接<br><br><br>以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。<br><br><br>当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。<br><br><br>但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i tcp:<span class="number">6379</span></span><br></pre></td></tr></table></figure><p><br>使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。<br><br><br>但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想<br>直到我运行：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404912911-97736d80-2ee3-455d-a948-6d134f4e2663.png#align=left&display=inline&height=534&name=2.png&originHeight=534&originWidth=3338&size=980508&status=done&style=none&width=3338" alt="2.png"><br>发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了<br><a name="-5"></a></p><h1 id="做了哪些急救工作"><a href="#做了哪些急救工作" class="headerlink" title="做了哪些急救工作"></a>做了哪些急救工作</h1><p>这时，感觉自己开始投入了一场对抗战<br><br><br>1.首先查找植入程序的位置。<br>在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。<br><br><br>2.删除清理可疑文件<br><br><br>杀死进程<br><br><br>删除了正在运行的程序文件还有安装包<br>3.查看所有用户的定时任务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd |cut -f <span class="number">1</span> -d:crontab -uXXX -l</span><br></pre></td></tr></table></figure><p><br>4.开启防火墙<br><br><br>仅开放会使用到的端口<br>5.修改redis默认端口<br><br><br>redis.conf中的port<br>6.添加redis授权密码<br><br><br>redis.conf中的requirepass<br>7.修改绑定远程绑定ip<br><br><br>redis.conf中的bind<br>最后重启redis服务！<br><a name="-6"></a></p><h1 id="从中学到了什么"><a href="#从中学到了什么" class="headerlink" title="从中学到了什么"></a>从中学到了什么</h1><p>明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～<br>最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;服务器自述&quot;&gt;&lt;a href=&quot;#服务器自述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫 - 动态爬取</title>
    <link href="cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
    <id>cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</id>
    <published>2019-06-12T15:26:15.000Z</published>
    <updated>2020-04-04T17:11:17.062Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。<br><br><br>到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。<br><br><br>那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。<br><br><br>那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843101-98e57df8-c124-4923-aa9f-7c42ce2288b6.png#align=left&display=inline&height=1300&originHeight=1300&originWidth=3356&size=0&status=done&style=none&width=3356" alt><br><br><br>注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843056-e12f0f79-905f-4420-abfd-fb85625767ac.png#align=left&display=inline&height=1150&originHeight=1150&originWidth=2266&size=0&status=done&style=none&width=2266" alt><br><br><br>点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：<br>Headers：请求和响应的详细信息<br>Preview：响应体格式化之后的显示<br>Response：响应体原始内容<br>Cookies：Cookies<br>Timing：时间开销<br><br><br>通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401842263-072689cf-5dea-4f8b-b103-0758d9c5e52e.png#align=left&display=inline&height=262&originHeight=262&originWidth=1276&size=0&status=done&style=none&width=1276" alt><br><br><br>上面的截图展示了这次请求的请求方式、请求地址等信息。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843887-cf8bff5f-4570-4ad4-8681-f0f3eae929b3.png#align=left&display=inline&height=884&originHeight=884&originWidth=2652&size=0&status=done&style=none&width=2652" alt><br><br><br>上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843009-4797a28d-3070-421e-a45c-86781d7f580d.png#align=left&display=inline&height=188&originHeight=188&originWidth=848&size=0&status=done&style=none&width=848" alt><br><br><br>上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。<br><br><br>那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。<br><br><br>我们回过头重新观察一下返回的json，格式化之后的层级关系如下：<br><br><br>很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">json_obj = json.loads(<span class="string">"&#123;'key': 'value'&#125;"</span>)  <span class="comment"># 字符串到对象</span></span><br><span class="line">json_str = json.dumps(json_obj)            <span class="comment"># 对象到字符串</span></span><br></pre></td></tr></table></figure><p><br>json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="爬虫" scheme="cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>SHC：使用 Spark SQL 高效地读写 HBase</title>
    <link href="cpeixin.cn/2019/05/16/SHC%EF%BC%9A%E4%BD%BF%E7%94%A8-Spark-SQL-%E9%AB%98%E6%95%88%E5%9C%B0%E8%AF%BB%E5%86%99-HBase/"/>
    <id>cpeixin.cn/2019/05/16/SHC%EF%BC%9A%E4%BD%BF%E7%94%A8-Spark-SQL-%E9%AB%98%E6%95%88%E5%9C%B0%E8%AF%BB%E5%86%99-HBase/</id>
    <published>2019-05-16T02:27:59.000Z</published>
    <updated>2020-05-16T02:31:46.066Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --><p><br>Apache <a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a> 和 Apache <a href="https://www.iteblog.com/archives/tag/hbase/" target="_blank" rel="external nofollow noopener noreferrer">HBase</a> 是两个使用比较广泛的大数据组件。很多场景需要使用 <a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a> 分析/查询 <a href="https://www.iteblog.com/archives/tag/hbase/" target="_blank" rel="external nofollow noopener noreferrer">HBase</a> 中的数据，而目前 <a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a> 内置是支持很多数据源的，其中就包括了 HBase，但是内置的读取数据源还是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：</p><ul><li>一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；</li><li>TableInputFormat 中不支持 BulkGet；</li><li>不能享受到 Spark SQL 内置的 catalyst 引擎的优化。</li></ul><p><br>从另一个方面来讲，如果先不谈关于性能的方面，那么你在写HBase的过程中，是不是步骤感觉很麻烦呢，Foreach每条数据Put到HBase中，往往写入逻辑部分的代码就要写很长很长的一段。<br><br><br>基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。<br><br><br><a href="https://github.com/hortonworks-spark/shc" target="_blank" rel="external nofollow noopener noreferrer">项目地址</a><br>具体的引入方式，在之前的Spark SQL入门中有写到<br><a name="EjSTz"></a></p><h2 id="Catalog"><a href="#Catalog" class="headerlink" title="Catalog"></a>Catalog</h2><p>对于每个表，必须提供一个目录，其中包括行键和具有预定义列族的数据类型的列，并定义hbase列与表模式之间的映射。目录是用户定义的json格式。<br></p><p><a name="eXdJt"></a></p><h2 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h2><p>支持Java基本类型。将来，将支持其他数据类型，这些数据类型依赖于用户指定的Serdes。SHC支持三种内部Serdes：Avro，Phoenix和PrimitiveType。用户可以通过在目录中定义“ tableCoder”来指定要使用的serde。为此，请参考示例和单元测试。以Avro为例。用户定义的Serdes将负责将字节数组转换为Avro对象，而连接器将负责将Avro对象转换为催化剂支持的数据类型。用户定义新Serde时，需要使其“实现”特征’SHCDataType’。<br><br><br>请注意，如果用户希望DataFrame仅处理字节数组，则可以指定二进制类型。然后，用户可以获得每个列为字节数组的催化剂行。用户可以使用定制的解串器进一步对它进行反序列化，或者直接在DataFrame的RDD上进行操作。<br></p><p><a name="j3Svo"></a></p><h2 id="数据局部性"><a href="#数据局部性" class="headerlink" title="数据局部性"></a>数据局部性</h2><p>当Spark Worker节点与hbase区域服务器位于同一位置时，通过标识区域服务器位置并将执行程序与区域服务器一起定位来实现数据局部性。每个执行程序将仅对位于同一主机上的同一部分数据执行Scan / BulkGet。<br></p><p><a name="TccVy"></a></p><h2 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h2><p>该库使用HBase提供的现有标准HBase过滤器，并且不能在协处理器上运行。<br></p><p><a name="eEPLD"></a></p><h2 id="分区修剪"><a href="#分区修剪" class="headerlink" title="分区修剪"></a>分区修剪</h2><p>通过从谓词中提取行键，我们将scan / BulkGet划分为多个非重叠区域，只有具有请求数据的区域服务器才会执行scan / BulkGet。当前，分区修剪是在行键的第一维上执行的。请注意，需要仔细定义WHERE条件。否则，结果扫描可能会包含一个比用户预期大的区域。例如，以下条件将导致完全扫描（rowkey1是行键的第一维，而column是常规的hbase列）。其中rowkey1&gt;“ abc” OR列=“ xyz”<br></p><p><a name="lDJh4"></a></p><h2 id="扫描和批量获取"><a href="#扫描和批量获取" class="headerlink" title="扫描和批量获取"></a>扫描和批量获取</h2><p>都通过指定WHERE子句向用户公开，例如，其中column&gt; x和column &lt;y用于扫描，而column = x用于get。所有操作都在执行程序中执行，而驱动程序仅构造这些操作。在内部，我们将它们转换为扫描或获取或两者结合，从而将Iterator [Row]返回至催化剂引擎。<br></p><p><a name="P0Sof"></a></p><h2 id="可创建的数据源"><a href="#可创建的数据源" class="headerlink" title="可创建的数据源"></a>可创建的数据源</h2><p>该库支持从HBase读取/向HBase写入。<br><a name="qW63X"></a></p><h2 id="应用用途"><a href="#应用用途" class="headerlink" title="应用用途"></a>应用用途</h2><p>下面说明了如何使用连接器的基本步骤。有关更多详细信息和高级用例（例如Avro和复合键支持），请参考存储库中的示例。<br><a name="DjKzs"></a></p><h3 id="定义了HBase目录"><a href="#定义了HBase目录" class="headerlink" title="定义了HBase目录"></a>定义了HBase目录</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">        |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span>table1<span class="string">"&#125;,</span></span><br><span class="line"><span class="string">        |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">        |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">          |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">      |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>上面定义了一个名称为table1，行键为键，列数为（col1-col8）的HBase表的架构。请注意，行键还必须详细定义为具有特定cf（行键）的列（col0）。<br></p><p><a name="xic7k"></a></p><h3 id="写入HBase表以填充数据"><a href="#写入HBase表以填充数据" class="headerlink" title="写入HBase表以填充数据"></a>写入HBase表以填充数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(data).toDF.write.options(</span><br><span class="line">  <span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; catalog, <span class="type">HBaseTableCatalog</span>.newTable -&gt; <span class="string">"5"</span>))</span><br><span class="line">  .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure><p>给定具有指定架构的DataFrame，上面将创建一个具有5个区域的HBase表并将该DataFrame保存在其中。请注意，如果未指定HBaseTableCatalog.newTable，则必须预先创建表。<br></p><p><a name="wSV5W"></a></p><h3 id="在HBase表的顶部执行DataFrame操作"><a href="#在HBase表的顶部执行DataFrame操作" class="headerlink" title="在HBase表的顶部执行DataFrame操作"></a>在HBase表的顶部执行DataFrame操作</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withCatalog</span></span>(cat: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  sqlContext</span><br><span class="line">  .read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;cat))</span><br><span class="line">  .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">  .load()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a name="yQeDt"></a></p><h3 id="复杂查询"><a href="#复杂查询" class="headerlink" title="复杂查询"></a>复杂查询</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br><span class="line"><span class="keyword">val</span> s = df.filter((($<span class="string">"col0"</span> &lt;= <span class="string">"row050"</span> &amp;&amp; $<span class="string">"col0"</span> &gt; <span class="string">"row040"</span>) ||</span><br><span class="line">  $<span class="string">"col0"</span> === <span class="string">"row005"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> === <span class="string">"row020"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> ===  <span class="string">"r20"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> &lt;= <span class="string">"row005"</span>) &amp;&amp;</span><br><span class="line">  ($<span class="string">"col4"</span> === <span class="number">1</span> ||</span><br><span class="line">  $<span class="string">"col4"</span> === <span class="number">42</span>))</span><br><span class="line">  .select(<span class="string">"col0"</span>, <span class="string">"col1"</span>, <span class="string">"col4"</span>)</span><br><span class="line">s.show</span><br></pre></td></tr></table></figure><p><a name="mKwKp"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="szhEb"></a></p><h3 id="SQL支持"><a href="#SQL支持" class="headerlink" title="SQL支持"></a>SQL支持</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Load the dataframe</span></span><br><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br><span class="line"><span class="comment">//SQL example</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"table"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select count(col1) from table"</span>).show</span><br></pre></td></tr></table></figure><br><a name="raaVM"></a> ## 支持Avro模式 该连接器完全支持所有avro模式。用户可以在其目录中使用完整记录模式或部分字段模式作为数据类型（有关更多详细信息，请参阅[此处](https://github.com/hortonworks-spark/shc/wiki/2.-Native-Avro-Support)）。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schema_array = <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">type": "</span><span class="string">array", "</span><span class="string">items": ["</span><span class="string">string","</span><span class="string">null"]&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> schema_record =</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">namespace": "</span>example.<span class="string">avro",</span></span><br><span class="line"><span class="string">     |   "</span><span class="string">type": "</span><span class="string">record",      "</span><span class="string">name": "</span><span class="type">User</span><span class="string">",</span></span><br><span class="line"><span class="string">     |    "</span><span class="string">fields": [      &#123;"</span><span class="string">name": "</span><span class="string">name", "</span><span class="string">type": "</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">     |      &#123;"</span><span class="string">name": "</span>favorite_<span class="string">number",  "</span><span class="string">type": ["</span><span class="string">int", "</span><span class="string">null"]&#125;,</span></span><br><span class="line"><span class="string">     |        &#123;"</span><span class="string">name": "</span>favorite_<span class="string">color", "</span><span class="string">type": ["</span><span class="string">string", "</span><span class="string">null"]&#125;      ]    &#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> catalog = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">        |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">htable"&#125;,</span></span><br><span class="line"><span class="string">        |"</span><span class="string">rowkey":"</span>key1<span class="string">",</span></span><br><span class="line"><span class="string">        |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">          |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key1<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">avro":"</span>schema_<span class="string">array"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">avro":"</span>schema_<span class="string">record"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;</span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">      |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"> <span class="keyword">val</span> df = sqlContext.read.options(<span class="type">Map</span>(<span class="string">"schema_array"</span>-&gt;schema_array,<span class="string">"schema_record"</span>-&gt;schema_record, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).load()</span><br><span class="line">df.write.options(<span class="type">Map</span>(<span class="string">"schema_array"</span>-&gt;schema_array,<span class="string">"schema_record"</span>-&gt;schema_record, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).save()</span><br></pre></td></tr></table></figure><a name="LqW7H"></a> ####<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> complex = <span class="string">s""</span><span class="string">"MAP&lt;int, struct&lt;varchar:string&gt;&gt;"</span><span class="string">""</span></span><br><span class="line"><span class="keyword">val</span> schema =</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">namespace": "</span>example.<span class="string">avro",</span></span><br><span class="line"><span class="string">     |   "</span><span class="string">type": "</span><span class="string">record",      "</span><span class="string">name": "</span><span class="type">User</span><span class="string">",</span></span><br><span class="line"><span class="string">     |    "</span><span class="string">fields": [      &#123;"</span><span class="string">name": "</span><span class="string">name", "</span><span class="string">type": "</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">     |      &#123;"</span><span class="string">name": "</span>favorite_<span class="string">number",  "</span><span class="string">type": ["</span><span class="string">int", "</span><span class="string">null"]&#125;,</span></span><br><span class="line"><span class="string">     |        &#123;"</span><span class="string">name": "</span>favorite_<span class="string">color", "</span><span class="string">type": ["</span><span class="string">string", "</span><span class="string">null"]&#125;      ]    &#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> catalog = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">        |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">htable"&#125;,</span></span><br><span class="line"><span class="string">        |"</span><span class="string">rowkey":"</span>key1:key2<span class="string">",</span></span><br><span class="line"><span class="string">        |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">          |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key1<span class="string">", "</span><span class="string">type":"</span><span class="string">binary"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">avro":"</span>schema1<span class="string">"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">double",        "</span><span class="string">sedes":"</span>org.apache.spark.sql.execution.datasources.hbase.<span class="type">DoubleSedes</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span>$<span class="string">complex"&#125;</span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">      |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line">   </span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.options(<span class="type">Map</span>(<span class="string">"schema1"</span>-&gt;schema, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).load()</span><br><span class="line">df.write.options(<span class="type">Map</span>(<span class="string">"schema1"</span>-&gt;schema, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).save()</span><br></pre></td></tr></table></figure><p>以上说明了我们的下一步，其中包括复合键支持，复杂数据类型，客户化Serde和Avro的支持。请注意，尽管所有主要部分都包含在当前代码库中，但现在可能无法运行。<br><br><br></p><p><a name="XtWKn"></a></p><h2 id="SHC查询优化"><a href="#SHC查询优化" class="headerlink" title="SHC查询优化"></a>SHC查询优化</h2><p><br>SHC 主要使用下面的几种优化，使得 Spark 获取 HBase 的数据扫描范围得到减少，提高了数据读取的效率。<br><strong><br></strong>将使用 Rowkey 的查询转换成 get 查询**<br>我们都知道，HBase 中使用 Get 查询的效率是非常高的，所以如果查询的过滤条件是针对 RowKey 进行的，那么我们可以将它转换成 Get 查询。为了说明这点，我们使用下面的例子进行说明。假设我们定义好的 HBase catalog 如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> catalog = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">  |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">iteblog", "</span>tableC<span class="string">oder":"</span><span class="type">PrimitiveType</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">  |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">  |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">    |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">id", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">  |&#125;</span></span><br><span class="line"><span class="string">|&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>那么如果有类似下面的查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"iteblog_table"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from iteblog_table where id = 1"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from iteblog_table where id = 1 or id = 2"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from iteblog_table where id in (1, 2)"</span>)</span><br></pre></td></tr></table></figure><p>因为查询条件直接是针对 RowKey 进行的，所以这种情况直接可以转换成 Get 或者 BulkGet 请求的。第一个 SQL 查询过程类似于下面过程<br><a href="https://www.iteblog.com/pic/hbase/shc_get-iteblog.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1589595330346-3b5f02d0-85b1-4e32-98d8-7d65433517d4.png#align=left&display=inline&height=409&margin=%5Bobject%20Object%5D&originHeight=409&originWidth=1032&size=0&status=done&style=none&width=1032" alt></a><br><br><br>但是，如果碰到非 RowKey 的过滤，那么这种查询是需要扫描 HBase 的全表的。上面的查询在 shc 里面就是将 HBase 里面的所有数据拿到，然后<strong>传输到 Spark ，再通过 Spark 里面进行过滤</strong>，可见 shc 在这种情况下效率是很低下的。<br><br><br>注意，上面的查询在 shc 返回的结果是错误的。具体原因是在将 id = 1 or col7 = ‘xxx’ 查询条件进行合并时，丢弃了所有的查找条件，相当于返回表的所有数据。定位到代码可以参见下面的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">or</span></span>[<span class="type">T</span>](left: <span class="type">HRF</span>[<span class="type">T</span>],</span><br><span class="line">            right: <span class="type">HRF</span>[<span class="type">T</span>])(<span class="keyword">implicit</span> ordering: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">HRF</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ranges = <span class="type">ScanRange</span>.or(left.ranges, right.ranges)</span><br><span class="line">    <span class="keyword">val</span> typeFilter = <span class="type">TypedFilter</span>.or(left.tf, right.tf)</span><br><span class="line">    <span class="type">HRF</span>(ranges, typeFilter, left.handled &amp;&amp; right.handled)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同理，类似于下面的查询在 shc 里面其实都是全表扫描，并且将所有的数据返回到 Spark 层面上再进行一次过滤。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sql(<span class="string">"select id, col6, col8 from iteblog_table where id = 1 or col7 &lt;= 'xxx'"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select id, col6, col8 from iteblog_table where id = 1 or col7 &gt;= 'xxx'"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select id, col6, col8 from iteblog_table where col7 = 'xxx'"</span>)</span><br></pre></td></tr></table></figure><p>很显然，这种方式查询效率并不高，一种可行的方案是将算子下推到 HBase 层面，在 HBase 层面通过 SingleColumnValueFilter 过滤一部分数据，然后再返回到 Spark，这样可以节省很多数据的传输。<br></p><p><a name="RHA0U"></a></p><h2 id="组合-RowKey-的查询优化"><a href="#组合-RowKey-的查询优化" class="headerlink" title="组合 RowKey 的查询优化"></a>组合 RowKey 的查询优化</h2><p><br>shc 还支持组合 RowKey 的方式来建表，具体如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cat</span> </span>=</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">     |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">iteblog", "</span>tableC<span class="string">oder":"</span><span class="type">PrimitiveType</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">     |"</span><span class="string">rowkey":"</span>key1:key2<span class="string">",</span></span><br><span class="line"><span class="string">     |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">     |"</span>col00<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key1<span class="string">", "</span><span class="string">type":"</span><span class="string">string", "</span><span class="string">length":"</span><span class="number">6</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col01<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key2<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">     |&#125;</span></span><br><span class="line"><span class="string">     |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>上面的 col00 和 col01 两列组合成一个 rowkey，并且 col00 排在前面，col01 排在后面。比如 col00 =’row002’，col01 = 2，那么组合的 rowkey 为 row002\x00\x00\x00\x02。那么在组合 Rowkey 的查询 shc 都有哪些优化呢？现在我们有如下查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0"</span>).show()</span><br></pre></td></tr></table></figure><p>根据上面的信息，RowKey 其实是由 col00 和 col01 组合而成的，那么上面的查询其实可以将 col00 和 col01 进行拼接，然后组合成一个 RowKey，然后上面的查询其实可以转换成一个 Get 查询。但是在 shc 里面，上面的查询是转换成一个 scan 和一个 get 查询的。scan 的 startRow 为 row000，endRow 为 row000\xff\xff\xff\xff；get 的 rowkey 为 row000\xff\xff\xff\xff，然后再将所有符合条件的数据返回，最后再在 Spark 层面上做一次过滤，得到最后查询的结果。因为 shc 里面组合键查询的代码还没完善，所以当前实现应该不是最终的。<br>在 shc 里面下面两条 SQL 查询下沉到 HBase 的逻辑一致</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 = 'row000'"</span>).show()df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0"</span>).show()</span><br></pre></td></tr></table></figure><p>唯一区别是在 Spark 层面上的过滤。<br><a name="aR7vS"></a></p><h2 id="scan-查询优化"><a href="#scan-查询优化" class="headerlink" title="scan 查询优化"></a>scan 查询优化</h2><p>如果我们的查询有 &lt; 或 &gt; 等查询过滤条件，比如下面的查询条件：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 &gt; 'row000' and col00 &lt; 'row005'"</span>).show()</span><br></pre></td></tr></table></figure><p>这个在 shc 里面转换成 HBase 的过滤为一条 get 和 一个 scan，具体为 get 的 Rowkey 为 row0005\xff\xff\xff\xff；scan 的 startRow 为 row000，endRow 为 row005\xff\xff\xff\xff，然后将查询的结果返回到 spark 层面上进行过滤。<br>总体来说，shc 能在一定程度上对查询进行优化，避免了全表扫描。但是经过评测，shc 其实还有很多地方不够完善，算子下沉并没有下沉到 HBase 层面上进行。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;Apache &lt;a href=&quot;https://www.iteblog.com/archives/tag/spark/&quot; target=
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>shadowsock-vps搭建VPN</title>
    <link href="cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/"/>
    <id>cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/</id>
    <published>2019-04-19T15:26:15.000Z</published>
    <updated>2020-04-04T17:11:07.011Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p><a name="-1"></a></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>还有10天左右就要回国了，由于职业的需要，对Google的依赖的越来越大的，那么回国后怎么才能‘科学上网’呢？之前在国内的时候，有使用过Lantern，稳定性和速度都还是不错了，可惜后来被和谐了。所以今天准备尝试搭建VPN，自己独立使用，一边搭建一边将过程记录下来。<br><a name="-2"></a></p><h1 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1><p>VPS: VPS（Virtual Private Server 虚拟专用服务器）技术，将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器 [1] 技术，和虚拟化技术 [2] 。在容器或虚拟机中，每个VPS都可分配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。<br><br><br>VPS为使用者提供了管理配置的自由，可用于企业虚拟化，也可以用于IDC资源租用。<br><br><br>VPN: VPN的学名叫虚拟专用网，洋文叫“Virtual Private Network”。维基百科的介绍在“这里”。本来这玩意儿主要是用于商业公司，为了让那些不在公司里的员工（比如出差在外的）能够方便地访问公司的内部网络。为了防止黑客冒充公司的员工，从外部访问公司的内部网络，VPN 软件都会提供强大的加密功能。而这个加密功能，也就让它顺便成为翻墙的利器。<br><a name="-3"></a></p><h1 id="科学上网原理"><a href="#科学上网原理" class="headerlink" title="科学上网原理"></a>科学上网原理</h1><p>VPN浏览外网的原理<br><br><br>使用 VPN 通常需要先安装客户端软件。当你运行 VPN 客户端，它会尝试联到 VPN 服务器（这点跟加密代理类似）。一旦和 VPN 服务器建立连接，VPN 客户端就会在你的系统中建立了一个虚拟局域网。而且，你的系统中也会多出一个虚拟网卡（在 Windows 下，可以用 ipconfig /all 命令，看到这多出来的网卡）。这样一来，你的系统中就有不止一块网卡。这就引出一个问题：那些访问网络的程序，它的数据流应该通过哪个网卡进出？<br>为了解决此问题，VPN 客户端通常会修改你系统的路由表，让那些数据流，优先从虚拟的网卡进出。由于虚拟的网卡是通往 VPN 服务器的，当数据流到达 VPN 服务器之后，VPN 服务器再帮你把数据流转向到真正的目的地。<br><br><br>前面说了，VPN 为了保证安全，都采用强加密的方式传输数据。这样一来，GFW 就无法分析你的网络数据流，进行敏感词过滤。所以，使用墙外的VPN服务器，无形中就能达到翻墙的效果。<br><a name="-4"></a></p><h1 id="方案选择"><a href="#方案选择" class="headerlink" title="方案选择"></a>方案选择</h1><p>VPN是一个大类，其中有很多实现的方法，防火长城现在将 VPN 屏蔽的已经所剩无几，后来大家看到了SSH，使用SSH的sock5很稳定，但是特征也十分明显，防火长城可以对其直接进行定向干扰。<br><br><br>而除了VPN，对于翻墙大家仍然有很多方法，比如Shadowsocks 、Lantern、VPNGate 等等，而实际上无论哪种方式，他们本身都需要一台服务器作为中间人进行消息传递。而VPS虚拟专用服务器就十分适合担当这个角色，并且由于VPS平时就作为商品在各类云服务器平台上售卖，自行购买并搭建相当方便，唯一需要的就是人们对于服务器的操作技术。<br><br><br><strong>而这次选择的方案是：VPS+Shadowsocks</strong><br>**<br>Shadowsocks特点：<br><br><br>省电，在电量查看里几乎看不到它的身影；<br><br><br>支持开机自启动，且断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴；<br><br><br>可使用自己的服务器，安全和速度的保证；<br><br><br>支持区分国内外流量，传统VPN在翻出墙外后访问国内站点会变慢；<br><br><br>可对应用设置单独代理，5.0之后的系统无需root。<br><br><br>Shadowsocks 目前不容易被封杀主要是因为：<br>建立在socks5协议之上，socks5是运用很广泛的协议，所以没办法直接封杀socks5协议<br>使用socks5协议建立连接，而没有使用VPN中的服务端身份验证和密钥协商过程。而是在服务端和客户端直接写死密钥和加密算法。所以防火墙很难找到明显的特征，因为这就是个普通的socks5协议。<br><br><br>Shadowsock搭建也比较简单，所以很多人自己架设VPS搭建，个人使用流量也很小，没法通过流量监控方式封杀。<br>自定义加密方式和密钥。因为加密主要主要是防止被检测，所以要选择安全系数高的加密方式。之前RC4会很容易被破解，而导致被封杀。所以现在推荐使用AES加密。而在客户端和服务端自定义密钥，泄露的风险相对较小。<br>所以如果是自己搭建的Shadosocks被封的概率很小，但是如果是第三方的Shadeowsocks，密码是server定的，你的数据很可能遭受到中间人攻击。<br><a name="-5"></a></p><h1 id="开工"><a href="#开工" class="headerlink" title="开工"></a>开工</h1><p><a name="vps"></a></p><h2 id="购买vps"><a href="#购买vps" class="headerlink" title="购买vps"></a>购买vps</h2><p>首先我们需要购买一台境外的服务器，接着我们在这台云服务器里面安装代理服务，那么以后我们上网的时候就可以通过它来中转，轻松畅快的畅游全网了。<br>购买VPS,我选择了<a href="https://www.vultr.com/" target="_blank" rel="external nofollow noopener noreferrer">vultr</a>，大家用过都说好，购买的过程也很方便。<br><br><br>第一步：选择离中国较近国家的服务器。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400540784-be727007-a613-4b5b-a3d5-9b5fbf9734a7.png#align=left&display=inline&height=1468&name=step1.png&originHeight=1468&originWidth=3066&size=659813&status=done&style=none&width=3066" alt="step1.png"><br><br><br><br><br>第二步：选择服务器配置和系统<br><br><br>这里，系统选择的是CentOS 7,配置的话，如果只是自己浏览网页的话，选择最低配置就好。其他的选项可以略过。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400556831-e2446e46-8c7e-4292-97a8-b89e6d983c9c.png#align=left&display=inline&height=1594&name=step2.png&originHeight=1594&originWidth=2792&size=669959&status=done&style=none&width=2792" alt="step2.png"><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235703-e6afdea2-828f-42a0-a955-67960c8710bc.png#align=left&display=inline&height=1812&originHeight=1812&originWidth=2600&size=0&status=done&style=none&width=2600" alt><br><br><br>第三步：支付和部署<br><br><br>支付可以选择支付宝支付，非常方便。购买成功后，点击Server中的“+”号，来部署你刚刚选择的服务器。<br>第四步：登陆服务器<br><br><br>查看服务器详情 Server Details,根据提供的服务器信息，登陆服务器。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235150-95a87ca7-8392-404e-a5d9-c4a606b8ae7e.png#align=left&display=inline&height=1166&originHeight=1166&originWidth=2728&size=0&status=done&style=none&width=2728" alt><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235639-e36e0afa-200d-49ad-a324-350004d08d09.png#align=left&display=inline&height=1232&originHeight=1232&originWidth=2612&size=0&status=done&style=none&width=2612" alt><br><br><br>我是使用Mac本身终端ssh到服务器上的，因为Mac上多数的SSH客户端要么收费，要么不好用，要么安装过程非常繁琐。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p <span class="number">22</span> root@ip</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235316-408c45f7-df95-4c8c-9428-9f476c6c7190.png#align=left&display=inline&height=244&originHeight=244&originWidth=1304&size=0&status=done&style=none&width=1304" alt><br><a name="shadowsocks"></a></p><h2 id="搭建shadowsocks服务器"><a href="#搭建shadowsocks服务器" class="headerlink" title="搭建shadowsocks服务器"></a>搭建shadowsocks服务器</h2><p>连接到你的 vultr 服务器之后，接下来就可以使用几个命令让你快速搭建一个属于自己的 ss 服务器：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install wget</span><br></pre></td></tr></table></figure><p><br>接着执行安装shadowsocks：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span><br></pre></td></tr></table></figure><p><br>获取 <a href="http://shadowsocks.sh/" target="_blank" rel="external nofollow noopener noreferrer">shadowsocks.sh</a> 读取权限：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x shadowsocks.sh</span><br></pre></td></tr></table></figure><p><br>设置你的 ss 密码和端口号：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./shadowsocks.sh <span class="number">2</span>&gt;&amp;<span class="number">1</span> | tee shadowsocks.log</span><br></pre></td></tr></table></figure><p><br>接下来后就可以设置密码和端口号了<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400236071-267db22c-ea03-4f8b-969d-86ae5efc4d68.png#align=left&display=inline&height=306&originHeight=306&originWidth=1086&size=0&status=done&style=none&width=1086" alt><br><br><br>密码和端口号可以使用默认的，也可以直接重新输入新的。<br>选择加密方式<br><br><br>设置完密码和端口号之后，我们选择加密方式，这里选择 7 ，使用aes-256-cfb的加密模式<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235435-525e1564-eb61-47e0-b9b1-a28683979702.png#align=left&display=inline&height=684&originHeight=684&originWidth=914&size=0&status=done&style=none&width=914" alt><br><br><br>接着按任意键进行安装。<br>安装ss完成后<br><br><br>会给你显示你需要连接 vpn 的信息：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400239145-4d70810e-bc73-4cfa-bc8c-4da35e3e0dcf.png#align=left&display=inline&height=464&originHeight=464&originWidth=1186&size=0&status=done&style=none&width=1186" alt><br>搞定，将这些信息保存起来，那么这时候你就可以使用它们来科学上网啦。<br><a name="bbr"></a></p><h2 id="使用BBR加速上网"><a href="#使用BBR加速上网" class="headerlink" title="使用BBR加速上网"></a>使用BBR加速上网</h2><p>安装 BBR<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh</span><br></pre></td></tr></table></figure><p><br>获取读写权限<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x bbr.sh</span><br></pre></td></tr></table></figure><p><br>启动BBR安装<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bbr.sh</span><br></pre></td></tr></table></figure><p><br>接着按任意键，开始安装，坐等一会。安装完成一会之后它会提示我们是否重新启动vps，我们输入 y 确定重启服务器。<br>重新启动之后，输入：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep bbr</span><br></pre></td></tr></table></figure><p><br>如果看到 tcp_bbr 就说明 BBR 已经启动了。<br><a name="-6"></a></p><h2 id="客户端进行连接"><a href="#客户端进行连接" class="headerlink" title="客户端进行连接"></a>客户端进行连接</h2><p><a name="windowsshadowsocks"></a></p><h3 id="windows使用Shadowsocks"><a href="#windows使用Shadowsocks" class="headerlink" title="windows使用Shadowsocks"></a>windows使用Shadowsocks</h3><p>windows点击下载：<a href="https://pan.baidu.com/s/19m0AfTkPDSRj0bfYrGpbIg" target="_blank" rel="external nofollow noopener noreferrer">Shadowsocks windows客户端</a><br>打开 Shadowsocks 客户端，输入ip地址，密码，端口，和加密方式。接着点击确定，右下角会有个小飞机按钮，右键–&gt;启动代理。<br><a name="androidshadowsocks"></a></p><h3 id="Android使用Shadowsocks"><a href="#Android使用Shadowsocks" class="headerlink" title="Android使用Shadowsocks"></a>Android使用Shadowsocks</h3><p>Android点击下载：<a href="https://pan.baidu.com/s/1coAkZn-GuYHu5eIKaHECxA" target="_blank" rel="external nofollow noopener noreferrer">Shadowsocks Android客户端</a><br>打开apk安装，接着打开APP，输入ip地址，密码，端口，和加密方式。即可科学上网。<br><a name="iphoneshadowsocks"></a></p><h3 id="iPhone使用Shadowsocks"><a href="#iPhone使用Shadowsocks" class="headerlink" title="iPhone使用Shadowsocks"></a>iPhone使用Shadowsocks</h3><p>iPhone要下载的app需要在appstore下载，但是需要用美区账号才能下载，而且这个APP需要钱。在这里提供一种解决方案，就是可以再搭建一个<a href="https://wistbean.github.io/ipsec,l2tp_vpn.html#%E4%BD%BF%E7%94%A8-IPsec-L2TP-%E8%84%9A%E6%9C%AC%E6%90%AD%E5%BB%BA" target="_blank" rel="external nofollow noopener noreferrer">IPsec/L2TP</a> VPN,专门给你的iPhone使用。<br><a name="mac"></a></p><h3 id="Mac配置"><a href="#Mac配置" class="headerlink" title="Mac配置"></a>Mac配置</h3><p>用的是Mac电脑，所以点击相关链接。东西都挂在github上，下载对应的zip文件，下载完成后安装并运行起来。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235178-7ccd25cd-8b34-4103-8cc8-204e84672cfe.png#align=left&display=inline&height=748&originHeight=748&originWidth=1302&size=0&status=done&style=none&width=1302" alt><br><br><br>点击图标，进入 服务器设置<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400239120-45698c54-ac3b-48b9-9c4d-9f3537938f87.png#align=left&display=inline&height=926&originHeight=926&originWidth=1302&size=0&status=done&style=none&width=1302" alt><br><br><br>主要有四个地方要填，服务器的地址，端口号，加密方法，密码。服务器地址即为之前 Main controls选项中的IP地址。端口号、加密方法、密码必须与之前 Shadowsocks Server 中的信息一一匹配，否则会连接失败。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235629-2861fc8f-5bfc-4610-993f-49064774e693.png#align=left&display=inline&height=1156&originHeight=1156&originWidth=1292&size=0&status=done&style=none&width=1292" alt><br><br><br>设置完成后点击确定，然后服务器选择这个配置，默认选中PAC自动模式，确保Shadowsocks状态为On，这时候打开谷歌试试~<br>接着就可以上外网了 😂</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="工具" scheme="cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="shadowsock" scheme="cpeixin.cn/tags/shadowsock/"/>
    
  </entry>
  
  <entry>
    <title>Structured Streaming 重温</title>
    <link href="cpeixin.cn/2019/02/10/Structured-Streaming-%E9%87%8D%E6%B8%A9/"/>
    <id>cpeixin.cn/2019/02/10/Structured-Streaming-%E9%87%8D%E6%B8%A9/</id>
    <published>2019-02-10T08:03:44.000Z</published>
    <updated>2020-05-09T04:09:47.215Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p><a name="N4GjR"></a></p><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>_<br>Structured Streaming 则是在 Spark 2.0 加入的经过重新设计的全新流式引擎。它的模型十分简洁，易于理解。一个流的数据源从逻辑上来说就是一个不断增长的动态表格，随着时间的推移，新数据被持续不断地添加到表格的末尾。用户可以使用 Dataset/DataFrame 或者 SQL 来对这个动态数据源进行实时查询。每次查询在逻辑上就是对当前的表格内容执行一次 SQL 查询。如何执行查询则是由用户通过触发器（Trigger）来设定。用户既可以设定定期执行，也可以让查询尽可能快地执行，从而达到实时的效果。最后，系统通过 checkpointing 和 Write-Ahead Logs来确保端到端的一次容错保证。一个流的输出有多种模式，既可以是基于整个输入执行查询后的完整结果，也可以选择只输出与上次查询相比的差异，或者就是简单地追加最新的结果。这个模型对于熟悉 SQL 的用户来说很容易掌握，对流的查询跟查询一个表格几乎完全一样。<br><br><br>在内部，默认情况下，结构化流查询是使用_微批量处理_引擎_处理的_，该引擎将数据流作为一系列小批量作业进行处理，从而实现了低至100毫秒的端到端延迟以及 exactly-once的容错保证。但是，自Spark 2.3起，我们引入了一种称为“ <strong>连续处理”</strong>的新低延迟处理模式，该模式可以实现一次最少保证的低至1毫秒的端到端延迟。在不更改查询中的Dataset / DataFrame操作的情况下，您将能够根据应用程序需求选择模式<br><br><br></p><p><a name="YF5tx"></a></p><h2 id="Structured-Streaming-是对-Spark-Streaming-的改进么？"><a href="#Structured-Streaming-是对-Spark-Streaming-的改进么？" class="headerlink" title="Structured Streaming 是对 Spark Streaming 的改进么？"></a>Structured Streaming 是对 Spark Streaming 的改进么？</h2><p><br>Structured Streaming 并不是对 Spark Streaming 的简单改进，而是我们吸取了过去几年在开发 Spark SQL 和 Spark Streaming 过程中的经验教训，以及 Spark 社区和 Databricks 众多客户的反馈，重新开发的全新流式引擎，致力于为批处理和流处理提供统一的高性能 API。同时，在这个新的引擎中，我们也很容易实现之前在 Spark Streaming 中很难实现的一些功能，<strong>比如 Event Time 的支持</strong>，Stream-Stream Join，毫秒级延迟<br><br><br>类似于 Dataset/DataFrame 代替 Spark Core 的 RDD 成为为 Spark 用户编写批处理程序的首选，Dataset/DataFrame 也将替代 Spark Streaming 的 DStream，成为编写流处理程序的首选。<br></p><p><a name="HuvDe"></a></p><h2 id="Structured-Streaming-的-Spark-有什么优劣势吗？"><a href="#Structured-Streaming-的-Spark-有什么优劣势吗？" class="headerlink" title="Structured Streaming 的 Spark 有什么优劣势吗？"></a>Structured Streaming 的 Spark 有什么优劣势吗？</h2><ul><li>简洁的模型。Structured Streaming 的模型很简洁，易于理解。用户可以直接把一个流想象成是无限增长的表格。</li><li>一致的 API。由于和 Spark SQL 共用大部分 API，对 Spaprk SQL 熟悉的用户很容易上手，代码也十分简洁。同时批处理和流处理程序还可以共用代码，不需要开发两套不同的代码，显著提高了开发效率。</li><li>卓越的性能。Structured Streaming 在与 Spark SQL 共用 API 的同时，也直接使用了 Spark SQL 的 Catalyst 优化器和 Tungsten，数据处理性能十分出色。此外，Structured Streaming 还可以直接从未来 Spark SQL 的各种性能优化中受益。</li><li>多语言支持。Structured Streaming 直接支持目前 Spark SQL 支持的语言，包括 Scala，Java，Python，R 和 SQL。用户可以选择自己喜欢的语言进行开发。</li></ul><br><br>呃～～ 关于Structured Streaming的介绍就说到这里，如果想看更详细更准确的介绍呢，还是乖乖的去官网吧。在2017年10月份的时候，新立项的一个app用户行为实时项目，我有意使用Structured Streaming，所以就调研了一下，自己写了一个demo，我记忆里那时写Structured很别扭，就像一个模版一样，输入源和输出源都被规定好了函数和参数，并且在那时候测试后的时候，不怎么稳定，而且官方并没有给出成熟的版本，当时所测试的功能还都是 alpha 版本，所以当时就还是使用了Spark Streaming<p>不过现在来看，Structured Streaming 越来越成熟，Spark Streaming感觉似乎停止了更新。Structured streaming应该是Spark流处理的未来，但是很难替代Flink。Flink在流处理上的天然优势很难被Spark超越。<br><br><br>在读完了Structured Streaming官网后，还是亲手的写一些实例感受一下，以后做架构的时候，如果适合的话，还可以加进来。</p><p><a name="9BWc2"></a></p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><br><a name="TJ1U5"></a> ### complete, append, update<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">StreamingQuery</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.&#123;<span class="type">JSON</span>, <span class="type">JSONObject</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">structured_kafka</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger:<span class="type">Logger</span> = <span class="type">Logger</span>.getRootLogger</span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">kafka_format</span>(<span class="params">date_time: <span class="type">String</span>, keyword_list: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">spark</span></span>: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Structrued-Streaming"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafka_df: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(<span class="string">"kafka"</span>)</span><br><span class="line">      .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">      .option(<span class="string">"subscribe"</span>, <span class="string">"weibo_keyword"</span>)</span><br><span class="line">      .option(<span class="string">"startingOffsets"</span>, <span class="string">"earliest"</span>)</span><br><span class="line">      .option(<span class="string">"includeTimestamp"</span>, value = <span class="literal">true</span>)</span><br><span class="line"><span class="comment">//      .option("endingOffsets", "latest")</span></span><br><span class="line"><span class="comment">//      .option("startingOffsets", """&#123;"topic1":&#123;"0":23,"1":-2&#125;,"topic2":&#123;"0":-2&#125;&#125;""")</span></span><br><span class="line"><span class="comment">//      .option("endingOffsets", """&#123;"topic1":&#123;"0":50,"1":-1&#125;,"topic2":&#123;"0":-1&#125;&#125;""")</span></span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyvalue_df: <span class="type">DataFrame</span> = kafka_df</span><br><span class="line">      .selectExpr(<span class="string">"CAST(value AS STRING)"</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map((x: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> date_time: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"datetime"</span>)</span><br><span class="line">        <span class="keyword">val</span> keyword_list: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"keywordList"</span>)</span><br><span class="line">        (date_time, keyword_list)</span><br><span class="line">      &#125;)</span><br><span class="line">      .flatMap((x: (<span class="type">String</span>, <span class="type">String</span>)) =&gt;&#123;</span><br><span class="line">        x._2.split(<span class="string">","</span>).map((word: <span class="type">String</span>) =&gt;(x._1,word))</span><br><span class="line">      &#125;)</span><br><span class="line">      .toDF(<span class="string">"date_time"</span>, <span class="string">"keyword"</span>)</span><br><span class="line">      .groupBy(<span class="string">"keyword"</span>).count()</span><br><span class="line">      .orderBy($<span class="string">"count"</span>.desc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> query: <span class="type">StreamingQuery</span> = keyvalue_df.writeStream</span><br><span class="line">      .outputMode(<span class="string">"complete"</span>) <span class="comment">//append</span></span><br><span class="line">      .format(<span class="string">"console"</span>)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>其中需要注意：<br>_<br>_Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;_<br>_<br>Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;<br><br>未进行aggregate的stream不能sort<br>_<br>_ <a name="5prwS"></a> ### window窗口 _<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.&#123;<span class="type">StreamingQuery</span>, <span class="type">Trigger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">structured_kafka_window</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger:<span class="type">Logger</span> = <span class="type">Logger</span>.getRootLogger</span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">kafka_format</span>(<span class="params">date_time: <span class="type">String</span>, keyword_list: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">spark</span></span>: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Structrued-Streaming"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafka_df: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(<span class="string">"kafka"</span>)</span><br><span class="line">      .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">      .option(<span class="string">"subscribe"</span>, <span class="string">"weibo_keyword"</span>)</span><br><span class="line">      .option(<span class="string">"startingOffsets"</span>, <span class="string">"latest"</span>)</span><br><span class="line">      .option(<span class="string">"includeTimestamp"</span>, value = <span class="literal">true</span>)</span><br><span class="line"><span class="comment">//      .option("endingOffsets", "latest")</span></span><br><span class="line"><span class="comment">//      .option("startingOffsets", """&#123;"topic1":&#123;"0":23,"1":-2&#125;,"topic2":&#123;"0":-2&#125;&#125;""")</span></span><br><span class="line"><span class="comment">//      .option("endingOffsets", """&#123;"topic1":&#123;"0":50,"1":-1&#125;,"topic2":&#123;"0":-1&#125;&#125;""")</span></span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyvalue_df: <span class="type">DataFrame</span> = kafka_df</span><br><span class="line">      .selectExpr(<span class="string">"CAST(value AS STRING)"</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map((x: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> date_time: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"datetime"</span>)</span><br><span class="line">        <span class="keyword">val</span> keyword_list: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"keywordList"</span>)</span><br><span class="line">        (date_time, keyword_list)</span><br><span class="line">      &#125;)</span><br><span class="line">      .flatMap((x: (<span class="type">String</span>, <span class="type">String</span>)) =&gt;&#123;</span><br><span class="line">        x._2.split(<span class="string">","</span>).map((word: <span class="type">String</span>) =&gt;(x._1,word))</span><br><span class="line">      &#125;)</span><br><span class="line">      .toDF(<span class="string">"date_time"</span>, <span class="string">"keyword"</span>)</span><br><span class="line">      .groupBy(window($<span class="string">"date_time"</span>, <span class="string">"5 minutes"</span>, <span class="string">"1 minutes"</span>),$<span class="string">"keyword"</span>)</span><br><span class="line">      .count()</span><br><span class="line">      .orderBy(<span class="string">"window"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> query: <span class="type">StreamingQuery</span> = keyvalue_df.writeStream</span><br><span class="line">      .outputMode(<span class="string">"complete"</span>) <span class="comment">//append</span></span><br><span class="line">      .format(<span class="string">"console"</span>)</span><br><span class="line">      .option(<span class="string">"truncate"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">"5 seconds"</span>))</span><br><span class="line">      .start()</span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>这里关于window窗口的划分，我建议大家好好的研读一下源码：<br>**位置：package **org.apache.spark.sql.catalyst.analysis<br>![屏幕快照 2020-05-08 下午11.29.50.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1588952005543-007c1291-12fd-4148-a625-587b1a6149f3.png#align=left&display=inline&height=1694&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-08%20%E4%B8%8B%E5%8D%8811.29.50.png&originHeight=1694&originWidth=1936&size=488433&status=done&style=none&width=1936)<br><p><a name="cuL0b"></a></p><h3 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基于event-time的window，words包含timestamp和word两列</span></span><br><span class="line">word</span><br><span class="line">  .withWatermark(<span class="string">"timestamp"</span>, <span class="string">"30 minutes"</span>)<span class="comment">//某窗口结果为x，但是部分数据在这个窗口的最后一个timestamp过后还没到达，Spark在这会等30min，过后就不再更新x了。</span></span><br><span class="line">  .dropDuplicates(<span class="string">"User"</span>, <span class="string">"timestamp"</span>)</span><br><span class="line">  .groupBy(window(col(<span class="string">"timestamp"</span>), <span class="string">"10 minutes"</span>),col(<span class="string">"User"</span>))<span class="comment">// 10min后再加一个参数变为Sliding windows，表示每隔多久计算一次。</span></span><br><span class="line">  .count() </span><br><span class="line">  .writeStream</span><br><span class="line">  .queryName(<span class="string">"events_per_window"</span>)</span><br><span class="line">  .format(<span class="string">"memory"</span>)</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM events_per_window"</span>)</span><br></pre></td></tr></table></figure><p>watermark = max event time seen by the engine - late threshold，相当于Flink的BoundedOutOfOrdernessTximestampExtractor。<br><br><br>在window计算被触发时，Spark会删除结束时间低于当前wm的window的中间结果，属于该window的迟到数据“可能”会被忽略，越迟越可能被忽略，删除完后才更新wm，所以即便下一批没有数据加入，Spark所依据的wm也是新的，下下一批wm不变。<br><br><br>上面是update mode，如果是append模式，那么结果要等到trigger后发现window的结束时间低于更新后的水位线时才会出来。另外，max event time seen by the engine - late threshold机制意味着如果下一批计算没有更晚的数据加入，那么wm就不会前进，那么数据的append就会被延后。<br><br><br>Conditions for watermarking to clean aggregation state(as of Spark 2.1.1, subject to change in the future)</p><ul><li>不支持complete模式。</li><li>groupBy必须包含timestamp列或者window(col(timestamp))，withWatermark中的列要和前面的timestamp列相同</li><li>顺序必须是先withWatermark再到groupBy</li></ul><p><br>参考：<br><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="external nofollow noopener noreferrer">结构化流编程指南</a><br><a href="https://www.cnblogs.com/code2one/p/9872355.html" target="_blank" rel="external nofollow noopener noreferrer">Spark之Structured Streaming</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;N4GjR&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;总览&quot;&gt;&lt;a href=&quot;#总览&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 Let&#39;s go</title>
    <link href="cpeixin.cn/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Let-s-go/"/>
    <id>cpeixin.cn/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Let-s-go/</id>
    <published>2019-01-01T15:03:48.000Z</published>
    <updated>2020-05-06T16:54:06.970Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --><p>前段时间讲了数据挖掘十大经典算法，在实战中也了解了随机森林、逻辑回归的概念及工具使用。这些算法都属于传统的机器学习算法。你肯定听说过这两年很火的深度学习，那么机器学习算法和深度学习有什么关联呢？在这篇文章中，我们会通过以下几个方面了解深度学习：</p><ol><li>数据挖掘、机器学习和深度学习的区别是什么？这些概念都代表什么？</li><li>我们通过深度学习让机器具备人的能力，甚至某些技能的水平超过人类，比如图像识别、下棋对弈等。那么深度学习的大脑是如何工作的？</li><li>深度学习是基于神经网络构建的，都有哪些常用的网络模型？</li><li>深度学习有三个重要的应用领域，这三个应用领域分别是什么？</li></ol><p><a name="a1lJh"></a></p><h3 id="数据挖掘，机器学习，深度学习的区别是什么？"><a href="#数据挖掘，机器学习，深度学习的区别是什么？" class="headerlink" title="数据挖掘，机器学习，深度学习的区别是什么？"></a>数据挖掘，机器学习，深度学习的区别是什么？</h3><p><br>实际上数据挖掘和机器学习在很大程度上是重叠的。一些常用算法，比如 K-Means、KNN、SVM、决策树和朴素贝叶斯等，既可以说是数据挖掘算法，又可以说是机器学习算法。那么数据挖掘和机器学习之间有什么区别呢？数据挖掘通常是从现有的数据中提取规律模式（pattern）以及使用算法模型（model）。核心目的是找到这些数据变量之间的关系，因此我们也会通过数据可视化对变量之间的关系进行呈现，用算法模型挖掘变量之间的关联关系。通常情况下，我们只能判断出来变量 A 和变量 B 是有关系的，但并不一定清楚这两者之间有什么具体关系。在我们谈论数据挖掘的时候，更强调的是从数据中挖掘价值。<br><br><br>机器学习是人工智能的一部分，它指的是通过训练数据和算法模型让机器具有一定的智能。一般是通过已有的数据来学习知识，并通过各种算法模型形成一定的处理能力，比如分类、聚类、预测、推荐能力等。这样当有新的数据进来时，就可以通过训练好的模型对这些数据进行预测，也就是通过机器的智能帮我们完成某些特定的任务。深度学习属于机器学习的一种，它的目标同样是让机器具有智能，只是与传统的机器学习算法不同，它是通过神经网络来实现的。<br><br><br>神经网络就好比是机器的大脑，刚开始就像一个婴儿一样，是一张白纸。但通过多次训练之后，“大脑”就可以逐渐具备某种能力。这个训练过程中，我们只需要告诉这个大脑输入数据是什么，以及对应的输出结果是什么即可。通过多次训练，“大脑”中的多层神经网络的参数就会自动优化，从而得到一个适应于训练数据的模型。所以你能看到在传统的机器学习模型中，我们都会讲解模型的算法原理，比如 K-Means 的算法原理，KNN 的原理等。而到了神经网络，我们更关注的是网络结构，以及网络结构中每层神经元的传输机制。我们不需要告诉机器具体的特征规律是什么，只需把我们想要训练的数据和对应的结果告诉机器大脑即可。深度学习会自己找到数据的特征规律！而传统机器学习往往需要专家（我们）来告诉机器采用什么样的模型算法，这就是深度学习与传统机器学习最大的区别。<br><br><br>另外深度学习的神经网络结构通常比较深，一般都是 5 层以上，甚至也有 101 层或更多的层数。这些深度的神经网络可以让机器更好地自动捕获数据的特征。<br><a name="FSGJb"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Eijqr"></a></p><h3 id="神经网络是如何工作的"><a href="#神经网络是如何工作的" class="headerlink" title="神经网络是如何工作的"></a>神经网络是如何工作的</h3><p><br>神经网络可以说是机器的大脑，经典的神经网络结构可以用下面的图来表示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1588344895471-60b8702d-0d33-4043-93a6-2fe227044890.jpeg#align=left&display=inline&height=1327&margin=%5Bobject%20Object%5D&name=image.jpeg&originHeight=1327&originWidth=2237&size=290312&status=done&style=none&width=2237" alt="image.jpeg"><br><br><br>这里有一些概念你需要了解。<br><br><br>节点：神经网络是由神经元组成的，也称之为节点，它们分布在神经网络的各个层中，这些层包括输入层，输出层和隐藏层。<br><br><br>输入层：负责接收信号，并分发到隐藏层。一般我们将数据传给输入层。<br><br><br>输出层：负责输出计算结果，一般来说输出层节点数等于我们要分类的个数。<br><br><br>隐藏层：除了输入层和输出层外的神经网络都属于隐藏层，隐藏层可以是一层也可以是多层，每个隐藏层都会把前一层节点传输出来的数据进行计算（你可以理解是某种抽象表示），这相当于把数据抽象到另一个维度的空间中，可以更好地提取和计算数据的特征。<br><br><br>工作原理：神经网络就好比一个黑盒子，我们只需要告诉这个黑盒子输入数据和输出数据，神经网络就可以自我训练。一旦训练好之后，就可以像黑盒子一样使用，当你传入一个新的数据时，它就会告诉你对应的输出结果。在训练过程中，神经网络主要是通过前向传播和反向传播机制运作的。<br><br><br>什么是前向传播和反向传播呢？<br><br><br>前向传播：数据从输入层传递到输出层的过程叫做前向传播。这个过程的计算结果通常是通过上一层的神经元的输出经过矩阵运算和激活函数得到的。这样就完成了每层之间的神经元数据的传输。<br><br><br>反向传播：当前向传播作用到输出层得到分类结果之后，我们需要与实际值进行比对，从而得到误差。反向传播也叫作误差反向传播，核心原理是通过代价函数对网络中的参数进行修正，这样更容易让网络参数得到收敛。所以，整个神经网络训练的过程就是不断地通过前向 - 反向传播迭代完成的，当达到指定的迭代次数或者达到收敛标准的时候即可以停止训练。然后我们就可以拿训练好的网络模型对新的数据进行预测。当然，深度神经网络是基于神经网络发展起来的，它的原理与神经网络的原理一样，只不过强调了模型结构的深度，通常有 5 层以上，这样模型的学习能力会更强大。常用的神经网络都有哪些按照中间层功能的不同，神经网络可以分为三种网络结构，分别为 FNN、CNN 和 RNN。<br><br><br>FNN（Fully-connected Neural Network）指的是全连接神经网络，全连接的意思是每一层的神经元与上一层的所有神经元都是连接的。不过在实际使用中，全连接的参数会过多，导致计算量过大。因此在实际使用中全连接神经网络的层数一般比较少。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1588344980493-f83f763c-164a-4c40-9f68-cc8f9368eea5.jpeg#align=left&display=inline&height=1355&margin=%5Bobject%20Object%5D&name=c2bfd532360e5cc026aa12f45c86957a.jpg&originHeight=1355&originWidth=2448&size=424117&status=done&style=none&width=2448" alt="c2bfd532360e5cc026aa12f45c86957a.jpg"></p><p>CNN 叫作卷积神经网络，在图像处理中有广泛的应用，了解图像识别的同学对这个词一定不陌生。CNN 网络中，包括了卷积层、池化层和全连接层。这三个层都有什么作用呢？卷积层相当于一个滤镜的作用，它可以把图像进行分块，对每一块的图像进行变换操作。池化层相当于对神经元的数据进行降维处理，这样输出的维数就会减少很多，从而降低整体的计算量。全连接层通常是输出层的上一层，它将上一层神经元输出的数据转变成一维的向量。<br><br><br>RNN 称为循环神经网络，它的特点是神经元的输出可以在下一个时刻作用到自身，这样 RNN 就可以看做是在时间上传递的神经网络。它可以应用在语音识别、自然语言处理等与上下文相关的场景。深度学习网络往往包括了这三种网络的变种形成，常用的深度神经网络包括 AlexNet、VGG19、GoogleNet、ResNet 等，我总结了这些网络的特点，你可以看下：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1588344996865-24933d1e-03b9-4fe1-9255-402068146252.png#align=left&display=inline&height=205&margin=%5Bobject%20Object%5D&name=490ced74dc6cf70403e73c05f302449f.png&originHeight=205&originWidth=560&size=33554&status=done&style=none&width=560" alt="490ced74dc6cf70403e73c05f302449f.png"><br>你能看出随着时间的推进，提出的深度学习网络层数越来越深，Top-5 错误率越来越低。你可能会问什么是 Top-5 错误率，实际上这些网络结构的提出和一个比赛相关，这个比赛叫做 ILSVRC，英文全称叫做 Large Scale Visual Recognition Challenge。它是一个关于大规模图像可视化识别的比赛，所基于的数据集就是著名的 ImageNet 数据集，一共包括了 1400 万张图片，涵盖 2 万多个类别。表格中的 AlexNet 就是 2012 年的 ILSVRC 冠军，当时的 Top-5 正确率是 84.7%，VGG 和 GoogleNet 是 2014 年 ILSVRC 比赛的模型，其中 GoogleNet 是当时比赛的冠军，而 VGG 是当时比赛的亚军，它的效率低于 GoogleNet。VGG 有两个版本，VGG16 和 VGG19，分别是 16 层和 19 层的 VGG 网络，这两者没有本质的区别，只是网络深度不同。到了 2015 年，比赛冠军是 ResNet，Top-5 正确率达到了 96.43%。ResNet 也有不同的版本，比如 ResNet50、ResNet101 和 ResNet152 等，名称后面的数字代表的是不同的网络深度。之后 ResNet 在其他图像比赛中也多次拿到冠军。<br></p><p><a name="OxXl6"></a></p><h3 id="深度学习的应用领域"><a href="#深度学习的应用领域" class="headerlink" title="深度学习的应用领域"></a>深度学习的应用领域</h3><p><br>从 ImageNet 跑出来的这些优秀模型都是基于 CNN 卷积神经网络的。实际上深度学习有三大应用领域，图像识别就是其中之一，其他领域分别是语音识别和自然语言处理。这三个应用领域有一个共同的特性，就是都来自于信号处理。我们人类平时会处理图像信息，语音信息以及语言文字信息。机器可以帮助我们完成这三个应用里的某些工作。<br><br><br>比如图像识别领域中图像分类和物体检测就是两个核心的任务。我们可以让机器判断图像中都有哪些物体，类别是什么，以及这些物体所处的位置。图像识别被广泛应用在安防检测中。此外人脸识别也是图像识别重要的应用场景。Siri 大家一定不陌生，此外还有我们使用的智能电视等，都采用了语音识别技术。语音识别技术可以识别人类的语音指令并进行交互。在语音导航中，还采用了语音合成技术，这样就可以让机器模拟人的声音为我们服务，Siri 语音助手也采用了语音识别和合成的技术。自然语言处理的英文缩写是 NLP，它被广泛应用到自动问答、智能客服、过滤垃圾邮件和短信等领域中。在电商领域，我们可以通过 NLP 自动给商品评论打标签，在用户决策的时候提供数据支持。在自动问答中，我们可以输入自己想问的问题，让机器来回答，比如在百度中输入“姚明的老婆”，就会自动显示出”叶莉“。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1588345111082-f2216446-9de9-4195-aebd-d2be302b80db.png#align=left&display=inline&height=700&margin=%5Bobject%20Object%5D&name=image.png&originHeight=700&originWidth=1729&size=364371&status=done&style=none&width=1729" alt="image.png"><br><br><br>此外这些技术还可以相互组合为我们提供服务，比如在无人驾驶中就采用了图像识别、语音识别等技术。在超市购物中也采用了集成图像识别、意图识别等技术等。<br></p><p><a name="HkLRt"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><br>今天我们大概了解了一下深度学习。深度学习也是机器学习的一种。我们之前讲解了数据挖掘十大经典算法，还有逻辑回归、随机森林算法等，这些都是传统的机器学习算法。在日常工作中，可以满足大部分的机器学习任务。但是对于数据量更大，更开放性的问题，我们就可以采用深度学习的算法，让机器自己来找规律，而不是通过我们指定的算法来找分类规律。所以深度学习的普适性会更强一些，但也并不代表深度学习就优于机器学习。<br><br><br>一方面深度学习需要大量的数据，另一方面深度学习的学习时间，和需要的计算资源都要大于传统的机器学习。你能看到各种深度学习的训练集一般都还是比较大的，比如 ImageNet 就包括了 1400 万张图片。如果我们没有提供大量的训练数据，训练出来的深度模型识别结果未必好于传统的机器学习。<br><br><br>实际上神经网络最早是在 1986 年提出来的，之后不温不火，直到 ImageNet 于 2009 年提出，在 2010 年开始举办每年的 ImageNet 大规模视觉识别挑战赛（ILSVRC），深度学习才得到迅猛发展。2016 年 Google 研发的 AlphaGo 击败了人类冠军李世石，更是让人们看到了深度学习的力量。一个好问题的提出，可以激发无穷的能量，这是科技进步的源泉，也是为什么在科学上，我们会有各种公开的数据集。一个好的数据集就代表了一个好的问题和使用场景。正是这些需求的出现，才能让我们的算法有更好的用武之地，同时也有了各种算法相互比拼的平台。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;前段时间讲了数据挖掘十大经典算法，在实战中也了解了随机森林、逻辑回归的概念及工具使用。这些算法都属于传统的机器学习算法。你肯定听说过这两年很火的
      
    
    </summary>
    
    
      <category term="深度学习" scheme="cpeixin.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="cpeixin.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>比特币走势预测</title>
    <link href="cpeixin.cn/2018/10/05/%E6%AF%94%E7%89%B9%E5%B8%81%E8%B5%B0%E5%8A%BF%E9%A2%84%E6%B5%8B/"/>
    <id>cpeixin.cn/2018/10/05/%E6%AF%94%E7%89%B9%E5%B8%81%E8%B5%B0%E5%8A%BF%E9%A2%84%E6%B5%8B/</id>
    <published>2018-10-04T16:43:58.000Z</published>
    <updated>2020-05-01T14:21:22.500Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --><p>我们之前介绍了数据挖掘算法中的分类、聚类、回归和关联分析算法，那么对于比特币走势的预测，采用哪种方法比较好呢？<br><br><br>可能有些人会认为采用回归分析会好一些，因为预测的结果是连续的数值类型。实际上，数据挖掘算法还有一种叫时间序列分析的算法，时间序列分析模型建立了观察结果与时间变化的关系，能帮我们预测未来一段时间内的结果变化情况。那么时间序列分析和回归分析有哪些区别呢？<br><br><br>首先，在选择模型前，我们需要确定结果与变量之间的关系。回归分析训练得到的是目标变量 y 与自变量 x（一个或多个）的相关性，然后通过新的自变量 x 来预测目标变量 y。而时间序列分析得到的是目标变量 y 与时间的相关性。<br><br><br>另外，回归分析擅长的是多变量与目标结果之间的分析，即便是单一变量，也往往与时间无关。而时间序列分析建立在时间变化的基础上，它会分析目标变量的趋势、周期、时期和不稳定因素等。这些趋势和周期都是在时间维度的基础上，我们要观察的重要特征。<br><br><br>那么针对今天要进行的预测比特币走势的项目，我们都需要掌握哪些目标呢？</p><ol><li>了解时间序列预测的概念，以及常用的模型算法，包括 AR、MA、ARMA、ARIMA 模型等；</li><li>掌握并使用 ARMA 模型工具，对一个时间序列数据进行建模和预测；</li><li>对比特币的历史数据进行时间序列建模，并预测未来 6 个月的走势。</li></ol><p><a name="cFXtt"></a></p><h3 id="时间序列预测"><a href="#时间序列预测" class="headerlink" title="时间序列预测"></a>时间序列预测</h3><p>关于时间序列，你可以把它理解为按照时间顺序组成的数字序列。实际上在中国古代的农业社会中，人们就将一年中不同时间节点和天气的规律总结了下来，形成了二十四节气，也就是从时间序列中观察天气和太阳的规律（只是当时没有时间序列模型和相应工具），从而使得农业得到迅速发展。</p><p>在现代社会，时间序列在金融、经济、商业领域拥有广泛的应用。在时间序列预测模型中，有一些经典的模型，包括 AR、MA、ARMA、ARIMA。我来给你简单介绍一下。</p><p>AR 的英文全称叫做 Auto Regressive，中文叫自回归模型。这个算法的思想比较简单，它认为过去若干时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点。在我们日常生活环境中就存在白噪声，在数据挖掘的过程中，你可以把它理解为一个期望为 0，方差为常数的纯随机过程。AR 模型还存在一个阶数，称为 AR（p）模型，也叫作 p 阶自回归模型。它指的是通过这个时刻点的前 p 个点，通过线性组合再加上白噪声来预测当前时刻点的值。</p><p>MA 的英文全称叫做 Moving Average，中文叫做滑动平均模型。它与 AR 模型大同小异，AR 模型是历史时序值的线性组合，MA 是通过历史白噪声进行线性组合来影响当前时刻点。AR 模型中的历史白噪声是通过影响历史时序值，从而间接影响到当前时刻点的预测值。同样 MA 模型也存在一个阶数，称为 MA(q) 模型，也叫作 q 阶移动平均模型。我们能看到 AR 和 MA 模型都存在阶数，在 AR 模型中，我们用 p 表示，在 MA 模型中我们用 q 表示，这两个模型大同小异，与 AR 模型不同的是 MA 模型是历史白噪声的线性组合。</p><p>ARMA 的英文全称是 Auto Regressive Moving Average，中文叫做自回归滑动平均模型，也就是 AR 模型和 MA 模型的混合。相比 AR 模型和 MA 模型，它有更准确的估计。同样 ARMA 模型存在 p 和 q 两个阶数，称为 ARMA(p,q) 模型。</p><p>ARIMA 的英文全称是 Auto Regressive Integrated Moving Average 模型，中文叫差分自回归滑动平均模型，也叫求合自回归滑动平均模型。相比于 ARMA，ARIMA 多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARIMA 的原理和ARMA 模型一样。相比于 ARMA(p,q) 的两个阶数，ARIMA 是一个三元组的阶数 (p,d,q)，称为 ARIMA(p,d,q) 模型。其中 d 是差分阶数。<br><br><br></p><p><a name="3RhDh"></a></p><h3 id="ARMA-模型工具"><a href="#ARMA-模型工具" class="headerlink" title="ARMA 模型工具"></a>ARMA 模型工具</h3><p>上面介绍的 AR，MA，ARMA，ARIMA 四种模型，你只需要了解基础概念即可，中间涉及到的一些数学公式这里不进行展开。在实际工作中，我们更多的是使用工具，我在这里主要讲解下如何使用 ARMA 模型工具。在使用 ARMA 工具前，你需要先引用相关工具包：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.tsa.arima_model <span class="keyword">import</span> ARMA</span><br></pre></td></tr></table></figure><p><br>然后通过 ARMA(endog,order,exog=None) 创建 ARMA 类，这里有一些主要的参数简单说明下：<br><br><br>endog：英文是 endogenous variable，代表内生变量，又叫非政策性变量，它是由模型决定的，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目中需要用到的变量。<br><br><br>order：代表是 p 和 q 的值，也就是 ARMA 中的阶数。<br><br><br>exog：英文是 exogenous variables，代表外生变量。外生变量和内生变量一样是经济模型中的两个重要变量。相对于内生变量而言，外生变量又称作为政策性变量，在经济机制内受外部因素的影响，不是我们模型要研究的变量。<br><br><br>举个例子，如果我们想要创建 ARMA(7,0) 模型，可以写成：ARMA(data,(7,0))，其中 data 是我们想要观察的变量，(7,0) 代表 (p,q) 的阶数。创建好之后，我们可以通过 fit 函数进行拟合，通过 predict(start, end) 函数进行预测，其中 start 为预测的起始时间，end 为预测的终止时间。下面我们使用 ARMA 模型对一组时间序列做建模，代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 用ARMA进行时间序列预测</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> statsmodels.tsa.arima_model <span class="keyword">import</span> ARMA</span><br><span class="line"><span class="keyword">from</span> statsmodels.graphics.api <span class="keyword">import</span> qqplot</span><br><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">data = [<span class="number">5922</span>, <span class="number">5308</span>, <span class="number">5546</span>, <span class="number">5975</span>, <span class="number">2704</span>, <span class="number">1767</span>, <span class="number">4111</span>, <span class="number">5542</span>, <span class="number">4726</span>, <span class="number">5866</span>, <span class="number">6183</span>, <span class="number">3199</span>, <span class="number">1471</span>, <span class="number">1325</span>, <span class="number">6618</span>, <span class="number">6644</span>, <span class="number">5337</span>, <span class="number">7064</span>, <span class="number">2912</span>, <span class="number">1456</span>, <span class="number">4705</span>, <span class="number">4579</span>, <span class="number">4990</span>, <span class="number">4331</span>, <span class="number">4481</span>, <span class="number">1813</span>, <span class="number">1258</span>, <span class="number">4383</span>, <span class="number">5451</span>, <span class="number">5169</span>, <span class="number">5362</span>, <span class="number">6259</span>, <span class="number">3743</span>, <span class="number">2268</span>, <span class="number">5397</span>, <span class="number">5821</span>, <span class="number">6115</span>, <span class="number">6631</span>, <span class="number">6474</span>, <span class="number">4134</span>, <span class="number">2728</span>, <span class="number">5753</span>, <span class="number">7130</span>, <span class="number">7860</span>, <span class="number">6991</span>, <span class="number">7499</span>, <span class="number">5301</span>, <span class="number">2808</span>, <span class="number">6755</span>, <span class="number">6658</span>, <span class="number">7644</span>, <span class="number">6472</span>, <span class="number">8680</span>, <span class="number">6366</span>, <span class="number">5252</span>, <span class="number">8223</span>, <span class="number">8181</span>, <span class="number">10548</span>, <span class="number">11823</span>, <span class="number">14640</span>, <span class="number">9873</span>, <span class="number">6613</span>, <span class="number">14415</span>, <span class="number">13204</span>, <span class="number">14982</span>, <span class="number">9690</span>, <span class="number">10693</span>, <span class="number">8276</span>, <span class="number">4519</span>, <span class="number">7865</span>, <span class="number">8137</span>, <span class="number">10022</span>, <span class="number">7646</span>, <span class="number">8749</span>, <span class="number">5246</span>, <span class="number">4736</span>, <span class="number">9705</span>, <span class="number">7501</span>, <span class="number">9587</span>, <span class="number">10078</span>, <span class="number">9732</span>, <span class="number">6986</span>, <span class="number">4385</span>, <span class="number">8451</span>, <span class="number">9815</span>, <span class="number">10894</span>, <span class="number">10287</span>, <span class="number">9666</span>, <span class="number">6072</span>, <span class="number">5418</span>]</span><br><span class="line">data=pd.Series(data)</span><br><span class="line">data_index = sm.tsa.datetools.dates_from_range(<span class="string">'1901'</span>,<span class="string">'1990'</span>)</span><br><span class="line"><span class="comment"># 绘制数据图</span></span><br><span class="line">data.index = pd.Index(data_index)</span><br><span class="line">data.plot(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 创建ARMA模型# 创建ARMA模型</span></span><br><span class="line">arma = ARMA(data,(<span class="number">7</span>,<span class="number">0</span>)).fit()</span><br><span class="line">print(<span class="string">'AIC: %0.4lf'</span> %arma.aic)</span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">predict_y = arma.predict(<span class="string">'1990'</span>, <span class="string">'2000'</span>)</span><br><span class="line"><span class="comment"># 预测结果绘制</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">ax = data.ix[<span class="string">'1901'</span>:].plot(ax=ax)</span><br><span class="line">predict_y.plot(ax=ax)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587566465755-51d3a226-d74c-47a7-83a2-3f19889b168c.png#align=left&display=inline&height=1153&margin=%5Bobject%20Object%5D&name=71c5fe77926ba65e4b4aca48c337c66a.png&originHeight=1153&originWidth=1726&size=295229&status=done&style=none&width=1726" alt="71c5fe77926ba65e4b4aca48c337c66a.png">)<img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587566483385-5c01d718-1103-4b3b-be36-1f1bc4e0d11d.png#align=left&display=inline&height=1131&margin=%5Bobject%20Object%5D&name=082ff6d7e85e176b8fd5c38f528314fc.png&originHeight=1131&originWidth=1727&size=291062&status=done&style=none&width=1727" alt="082ff6d7e85e176b8fd5c38f528314fc.png"><br><br><br>我创建了 1901 年 -1990 年之间的时间序列数据 data，然后创建 ARMA(7,0) 模型，并传入时间序列数据 data，使用 fit 函数拟合，然后对 1990 年 -2000 年之间的数据进行预测，最后绘制预测结果。<br><br><br>你能看到 ARMA 工具的使用还是很方便的，只是我们需要 p 和 q 的取值。实际项目中，我们可以给 p 和 q 指定一个范围，让 ARMA 都运行一下，然后选择最适合的模型。<br><br><br>你可能会问，怎么判断一个模型是否适合？我们需要引入 AIC 准则，也叫作赤池消息准则，它是衡量统计模型拟合好坏的一个标准，数值越小代表模型拟合得越好。在这个例子中，你能看到 ARMA(7,0) 这个模型拟合出来的 AIC 是 1619.6323（并不一定是最优）。<br></p><p><a name="NqVK6"></a></p><h3 id="对比特币走势进行预测"><a href="#对比特币走势进行预测" class="headerlink" title="对比特币走势进行预测"></a>对比特币走势进行预测</h3><p><br>我们都知道比特币的走势除了和历史数据以外，还和很多外界因素相关，比如用户的关注度，各国的政策，币圈之间是否打架等等。当然这些外界的因素不是我们这节课需要考虑的对象。假设我们只考虑比特币以往的历史数据，用 ARMA 这个时间序列模型预测比特币的走势。比特币历史数据（从 2012-01-01 到 2018-10-31）可以从 GitHub 上下载：<a href="https://github.com/cystanford/bitcoin。" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/cystanford/bitcoin。</a><br><br><br>你能看到数据一共包括了 8 个字段，代表的含义如下：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587566670077-1f6dd13e-fd86-40c8-8be1-f125067a6e36.png#align=left&display=inline&height=288&margin=%5Bobject%20Object%5D&name=b0db4047723ec5e649240e2a87196a36%20%281%29.png&originHeight=288&originWidth=468&size=33146&status=done&style=none&width=468" alt="b0db4047723ec5e649240e2a87196a36 (1).png"><br>我们的目标是构造 ARMA 时间序列模型，预测比特币（平均）价格走势。p 和 q 参数具体选择多少呢？我们可以设置一个区间范围，然后选择 AIC 最低的 ARMA 模型。<br><br><br>首先我们需要加载数据。在准备阶段，我们需要先探索数据，采用数据可视化方式查看比特币的历史走势。按照不同的时间尺度（天，月，季度，年）可以将数据压缩，得到不同尺度的数据，然后做可视化呈现。<br><br><br>这 4 个时间尺度上，我们选择月作为预测模型的时间尺度，相应的，我们选择 Weighted_Price 这个字段的数值作为观察结果，在原始数据中，Weighted_Price 对应的是比特币每天的平均价格，当我们以“月”为单位进行压缩的时候，对应的 Weighted_Price 得到的就是当月的比特币平均价格。压缩代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_month = df.resample(<span class="string">'M'</span>).mean()</span><br></pre></td></tr></table></figure><p>最后在预测阶段创建 ARMA 时间序列模型。我们并不知道 p 和 q 取什么值时，模型最优，因此我们可以给它们设置一个区间范围，比如都是 range(0,3)，然后计算不同模型的 AIC 数值，选择最小的 AIC 数值对应的那个 ARMA 模型。最后用这个最优的 ARMA 模型预测未来 8 个月的比特币平均价格走势，并将结果做可视化呈现。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 比特币走势预测，使用时间序列ARMA</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> statsmodels.tsa.arima_model <span class="keyword">import</span> ARMA</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">df = pd.read_csv(<span class="string">'./bitcoin_2012-01-01_to_2018-10-31.csv'</span>)</span><br><span class="line"><span class="comment"># 将时间作为df的索引</span></span><br><span class="line">df.Timestamp = pd.to_datetime(df.Timestamp)</span><br><span class="line">df.index = df.Timestamp</span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 按照月，季度，年来统计</span></span><br><span class="line">df_month = df.resample(<span class="string">'M'</span>).mean()</span><br><span class="line">df_Q = df.resample(<span class="string">'Q-DEC'</span>).mean()</span><br><span class="line">df_year = df.resample(<span class="string">'A-DEC'</span>).mean()</span><br><span class="line"><span class="comment"># 按照天，月，季度，年来显示比特币的走势</span></span><br><span class="line">fig = plt.figure(figsize=[<span class="number">15</span>, <span class="number">7</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.suptitle(<span class="string">'比特币金额（美金）'</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(df.Weighted_Price, <span class="string">'-'</span>, label=<span class="string">'按天'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(df_month.Weighted_Price, <span class="string">'-'</span>, label=<span class="string">'按月'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(df_Q.Weighted_Price, <span class="string">'-'</span>, label=<span class="string">'按季度'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(df_year.Weighted_Price, <span class="string">'-'</span>, label=<span class="string">'按年'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 设置参数范围</span></span><br><span class="line">ps = range(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">qs = range(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">parameters = product(ps, qs)</span><br><span class="line">parameters_list = list(parameters)</span><br><span class="line"><span class="comment"># 寻找最优ARMA模型参数，即best_aic最小</span></span><br><span class="line">results = []</span><br><span class="line">best_aic = float(<span class="string">"inf"</span>) <span class="comment"># 正无穷</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> parameters_list:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        model = ARMA(df_month.Weighted_Price,order=(param[<span class="number">0</span>], param[<span class="number">1</span>])).fit()</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        print(<span class="string">'参数错误:'</span>, param)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    aic = model.aic</span><br><span class="line">    <span class="keyword">if</span> aic &lt; best_aic:</span><br><span class="line">        best_model = model</span><br><span class="line">        best_aic = aic</span><br><span class="line">        best_param = param</span><br><span class="line">    results.append([param, model.aic])</span><br><span class="line"><span class="comment"># 输出最优模型</span></span><br><span class="line">result_table = pd.DataFrame(results)</span><br><span class="line">result_table.columns = [<span class="string">'parameters'</span>, <span class="string">'aic'</span>]</span><br><span class="line">print(<span class="string">'最优模型: '</span>, best_model.summary())</span><br><span class="line"><span class="comment"># 比特币预测</span></span><br><span class="line">df_month2 = df_month[[<span class="string">'Weighted_Price'</span>]]</span><br><span class="line">date_list = [datetime(<span class="number">2018</span>, <span class="number">11</span>, <span class="number">30</span>), datetime(<span class="number">2018</span>, <span class="number">12</span>, <span class="number">31</span>), datetime(<span class="number">2019</span>, <span class="number">1</span>, <span class="number">31</span>), datetime(<span class="number">2019</span>, <span class="number">2</span>, <span class="number">28</span>), datetime(<span class="number">2019</span>, <span class="number">3</span>, <span class="number">31</span>), </span><br><span class="line">             datetime(<span class="number">2019</span>, <span class="number">4</span>, <span class="number">30</span>), datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">31</span>), datetime(<span class="number">2019</span>, <span class="number">6</span>, <span class="number">30</span>)]</span><br><span class="line">future = pd.DataFrame(index=date_list, columns= df_month.columns)</span><br><span class="line">df_month2 = pd.concat([df_month2, future])</span><br><span class="line">df_month2[<span class="string">'forecast'</span>] = best_model.predict(start=<span class="number">0</span>, end=<span class="number">91</span>)</span><br><span class="line"><span class="comment"># 比特币预测结果显示</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">7</span>))</span><br><span class="line">df_month2.Weighted_Price.plot(label=<span class="string">'实际金额'</span>)</span><br><span class="line">df_month2.forecast.plot(color=<span class="string">'r'</span>, ls=<span class="string">'--'</span>, label=<span class="string">'预测金额'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'比特币金额（月）'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'时间'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'美金'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587568120533-48e72652-d6c8-48d7-89b5-e85444e01f0b.png#align=left&display=inline&height=387&margin=%5Bobject%20Object%5D&name=image.png&originHeight=896&originWidth=1726&size=185308&status=done&style=none&width=746" alt="image.png">)<img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587568146325-8a757836-53be-48dc-bd78-db7c3ac4b303.png#align=left&display=inline&height=661&margin=%5Bobject%20Object%5D&name=image.png&originHeight=661&originWidth=1729&size=105845&status=done&style=none&width=1729" alt="image.png">)<img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587568269473-039c43af-0ead-4c73-934f-240ea39180db.png#align=left&display=inline&height=932&margin=%5Bobject%20Object%5D&name=image.png&originHeight=932&originWidth=1729&size=516959&status=done&style=none&width=1729" alt="image.png"><br><br><br>我们通过 product 函数创建了 (p,q) 在 range(0,3) 范围内的所有可能组合，并对每个 ARMA(p,q) 模型进行了 AIC 数值计算，保存了 AIC 数值最小的模型参数。然后用这个模型对比特币的未来 8 个月进行了预测。<br><br><br>从结果中你能看到，在 2018 年 10 月之后 8 个月的时间里，比特币会触底到 4000 美金左右，实际上比特币在这个阶段确实降低到了 4000 元美金甚至更低。在时间尺度的选择上，我们选择了月，这样就对数据进行了降维，也节约了 ARMA 的模型训练时间。你能看到比特币金额（美金）这张图中，按月划分的比特币走势和按天划分的比特币走势差别不大，在减少了局部的波动的同时也能体现出比特币的趋势，这样就节约了 ARMA 的模型训练时间。<br></p><p><a name="3wkzY"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><br>今天我给你讲了一个比特币趋势预测的实战项目。通过这个项目你应该能体会到，当我们对一个数值进行预测的时候，如果考虑的是多个变量和结果之间的关系，可以采用回归分析，如果考虑单个时间维度与结果的关系，可以使用时间序列分析。<br><br><br>根据比特币的历史数据，我们使用 ARMA 模型对比特币未来 8 个月的走势进行了预测，并对结果进行了可视化显示。你能看到 ARMA 工具还是很好用的，虽然比特币的走势受很多外在因素影响，比如政策环境。不过当我们掌握了这些历史数据，也不妨用时间序列模型来分析预测一下。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;我们之前介绍了数据挖掘算法中的分类、聚类、回归和关联分析算法，那么对于比特币走势的预测，采用哪种方法比较好呢？&lt;br&gt;&lt;br&gt;&lt;br&gt;可能有些人
      
    
    </summary>
    
    
      <category term="机器学习" scheme="cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="时间序列" scheme="cpeixin.cn/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>信用卡诈骗分析</title>
    <link href="cpeixin.cn/2018/10/03/%E4%BF%A1%E7%94%A8%E5%8D%A1%E8%AF%88%E9%AA%97%E5%88%86%E6%9E%90/"/>
    <id>cpeixin.cn/2018/10/03/%E4%BF%A1%E7%94%A8%E5%8D%A1%E8%AF%88%E9%AA%97%E5%88%86%E6%9E%90/</id>
    <published>2018-10-02T16:43:58.000Z</published>
    <updated>2020-05-01T14:20:36.160Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --><p>上一篇文章中，我们用随机森林以及之前讲过的 SVM、决策树和 KNN 分类器对信用卡违约数据进行了分析，这节课我们来研究下信用卡欺诈。<br><br><br>相比于信用卡违约的比例，信用卡欺诈的比例更小，但是危害极大。如何通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷风险是我们这次项目的目标。<br><br><br>通过今天的学习，你需要掌握以下几个方面：</p><ul><li>了解逻辑回归分类，以及如何在 sklearn 中使用它；</li><li>信用卡欺诈属于二分类问题，欺诈交易在所有交易中的比例很小，对于这种数据不平衡的情况，到底采用什么样的模型评估标准会更准确；</li><li>完成信用卡欺诈分析的实战项目，并通过数据可视化对数据探索和模型结果评估进一步加强了解。<br><a name="wwZpI"></a><h3><a href="#" class="headerlink"></a></h3><a name="evkL5"></a><h3 id="构建逻辑回归分类器"><a href="#构建逻辑回归分类器" class="headerlink" title="构建逻辑回归分类器"></a>构建逻辑回归分类器</h3></li></ul><p><br>逻辑回归虽然不在我们讲解的十大经典数据挖掘算法里面，但也是常用的数据挖掘算法。逻辑回归，也叫作 logistic 回归。虽然名字中带有“回归”，但它实际上是分类方法，主要解决的是二分类问题，当然它也可以解决多分类问题，只是二分类更常见一些。<br><br><br>在逻辑回归中使用了 Logistic 函数，也称为 Sigmoid 函数。Sigmoid 函数是在深度学习中经常用到的函数之一，函数公式为：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586964265219-9919addd-92e8-42f4-8d14-fab4af3e5bb0.png#align=left&display=inline&height=204&margin=%5Bobject%20Object%5D&name=image.png&originHeight=204&originWidth=444&size=11021&status=done&style=none&width=444" alt="image.png"><br>函数的图形如下所示，类似 S 状：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586964264055-79ceda30-ba6d-492e-808c-de24a6c1299d.png#align=left&display=inline&height=206&margin=%5Bobject%20Object%5D&name=b7a5d39d91fda02b21669137a489743b.png&originHeight=206&originWidth=477&size=7332&status=done&style=none&width=477" alt="b7a5d39d91fda02b21669137a489743b.png"><br><br><br>你能看出 g(z) 的结果在 0-1 之间，当 z 越大的时候，g(z) 越大，当 z 趋近于无穷大的时候，g(z) 趋近于 1。同样当 z 趋近于无穷小的时候，g(z) 趋近于 0。同时，函数值以 0.5 为中心。<br><br><br>为什么逻辑回归算法是基于 Sigmoid 函数实现的呢？你可以这样理解：我们要实现一个二分类任务，0 即为不发生，1 即为发生。我们给定一些历史数据 X 和 y。其中 X 代表样本的 n 个特征，y 代表正例和负例，也就是 0 或 1 的取值。通过历史样本的学习，我们可以得到一个模型，当给定新的 X 的时候，可以预测出 y。这里我们得到的 y 是一个预测的概率，通常不是 0% 和 100%，而是中间的取值，那么我们就可以认为概率大于 50% 的时候，即为发生（正例），概率小于 50% 的时候，即为不发生（负例）。这样就完成了二分类的预测。<br><br><br>逻辑回归模型的求解这里不做介绍，我们来看下如何使用 sklearn 中的逻辑回归工具。在 sklearn 中，我们使用 LogisticRegression() 函数构建逻辑回归分类器，函数里有一些常用的构造参数：<br></p><ol><li>penalty：惩罚项，取值为 l1 或 l2，默认为 l2。当模型参数满足高斯分布的时候，使用 l2，当模型参数满足拉普拉斯分布的时候，使用 l1；</li><li>solver：代表的是逻辑回归损失函数的优化方法。有 5 个参数可选，分别为 liblinear、lbfgs、newton-cg、sag 和 saga。默认为 liblinear，适用于数据量小的数据集，当数据量大的时候可以选用 sag 或 saga 方法。</li><li>max_iter：算法收敛的最大迭代次数，默认为 10。</li><li>n_jobs：拟合和预测的时候 CPU 的核数，默认是 1，也可以是整数，如果是 -1 则代表 CPU 的核数。</li></ol><p><br>当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。<br><br><br></p><p><a name="gdmvF"></a></p><h3 id="模型评估指标"><a href="#模型评估指标" class="headerlink" title="模型评估指标"></a>模型评估指标</h3><p><br>我们之前对模型做评估时，通常采用的是准确率 (accuracy)，它指的是分类器正确分类的样本数与总体样本数之间的比例。这个指标对大部分的分类情况是有效的，不过当分类结果严重不平衡的时候，准确率很难反应模型的好坏。<br><br><br>举个例子，对于机场安检中恐怖分子的判断，就不能采用准确率对模型进行评估。我们知道恐怖分子的比例是极低的，因此当我们用准确率做判断时，如果准确率高达 99.999%，就说明这个模型一定好么？其实正因为现实生活中恐怖分子的比例极低，就算我们不能识别出一个恐怖分子，也会得到非常高的准确率。因为准确率的评判标准是正确分类的样本个数与总样本数之间的比例。因此非恐怖分子的比例会很高，就算我们识别不出来恐怖分子，正确分类的个数占总样本的比例也会很高，也就是准确率高。<br><br><br>实际上我们应该更关注恐怖分子的识别，这里先介绍下数据预测的四种情况：TP、FP、TN、FN。我们用第二个字母 P 或 N 代表预测为正例还是负例，P 为正，N 为负。第一个字母 T 或 F 代表的是预测结果是否正确，T 为正确，F 为错误。所以四种情况分别为：</p><ul><li>TP：预测为正，判断正确；</li><li>FP：预测为正，判断错误；</li><li>TN：预测为负，判断正确；</li><li>FN：预测为负，判断错误。</li></ul><p><br>我们知道样本总数 =TP+FP+TN+FN，预测正确的样本数为 TP+TN，因此准确率 Accuracy = (TP+TN)/(TP+TN+FN+FP)。<br><br><br>实际上，对于分类不平衡的情况，有两个指标非常重要，<strong>它们分别是精确度和召回率</strong>。<br><br><br>精确率 P = TP/ (TP+FP)，对应上面恐怖分子这个例子，在所有判断为恐怖分子的人数中，真正是恐怖分子的比例。<br><br><br>召回率 R = TP/ (TP+FN)，也称为查全率。代表的是恐怖分子被正确识别出来的个数与恐怖分子总数的比例。为什么要统计召回率和精确率这两个指标呢？假设我们只统计召回率，当召回率等于 100% 的时候，模型是否真的好呢？<br><br><br>举个例子，假设我们把机场所有的人都认为是恐怖分子，恐怖分子都会被正确识别，这个数字与恐怖分子的总数比例等于 100%，但是这个结果是没有意义的。如果我们认为机场里所有人都是恐怖分子的话，那么非恐怖分子（极高比例）都会认为是恐怖分子，误判率太高了，所以我们还需要统计精确率作为召回率的补充。实际上有一个指标综合了精确率和召回率，可以更好地评估模型的好坏。这个指标叫做 F1，用公式表示为：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586964834238-6a4ec2e1-306d-4057-9a97-7df5e831b85c.png#align=left&display=inline&height=182&margin=%5Bobject%20Object%5D&name=b122244eae9a74eded619d14c0bc12ce.png&originHeight=182&originWidth=398&size=11527&status=done&style=none&width=398" alt="b122244eae9a74eded619d14c0bc12ce.png"><br><br><br>F1 作为精确率 P 和召回率 R 的调和平均，数值越大代表模型的结果越好。<br><br><br></p><p><a name="jZ1Vb"></a></p><h3 id="信用卡欺诈分析"><a href="#信用卡欺诈分析" class="headerlink" title="信用卡欺诈分析"></a>信用卡欺诈分析</h3><p><br>我们来看一个信用卡欺诈分析的项目，这个数据集你可以从百度网盘（因为数据集大于 100M，所以采用了网盘）中下载：链接：<a href="https://pan.baidu.com/s/14F8WuX0ZJntdB_r1EC08HA" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/14F8WuX0ZJntdB_r1EC08HA</a> 提取码：58gp<br><br><br>数据集包括了 2013 年 9 月份两天时间内的信用卡交易数据，284807 笔交易中，一共有 492 笔是欺诈行为。输入数据一共包括了 28 个特征 V1，V2，……V28 对应的取值，以及交易时间 Time 和交易金额 Amount。为了保护数据隐私，我们不知道 V1 到 V28 这些特征代表的具体含义，只知道这 28 个特征值是通过 PCA 变换得到的结果。另外字段 Class 代表该笔交易的分类，Class=0 为正常（非欺诈），Class=1 代表欺诈。<br><br><br>我们的目标是针对这个数据集构建一个信用卡欺诈分析的分类器，采用的是逻辑回归。从数据中你能看到欺诈行为只占到了 492/284807=0.172%，数据分类结果的分布是非常不平衡的，因此我们不能使用准确率评估模型的好坏，而是需要统计 F1 值（综合精确率和召回率）。我们先梳理下整个项目的流程：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586965086920-af9f8f64-ed51-45a0-99bc-b5d7dbe3d7fb.jpeg#align=left&display=inline&height=1079&margin=%5Bobject%20Object%5D&name=image.jpeg&originHeight=1079&originWidth=2350&size=262963&status=done&style=none&width=2350" alt="image.jpeg"><br></p><ol><li>加载数据；</li></ol><ol start="2"><li>准备阶段：我们需要探索数据，用数据可视化的方式查看分类结果的情况，以及随着时间的变化，欺诈交易和正常交易的分布情况。上面已经提到过，V1-V28 的特征值都经过 PCA 的变换，但是其余的两个字段，Time 和 Amount 还需要进行规范化。Time 字段和交易本身是否为欺诈交易无关，因此我们不作为特征选择，只需要对 Amount 做数据规范化就行了。同时数据集没有专门的测试集，使用 train_test_split 对数据集进行划分；</li></ol><ol start="3"><li>分类阶段：我们需要创建逻辑回归分类器，然后传入训练集数据进行训练，并传入测试集预测结果，将预测结果与测试集的结果进行比对。这里的模型评估指标用到了<strong>精确率、召回率和 F1 值</strong>。同时我们将精确率 - 召回率进行了可视化呈现。</li></ol><p><br>基于上面的流程，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># 使用逻辑回归对信用卡欺诈进行分类</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, precision_recall_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 混淆矩阵可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(cm, classes, normalize = False, title = <span class="string">'Confusion matrix"'</span>, cmap = plt.cm.Blues)</span> :</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.imshow(cm, interpolation = <span class="string">'nearest'</span>, cmap = cmap)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    tick_marks = np.arange(len(classes))</span><br><span class="line">    plt.xticks(tick_marks, classes, rotation = <span class="number">0</span>)</span><br><span class="line">    plt.yticks(tick_marks, classes)</span><br><span class="line"> </span><br><span class="line">    thresh = cm.max() / <span class="number">2.</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(range(cm.shape[<span class="number">0</span>]), range(cm.shape[<span class="number">1</span>])) :</span><br><span class="line">        plt.text(j, i, cm[i, j],</span><br><span class="line">                 horizontalalignment = <span class="string">'center'</span>,</span><br><span class="line">                 color = <span class="string">'white'</span> <span class="keyword">if</span> cm[i, j] &gt; thresh <span class="keyword">else</span> <span class="string">'black'</span>)</span><br><span class="line"> </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.ylabel(<span class="string">'True label'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Predicted label'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 显示模型评估结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_metrics</span><span class="params">()</span>:</span></span><br><span class="line">    tp = cm[<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">    fn = cm[<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">    fp = cm[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">    tn = cm[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'精确率: &#123;:.3f&#125;'</span>.format(tp/(tp+fp)))</span><br><span class="line">    print(<span class="string">'召回率: &#123;:.3f&#125;'</span>.format(tp/(tp+fn)))</span><br><span class="line">    print(<span class="string">'F1值: &#123;:.3f&#125;'</span>.format(<span class="number">2</span>*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))))</span><br><span class="line"><span class="comment"># 绘制精确率-召回率曲线</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall</span><span class="params">()</span>:</span></span><br><span class="line">    plt.step(recall, precision, color = <span class="string">'b'</span>, alpha = <span class="number">0.2</span>, where = <span class="string">'post'</span>)</span><br><span class="line">    plt.fill_between(recall, precision, step =<span class="string">'post'</span>, alpha = <span class="number">0.2</span>, color = <span class="string">'b'</span>)</span><br><span class="line">    plt.plot(recall, precision, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlim([<span class="number">0.0</span>,<span class="number">1</span>])</span><br><span class="line">    plt.ylim([<span class="number">0.0</span>,<span class="number">1.05</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'召回率'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'精确率'</span>)</span><br><span class="line">    plt.title(<span class="string">'精确率-召回率 曲线'</span>)</span><br><span class="line">    plt.show();</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./creditcard.csv'</span>)</span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(data.describe())</span><br><span class="line"><span class="comment"># 设置plt正确显示中文</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line"><span class="comment"># 绘制类别分布</span></span><br><span class="line">plt.figure()</span><br><span class="line">ax = sns.countplot(x = <span class="string">'Class'</span>, data = data)</span><br><span class="line">plt.title(<span class="string">'类别分布'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 显示交易笔数，欺诈交易笔数</span></span><br><span class="line">num = len(data)</span><br><span class="line">num_fraud = len(data[data[<span class="string">'Class'</span>]==<span class="number">1</span>]) </span><br><span class="line">print(<span class="string">'总交易笔数: '</span>, num)</span><br><span class="line">print(<span class="string">'诈骗交易笔数：'</span>, num_fraud)</span><br><span class="line">print(<span class="string">'诈骗交易比例：&#123;:.6f&#125;'</span>.format(num_fraud/num))</span><br><span class="line"><span class="comment"># 欺诈和正常交易可视化</span></span><br><span class="line">f, (ax1, ax2) = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, sharex=<span class="literal">True</span>, figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line">bins = <span class="number">50</span></span><br><span class="line">ax1.hist(data.Time[data.Class == <span class="number">1</span>], bins = bins, color = <span class="string">'deeppink'</span>)</span><br><span class="line">ax1.set_title(<span class="string">'诈骗交易'</span>)</span><br><span class="line">ax2.hist(data.Time[data.Class == <span class="number">0</span>], bins = bins, color = <span class="string">'deepskyblue'</span>)</span><br><span class="line">ax2.set_title(<span class="string">'正常交易'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'时间'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'交易次数'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 对Amount进行数据规范化</span></span><br><span class="line">data[<span class="string">'Amount_Norm'</span>] = StandardScaler().fit_transform(data[<span class="string">'Amount'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">y = np.array(data.Class.tolist())</span><br><span class="line">data = data.drop([<span class="string">'Time'</span>,<span class="string">'Amount'</span>,<span class="string">'Class'</span>],axis=<span class="number">1</span>)</span><br><span class="line">X = np.array(data.as_matrix())</span><br><span class="line"><span class="comment"># 准备训练集和测试集</span></span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split (X, y, test_size = <span class="number">0.1</span>, random_state = <span class="number">33</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 逻辑回归分类</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line">clf.fit(train_x, train_y)</span><br><span class="line">predict_y = clf.predict(test_x)</span><br><span class="line"><span class="comment"># 预测样本的置信分数</span></span><br><span class="line">score_y = clf.decision_function(test_x)  </span><br><span class="line"><span class="comment"># 计算混淆矩阵，并显示</span></span><br><span class="line">cm = confusion_matrix(test_y, predict_y)</span><br><span class="line">class_names = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 显示混淆矩阵</span></span><br><span class="line">plot_confusion_matrix(cm, classes = class_names, title = <span class="string">'逻辑回归 混淆矩阵'</span>)</span><br><span class="line"><span class="comment"># 显示模型评估分数</span></span><br><span class="line">show_metrics()</span><br><span class="line"><span class="comment"># 计算精确率，召回率，阈值用于可视化</span></span><br><span class="line">precision, recall, thresholds = precision_recall_curve(test_y, score_y)</span><br><span class="line">plot_precision_recall()</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                Time            V1      ...               Amount          Class</span><br><span class="line">count  <span class="number">284807.000000</span>  <span class="number">2.848070e+05</span>      ...        <span class="number">284807.000000</span>  <span class="number">284807.000000</span></span><br><span class="line">mean    <span class="number">94813.859575</span>  <span class="number">1.165980e-15</span>      ...            <span class="number">88.349619</span>       <span class="number">0.001727</span></span><br><span class="line">std     <span class="number">47488.145955</span>  <span class="number">1.958696e+00</span>      ...           <span class="number">250.120109</span>       <span class="number">0.041527</span></span><br><span class="line">min         <span class="number">0.000000</span> <span class="number">-5.640751e+01</span>      ...             <span class="number">0.000000</span>       <span class="number">0.000000</span></span><br><span class="line"><span class="number">25</span>%     <span class="number">54201.500000</span> <span class="number">-9.203734e-01</span>      ...             <span class="number">5.600000</span>       <span class="number">0.000000</span></span><br><span class="line"><span class="number">50</span>%     <span class="number">84692.000000</span>  <span class="number">1.810880e-02</span>      ...            <span class="number">22.000000</span>       <span class="number">0.000000</span></span><br><span class="line"><span class="number">75</span>%    <span class="number">139320.500000</span>  <span class="number">1.315642e+00</span>      ...            <span class="number">77.165000</span>       <span class="number">0.000000</span></span><br><span class="line">max    <span class="number">172792.000000</span>  <span class="number">2.454930e+00</span>      ...         <span class="number">25691.160000</span>       <span class="number">1.000000</span></span><br><span class="line"></span><br><span class="line">[<span class="number">8</span> rows x <span class="number">31</span> columns]</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586967850283-e9af3698-331c-4e3e-ae8f-d490803e2811.png#align=left&display=inline&height=1363&margin=%5Bobject%20Object%5D&name=5e98974d6c2e87168b40e5f751d00f61.png&originHeight=1363&originWidth=1728&size=89241&status=done&style=none&width=1728" alt="5e98974d6c2e87168b40e5f751d00f61.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">总交易笔数:  284807</span><br><span class="line">诈骗交易笔数： 492</span><br><span class="line">诈骗交易比例：0.001727</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586967935125-87c5538f-27a4-44d1-8a27-c67cb6cbde4d.png#align=left&display=inline&height=1458&margin=%5Bobject%20Object%5D&name=bfe65c34b74de661477d9b59d4db6a39.png&originHeight=1458&originWidth=1729&size=121793&status=done&style=none&width=1729" alt="bfe65c34b74de661477d9b59d4db6a39.png">)<img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586967952349-d6855347-0edd-4caa-819e-7c98e253769b.png#align=left&display=inline&height=1022&margin=%5Bobject%20Object%5D&name=c8a59cb4f3d94c91eb6648be1b0429d2.png&originHeight=1022&originWidth=1726&size=88131&status=done&style=none&width=1726" alt="c8a59cb4f3d94c91eb6648be1b0429d2.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">精确率: 0.841</span><br><span class="line">召回率: 0.617</span><br><span class="line">F1值: 0.712</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586967968745-3cf776d3-884d-4af6-a270-b33cefc550c9.png#align=left&display=inline&height=446&margin=%5Bobject%20Object%5D&name=28ccd0f8d609046b2bafb27fb1195269.png&originHeight=446&originWidth=567&size=16903&status=done&style=none&width=567" alt="28ccd0f8d609046b2bafb27fb1195269.png"><br>你能看出来欺诈交易的笔数为 492 笔，占所有交易的比例是很低的，即 0.001727，我们可以通过数据可视化的方式对欺诈交易和正常交易的分布进行呈现。另外通过可视化，我们也能看出精确率和召回率之间的关系，当精确率高的时候，召回率往往很低，召回率高的时候，精确率会比较低。<br><br><br>代码有一些模块需要说明下。我定义了 plot_confusion_matrix 函数对混淆矩阵进行可视化。什么是混淆矩阵呢？混淆矩阵也叫误差矩阵，实际上它就是 TP、FP、TN、FN 这四个数值的矩阵表示，帮助我们判断预测值和实际值相比，对了多少。从这个例子中，你能看出 TP=37，FP=7，FN=23。所以精确率 P=TP/(TP+FP)=37/(37+7)=0.841，召回率 R=TP/(TP+FN)=37/(37+23)=0.617。然后使用了 sklearn 中的 precision_recall_curve 函数，通过预测值和真实值来计算精确率 - 召回率曲线。<br><br><br>precision_recall_curve 函数会计算在不同概率阈值情况下的精确率和召回率。最后定义 plot_precision_recall 函数，绘制曲线。<br></p><p><a name="ig5Sr"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><br>今天我给你讲了逻辑回归的概念和相关工具的使用，另外学习了在数据样本不平衡的情况下，如何评估模型。这里你需要了解精确率，召回率和 F1 的概念和计算方式。<br><br><br>最后在信用卡欺诈分析的项目中，我们使用了逻辑回归工具，并对混淆矩阵进行了计算，同时在模型结果评估中，使用了精确率、召回率和 F1 值，最后得到精确率 - 召回率曲线的可视化结果。从这个项目中你能看出来，不是所有的分类都是样本平衡的情况，针对正例比例极低的情况，比如信用卡欺诈、某些疾病的识别，或者是恐怖分子的判断等，都需要采用精确率 - 召回率来进行统计。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586968908701-9553e7eb-6051-4c47-86c4-3af7e07d490d.png#align=left&display=inline&height=356&margin=%5Bobject%20Object%5D&name=abee1a58b99814f1e0218778b98a6950.png&originHeight=824&originWidth=1729&size=318818&status=done&style=none&width=746" alt="abee1a58b99814f1e0218778b98a6950.png"><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;上一篇文章中，我们用随机森林以及之前讲过的 SVM、决策树和 KNN 分类器对信用卡违约数据进行了分析，这节课我们来研究下信用卡欺诈。&lt;br&gt;&lt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="逻辑回归" scheme="cpeixin.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>信用卡风险评估</title>
    <link href="cpeixin.cn/2018/10/01/%E4%BF%A1%E7%94%A8%E5%8D%A1%E9%A3%8E%E9%99%A9%E8%AF%84%E4%BC%B0/"/>
    <id>cpeixin.cn/2018/10/01/%E4%BF%A1%E7%94%A8%E5%8D%A1%E9%A3%8E%E9%99%A9%E8%AF%84%E4%BC%B0/</id>
    <published>2018-09-30T17:21:02.000Z</published>
    <updated>2020-05-01T14:20:47.150Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --><p>今天我来带你做一个数据挖掘的项目。在数据挖掘的过程中，我们经常会遇到一些问题，比如：如何选择各种分类器，到底选择哪个分类算法，是 SVM，决策树，还是 KNN？如何优化分类器的参数，以便得到更好的分类准确率？这两个问题，是数据挖掘核心的问题。<br><br><br>当然对于一个新的项目，我们还有其他的问题需要了解，比如掌握数据探索和数据可视化的方式，还需要对数据的完整性和质量做评估。这些内容我在之前的课程中都有讲到过。今天的学习主要围绕下面的三个目标，并通过它们完成信用卡违约率项目的实战<br><br><br>这三个目标分别是：</p><ul><li>创建各种分类器，包括已经掌握的 SVM、决策树、KNN 分类器，以及随机森林分类器；</li><li>掌握 GridSearchCV 工具，优化算法模型的参数；</li><li>使用 Pipeline 管道机制进行流水线作业。因为在做分类之前，我们还需要一些准备过程，比如数据规范化，或者数据降维等。</li></ul><p><a name="qVKos"></a></p><h3 id="构建随机森林分类器"><a href="#构建随机森林分类器" class="headerlink" title="构建随机森林分类器"></a>构建随机森林分类器</h3><p><br>在算法篇中，我主要讲了数据挖掘十大经典算法。实际工作中，你也可能会用到随机森林。随机森林的英文是 Random Forest，英文简写是 RF。它实际上是一个包含多个决策树的分类器，每一个子分类器都是一棵 CART 分类回归树。所以随机森林既可以做分类，又可以做回归。<br><br><br>当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。当它做回归的时候，输出结果是每棵 CART 树的回归结果的平均值。在 sklearn 中，我们使用 RandomForestClassifier() 构造随机森林模型，函数里有一些常用的构造参数：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586881198201-8f4e3d3f-b0d2-4657-ba91-0693591eab3b.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=352035fe3e92d412d652fd55c77f23f9.png&originHeight=210&originWidth=629&size=52416&status=done&style=none&width=629" alt="352035fe3e92d412d652fd55c77f23f9.png"><br>当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。<br></p><p><a name="nOoSM"></a></p><h3 id="使用-GridSearchCV-工具对模型参数进行调优"><a href="#使用-GridSearchCV-工具对模型参数进行调优" class="headerlink" title="使用 GridSearchCV 工具对模型参数进行调优"></a>使用 GridSearchCV 工具对模型参数进行调优</h3><p><br>在做分类算法的时候，我们需要经常调节网络参数（对应上面的构造参数），目的是得到更好的分类结果。实际上一个分类算法有很多参数，取值范围也比较广，那么该如何调优呢？<br><br><br>Python 给我们提供了一个很好用的工具 GridSearchCV，它是 Python 的参数自动搜索模块。<strong>我们只要告诉它想要调优的参数有哪些以及参数的取值范围，它就会把所有的情况都跑一遍，然后告诉我们哪个参数是最优的，结果如何。</strong><br><br><br>使用 GridSearchCV 模块需要先引用工具包，方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br></pre></td></tr></table></figure><p><br>然后我们使用 GridSearchCV(estimator, param_grid, cv=None, scoring=None) 构造参数的自动搜索模块，这里有一些主要的参数需要说明下：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586882395328-f0d75ba6-0707-4045-b367-5e902287ad6d.png#align=left&display=inline&height=183&margin=%5Bobject%20Object%5D&name=image.png&originHeight=183&originWidth=630&size=51879&status=done&style=none&width=630" alt="image.png"><br><br><br>构造完 GridSearchCV 之后，我们就可以使用 fit 函数拟合训练，使用 predict 函数预测，这时预测采用的是最优参数情况下的分类器。<br><br><br>这里举一个简单的例子，我们用 sklearn 自带的 IRIS 数据集，采用随机森林对 IRIS 数据分类。假设我们想知道 n_estimators 在 1-10 的范围内取哪个值的分类结果最好，可以编写代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 使用RandomForest对IRIS数据集进行分类</span></span><br><span class="line"><span class="comment"># 利用GridSearchCV寻找最优参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">rf = RandomForestClassifier()</span><br><span class="line">parameters = &#123;<span class="string">"n_estimators"</span>: range(<span class="number">1</span>,<span class="number">11</span>)&#125;</span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="comment"># 使用GridSearchCV进行参数调优</span></span><br><span class="line">clf = GridSearchCV(estimator=rf, param_grid=parameters)</span><br><span class="line"><span class="comment"># 对iris数据集进行分类</span></span><br><span class="line">clf.fit(iris.data, iris.target)</span><br><span class="line">print(<span class="string">"最优分数： %.4lf"</span> %clf.best_score_)</span><br><span class="line">print(<span class="string">"最优参数："</span>, clf.best_params_)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行结果如下：</span><br><span class="line">最优分数： <span class="number">0.9667</span></span><br><span class="line">最优参数： &#123;<span class="string">'n_estimators'</span>: <span class="number">6</span>&#125;</span><br></pre></td></tr></table></figure><p><br>你能看到当我们采用随机森林作为分类器的时候，最优准确率是 0.9667，当 n_estimators=6 的时候，是最优参数，也就是随机森林一共有 6 个子决策树。<br></p><p><a name="nK5Sl"></a></p><h3 id="使用-Pipeline-管道机制进行流水线作业"><a href="#使用-Pipeline-管道机制进行流水线作业" class="headerlink" title="使用 Pipeline 管道机制进行流水线作业"></a>使用 Pipeline 管道机制进行流水线作业</h3><p><br>做分类的时候往往都是有步骤的，比如先对数据进行规范化处理，你也可以用 PCA 方法（一种常用的降维方法）对数据降维，最后使用分类器分类。Python 有一种 Pipeline 管道机制。管道机制就是让我们把每一步都按顺序列下来，从而创建 Pipeline 流水线作业。每一步都采用 (‘名称’, 步骤) 的方式来表示。我们需要先采用 StandardScaler 方法对数据规范化，即采用数据规范化为均值为 0，方差为 1 的正态分布，然后采用 PCA 方法对数据进行降维，最后采用随机森林进行分类。具体代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">pipeline = Pipeline([</span><br><span class="line">        (<span class="string">'scaler'</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">'pca'</span>, PCA()),</span><br><span class="line">        (<span class="string">'randomforestclassifier'</span>, RandomForestClassifier())</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>那么我们现在采用 Pipeline 管道机制，用随机森林对 IRIS 数据集做一下分类。先用 StandardScaler 方法对数据规范化，然后再用随机森林分类，编写代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 使用RandomForest对IRIS数据集进行分类</span></span><br><span class="line"><span class="comment"># 利用GridSearchCV寻找最优参数,使用Pipeline进行流水作业</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">rf = RandomForestClassifier()</span><br><span class="line">parameters = &#123;<span class="string">"randomforestclassifier__n_estimators"</span>: range(<span class="number">1</span>,<span class="number">11</span>)&#125;</span><br><span class="line">iris = load_iris()</span><br><span class="line">pipeline = Pipeline([</span><br><span class="line">        (<span class="string">'scaler'</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">'randomforestclassifier'</span>, rf)</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 使用GridSearchCV进行参数调优</span></span><br><span class="line">clf = GridSearchCV(estimator=pipeline, param_grid=parameters)</span><br><span class="line"><span class="comment"># 对iris数据集进行分类</span></span><br><span class="line">clf.fit(iris.data, iris.target)</span><br><span class="line">print(<span class="string">"最优分数： %.4lf"</span> %clf.best_score_)</span><br><span class="line">print(<span class="string">"最优参数："</span>, clf.best_params_)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行结果：</span><br><span class="line">最优分数： <span class="number">0.9667</span></span><br><span class="line">最优参数： &#123;<span class="string">'randomforestclassifier__n_estimators'</span>: <span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure><p><br>你能看到是否采用数据规范化对结果还是有一些影响的，有了 GridSearchCV 和 Pipeline 这两个工具之后，我们在使用分类器的时候就会方便很多。<br></p><p><a name="3TqBP"></a></p><h3 id="对信用卡违约率进行分析"><a href="#对信用卡违约率进行分析" class="headerlink" title="对信用卡违约率进行分析"></a>对信用卡违约率进行分析</h3><p><br>我们现在来做一个信用卡违约率的项目，这个数据集你可以从 GitHub 上下载：<a href="https://github.com/cystanford/credit_default" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/cystanford/credit_default</a>。<br><br><br>这个数据集是台湾某银行 2005 年 4 月到 9 月的信用卡数据，数据集一共包括 25 个字段，具体含义如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586883521178-d88ca06b-f952-481a-8f92-39bf0d5059a8.jpeg#align=left&display=inline&height=843&margin=%5Bobject%20Object%5D&name=1730fb3a809c99950739e7f50e1a6988.jpg&originHeight=843&originWidth=627&size=231191&status=done&style=none&width=627" alt="1730fb3a809c99950739e7f50e1a6988.jpg"><br>现在我们的目标是要针对这个数据集构建一个分析信用卡违约率的分类器。具体选择哪个分类器，以及分类器的参数如何优化，我们可以用 GridSearchCV 这个工具跑一遍。先梳理下整个项目的流程：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586883546533-c7be28a5-fefc-4c95-a7f5-1b34d8dbc07e.jpeg#align=left&display=inline&height=1079&margin=%5Bobject%20Object%5D&name=929c96584cbc25972f63ef39101c96a5.jpg&originHeight=1079&originWidth=2350&size=262963&status=done&style=none&width=2350" alt="929c96584cbc25972f63ef39101c96a5.jpg"><br></p><ol><li>加载数据；</li><li>准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更直观的了解，比如我们想要了解信用卡违约率和不违约率的人数。因为数据集没有专门的测试集，我们还需要使用 train_test_split 划分数据集。</li><li>分类阶段：之所以把数据规范化放到这个阶段，是因为我们可以使用 Pipeline 管道机制，将数据规范化设置为第一步，分类为第二步。因为我们不知道采用哪个分类器效果好，所以我们需要多用几个分类器，比如 SVM、决策树、随机森林和 KNN。然后通过 GridSearchCV 工具，找到每个分类器的最优参数和最优分数，最终找到最适合这个项目的分类器和该分类器的参数。</li></ol><p><br>基于上面的流程，具体代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造各种分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">classifiers = [</span><br><span class="line">    SVC(random_state = <span class="number">1</span>, kernel = <span class="string">'rbf'</span>),</span><br><span class="line">    DecisionTreeClassifier(random_state = <span class="number">1</span>, criterion = <span class="string">'gini'</span>),</span><br><span class="line">    RandomForestClassifier(random_state = <span class="number">1</span>, criterion = <span class="string">'gini'</span>),</span><br><span class="line">    KNeighborsClassifier(metric = <span class="string">'minkowski'</span>),</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 分类器名称</span></span><br><span class="line">classifier_names = [</span><br><span class="line">            <span class="string">'svc'</span>,</span><br><span class="line">            <span class="string">'decisiontreeclassifier'</span>,</span><br><span class="line">            <span class="string">'randomforestclassifier'</span>,</span><br><span class="line">            <span class="string">'kneighborsclassifier'</span>,</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 分类器参数</span></span><br><span class="line">classifier_param_grid = [</span><br><span class="line">            &#123;<span class="string">'svc__C'</span>:[<span class="number">1</span>], <span class="string">'svc__gamma'</span>:[<span class="number">0.01</span>]&#125;,</span><br><span class="line">            &#123;<span class="string">'decisiontreeclassifier__max_depth'</span>:[<span class="number">6</span>,<span class="number">9</span>,<span class="number">11</span>]&#125;,</span><br><span class="line">            &#123;<span class="string">'randomforestclassifier__n_estimators'</span>:[<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>]&#125; ,</span><br><span class="line">            &#123;<span class="string">'kneighborsclassifier__n_neighbors'</span>:[<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>]&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对具体的分类器进行GridSearchCV参数调优</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GridSearchCV_work</span><span class="params">(pipeline, train_x, train_y, test_x, test_y, param_grid, score = <span class="string">'accuracy'</span>)</span>:</span></span><br><span class="line">    response = &#123;&#125;</span><br><span class="line">    gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score)</span><br><span class="line">    <span class="comment"># 寻找最优的参数 和最优的准确率分数</span></span><br><span class="line">    search = gridsearch.fit(train_x, train_y)</span><br><span class="line">    print(<span class="string">"GridSearch最优参数："</span>, search.best_params_)</span><br><span class="line">    print(<span class="string">"GridSearch最优分数： %0.4lf"</span> %search.best_score_)</span><br><span class="line">    predict_y = gridsearch.predict(test_x)</span><br><span class="line">    print(<span class="string">"准确率 %0.4lf"</span> %accuracy_score(test_y, predict_y))</span><br><span class="line">    response[<span class="string">'predict_y'</span>] = predict_y</span><br><span class="line">    response[<span class="string">'accuracy_score'</span>] = accuracy_score(test_y,predict_y)</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    data = pd.read_csv(<span class="string">'UCI_Credit_Card.csv'</span>)</span><br><span class="line">    <span class="comment"># 数据条数和字段数量</span></span><br><span class="line">    print(data.shape)</span><br><span class="line">    <span class="comment"># 数据探索</span></span><br><span class="line">    print(data.describe)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># 查看下一个月的违约率</span></span><br><span class="line">    next_month = data[<span class="string">'default.payment.next.month'</span>].value_counts()</span><br><span class="line"></span><br><span class="line">    print(next_month)</span><br><span class="line">    <span class="comment"># 统计违约率结果</span></span><br><span class="line">    df = pd.DataFrame(&#123;<span class="string">'default.payment.next.month'</span>: next_month.index, <span class="string">'values'</span>: next_month.values&#125;)</span><br><span class="line">    print(df)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正常显示中文标签</span></span><br><span class="line">    <span class="comment"># plt.rcParams['font.sans-serif'] = ['FangSong']  # 用来正常显示中文标签</span></span><br><span class="line">    plt.rcParams[<span class="string">"font.family"</span>] = <span class="string">'Arial Unicode MS'</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'信用卡违约率客户 (违约：1，守约：0)'</span>)</span><br><span class="line">    sns.set_color_codes(<span class="string">'pastel'</span>)</span><br><span class="line">    sns.barplot(x=<span class="string">'default.payment.next.month'</span>, y=<span class="string">"values"</span>, data=df)</span><br><span class="line">    locs, labels = plt.xticks()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># 特征选择，去掉ID字段、最后一个结果字段即可</span></span><br><span class="line">    data.drop([<span class="string">'ID'</span>], inplace=<span class="literal">True</span>, axis=<span class="number">1</span>)  <span class="comment"># ID这个字段没有用</span></span><br><span class="line">    target = data[<span class="string">'default.payment.next.month'</span>].values</span><br><span class="line">    columns = data.columns.tolist()</span><br><span class="line">    columns.remove(<span class="string">'default.payment.next.month'</span>)</span><br><span class="line">    features = data[columns].values</span><br><span class="line">    <span class="keyword">return</span> features, target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data = get_data()</span><br><span class="line">    show_data(data)</span><br><span class="line">    features, target = get_feature(data)</span><br><span class="line">    <span class="comment"># 30%作为测试集，其余作为训练集</span></span><br><span class="line">    train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=<span class="number">0.30</span>, stratify = target, random_state = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> model, model_name, model_param_grid <span class="keyword">in</span> zip(classifiers, classifier_names, classifier_param_grid):</span><br><span class="line">        pipeline = Pipeline([</span><br><span class="line">            (<span class="string">'scaler'</span>, StandardScaler()),</span><br><span class="line">            (model_name, model)</span><br><span class="line">        ])</span><br><span class="line">        result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid, score=<span class="string">'accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">30000</span>, <span class="number">25</span>)</span><br><span class="line">&lt;bound method NDFrame.describe of           ID  LIMIT_BAL  SEX  ...  PAY_AMT5  PAY_AMT6  default.payment.next.month</span><br><span class="line"><span class="number">0</span>          <span class="number">1</span>    <span class="number">20000.0</span>    <span class="number">2</span>  ...       <span class="number">0.0</span>       <span class="number">0.0</span>                           <span class="number">1</span></span><br><span class="line"><span class="number">1</span>          <span class="number">2</span>   <span class="number">120000.0</span>    <span class="number">2</span>  ...       <span class="number">0.0</span>    <span class="number">2000.0</span>                           <span class="number">1</span></span><br><span class="line"><span class="number">2</span>          <span class="number">3</span>    <span class="number">90000.0</span>    <span class="number">2</span>  ...    <span class="number">1000.0</span>    <span class="number">5000.0</span>                           <span class="number">0</span></span><br><span class="line"><span class="number">3</span>          <span class="number">4</span>    <span class="number">50000.0</span>    <span class="number">2</span>  ...    <span class="number">1069.0</span>    <span class="number">1000.0</span>                           <span class="number">0</span></span><br><span class="line"><span class="number">4</span>          <span class="number">5</span>    <span class="number">50000.0</span>    <span class="number">1</span>  ...     <span class="number">689.0</span>     <span class="number">679.0</span>                           <span class="number">0</span></span><br><span class="line">      ...        ...  ...  ...       ...       ...                         ...</span><br><span class="line"><span class="number">29995</span>  <span class="number">29996</span>   <span class="number">220000.0</span>    <span class="number">1</span>  ...    <span class="number">5000.0</span>    <span class="number">1000.0</span>                           <span class="number">0</span></span><br><span class="line"><span class="number">29996</span>  <span class="number">29997</span>   <span class="number">150000.0</span>    <span class="number">1</span>  ...       <span class="number">0.0</span>       <span class="number">0.0</span>                           <span class="number">0</span></span><br><span class="line"><span class="number">29997</span>  <span class="number">29998</span>    <span class="number">30000.0</span>    <span class="number">1</span>  ...    <span class="number">2000.0</span>    <span class="number">3100.0</span>                           <span class="number">1</span></span><br><span class="line"><span class="number">29998</span>  <span class="number">29999</span>    <span class="number">80000.0</span>    <span class="number">1</span>  ...   <span class="number">52964.0</span>    <span class="number">1804.0</span>                           <span class="number">1</span></span><br><span class="line"><span class="number">29999</span>  <span class="number">30000</span>    <span class="number">50000.0</span>    <span class="number">1</span>  ...    <span class="number">1000.0</span>    <span class="number">1000.0</span>                           <span class="number">1</span></span><br><span class="line">[<span class="number">30000</span> rows x <span class="number">25</span> columns]&gt;</span><br><span class="line"><span class="number">0</span>    <span class="number">23364</span></span><br><span class="line"><span class="number">1</span>     <span class="number">6636</span></span><br><span class="line">Name: default.payment.next.month, dtype: int64</span><br><span class="line">   default.payment.next.month  values</span><br><span class="line"><span class="number">0</span>                           <span class="number">0</span>   <span class="number">23364</span></span><br><span class="line"><span class="number">1</span>                           <span class="number">1</span>    <span class="number">6636</span></span><br><span class="line">GridSearch最优参数： &#123;<span class="string">'svc__C'</span>: <span class="number">1</span>, <span class="string">'svc__gamma'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">GridSearch最优分数： <span class="number">0.8186</span></span><br><span class="line">准确率 <span class="number">0.8172</span></span><br><span class="line">GridSearch最优参数： &#123;<span class="string">'decisiontreeclassifier__max_depth'</span>: <span class="number">6</span>&#125;</span><br><span class="line">GridSearch最优分数： <span class="number">0.8208</span></span><br><span class="line">准确率 <span class="number">0.8113</span></span><br><span class="line">GridSearch最优参数： &#123;<span class="string">'randomforestclassifier__n_estimators'</span>: <span class="number">6</span>&#125;</span><br><span class="line">GridSearch最优分数： <span class="number">0.8004</span></span><br><span class="line">准确率 <span class="number">0.7994</span></span><br><span class="line">GridSearch最优参数： &#123;<span class="string">'kneighborsclassifier__n_neighbors'</span>: <span class="number">8</span>&#125;</span><br><span class="line">GridSearch最优分数： <span class="number">0.8040</span></span><br><span class="line">准确率 <span class="number">0.8036</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586884510276-5fe1de3d-a2b8-4c93-b940-372069fbed52.png#align=left&display=inline&height=1774&margin=%5Bobject%20Object%5D&name=187d0233d4fb5f07a9653e5ae4754372.png&originHeight=1774&originWidth=1729&size=124838&status=done&style=none&width=1729" alt="187d0233d4fb5f07a9653e5ae4754372.png"><br><br><br>从结果中，我们能看到 SVM 分类器的准确率最高，测试准确率为 0.8172。在决策树分类中，我设置了 3 种最大深度，当最大深度 =6 时结果最优，测试准确率为 0.8113；在随机森林分类中，我设置了 3 个决策树个数的取值，取值为 6 时结果最优，测试准确率为 0.7994；在 KNN 分类中，我设置了 3 个 n 的取值，取值为 8 时结果最优，测试准确率为 0.8036。<br></p><p><a name="cfCD6"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><br>今天我给你讲了随机森林的概念及工具的使用，另外针对数据挖掘算法中经常采用的参数调优，也介绍了 GridSearchCV 工具这个利器。并将这两者结合起来，在信用卡违约分析这个项目中进行了使用。<br><br><br>很多时候，我们不知道该采用哪种分类算法更适合。即便是对于一种分类算法，也有很多参数可以调优，每个参数都有一定的取值范围。我们可以把想要采用的分类器，以及这些参数的取值范围都设置到数组里，然后使用 GridSearchCV 工具进行调优。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586884632239-1bcdafa2-dde8-4fe6-9acb-52e5dca88f2f.png#align=left&display=inline&height=1019&margin=%5Bobject%20Object%5D&name=14f9cddc17d6cceb0b8cbc4381c65216.png&originHeight=1019&originWidth=1728&size=401439&status=done&style=none&width=1728" alt="14f9cddc17d6cceb0b8cbc4381c65216.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri May 22 2020 23:24:18 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;今天我来带你做一个数据挖掘的项目。在数据挖掘的过程中，我们经常会遇到一些问题，比如：如何选择各种分类器，到底选择哪个分类算法，是 SVM，决策树
      
    
    </summary>
    
    
      <category term="机器学习" scheme="cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Random Forest" scheme="cpeixin.cn/tags/Random-Forest/"/>
    
  </entry>
  
</feed>
