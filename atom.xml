<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布兰特 | 不忘初心</title>
  
  <subtitle>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="cpeixin.cn/"/>
  <updated>2020-05-28T15:08:18.891Z</updated>
  <id>cpeixin.cn/</id>
  
  <author>
    <name>Brent</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux直接下载Google Drive文件</title>
    <link href="cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/"/>
    <id>cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/</id>
    <published>2020-05-27T17:03:54.000Z</published>
    <updated>2020-05-28T15:08:18.891Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测试。<br><br><br>首选方案是将整个项目上传到GitHub中，随后在服务器中直接wget，但是模型文件过大，GitHub单个文件的限制是100MB。<br><br><br>突然想到可不可以直接从Google Drive上进行下载模型文件到服务器😅<br><br><br><strong>下载小文件：</strong><br><br><br>选择要下载的文件右键<br><br><br>点击“共享”<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598566164-b176f9b4-62bf-41d6-82bf-36e8837816f6.png#align=left&display=inline&height=1066&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.51.36.png&originHeight=1066&originWidth=1712&size=193930&status=done&style=none&width=1712" alt="屏幕快照 2020-05-28 上午12.51.36.png"><br><br><br><br><br>点击“更改”，设置分享权限<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598584015-3b9be292-6dd3-4edc-86ef-de50b58be55b.png#align=left&display=inline&height=1088&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.52.08.png&originHeight=1088&originWidth=2036&size=141068&status=done&style=none&width=2036" alt="屏幕快照 2020-05-28 上午12.52.08.png"><br><br><br>这是复制图中选中部分的ID<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598604200-c2f98348-2daf-423b-9792-df03ad395236.png#align=left&display=inline&height=1078&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.53.24.png&originHeight=1078&originWidth=1858&size=172147&status=done&style=none&width=1858" alt="屏幕快照 2020-05-28 上午12.53.24.png"><br><br><br>拼接下载链接，进行下载<br><br><br>wget <a href="https://drive.google.com/uc?id=1sT6GvdtCG3AnV-62beWCP6LNdtFgmX-o" target="_blank" rel="external nofollow noopener noreferrer">https://drive.google.com/uc?id=</a>复制下来的共享id -O your_file_name</p><p><strong>下载大文件：</strong></p><p>上面的方法，适合下载一些小文件，大文件就不可以了。更换下面命令的id选项，并且准备好cookies.txt<br><br><br>关于cookies.txt，可以在Chrome浏览器中下载cookie.txt这个插件，点击下载，上传到服务器中/tmp目录下即可<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590678181832-6ba38a09-0ea7-4d48-a6a3-5cd116d19e1d.png#align=left&display=inline&height=1472&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8B%E5%8D%8810.59.19.png&originHeight=1472&originWidth=3094&size=893365&status=done&style=none&width=3094" alt="屏幕快照 2020-05-28 下午10.59.19.png"><br><br><br>关于文件id，和上面方法获取一致，接下来运行下面命令即可。<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZag' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZagARe" -O pytorch_model.bin</span><br></pre></td></tr></table></figure><br><br><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测
      
    
    </summary>
    
    
      <category term="Tools" scheme="cpeixin.cn/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>def neverGrowUp()</title>
    <link href="cpeixin.cn/2020/04/06/def-neverGrowUp/"/>
    <id>cpeixin.cn/2020/04/06/def-neverGrowUp/</id>
    <published>2020-04-05T16:00:00.000Z</published>
    <updated>2020-04-05T15:10:24.541Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neverGrowUp</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="title">while</span> <span class="title">true</span>:</span></span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗疫英雄</title>
    <link href="cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/"/>
    <id>cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/</id>
    <published>2020-04-04T14:45:15.000Z</published>
    <updated>2020-04-05T14:46:33.308Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>致敬缅怀每一位抗疫英雄<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586098032229-b4c6c795-bf87-4105-8f82-87a86e48a89a.jpeg#align=left&display=inline&height=1796&name=WechatIMG86.jpeg&originHeight=1796&originWidth=1072&size=175464&status=done&style=none&width=1072" alt="WechatIMG86.jpeg"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;致敬缅怀每一位抗疫英雄&lt;br&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/jpeg/1072113
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python Flask &amp; Ajax 数据传输</title>
    <link href="cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/"/>
    <id>cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/</id>
    <published>2020-03-11T14:43:01.000Z</published>
    <updated>2020-04-04T17:13:00.080Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂</p><p>忙活三个小时，终于把前端和后端打通了～～</p><p>前端demo：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"&#123;&#123; url_for('send_message') &#125;&#125;"</span> <span class="attr">method</span>=<span class="string">"post"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span> =<span class="string">"domain"</span> <span class="attr">rows</span>=<span class="string">"30"</span> <span class="attr">cols</span>=<span class="string">"100"</span> <span class="attr">placeholder</span>=<span class="string">"请输入需要查询的域名,如cq5999.com"</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;input id="submit" type="submit" value="发送"&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">id</span>=<span class="string">"btn-bq"</span> <span class="attr">data-toggle</span>=<span class="string">"modal"</span> <span class="attr">data-target</span>=<span class="string">"#myModal"</span>&gt;</span>查询<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"send_content"</span>&gt;</span>向后台发送消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"send_content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send"</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"发送"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"recv_content"</span>&gt;</span>从后台接收消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"recv_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"recv_content"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 发送 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> message = $(<span class="string">"#send_content"</span>).val()</span></span><br><span class="line">        alert(message)</span><br><span class="line"><span class="javascript">        $.ajax(&#123;</span></span><br><span class="line"><span class="actionscript">            url:<span class="string">"/send_message"</span>,</span></span><br><span class="line"><span class="actionscript">            type:<span class="string">"POST"</span>,</span></span><br><span class="line">            data:&#123;</span><br><span class="line">                message:message</span><br><span class="line">            &#125;,</span><br><span class="line"><span class="actionscript">            dataType: <span class="string">'json'</span>,</span></span><br><span class="line"><span class="actionscript">            success:<span class="function"><span class="keyword">function</span> <span class="params">(data)</span> </span>&#123;</span></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 接收 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        $.getJSON(<span class="string">"/change_to_json"</span>,<span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#recv_content"</span>).val(data.message) <span class="comment">//将后端数据显示在前端</span></span></span><br><span class="line"><span class="javascript">            <span class="built_in">console</span>.log(<span class="string">"传到前端的数据的类型："</span> + <span class="keyword">typeof</span> (data.message))</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#send_content"</span>).val(<span class="string">""</span>)<span class="comment">//发送的输入框清空</span></span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>后端demo:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template, request, jsonify</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"index_v6.html"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/send_message', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_message</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_get = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    message_get = request.form[<span class="string">"domain"</span>].split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># message_get = request.form['message'] #input提交</span></span><br><span class="line">    print(<span class="string">"收到前端发过来的信息：%s"</span> % message_get)</span><br><span class="line">    print(<span class="string">"收到数据的类型为："</span> + str(type(message_get)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"收到消息"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/change_to_json', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_to_json</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_json = &#123;</span><br><span class="line">        <span class="string">"message"</span>: message_get</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jsonify(message_json)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>,debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂&lt;/p&gt;&lt;p&gt;忙活三个小时，终于把前端和后端打通了～～&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Python Flask接口设计-示例</title>
    <link href="cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/"/>
    <id>cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/</id>
    <published>2020-03-10T15:08:35.000Z</published>
    <updated>2020-04-04T17:12:52.356Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><a name="LHF1q"></a></p><h3 id="Get-请求"><a href="#Get-请求" class="headerlink" title="Get 请求"></a>Get 请求</h3><p><strong><strong>开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。</strong></strong><br><strong><br>**</strong>方法 1:****</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["GET"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><p>此种方式对应的request请求方式：</p><ol><li>拼接请求链接, 直接请求：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0/test_1.0?name=ccc&amp;age=18</a></li><li>request 请求中带有参数，如下图</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583826674613-bc99538a-988e-4386-b8e6-9eb9fce1862f.png#align=left&display=inline&height=610&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%883.47.43.png&originHeight=610&originWidth=1424&size=98593&status=done&style=none&width=1424" alt="屏幕快照 2020-03-10 下午3.47.43.png"></p><p>方法 2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">banWordSingleStart</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> getWordStatus(word)</span><br></pre></td></tr></table></figure><p>此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字</p><p>请求链接：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0</a>/api/banWordSingle/输入词</p><p><a name="vJdOc"></a></p><h3 id="Post-请求"><a href="#Post-请求" class="headerlink" title="Post 请求"></a>Post 请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["POST"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">8080</span>)</span><br></pre></td></tr></table></figure><p>请求方式：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583831085097-3a858ae4-259d-408d-a162-6a4ed8c5e291.png#align=left&display=inline&height=692&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%885.00.28.png&originHeight=692&originWidth=1438&size=99272&status=done&style=none&width=1438" alt="屏幕快照 2020-03-10 下午5.00.28.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;LHF1q&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;Get-请求&quot;&gt;&lt;a href=&quot;#Get-请求&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.10版本发布</title>
    <link href="cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/"/>
    <id>cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</id>
    <published>2020-02-12T17:22:22.000Z</published>
    <updated>2020-06-03T14:38:01.179Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><br>Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡献了200多个贡献者，此版本引入了对Flink作业的整体性能和稳定性的重大改进，原生Kubernetes集成的预览以及Python支持的巨大进步（PyFlink）。(Spark对Python的支持也越来越好😂)<br><br><br>Flink 1.10还标志着<a href="https://flink.apache.org/news/2019/08/22/release-1.9.0.html#preview-of-the-new-blink-sql-query-processor" target="_blank" rel="external nofollow noopener noreferrer">Blink集成</a>的完成，强化了流数据SQL并通过可用于生产的Hive集成和TPC-DS覆盖将成熟的批处理引入Flink。这篇博客文章描述了所有主要的新功能和改进，需要注意的重要更改以及预期的发展。<br><br><br>现在可以在Flink网站的更新的“ <a href="https://flink.apache.org/downloads.html" target="_blank" rel="external nofollow noopener noreferrer">下载”页面</a>上找到二进制分发文件和源工件。有关更多详细信息，请查看完整的<a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&version=12345845" target="_blank" rel="external nofollow noopener noreferrer">发行变更日志</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/" target="_blank" rel="external nofollow noopener noreferrer">更新的文档</a>。我们鼓励您下载发行版，并通过<a href="https://flink.apache.org/community.html#mailing-lists" target="_blank" rel="external nofollow noopener noreferrer">Flink邮件列表</a>或<a href="https://issues.apache.org/jira/projects/FLINK/summary" target="_blank" rel="external nofollow noopener noreferrer">JIRA</a>与社区分享您的反馈。</p><hr><p><a name="JxUa7"></a></p><h2 id="新功能和改进"><a href="#新功能和改进" class="headerlink" title="新功能和改进"></a>新功能和改进</h2><p><a name="improved-memory-management-and-configuration"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="ze04S"></a></p><h3 id="改进的内存管理和配置"><a href="#改进的内存管理和配置" class="headerlink" title="改进的内存管理和配置"></a>改进的内存管理和配置</h3><p><br>目前Flink中的TaskExecutor内存配置存在一些缺点，这些缺点使得难以推理或优化资源利用率，例如：</p><ul><li>流处理和批处理执行中用于内存占用的不同配置模型；<br></li><li>流处理执行中堆外状态后端（即RocksDB）的复杂且依赖用户的配置。<br></li></ul><p><br>为了使内存选项对用户更明确和直观，Flink 1.10对TaskExecutor内存模型和配置逻辑（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors" target="_blank" rel="external nofollow noopener noreferrer">FLIP-49</a>）进行了重大更改。这些更改使Flink更适合于各种部署环境（例如Kubernetes，Yarn，Mesos），从而使用户可以严格控制其内存消耗。<br><br><br><strong>托管内存扩展</strong><br><br><br>托管内存已经扩展扩展，当然还考虑了RocksDB StateBackend的内存使用情况。虽然批处理作业可以使用堆内（on-heap）或堆外（off-heap）内存，但具有这些功能的流作业RocksDBStateBackend只能使用堆内内存。因此，为了允许用户在流执行和批处理执行之间切换而不必修改群集配置，托管内存现在始终处于堆外状态。<br><br><br><strong>简化RocksDB配置</strong><br>**<br>曾经配置像RocksDB这样的off-heap (堆外)state backend涉及大量的手动调整，例如减小JVM堆大小或将Flink设置为使用堆外内存。现在可以通过Flink的现成配置来实现，并且调整RocksDBStateBackend内存预算就像调整内存大小一样简单。<br><br><br>另一个重要的改进是允许Flink绑定RocksDB本地内存使用情况（<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="external nofollow noopener noreferrer">FLINK-7289</a>），从而防止其超出总内存预算-这在Kubernetes等容器化环境中尤其重要。有关如何启用和调整此功能的详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/state/large_state_tuning.html#tuning-rocksdb" target="_blank" rel="external nofollow noopener noreferrer">Tuning RocksDB</a>。<br>注意 FLIP-49更改了群集资源配置的过程，这可能需要调整群集以从以前的Flink版本进行升级。有关所引入更改和调整指南的全面概述，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_setup.html" target="_blank" rel="external nofollow noopener noreferrer">此设置</a>。<br></p><p><a name="unified-logic-for-job-submission"></a></p><h3 id="提交作业的统一逻辑"><a href="#提交作业的统一逻辑" class="headerlink" title="提交作业的统一逻辑"></a>提交作业的统一逻辑</h3><p><br>在此版本之前，提交作业是执行环境的一部分职责，并且与不同的部署目标（例如，Yarn，Kubernetes，Mesos）紧密相关。这导致关注点分离不佳，并且随着时间的流逝，用户需要单独配置和管理的定制环境越来越多。<br><br><br>在Flink 1.10中，作业提交逻辑被抽象到通用Executor接口（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-73%3A+Introducing+Executors+for+job+submission" target="_blank" rel="external nofollow noopener noreferrer">FLIP-73</a>）中。另外ExecutorCLI（<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=133631524" target="_blank" rel="external nofollow noopener noreferrer">FLIP-81</a>）引入了一个统一的方式去指定配置参数对于任何 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/cli.html#deployment-targets" target="_blank" rel="external nofollow noopener noreferrer">执行对象</a>。为了完善这项工作，结果检索的过程也与作业提交分离，引入了JobClient（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API" target="_blank" rel="external nofollow noopener noreferrer">FLINK-74</a>）来负责获取JobExecutionResult。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590943394674-2fd89b17-b61e-4180-986d-72062c95f4d4.png#align=left&display=inline&height=334&margin=%5Bobject%20Object%5D&originHeight=789&originWidth=1999&size=0&status=done&style=none&width=847" alt><br><br><br>特别是，这些更改通过为用户提供Flink的统一入口点，使在下游框架（例如Apache Beam或Zeppelin交互式笔记本）中以编程方式使用Flink变得更加容易。对于跨多个目标环境使用Flink的用户，向基于配置的执行过程的过渡还可以显着减少样板代码和可维护性开销。<br></p><p><a name="native-kubernetes-integration-beta"></a></p><h3 id="原生Kubernetes集成（测试版）"><a href="#原生Kubernetes集成（测试版）" class="headerlink" title="原生Kubernetes集成（测试版）"></a>原生Kubernetes集成（测试版）</h3><p><br>对于希望在容器化环境上开始使用Flink的用户，在Kubernetes之上部署和管理独立集群需要有关容器，算子和环境工具kubectl的一些知识。<br><br><br>在Flink 1.10中，我们推出了Active Kubernetes集成（<a href="https://jira.apache.org/jira/browse/FLINK-9953" target="_blank" rel="external nofollow noopener noreferrer">FLINK-9953</a>）的第一阶段，其中，“主动”指 Flink ResourceManager (K8sResMngr) 原生地与 Kubernetes 通信，像 Flink 在 Yarn 和 Mesos 上一样按需申请 pod。用户可以利用 namespace，在多租户环境中以较少的资源开销启动 Flink。这需要用户提前配置好 RBAC 角色和有足够权限的服务账号。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590943394177-de66ac47-ba2d-4182-b5b4-585ff643cfc8.png#align=left&display=inline&height=322&margin=%5Bobject%20Object%5D&originHeight=870&originWidth=1714&size=0&status=done&style=none&width=635" alt><br><br><br>正如刚刚讲到的，Flink 1.10中的所有命令行选项都映射到统一配置。因此，用户可以简单地引用Kubernetes配置选项，然后使用以下命令在CLI中将作业提交到Kubernetes上的现有Flink会话：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run -d -e kubernetes-session -Dkubernetes.cluster-id&#x3D;&lt;ClusterId&gt; examples&#x2F;streaming&#x2F;WindowJoin.jar</span><br></pre></td></tr></table></figure><p><br>如果您想尝试使用此预览功能，我们建议您逐步完成本<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/native_kubernetes.html" target="_blank" rel="external nofollow noopener noreferrer">机Kubernetes的安装</a>，试用并与社区分享反馈。<br></p><p><a name="table-apisql-production-ready-hive-integration"></a></p><h3 id="Table-API-SQL：生产就绪的Hive集成"><a href="#Table-API-SQL：生产就绪的Hive集成" class="headerlink" title="Table API / SQL：生产就绪的Hive集成"></a>Table API / SQL：生产就绪的Hive集成</h3><p><br>Hive集成在Flink 1.9中宣布为预览功能。此预览允许用户使用SQL DDL将Flink-specific元数据（例如Kafka表）保留在Hive Metastore中，调用Hive中定义的UDF并使用Flink读取和写入Hive表。Flink 1.10通过进一步的开发使这项工作更加圆满，这些开发使可立即投入生产的Hive集成到Flink，并具有与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/#supported-hive-versions" target="_blank" rel="external nofollow noopener noreferrer">大多数Hive版本的</a>完全兼容性。<br><a name="native-partition-support-for-batch-sql"></a></p><h4 id="-1"><a href="#-1" class="headerlink"></a></h4><p><a name="IuODz"></a></p><h4 id="批处理SQL的本地分区支持"><a href="#批处理SQL的本地分区支持" class="headerlink" title="批处理SQL的本地分区支持"></a>批处理SQL的本地分区支持</h4><p><br>1.10版本以前，仅支持对未分区的Hive表进行写入。在Flink 1.10中，Flink SQL语法已通过<code>INSERT OVERWRITE</code>和<code>PARTITION</code>（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-63%3A+Rework+table+partition+support" target="_blank" rel="external nofollow noopener noreferrer">FLIP-63</a>）进行了扩展，使用户能够在Hive中写入静态和动态分区。<br><strong><br></strong>静态分区写入**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;</span><br></pre></td></tr></table></figure><p><strong><br></strong>动态分区编写**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 select_statement1 FROM from_statement;</span><br></pre></td></tr></table></figure><p><br>Flink对于分区表的全面支持，允许用户利用读取时的分区修剪功能，通过减少需要扫描的数据量来显着提高这些操作的性能。<br></p><p><a name="further-optimizations"></a></p><h4 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h4><p>除了分区修剪外，Flink 1.10还为Hive集成引入了更多<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/read_write_hive.html#optimizations" target="_blank" rel="external nofollow noopener noreferrer">读取优化</a>，例如：<br></p><ul><li><strong>投影下推：</strong> Flink通过省略表扫描中不必要的字段，利用投影下推来最大程度地减少Flink和Hive表之间的数据传输。这对于具有大量列的表尤其有利。</li></ul><ul><li><strong>LIMIT下推：</strong>对于带有<code>LIMIT</code>子句的查询，Flink将尽可能限制输出记录的数量，以最大程度地减少通过网络传输的数据量。</li></ul><ul><li><strong>读取时</strong>进行<strong>ORC矢量化：</strong>为了提高ORC文件的读取性能，Flink现在默认将本机ORC矢量化阅读器用于2.0.0以上的Hive版本以及具有非复杂数据类型的列。</li></ul><p><a name="pluggable-modules-as-flink-system-objects-beta"></a></p><h4 id="可插拔模块作为Flink系统对象（Beta）"><a href="#可插拔模块作为Flink系统对象（Beta）" class="headerlink" title="可插拔模块作为Flink系统对象（Beta）"></a>可插拔模块作为Flink系统对象（Beta）</h4><p>Flink 1.10引入了Flink Table核心中可插拔模块的通用机制，首先关注系统功能（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-68%3A+Extend+Core+Table+System+with+Pluggable+Modules" target="_blank" rel="external nofollow noopener noreferrer">FLIP-68</a>）。使用该模块，用户可以扩展Flink的系统对象，例如，使用行为类似于Flink系统功能的Hive内置函数。该版本附带一个预先实现的<code>HiveModule</code>，支持多个Hive版本的版本，但用户也可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/modules.html" target="_blank" rel="external nofollow noopener noreferrer">编写自己的可插拔模块</a>。<br><a name="other-improvements-to-the-table-apisql"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="rCuCf"></a></p><h3 id="Table-API-SQL的其他改进"><a href="#Table-API-SQL的其他改进" class="headerlink" title="Table API / SQL的其他改进"></a>Table API / SQL的其他改进</h3><p><a name="watermarks-and-computed-columns-in-sql-ddl"></a></p><h4 id="-3"><a href="#-3" class="headerlink"></a></h4><p><a name="FlcDT"></a></p><h4 id="SQL-DDL中的水印和计算列"><a href="#SQL-DDL中的水印和计算列" class="headerlink" title="SQL DDL中的水印和计算列"></a>SQL DDL中的水印和计算列</h4><p>Flink 1.10支持特定于流的语法扩展，以在Flink SQL DDL（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-66%3A+Support+Time+Attribute+in+SQL+DDL" target="_blank" rel="external nofollow noopener noreferrer">FLIP-66</a>）中定义时间属性和水印生成。这允许基于时间的操作（例如加窗），以及在使用DDL语句创建的表上定义<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html#create-table" target="_blank" rel="external nofollow noopener noreferrer">水印策略</a>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE table_name (</span><br><span class="line">  WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;</span><br><span class="line">) WITH (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><br>此版本还引入了对虚拟计算列（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-70%3A+Flink+SQL+Computed+Column+Design" target="_blank" rel="external nofollow noopener noreferrer">FLIP-70</a>）的支持，该列可基于同一表中的其他列或确定性表达式（即，文字值，UDF和内置函数）派生。在Flink中，计算列可用于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html#create-table" target="_blank" rel="external nofollow noopener noreferrer">在创建表时</a>定义时间属性。<br><a name="additional-extensions-to-sql-ddl"></a></p><h4 id="-4"><a href="#-4" class="headerlink"></a></h4><p><a name="Exz4X"></a></p><h4 id="SQL-DDL的其他扩展"><a href="#SQL-DDL的其他扩展" class="headerlink" title="SQL DDL的其他扩展"></a>SQL DDL的其他扩展</h4><p><br>现在，temporary/persistent 和 system/catalog（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-57%3A+Rework+FunctionCatalog" target="_blank" rel="external nofollow noopener noreferrer">FLIP-57</a>）之间有明显的区别。这不仅消除了函数引用中的歧义，而且允许确定性的函数解析顺序（即，在命名冲突的情况下，系统函数将优先于目录函数，而临时函数的优先级高于两个维度的持久性函数）。<br><br><br>遵循FLIP-57的基础知识，我们扩展了SQL DDL语法以支持目录功能，临时功能和临时系统功能（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-79+Flink+Function+DDL+Support" target="_blank" rel="external nofollow noopener noreferrer">FLIP-79</a>）的创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION </span><br><span class="line">  [IF NOT EXISTS] [catalog_name.][db_name.]function_name </span><br><span class="line">AS identifier [LANGUAGE JAVA|SCALA]</span><br></pre></td></tr></table></figure><p><br>有关Flink SQL中DDL支持的当前状态的完整概述，请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/" target="_blank" rel="external nofollow noopener noreferrer">更新的文档</a>。<br><strong><br></strong>注意**为了将来能正确处理和保证元对象（表，视图，函数）之间的行为一致，不建议使用Table API中的某些对象声明方法，而应使用更接近标准SQL DDL的方法（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-64%3A+Support+for+Temporary+Objects+in+Table+module" target="_blank" rel="external nofollow noopener noreferrer">FLIP -64</a>）。<br><a name="full-tpc-ds-coverage-for-batch"></a></p><h4 id="-5"><a href="#-5" class="headerlink"></a></h4><p><a name="eHBxG"></a></p><h4 id="TPC-DS的完整覆盖范围可批量处理"><a href="#TPC-DS的完整覆盖范围可批量处理" class="headerlink" title="TPC-DS的完整覆盖范围可批量处理"></a>TPC-DS的完整覆盖范围可批量处理</h4><p><br>TPC-DS是一种广泛使用的行业标准决策支持基准，用于评估和衡量基于SQL的数据处理引擎的性能。在Flink 1.10中，端到端（<a href="https://issues.apache.org/jira/browse/FLINK-11491" target="_blank" rel="external nofollow noopener noreferrer">FLINK-11491</a>）支持所有TPC-DS查询，这反映了它的SQL引擎已准备就绪，可以满足类似现代数据仓库的工作负载的需求。<br><a name="pyflink-support-for-native-user-defined-functions-udfs"></a></p><h3 id="-6"><a href="#-6" class="headerlink"></a></h3><p><a name="TgP6u"></a></p><h3 id="PyFlink：支持本机用户定义的函数（UDF）"><a href="#PyFlink：支持本机用户定义的函数（UDF）" class="headerlink" title="PyFlink：支持本机用户定义的函数（UDF）"></a>PyFlink：支持本机用户定义的函数（UDF）</h3><p><br>在以前的发行版中引入了PyFlink的预览版，朝着实现Flink中完全Python支持的目标迈进了一步。对于此发行版，重点是使用户能够在表API / SQL（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-58%3A+Flink+Python+User-Defined+Stateless+Function+for+Table" target="_blank" rel="external nofollow noopener noreferrer">FLIP-58</a>）中注册和使用Python用户定义函数（UDF，已计划UDTF / UDAF ）。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1590945929231-b78d0f66-a731-4359-8aa2-442b5f78bc77.gif#align=left&display=inline&height=405&margin=%5Bobject%20Object%5D&name=flink_1.10_pyflink.gif&originHeight=405&originWidth=779&size=2561122&status=done&style=none&width=779" alt="flink_1.10_pyflink.gif"><br><br><br>如果您对基础实现感兴趣（利用Apache Beam的<a href="https://beam.apache.org/roadmap/portability/" target="_blank" rel="external nofollow noopener noreferrer">可移植性框架）</a>，请参考FLIP-58的“架构”部分，也请参考<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-78%3A+Flink+Python+UDF+Environment+and+Dependency+Management" target="_blank" rel="external nofollow noopener noreferrer">FLIP-78</a>。这些数据结构为Pandas支持和PyFlink最终到达DataStream API奠定了必要的基础。<br><br><br>从Flink 1.10开始，用户还可以<code>pip</code>使用以下方法轻松安装PyFlink ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install apache-flink</span><br></pre></td></tr></table></figure><p><br>有关PyFlink计划进行的其他改进的预览，请查看<a href="https://issues.apache.org/jira/browse/FLINK-14500" target="_blank" rel="external nofollow noopener noreferrer">FLINK-14500</a>并参与有关所需用户功能的<a href="http://apache-flink.147419.n8.nabble.com/Re-DISCUSS-What-parts-of-the-Python-API-should-we-focus-on-next-td1285.html" target="_blank" rel="external nofollow noopener noreferrer">讨论</a>。<br><a name="important-changes"></a></p><h2 id="重要变化"><a href="#重要变化" class="headerlink" title="重要变化"></a>重要变化</h2><ul><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-10725" target="_blank" rel="external nofollow noopener noreferrer">FLINK-10725</a> ] Flink现在可以编译并在Java 11上运行。<br></li><li>[ <a href="https://jira.apache.org/jira/browse/FLINK-15495" target="_blank" rel="external nofollow noopener noreferrer">FLINK-15495</a> ] Blink计划程序现在是SQL Client中的默认设置，因此用户可以从所有最新功能和改进中受益。在下一个版本中，还计划从Table API中的旧计划程序进行切换，因此我们建议用户开始熟悉Blink计划程序。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-13025" target="_blank" rel="external nofollow noopener noreferrer">FLINK-13025</a> ]有一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/elasticsearch.html#elasticsearch-connector" target="_blank" rel="external nofollow noopener noreferrer">新的Elasticsearch接收器连接器</a>，完全支持Elasticsearch 7.x版本。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-15115" target="_blank" rel="external nofollow noopener noreferrer">FLINK-15115</a> ] Kafka 0.8和0.9的连接器已标记为不推荐使用，将不再得到积极支持。如果您仍在使用这些版本或有任何其他相关问题，请联系@dev邮件列表。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-14516" target="_blank" rel="external nofollow noopener noreferrer">FLINK-14516</a> ]删除了非基于信用的网络流控制代码以及配置选项<code>taskmanager.network.credit.model</code>。展望未来，Flink将始终使用基于信用的流量控制。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-12122" target="_blank" rel="external nofollow noopener noreferrer">FLINK-12122</a> ] <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external nofollow noopener noreferrer">FLIP-6</a>在Flink 1.5.0中推出，并引入了与从中分配插槽方式有关的代码回归<code>TaskManagers</code>。要使用更接近FLIP之前行为的调度策略（Flink尝试将工作负载分散到所有当前可用的行为中）<code>TaskManagers</code>，用户可以<code>cluster.evenly-spread-out-slots: true</code>在中设置<code>flink-conf.yaml</code>。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-11956" target="_blank" rel="external nofollow noopener noreferrer">FLINK-11956</a> ] <code>s3-hadoop</code>和<code>s3-presto</code>文件系统不再使用类重定位，而应通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/filesystems/#pluggable-file-systems" target="_blank" rel="external nofollow noopener noreferrer">插件</a>加载，但现在可以与所有凭据提供程序无缝集成。强烈建议将其他文件系统仅用作插件，因为我们将继续删除重定位。<br></li><li>Flink 1.9带有重构的Web UI，保留了旧版的UI作为备份，以防万一某些功能无法正常工作。到目前为止，尚未报告任何问题，因此<a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Remove-old-WebUI-td35218.html" target="_blank" rel="external nofollow noopener noreferrer">社区投票决定</a>在Flink 1.10中删除旧版Web UI。<br><br><a name="release-notes"></a><h2 id="发行说明"><a href="#发行说明" class="headerlink" title="发行说明"></a>发行说明</h2>如果您打算将设置升级到Flink 1.10，请仔细查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/flink-1.10.html" target="_blank" rel="external nofollow noopener noreferrer">发行说明</a>，以获取详细的更改和新功能列表。此版本与以前的1.x版本的API兼容，这些版本的API使用@Public注释进行了注释。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="cpeixin.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>IDEA install TabNine</title>
    <link href="cpeixin.cn/2020/01/22/IDEA-install-TabNine/"/>
    <id>cpeixin.cn/2020/01/22/IDEA-install-TabNine/</id>
    <published>2020-01-22T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:48.223Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>TabNine是我目前遇到过最好的智能补全工具</p><p>TabNine基于GPT-2的插件</p><p>安装<br>IDEA编译器，找到plugins</p><p>Windows pycharm：File&gt;settings&gt;plugins;<br>Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins</p><p>查找 TabNine, 点击 install, 随后 restart</p><p>重启后：Help&gt;Edit Custom Properties…&gt;Create;</p><p>在跳出来的idea.properties中输入（注：英文字符） TabNine::config</p><p>随即会自动弹出TabNine激活页面；</p><p>激活<br>点击Activation Key下面的here；</p><p>输入你的邮箱号；</p><p>复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）</p><p>等待自动安装，观察页面（最下面有log可以看当前进度）；</p><p>激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；</p><p>确认激活完成，重启pycharm即可；</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;TabNine是我目前遇到过最好的智能补全工具&lt;/p&gt;&lt;p&gt;TabNine基于GPT-2的插件&lt;/p&gt;&lt;p&gt;安装&lt;br&gt;IDEA编译器，找到pl
      
    
    </summary>
    
    
      <category term="开发工具" scheme="cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="IDEA" scheme="cpeixin.cn/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动 EB 级 HDFS 实践</title>
    <link href="cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/</id>
    <published>2020-01-02T15:25:55.000Z</published>
    <updated>2020-05-31T15:28:54.936Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>转载自<a href="https://juejin.im/post/5e0aac53e51d4575e82591e8" target="_blank" rel="external nofollow noopener noreferrer">字节跳动 EB 级 HDFS 实践</a>，学习学习字节跳动基础架构部门对上万节点的HDFS集群的管理方式<br><br><br>文章的最后，有写上自己的总结，工作这几年，确实没有遇到过这么庞大的数据量和集群，那么就不能先实战再总结了，目前是站在巨人的肩膀上看看远处的风景，当我到达的那一天，就会更从容的融入风景了。</p><hr><p><a name="MorZX"></a></p><h1 id="HDFS-简介"><a href="#HDFS-简介" class="headerlink" title="HDFS 简介"></a>HDFS 简介</h1><p>因为 HDFS 这样一个系统已经存在了非常长的时间，应用的场景已经非常成熟了，所以这部分我们会比较简单地介绍。<br>HDFS 全名 Hadoop Distributed File System，是业界使用最广泛的开源分布式文件系统。原理和架构与 Google 的 GFS 基本一致。它的特点主要有以下几项：</p><ul><li>和本地文件系统一样的目录树视图</li><li>Append Only 的写入（不支持随机写）</li><li>顺序和随机读</li><li>超大数据规模</li><li>易扩展，容错率高<br><a name="AUWUZ"></a><h1><a href="#" class="headerlink"></a></h1><a name="fQiBn"></a><h1 id="字节跳动特色的-HDFS"><a href="#字节跳动特色的-HDFS" class="headerlink" title="字节跳动特色的 HDFS"></a>字节跳动特色的 HDFS</h1></li></ul><p><br>字节跳动应用 HDFS 已经非常长的时间了，经历了 7 年的发展，目前已直接支持了十多种数据平台，间接支持了上百种业务发展。从集群规模和数据量来说，HDFS 平台在公司内部已经成长为总数几万台服务器的大平台，支持了 EB 级别的数据量。<br><br><br>在深入相关的技术细节之前，我们先看看字节跳动的 HDFS 架构。<br><a name="PdpVx"></a></p><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><p><a name="oK4fY"></a></p><h2 id="架构介绍"><a href="#架构介绍" class="headerlink" title="架构介绍"></a>架构介绍</h2><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590931343675-83b09747-e37a-438c-a445-2fdafdba9c83.webp#align=left&display=inline&height=749&margin=%5Bobject%20Object%5D&originHeight=749&originWidth=962&size=0&status=done&style=none&width=962" alt><br><a name="jC8CW"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="66HyQ"></a></p><h3 id="接入层"><a href="#接入层" class="headerlink" title="接入层"></a>接入层</h3><p><br>接入层是区别于社区版本最大的一层，社区版本中并无这一层定义。在字节跳动的落地实践中，由于集群的节点过于庞大，我们需要非常多的 NameNode 实现联邦机制来接入不同上层业务的数据服务。但当 NameNode 数量也变得非常多了以后，用户请求的统一接入及统一视图的管理也会有很大的问题。为了解决用户接入过于分散，我们需要一个独立的接入层来支持用户请求的统一接入，转发路由；同时也能结合业务提供用户权限和流量控制能力；另外，该接入层也需要提供对外的目录树统一视图。<br><br><br>该接入层从部署形态上来讲，依赖于一些外部组件如 Redis，MySQL 等，会有一批无状态的 NNProxy 组成，他们提供了请求路由，Quota 限制，Tracing 能力及流量限速等能力。<br><a name="gFV5T"></a></p><h3 id="-3"><a href="#-3" class="headerlink"></a></h3><p><a name="DTx6L"></a></p><h3 id="元数据层"><a href="#元数据层" class="headerlink" title="元数据层"></a>元数据层</h3><p><br>这一层主要模块有 Name Node，ZKFC，和 BookKeeper（不同于 QJM，BookKeeper 在大规模多节点数据同步上来讲会表现得更稳定可靠）。<br><br><br>Name Node 负责存储整个 HDFS 集群的元数据信息，是整个系统的大脑。一旦故障，整个集群都会陷入不可用状态。因此 Name Node 有一套基于 ZKFC 的主从热备的高可用方案。<br><br><br>Name Node 还面临着扩展性的问题，单机承载能力始终受限。于是 HDFS 引入了联邦（Federation）机制。一个集群中可以部署多组 Name Node，它们独立维护自己的元数据，共用 Data Node 存储资源。这样，一个 HDFS 集群就可以无限扩展了。但是这种 Federation 机制下，每一组 Name Node 的目录树都互相割裂的。于是又出现了一些解决方案，能够使整个 Federation 集群对外提供一个完整目录树的视图。<br></p><p><a name="OVDWh"></a></p><h3 id="数据层"><a href="#数据层" class="headerlink" title="数据层"></a>数据层</h3><p><br>相比元数据层，数据层主要节点是 Data Node。Data Node 负责实际的数据存储和读取。用户文件被切分成块复制成多副本，每个副本都存在不同的 Data Node 上，以达到容错容灾的效果。每个副本在 Data Node 上都以文件的形式存储，元信息在启动时被加载到内存中。<br><br><br>Data Node 会定时向 Name Node 做心跳汇报，并且周期性将自己所存储的副本信息汇报给 Name Node。这个过程对 Federation 中的每个集群都是独立完成的。在心跳汇报的返回结果中，会携带 Name Node 对 Data Node 下发的指令，例如，需要将某个副本拷贝到另外一台 Data Node 或者将某个副本删除等。<br></p><p><a name="NU3Ae"></a></p><h2 id="主要业务"><a href="#主要业务" class="headerlink" title="主要业务"></a>主要业务</h2><p>先来看一下当前在字节跳动 HDFS 承载的主要业务：</p><ul><li>Hive，HBase，日志服务，Kafka 数据存储</li><li>Yarn，Flink 的计算框架平台数据</li><li>Spark，MapReduce 的计算相关数据存储<br><a name="GHk55"></a><h2 id="发展阶段"><a href="#发展阶段" class="headerlink" title="发展阶段"></a>发展阶段</h2>在字节跳动，随着业务的快速发展，HDFS 的数据量和集群规模快速扩大，原来的 HDFS 的集群从几百台，迅速突破千台和万台的规模。这中间，踩了无数的坑，大的阶段归纳起来会有这样几个阶段。<br><a name="rzDSM"></a><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3>业务增长初期，集群规模增长趋势非常陡峭，单集群规模很快在元数据服务器 Name Node 侧遇到瓶颈。引入联邦机制（Federation）实现集群的横向扩展。<br><br><br>联邦又带来统一命名空间问题，因此，需要统一视图空间帮助业务构建统一接入。为了解决这个问题，我们引入了 Name Node Proxy 组件实现统一视图和多租户管理等功能，这部分会在下文的 NNProxy 章节中介绍。<br><a name="r78Fs"></a><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3></li></ul><p><br>数据量继续增大，Federation 方式下的目录树管理也存在瓶颈，主要体现在数据量增大后，Java 版本的 GC 变得更加频繁，跨子树迁移节点代价过大，节点启动时间太长等问题。因此我们通过重构的方式，解决了 GC，锁优化，启动加速等问题，将原 Name Node 的服务能力进一步提高。容纳更多的元数据信息。<br><br><br>为了解决这个问题，我们也实现了字节跳动特色的 DanceNN 组件，兼容了原有 Java 版本 NameNode 的全部功能基础上，大大增强了稳定性和性能。相关详细介绍会在下面的 DanceNN 章节中介绍。<br><a name="cKChL"></a></p><h3 id="-4"><a href="#-4" class="headerlink"></a></h3><p><a name="Ufy9s"></a></p><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p><br>当数据量跨过 EB，集群规模扩大到几万台的时候，慢节点问题，更细粒度服务分级问题，成本问题和元数据瓶颈进一步凸显。我们在架构上进一步在包括完善多租户体系构建，重构数据节点和元数据分层等方向进一步演进。这部分目前正在进行中，因为优化的点会非常多，本文会给出慢节点优化的落地实践。<br></p><p><a name="tEe39"></a></p><h2 id="关键改进"><a href="#关键改进" class="headerlink" title="关键改进"></a>关键改进</h2><p><br>在整个架构演进的过程中，我们做了非常多的探索和尝试。如上所述，结合之前提到的几个大的挑战和问题，我们就其中关键的 Name Node Proxy 和 Dance Name Node 这两个重点组件做一下介绍，同时，也会介绍一下我们在慢节点方面的优化和改进。<br><a name="TD5kf"></a></p><h3 id="-5"><a href="#-5" class="headerlink"></a></h3><p><a name="4I1Fk"></a></p><h3 id="NNProxy（Name-Node-Proxy）"><a href="#NNProxy（Name-Node-Proxy）" class="headerlink" title="NNProxy（Name Node Proxy）"></a>NNProxy（Name Node Proxy）</h3><p>作为系统的元数据操作接入端，NNProxy 提供了联邦模式下统一元数据视图，解决了用户请求的统一转发，业务流量的统一管控的问题。<br><br><br>先介绍一下 NNProxy 所处的系统上下游。<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590933295275-e12431d8-0bab-419e-bc29-4c3e03a5c6ff.webp#align=left&display=inline&height=572&margin=%5Bobject%20Object%5D&originHeight=572&originWidth=864&size=0&status=done&style=none&width=864" alt><br><br><br>我们先来看一下 NNProxy 都做了什么工作。<br><a name="rk9VU"></a></p><h4 id="-6"><a href="#-6" class="headerlink"></a></h4><p><a name="AYfYO"></a></p><h4 id="路由管理"><a href="#路由管理" class="headerlink" title="路由管理"></a>路由管理</h4><p><br>在上面 Federation 的介绍中提到，每个集群都维护自己独立的目录树，无法对外提供一个完整的目录树视图。NNProxy 中的路由管理就解决了这个问题。路由管理存储了一张 mount table，表中记录若干条路径到集群的映射关系。<br><br><br>例如 <strong>/user -&gt; hdfs://namenodeB</strong>，这条映射关系的含义就是 /user 及其子目录这个目录在 <strong>namenodeB</strong> 这个集群上，所有对 /user 及其子目录的访问都会由 NNProxy 转发给 <strong>namenodeB</strong>，获取结果后再返回给 Client。<br><br><br>匹配原则为最长匹配，例如我们还有另外一条映射 <strong>/user/tiger/dump -&gt; hdfs://namenodeC</strong>，那么 /user/tiger/dump 及其所有子目录都在 namenodeC，而 /user 目录下其他子目录都在 namenodeB 上。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590933295845-bb064f91-f470-4129-80b9-c49e55ddae75.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=864&size=0&status=done&style=none&width=864" alt><br><a name="kRRta"></a></p><h4 id="-7"><a href="#-7" class="headerlink"></a></h4><p><a name="DJKii"></a></p><h4 id="Quota-限制"><a href="#Quota-限制" class="headerlink" title="Quota 限制"></a>Quota 限制</h4><p>使用过 HDFS 的同学会知道 Quota 这个概念。我们给每个目录集合分配了额定的空间资源，一旦使用超过这个阈值，就会被禁止写入。这个工作就是由 NNProxy 完成的。NNProxy 会通过 Quota 实时监控系统获取最新 Quota 使用情况，当用户进行元数据操作的时候，NNProxy 就会根据用户的 Quota 情况作出判断，决定通过或者拒绝。<br><a name="e2E2o"></a></p><h4 id="-8"><a href="#-8" class="headerlink"></a></h4><p><a name="JG3Xt"></a></p><h4 id="Trace-支持"><a href="#Trace-支持" class="headerlink" title="Trace 支持"></a>Trace 支持</h4><p>ByteTrace 是一个 Trace 系统，记录追踪用户和系统以及系统之间的调用行为，以达到分析和运维的目的。其中的 Trace 信息会附在向 NNProxy 的请求 RPC 中。NNProxy 拿到 ByteTrace 以后就可以知道当前请求的上游模块，USER 及 Application ID 等信息。NNProxy 一方面将这些信息发到 Kafka 做一些离线分析，一方面实时聚合并打点，以便追溯线上流量。<br><a name="7lbZo"></a></p><h4 id="-9"><a href="#-9" class="headerlink"></a></h4><p><a name="9VGfn"></a></p><h4 id="流量限制"><a href="#流量限制" class="headerlink" title="流量限制"></a>流量限制</h4><p><br>虽然 NNProxy 非常轻量，可以承受很高的 QPS，但是后端的 Name Node 承载能力是有限的。因此突发的大作业造成高 QPS 的读写请求被全量转发到 Name Node 上时，会造成 Name Node 过载，延时变高，甚至出现 OOM，影响集群上所有用户。<br><br><br>因此 NNProxy 另一个非常重要的任务就是限流，以保护后端 Name Node。目前限流基于路径+RPC 以及 用户+RPC 维度，例如我们可以限制 /user/tiger/warhouse 路径的 create 请求为 100 QPS，或者某个用户的 delete 请求为 5 QPS。一旦该用户的访问量超过这个阈值，NNProxy 会返回一个可重试异常，Client 收到这个异常后会重试。因此被限流的路径或用户会感觉到访问 HDFS 变慢，但是并不会失败。<br><br><br></p><p><a name="a95em"></a></p><h3 id="Dance-NN（Dance-Name-Node）"><a href="#Dance-NN（Dance-Name-Node）" class="headerlink" title="Dance NN（Dance Name Node）"></a>Dance NN（Dance Name Node）</h3><p><a name="lhQQ9"></a></p><h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>如前所述，在数据量上到 EB 级别的场景后，原有的 Java 版本的 Name Node 存在了非常多的线上问题需要解决。以下是在实践过程中我们遇到的一些问题总结：<br></p><ul><li>Java 版本 Name Node 采用 Java 语言开发，在 INode 规模上亿时，不可避免的会带来严重的 GC 问题；</li><li>Java 版本 Name Node 将 INode meta 信息完全放置于内存，10 亿 INode 大约占用 800GB 内存（包含 JVM 自身占用的部分 native memory），更进一步加重了 GC；</li><li>我们目前的集群规模下，Name Node 从重启到恢复服务需要 6 个小时，在主备同时发生故障的情况下，严重影响上层业务；</li><li>Java 版本 Name Node 全局一把读写锁，任何对目录树的修改操作都会阻塞其他的读写操作，并发度较低；</li></ul><p>从上可以看出，在大数据量场景下，我们亟需一个新架构版本的 Name Node 来承载我们的海量元数据。除了 C++语言重写来规避 Java 带来的 GC 问题以外，我们还在一些场景下做了特殊的优化。<br></p><p><a name="1ILjM"></a></p><h4 id="目录树锁设计"><a href="#目录树锁设计" class="headerlink" title="目录树锁设计"></a>目录树锁设计</h4><p><br>HDFS 对内是一个分布式集群，对外提供的是一个 unified 的文件系统，因此对文件及目录的操作需要像操作 Linux 本地文件系统一样。这就要求 HDFS 满足类似于数据库系统中 ACID 特性一样的原子性，一致性、隔离性和持久性。因此 DanceNN 在面对多个用户同时操作同一个文件或者同一个目录时，需要保证不会破坏掉 ACID 属性，需要对操作做锁保护。<br><br><br>不同于传统的 KV 存储和数据库表结构，DanceNN 上维护的是一棵树状的数据结构，因此单纯的 key 锁或者行锁在 DanceNN 下不适用。而像数据库的表锁或者原生 NN 的做法，对整棵目录树加单独一把锁又会严重的影响整体吞吐和延迟，因此 DanceNN 重新设计了树状锁结构，做到保证 ACID 的情况下，读吞吐能够到 8w，写吞吐能够到 2w，是原生 NN 性能的 10 倍以上。<br><br><br>这里，我们会重新对 RPC 做分类，像 <code>createFile</code>，<code>getFileInfo</code>，<code>setXAttr</code> 这类 RPC 依然是简单的对某一个 INode 进行 CURD 操作；像 <code>delete</code> RPC，有可能删除一个文件，也有可能会删除目录，后者会影响整棵子树下的所有文件；像 <code>rename</code> RPC，则是更复杂的另外一类操作，可能会涉及到多个 INode，甚至是多棵子树下的所有 INode。<br></p><p><a name="FOHq3"></a></p><h4 id="DanceNN-启动优化"><a href="#DanceNN-启动优化" class="headerlink" title="DanceNN 启动优化"></a>DanceNN 启动优化</h4><p><br>由于我们的 DanceNN 底层元数据实现了本地目录树管理结构，因此我们 DanceNN 的启动优化都是围绕着这样的设计来做的。<br><a name="cwcfb"></a></p><h5 id="-10"><a href="#-10" class="headerlink"></a></h5><p><a name="ygdYf"></a></p><h5 id="多线程扫描和填充-BlockMap"><a href="#多线程扫描和填充-BlockMap" class="headerlink" title="多线程扫描和填充 BlockMap"></a>多线程扫描和填充 BlockMap</h5><p>在系统启动过程中，第一步就是读取目录树中保存的信息并且填入 BlockMap 中，类似 Java 版 NN 读取 FSImage 的操作。在具体实现过程中，首先起多个线程并行扫描静态目录树结构。将扫描的结果放入一个加锁的 Buffer 中。当 Buffer 中的元素个数达到设定的数量以后，重新生成一个新的 Buffer 接收请求，并在老 Buffer 上起一个线程将数据填入 BlockMap。<br><a name="wPkVT"></a></p><h5 id="-11"><a href="#-11" class="headerlink"></a></h5><p><a name="GvGcm"></a></p><h5 id="接收块上报优化"><a href="#接收块上报优化" class="headerlink" title="接收块上报优化"></a>接收块上报优化</h5><p>DanceNN 启动以后会首先进入安全模式，接收所有 Date Node 的块上报，完善 BlockMap 中保存的信息。当上报的 Date Node 达到一定比例以后，才会退出安全模式，这时候才能正式接收 client 的请求。所以接收块上报的速度也会影响 Date Node 的启动时长。DanceNN 这里做了一个优化，根据 BlockID 将不同请求分配给不同的线程处理，每个线程负责固定的 Slice，线程之间无竞争，这样就极大的加快了接收块上报的速度。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935398857-421715f0-1f68-40e2-a738-5d53fc518e03.webp#align=left&display=inline&height=412&margin=%5Bobject%20Object%5D&originHeight=412&originWidth=864&size=0&status=done&style=none&width=864" alt><br><br><br></p><p><a name="tCZ1r"></a></p><h3 id="慢节点优化"><a href="#慢节点优化" class="headerlink" title="慢节点优化"></a>慢节点优化</h3><p><br>慢节点问题在很多分布式系统中都存在。其产生的原因通常为上层业务的热点或者底层资源故障。上层业务热点，会导致一些数据在较短的时间段内被集中访问。而底层资源故障，如出现慢盘或者盘损坏，更多的请求就会集中到某一个副本节点上从而导致慢节点。<br><br><br>通常来说，慢节点问题的优化和上层业务需求及底层资源量有很大的关系，极端情况，上层请求很小，下层资源充分富裕的情况下，慢节点问题将会非常少，反之则会变得非常严重。在字节跳动的 HDFS 集群中，慢节点问题一度非常严重，尤其是磁盘占用百分比非常高以后，各种慢节点问题层出不穷。其根本原因就是资源的平衡滞后，许多机器的磁盘占用已经触及红线导致写降级；新增热资源则会集中到少量机器上，这种情况下，当上层业务的每秒请求数升高后，对于 P999 时延要求比较高的一些大数据分析查询业务就容易出现一大批数据访问（&gt;10000 请求）被卡在某个慢请求的处理上。<br><br><br>我们优化的方向会分为读慢节点和写慢节点两个方面。<br></p><p><a name="PwyJV"></a></p><h4 id="读慢节点优化"><a href="#读慢节点优化" class="headerlink" title="读慢节点优化"></a>读慢节点优化</h4><p>我们经历了几个阶段：</p><ul><li>最早，使用社区版本，其 Switch Read 以读取一个 packet 的时长为统计单位，当读取一个 packet 的时间超过阈值时，认为读取当前 packet 超时。如果一定时间窗口内超时 packet 的数量过多，则认为当前节点是慢节点。但这个问题在于以 packet 作为统计单位使得算法不够敏感，这样使得每次读慢节点发生的时候，对于小 IO 场景（字节跳动的一些业务是以大量随机小 IO 为典型使用场景的），这些个积攒的 Packet 已经造成了问题。</li></ul><ul><li>后续，我们研发了 Hedged Read 的读优化。Hedged Read 对每一次读取设置一个超时时间。如果读取超时，那么会另开一个线程，在新的线程中向第二个副本发起读请求，最后取第一第二个副本上优先返回的 response 作为读取的结果。但这种情况下，在慢节点集中发生的时候，会导致读流量放大。严重的时候甚至导致小范围带宽短时间内不可用。</li></ul><ul><li>基于之前的经验，我们进一步优化，开启了 Fast Switch Read 的优化，该优化方式使用吞吐量作为判断慢节点的标准，当一段时间窗口内的吞吐量小于阈值时，认为当前节点是慢节点。并且根据当前的读取状况动态地调整阈值，动态改变时间窗口的长度以及吞吐量阈值的大小。<br>下表是当时线上某业务测试的值：</li></ul><table><thead><tr><th>Host:X.X.X.X</th><th>3 副本 Switch Read</th><th>2 副本 Hedged Read</th><th>3 副本 Hedged Read</th><th>3 副本 Fast Switch Read（优化后算法）</th></tr></thead><tbody><tr><td>读取时长 p999</td><td>977 ms</td><td>549 ms</td><td>192 ms</td><td><strong>128 ms</strong></td></tr><tr><td>最长读取时间</td><td>300 s</td><td>125 s</td><td>60 s</td><td><strong>15.5 s</strong></td></tr><tr><td>长尾出现次数（大于 500ms）</td><td>238 次/天</td><td>75 次/天</td><td>15 次/天</td><td><strong>3 次/天</strong></td></tr><tr><td>长尾出现次数（大于 1000ms）</td><td>196 次/天</td><td>64 次/天</td><td>6 次/天</td><td><strong>3 次/天</strong></td></tr></tbody></table><p><br>进一步的相关测试数据：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935682709-6dceebc5-46fb-459b-a559-c238e04d66be.webp#align=left&display=inline&height=211&margin=%5Bobject%20Object%5D&originHeight=211&originWidth=746&size=0&status=done&style=none&width=746" alt><br><a name="aI0wv"></a></p><h4 id="-12"><a href="#-12" class="headerlink"></a></h4><p><a name="hBmkC"></a></p><h4 id="写慢节点优化"><a href="#写慢节点优化" class="headerlink" title="写慢节点优化"></a>写慢节点优化</h4><p><br>写慢节点优化的适用场景会相对简单一些。主要解决的是写过程中，Pipeline 的中间节点变慢的情况。为了解决这个问题，我们也发展了 Fast Failover 和 Fast Failover+两种算法。<br><a name="2ceD5"></a></p><h5 id="-13"><a href="#-13" class="headerlink"></a></h5><p><a name="yHKlv"></a></p><h5 id="Fast-Failover"><a href="#Fast-Failover" class="headerlink" title="Fast Failover"></a>Fast Failover</h5><p>Fast Failover 会维护一段时间内 ACK 时间过长的 packet 数目，当超时 ACK 的数量超过阈值后，会结束当前的 block，向 namenode 申请新块继续写入。<br><br><br>Fast Failover 的问题在于，随意结束当前的 block 会造成系统的小 block 数目增加，给之后的读取速度以及 namenode 的元数据维护都带来负面影响。所以 Fast Failover 维护了一个切换阈值，如果已写入的数据量（block 的大小）大于这个阈值，才会进行 block 切换。<br><br><br>但是往往为了达到这个写入数据大小阈值，就会造成用户难以接收的延迟，因此当数据量小于阈时需要进额外的优化。<br><a name="Ohd15"></a></p><h5 id="-14"><a href="#-14" class="headerlink"></a></h5><p><a name="Hq46w"></a></p><h5 id="Fast-Failover-1"><a href="#Fast-Failover-1" class="headerlink" title="Fast Failover+"></a>Fast Failover+</h5><p>为了解决上述的问题，当已写入的数据量（block 的大小）小于阈值时，我们引入了新的优化手段——Fast Failover+。该算法首先从 pipeline 中筛选出速度较慢的 datanode，将慢节点从当前 pipeline 中剔除，并进入 Pipeline Recovery 阶段。Pipeline Recovery 会向 namenode 申请一个新的 datanode，与剩下的 datanode 组成一个新的 pipeline，并将已写入的数据同步到新的 datanode 上（该步骤称为 transfer block）。由于已经写入的数据量较小，transfer block 的耗时并不高。统计 p999 平均耗时只有 150ms。由 Pipeline Recovery 所带来的额外消耗是可接受的。<br><br><br>下表是当时线上某业务测试的值：</p><table><thead><tr><th>Host:X.X.X.X</th><th>Fast Failover p99</th><th>Fast Failover+ p99 <strong>(优化后算法)</strong></th><th>Fast Failover p95</th><th>Fast Failover+ p95 <strong>(优化后算法)</strong></th></tr></thead><tbody><tr><td>平均 Flush 时长</td><td>1.49 s</td><td><strong>1.23 s</strong></td><td>182 ms</td><td><strong>147 ms</strong></td></tr><tr><td>最长 Flush 时间</td><td>80 s</td><td><strong>66 s</strong></td><td>9.7 s</td><td><strong>6.5 s</strong></td></tr><tr><td>长尾出现次数（p99 大于 10s, p95 大于 1s）</td><td>63 次/天</td><td><strong>38 次/天</strong></td><td>94 次/天</td><td><strong>55 次/天</strong></td></tr><tr><td>长尾出现次数（p99 大于 5s, p95 大于 0.5s）</td><td>133 次/天</td><td><strong>101 次/天</strong></td><td>173 次/天</td><td><strong>156 次/天</strong></td></tr></tbody></table><p>一些进一步的实际效果对比：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935682666-c4712d74-258e-44be-b16d-a6a773f9fcbb.webp#align=left&display=inline&height=232&margin=%5Bobject%20Object%5D&originHeight=232&originWidth=734&size=0&status=done&style=none&width=734" alt><br><a name="d4UFq"></a></p><h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><p>HDFS 在字节跳动的发展历程已经非常长了。从最初的几百台的集群规模支持 PB 级别的数据量，到现在几万台级别多集群的平台支持 EB 级别的数据量，我们经历了 7 年的发展。伴随着业务的快速上量，我们团队也经历了野蛮式爆发，规模化发展，平台化运营的阶段。这过程中我们踩了不少坑，也积累了相当丰富的经验。当然，最重要的，公司还在持续高速发展，而我们仍旧不忘初心，坚持“DAY ONE”，继续在路上。<br><br><br></p><p><a name="mZaR7"></a></p><h1 id="学习总结"><a href="#学习总结" class="headerlink" title="学习总结"></a>学习总结</h1><p><br>接入层：接入层是字节设计假如的一层，在上万节点的HDFS集群中，必然要使用多NameNode模式，那么对于用户大量的请求统一管理，字节引入了接入层，具体实现借用Redis，Mysql以及NNProxy转发路由等外界组件实现。<br><br><br>元数据层：这里面有一点，在字节的HDFS集群中，并没有使用社区版的QJM HA高可用方案，而是使用了BookKeeper。Apache bookkeeper是一个分布式，可扩展，容错（多副本），低延迟的存储系统，其提供了高性能，高吞吐的存储能力。而QJM/Qurom Journal Manager是Clouera提出的，这是一个基于Paxos算法实现的HDFS HA方案<br><br><br>数据层倒是没什么特别的改善<br><br><br>可以看出，字节的数据暴涨阶段，首先遇到的问题是Name Node的瓶颈，而此时字节的集群环境为单集群，此时的解决方案是采用Federation。</p><p>数据持续高速增长，Federation 方式下的目录树管理也存在瓶颈，主要原因是Java频繁GC，那么字节的解决方案就显得有些硬核了，重写了NameNode，在字节中叫做DanceNN 🐂🍺。</p><p>在数据超过EB级别之后，遇到的问题就更多了。不同粒度服务分级，元数据存储瓶颈，慢节点等问题。那么字节的解决方案则是考虑到一方面从存储方面的数据节点进行重构，另一方面对于大块的元数据进行分级。</p><p>这里要说一下上面提到的NNProxy，好用！！主要有两个功能很吸引我，在Hadoop集群原有的基础上，字节添加了NNProxy，一个是根据用户请求的路径转发到不同的HDFS空间，二呢，对多租户的场景下，对每个用户的请求做判断，如果某个请求量过大，则会对其限流。</p><p>在这里，我也领略到了一个场景，那就是在字节EB级别的集群规模下，集群重启到全部服务恢复，需要6个小时左右。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;转载自&lt;a href=&quot;https://juejin.im/post/5e0aac53e51d4575e82591e8&quot; target=&quot;_bl
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hdfs" scheme="cpeixin.cn/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 Chinese 自动生成文章 - 环境准备</title>
    <link href="cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <id>cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</id>
    <published>2020-01-01T14:28:43.000Z</published>
    <updated>2020-04-13T09:28:23.224Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><a name="R14AA"></a></p><h2 id="Google-Colab"><a href="#Google-Colab" class="headerlink" title="Google Colab"></a>Google Colab</h2><p><br>Colaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。<br><br><br>Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。<br><br><br>缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。<br><br><br><strong>优点 免费！ 免费！免费！</strong><br>**<br><a name="dpofS"></a></p><h3 id="谷歌云盘"><a href="#谷歌云盘" class="headerlink" title="谷歌云盘"></a>谷歌云盘</h3><p><br>当登录账号进入<a href="https://drive.google.com/drive/my-drive" target="_blank" rel="external nofollow noopener noreferrer">谷歌云盘</a>时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723531238-28bbbd81-69e8-472d-b048-1ac67166a201.png#align=left&display=inline&height=612&name=image.png&originHeight=612&originWidth=1268&size=104029&status=done&style=none&width=1268" alt="image.png"><br>可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：</p><p><a name="EHdj9"></a></p><h3 id="引入Colab"><a href="#引入Colab" class="headerlink" title="引入Colab"></a>引入Colab</h3><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723706098-527d9fff-e46e-4dd1-b92a-0640b0d61555.png#align=left&display=inline&height=674&name=image.png&originHeight=674&originWidth=1125&size=104056&status=done&style=none&width=1125" alt="image.png"><br><br><br><br><br></p><p><a name="kykCO"></a></p><h3 id="设置GPU环境"><a href="#设置GPU环境" class="headerlink" title="设置GPU环境"></a>设置GPU环境</h3><p><br>打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723911273-f07371f9-e982-44b2-af34-b3781f294879.png#align=left&display=inline&height=739&name=image.png&originHeight=739&originWidth=1191&size=94677&status=done&style=none&width=1191" alt="image.png"><br><br><br></p><p><a name="f4U2h"></a></p><h3 id="挂载和切换工作目录"><a href="#挂载和切换工作目录" class="headerlink" title="挂载和切换工作目录"></a>挂载和切换工作目录</h3><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录</span></span><br><span class="line"></span><br><span class="line">os.chdir(<span class="string">'/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese'</span>) <span class="comment">## 共享云盘的目录</span></span><br></pre></td></tr></table></figure><p>其中： My Drive 代表你的google网盘根目录</p><pre><code>code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录</code></pre><p><a name="MyewB"></a></p><h3 id="在Colab中运行任务"><a href="#在Colab中运行任务" class="headerlink" title="在Colab中运行任务"></a>在Colab中运行任务</h3><p>下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769633567-4e4118a0-5c52-4517-9233-71d897e7fd68.png#align=left&display=inline&height=1748&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.15.46.png&originHeight=1748&originWidth=3096&size=378104&status=done&style=none&width=3096" alt="屏幕快照 2020-04-13 下午5.15.46.png"></p><p><a name="GZDbL"></a></p><h3 id="ipynb文件内容"><a href="#ipynb文件内容" class="headerlink" title=".ipynb文件内容"></a>.ipynb文件内容</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769997876-4536842e-6bb3-4d6f-8df9-e220a66026a0.png#align=left&display=inline&height=1702&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.23.22.png&originHeight=1702&originWidth=3154&size=396138&status=done&style=none&width=3154" alt="屏幕快照 2020-04-13 下午5.23.22.png"><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;R14AA&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;Google-Colab&quot;&gt;&lt;a href=&quot;#Google-Colab&quot; cl
      
    
    </summary>
    
    
      <category term="NLP" scheme="cpeixin.cn/categories/NLP/"/>
    
    
      <category term="GPT-2" scheme="cpeixin.cn/tags/GPT-2/"/>
    
  </entry>
  
  <entry>
    <title>架构思想</title>
    <link href="cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/"/>
    <id>cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/</id>
    <published>2019-12-20T02:26:15.000Z</published>
    <updated>2020-04-04T11:23:45.206Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h2><a href="#" class="headerlink"></a></h2><p>关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。<br><br><br>从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。<br><br><br>具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。<br><br><br>架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。<br><a name="-3"></a></p><h4 id="架构思想"><a href="#架构思想" class="headerlink" title="架构思想"></a>架构思想</h4><p>CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度</p><ul><li><p>满足业务发展需求是最高准则</p></li><li><p>业务建模，抽象和枚举是两种方式，需要平衡，不能走极端</p></li><li><p>模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计</p></li><li><p>基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。</p></li><li><p>分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程</p></li><li><p>分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中</p></li><li><p>不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术</p></li><li><p>好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难</p></li><li><p>业务架构、系统架构、数据模型<br><a name="-4"></a></p><h4 id="面对一块新业务，如何系统架构？"><a href="#面对一块新业务，如何系统架构？" class="headerlink" title="面对一块新业务，如何系统架构？"></a>面对一块新业务，如何系统架构？</h4></li><li><p>业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。</p></li><li><p>系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？</p></li><li><p>系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？</p></li><li><p>模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。<br><a name="-5"></a></p><h4 id="如何模块化"><a href="#如何模块化" class="headerlink" title="如何模块化"></a>如何模块化</h4></li><li><p>基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。</p></li><li><p>基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。</p></li><li><p>基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。</p></li><li><p>基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。<br><a name="-6"></a></p><h4 id="提升系统的稳定性"><a href="#提升系统的稳定性" class="headerlink" title="提升系统的稳定性"></a>提升系统的稳定性</h4></li><li><p>流控</p></li></ul><p>双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。<br>另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。</p><ul><li>容灾</li></ul><p>最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。<br>另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。</p><ul><li>稳定性</li></ul><p>第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。</p><ul><li>容量规划</li></ul><p>做好容量规划、系统间强弱依赖关系梳理。<br>如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。<br><a name="-7"></a></p><h4 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h4><ul><li><p>分布式系统</p></li><li><p>分布式缓存</p></li><li><p>分布式数据<br><a name="api"></a></p><h4 id="API-和乐高积木有什么相似之处？"><a href="#API-和乐高积木有什么相似之处？" class="headerlink" title="API 和乐高积木有什么相似之处？"></a>API 和乐高积木有什么相似之处？</h4><p>相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。<br>对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。<br>你可以发现很多 API 和乐高积木的相似之处：</p></li><li><p>标准化：通用、标准化的组件，作为基本的构建块（building blocks）；<br></p></li><li><p>可用性：强调可用性，附有文档或使用说明；<br></p></li><li><p>可定制：为不同功能使用不同的API；<br></p></li><li><p>创造性：能够组合不同的 API 来创造混搭的结果；</p></li></ul><p><br>乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。<br>虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。<br><br><br>让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;关于什么
      
    
    </summary>
    
    
      <category term="架构" scheme="cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>kali中文设置</title>
    <link href="cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/"/>
    <id>cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/</id>
    <published>2019-12-01T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:21.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>更新源</p><p><a href="https://blog.csdn.net/qq_38333291/article/details/89764967" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_38333291/article/details/89764967</a></p><p>设置编码和中文字体安装</p><p><a href="http://www.linuxdiyf.com/linux/20701.html" target="_blank" rel="external nofollow noopener noreferrer">http://www.linuxdiyf.com/linux/20701.html</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;更新源&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_38333291/article/details/897
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="kali" scheme="cpeixin.cn/tags/kali/"/>
    
  </entry>
  
  <entry>
    <title>分布式下的数据hash分布</title>
    <link href="cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/"/>
    <id>cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/</id>
    <published>2019-11-19T15:05:08.000Z</published>
    <updated>2020-04-04T11:24:04.737Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动在Spark SQL上的核心优化实践</title>
    <link href="cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</id>
    <published>2019-11-12T10:57:27.000Z</published>
    <updated>2020-05-16T10:59:03.835Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><br>本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践<br>原文链接：<a href="https://juejin.im/post/5dc3ed336fb9a04a7847f25c" target="_blank" rel="external nofollow noopener noreferrer">https://juejin.im/post/5dc3ed336fb9a04a7847f25c</a><br><a name="KLy4v"></a></p><h2 id="Spark-SQL-架构简介"><a href="#Spark-SQL-架构简介" class="headerlink" title="Spark SQL 架构简介"></a>Spark SQL 架构简介</h2><p>我们先简单聊一下Spark SQL 的架构。下面这张图描述了一条 SQL 提交之后需要经历的几个阶段，结合这些阶段就可以看到在哪些环节可以做优化。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589617132571-55a23e83-cc96-437b-93a9-871d41c6724e.webp#align=left&display=inline&height=448&margin=%5Bobject%20Object%5D&originHeight=448&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>很多时候，做数据仓库建模的同学更倾向于直接写 SQL 而非使用 Spark 的 DSL。一条 SQL 提交之后会被 Parser 解析并转化为 Unresolved Logical Plan。它的重点是 Logical Plan 也即逻辑计划，它描述了希望做什么样的查询。Unresolved 是指该查询相关的一些信息未知，比如不知道查询的目标表的 Schema 以及数据位置。<br>上述信息存于 Catalog 内。在生产环境中，一般由 Hive Metastore 提供 Catalog 服务。Analyzer 会结合 Catalog 将 Unresolved Logical Plan 转换为 Resolved Logical Plan。</p><p>到这里还不够。不同的人写出来的 SQL 不一样，生成的 Resolved Logical Plan 也就不一样，执行效率也不一样。为了保证无论用户如何写 SQL 都可以高效的执行，Spark SQL 需要对 Resolved Logical Plan 进行优化，这个优化由 Optimizer 完成。Optimizer 包含了一系列规则，对 Resolved Logical Plan 进行等价转换，最终生成 Optimized Logical Plan。<strong>该 Optimized Logical Plan 不能保证是全局最优的，但至少是接近最优的。</strong><br>上述过程只与 SQL 有关，与查询有关，但是与 Spark 无关，因此无法直接提交给 Spark 执行。Query Planner 负责将 Optimized Logical Plan 转换为 Physical Plan，进而可以直接由 Spark 执行。<br><br><br>由于同一种逻辑算子可以有多种物理实现。如 Join 有多种实现，ShuffledHashJoin、BroadcastHashJoin、BroadcastNestedLoopJoin、SortMergeJoin 等。因此 Optimized Logical Plan 可被 Query Planner 转换为多个 Physical Plan。如何选择最优的 Physical Plan 成为一件非常影响最终执行性能的事情。一种比较好的方式是，<strong>构建一个 Cost Model，并对所有候选的 Physical Plan 应用该 Model 并挑选 Cost 最小的 Physical Plan 作为最终的 Selected Physical Plan。</strong></p><p>Physical Plan 可直接转换成 RDD 由 Spark 执行。我们经常说“计划赶不上变化”，在执行过程中，可能发现原计划不是最优的，后续执行计划如果能根据运行时的统计信息进行调整可能提升整体执行效率。这部分<strong>动态调整由 Adaptive Execution</strong> 完成。<br><br><br>后面介绍字节跳动在 Spark SQL 上做的一些优化，主要围绕这一节介绍的逻辑计划优化与物理计划优化展开。<br><a name="Z68Tc"></a></p><h2 id="Spark-SQL引擎优化"><a href="#Spark-SQL引擎优化" class="headerlink" title="Spark SQL引擎优化"></a>Spark SQL引擎优化</h2><p><a name="L4CUN"></a></p><h3 id="Bucket-Join改进"><a href="#Bucket-Join改进" class="headerlink" title="Bucket Join改进"></a>Bucket Join改进</h3><p>在 Spark 里，实际并没有 Bucket Join 算子。这里说的 Bucket Join 泛指不需要 Shuffle 的 Sort Merge Join。<br>下图展示了 Sort Merge Join 的基本原理。用虚线框代表的 Table 1 和 Table 2 是两张需要按某字段进行 Join 的表。虚线框内的 partition 0 到 partition m 是该表转换成 RDD 后的 Partition，而非表的分区。</p><p>假设 Table 1 与 Table 2 转换为 RDD 后分别包含 m 和 k 个 Partition。为了进行 Join，需要通过 Shuffle 保证相同 Join Key 的数据在同一个 Partition 内且 Partition 内按 Key 排序，同时保证 Table 1 与 Table 2 经过 Shuffle 后的 RDD 的 Partition 数相同。<br><br><br>如下图所示，经过 Shuffle 后只需要启动 n 个 Task，每个 Task 处理 Table 1 与 Table 2 中对应 Partition 的数据进行 Join 即可。如 Task 0 只需要顺序扫描 Shuffle 后的左右两边的 partition 0 即可完成 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443202-f22b73ca-17f1-450c-86ca-e47c9b934bca.webp#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&originHeight=629&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>该方法的优势是适用场景广，几乎可用于任意大小的数据集。劣势是每次 Join 都需要对全量数据进行 Shuffle，而 Shuffle 是最影响 Spark SQL 性能的环节。如果能避免 Shuffle 往往能大幅提升 Spark SQL 性能。</p><p>对于大数据的场景来讲，数据一般是一次写入多次查询。如果经常对两张表按相同或类似的方式进行 Join，每次都需要付出 Shuffle 的代价。与其这样，<strong>不如让数据在写的时候，就让数据按照利于 Join 的方式分布，从而使得 Join 时无需进行 Shuffle。</strong>如下图所示，Table 1 与 Table 2 内的数据按照相同的 Key 进行分桶且桶数都为 n，同时桶内按该 Key 排序。对这两张表进行 Join 时，可以避免 Shuffle，直接启动 n 个 Task 进行 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443345-00aac217-cf9d-4ca2-beea-3a27868e9709.webp#align=left&display=inline&height=697&margin=%5Bobject%20Object%5D&originHeight=697&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>字节跳动对 Spark SQL 的 BucketJoin 做了四项比较大的改进。<br><strong><br></strong>改进一：支持与 Hive 兼容**<br>在过去一段时间，字节跳动把大量的 Hive 作业迁移到了 SparkSQL。而 Hive 与 Spark SQL 的 Bucket 表不兼容。对于使用 Bucket 表的场景，如果直接更新计算引擎，会造成 Spark SQL 写入 Hive Bucket 表的数据无法被下游的 Hive 作业当成 Bucket 表进行 Bucket Join，从而造成作业执行时间变长，可能影响 SLA。</p><p>为了解决这个问题，我们让 Spark SQL 支持 Hive 兼容模式，从而保证 Spark SQL 写入的 Bucket 表与 Hive 写入的 Bucket 表效果一致，并且这种表可以被 Hive 和 Spark SQL 当成 Bucket 表进行 Bucket Join 而不需要 Shuffle。通过这种方式保证 Hive 向 Spark SQL 的透明迁移。</p><p>第一个需要解决的问题是，Hive 的一个 Bucket 一般只包含一个文件，而 Spark SQL 的一个 Bucket 可能包含多个文件。<strong>解决办法是动态增加一次以 Bucket Key 为 Key 并且并行度与 Bucket 个数相同的 Shuffle。</strong></p><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443237-9db92d7d-db57-44a7-8c38-d02bece98cc8.webp#align=left&display=inline&height=609&margin=%5Bobject%20Object%5D&originHeight=609&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br><br><br>第二个需要解决的问题是，Hive 1.x 的哈希方式与 Spark SQL 2.x 的哈希方式（Murmur3Hash）不同，使得相同的数据在 Hive 中的 Bucket ID 与 Spark SQL 中的 Bucket ID 不同而无法直接 Join。在 Hive 兼容模式下，我们让上述动态增加的 Shuffle 使用 Hive 相同的哈希方式，从而解决该问题。<br><strong><br></strong>改进二：支持倍数关系Bucket Join**<br>Spark SQL 要求只有 Bucket 相同的表才能（必要非充分条件）进行 Bucket Join。对于两张大小相差很大的表，比如几百 GB 的维度表与几十 TB （单分区）的事实表，它们的 Bucket 个数往往不同，并且个数相差很多，默认无法进行 Bucket Join。因此我们通过两种方式支持了倍数关系的 Bucket Join，即当两张 Bucket 表的 Bucket 数是倍数关系时支持 Bucket Join。</p><p>第一种方式，Task 个数与小表 Bucket 个数相同。如下图所示，Table A 包含 3 个 Bucket，Table B 包含 6 个 Bucket。此时 Table B 的 bucket 0 与 bucket 3 的数据合集应该与 Table A 的 bucket 0 进行 Join。这种情况下，可以启动 3 个 Task。其中 Task 0 对 Table A 的 bucket 0 与 Table B 的 bucket 0 + bucket 3 进行 Join。在这里，需要对 Table B 的 bucket 0 与 bucket 3 的数据再做一次 merge sort 从而保证合集有序。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443244-bfe94284-63b6-4abe-8e6f-1df98c09fd08.webp#align=left&display=inline&height=616&margin=%5Bobject%20Object%5D&originHeight=616&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444353-c4e9ece6-e014-4583-ba9d-28bda6a5fe62.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><p>如果 Table A 与 Table B 的 Bucket 个数相差不大，可以使用上述方式。如果 Table B 的 Bucket 个数是 Bucket A Bucket 个数的 10 倍，那上述方式虽然避免了 Shuffle，但可能因为并行度不够反而比包含 Shuffle 的 SortMergeJoin 速度慢。此时可以使用另外一种方式，即 Task 个数与大表 Bucket 个数相等，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443251-d514de79-4fd1-4960-9e27-2a4e408c88ba.webp#align=left&display=inline&height=555&margin=%5Bobject%20Object%5D&originHeight=555&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>在该方案下，可将 Table A 的 3 个 Bucket 读多次。在上图中，直接将 Table A 与 Table A 进行 Bucket Union （新的算子，与 Union 类似，但保留了 Bucket 特性），结果相当于 6 个 Bucket，与 Table B 的 Bucket 个数相同，从而可以进行 Bucket Join。<br><strong><br></strong>改进三：支持BucketJoin 降级**<br>公司内部过去使用 Bucket 的表较少，在我们对 Bucket 做了一系列改进后，大量用户希望将表转换为 Bucket 表。转换后，表的元信息显示该表为 Bucket 表，而历史分区内的数据并未按 Bucket 表要求分布，在查询历史数据时会出现无法识别 Bucket 的问题。</p><p>同时，由于数据量上涨快，平均 Bucket 大小也快速增长。这会造成单 Task 需要处理的数据量过大进而引起使用 Bucket 后的效果可能不如直接使用基于 Shuffle 的 Join。<br><br><br>为了解决上述问题，我们实现了支持降级的 Bucket 表。基本原理是，每次修改 Bucket 信息（包含上述两种情况——将非 Bucket 表转为 Bucket 表，以及修改 Bucket 个数）时，记录修改日期。并且在决定使用哪种 Join 方式时，对于 Bucket 表先检查所查询的数据是否只包含该日期之后的分区。如果是，则当成 Bucket 表处理，支持 Bucket Join；否则当成普通无 Bucket 的表。<br><strong><br></strong>改进四：支持超集<strong><br>对于一张常用表，可能会与另外一张表按 User 字段做 Join，也可能会与另外一张表按 User 和 App 字段做 Join，与其它表按 User 与 Item 字段进行 Join。而 Spark SQL 原生的 Bucket Join 要求 Join Key Set 与表的 Bucket Key Set 完全相同才能进行 Bucket Join。在该场景中，不同 Join 的 Key Set 不同，因此无法同时使用 Bucket Join。这极大的限制了 Bucket Join 的适用场景。<br><br><br>针对此问题，我们支持了超集场景下的 Bucket Join。只要 Join Key Set 包含了 Bucket Key Set，即可进行 Bucket Join。<br><br><br>如下图所示，Table X 与 Table Y，都按字段 A 分 Bucket。而查询需要对 Table X 与 Table Y 进行 Join，且 Join Key Set 为 A 与 B。此时，由于 A 相等的数据，在两表中的 Bucket ID 相同，那 A 与 B 各自相等的数据在两表中的 Bucket ID 肯定也相同，所以数据分布是满足 Join 要求的，不需要 Shuffle。同时，Bucket Join 还需要保证两表按 Join Key Set 即 A 和 B 排序，此时只需要对 Table X 与 Table Y 进行分区内排序即可。由于两边已经按字段 A 排序了，此时再按 A 与 B 排序，代价相对较低。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443338-9f06a217-9078-4734-84ce-860707b4612e.webp#align=left&display=inline&height=661&margin=%5Bobject%20Object%5D&originHeight=661&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444719-a5289c2c-0dea-4316-8924-1e4377bce1ea.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br></strong>物化列**<br>Spark SQL 处理嵌套类型数据时，存在以下问题：</p><ul><li><strong>读取大量不必要的数据</strong>：对于 Parquet / ORC 等列式存储格式，可只读取需要的字段，而直接跳过其它字段，从而极大节省 IO。而对于嵌套数据类型的字段，如下图中的 Map 类型的 people 字段，往往只需要读取其中的子字段，如 people.age。却需要将整个 Map 类型的 people 字段全部读取出来然后抽取出 people.age 字段。这会引入大量的无意义的 IO 开销。在我们的场景中，存在不少 Map 类型的字段，而且很多包含几十至几百个 Key，这也就意味着 IO 被放大了几十至几百倍。<br></li><li><strong>无法进行向量化读取</strong>：而向量化读能极大的提升性能。但截止到目前（2019年10月26日），Spark 不支持包含嵌套数据类型的向量化读取。这极大的影响了包含嵌套数据类型的查询性能<br></li><li><strong>不支持 Filter 下推</strong>：目前（2019年10月26日）的 Spark 不支持嵌套类型字段上的 Filter 的下推<br></li><li><strong>重复计算</strong>：JSON 字段，在 Spark SQL 中以 String 类型存在，严格来说不算嵌套数据类型。不过实践中也常用于保存不固定的多个字段，在查询时通过 JSON Path 抽取目标子字段，而大型 JSON 字符串的字段抽取非常消耗 CPU。对于热点表，频繁重复抽取相同子字段非常浪费资源。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443241-a0b575d3-4b99-451f-802f-991fd41107f6.webp#align=left&display=inline&height=668&margin=%5Bobject%20Object%5D&originHeight=668&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444769-6635f36d-d873-430c-8223-adcb532766d6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>对于这个问题，做数仓的同学也想了一些解决方案。如下图所示，<strong>在名为 base_table 的表之外创建了一张名为 sub_table 的表，并且将高频使用的子字段 people.age 设置为一个额外的 Integer 类型的字段</strong>。下游不再通过 base_table 查询 people.age，而是使用 sub_table 上的 age 字段代替。通过这种方式，将嵌套类型字段上的查询转为了 Primitive 类型字段的查询，同时解决了上述问题。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444716-a4b11d0a-899e-430c-af62-8d0151c63af6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443482-11bc4e9f-a7bb-4d7d-9026-746ea16d0d27.webp#align=left&display=inline&height=611&margin=%5Bobject%20Object%5D&originHeight=611&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>这种方案存在明显缺陷：</p><ul><li><strong>额外维护了一张表</strong>，引入了大量的额外存储/计算开销。<br></li><li>无法在新表上查询新增字段的历史数据（如要支持对历史数据的查询，需要重跑历史作业，开销过大，无法接受）。<br></li><li>表的维护方需要在修改表结构后修改插入数据的作业。<br></li><li>需要下游查询方修改查询语句，推广成本较大。<br></li><li><strong>运营成本高</strong>：如果高频子字段变化，需要删除不再需要的独立子字段，并添加新子字段为独立字段。删除前，需要确保下游无业务使用该字段。而新增字段需要通知并推进下游业务方使用新字段。<br></li></ul><p>为解决上述所有问题，我们设计并实现了物化列。它的原理是：</p><ul><li>新增一个 Primitive 类型字段，比如 Integer 类型的 age 字段，并且指定它是 people.age 的物化字段。<br></li><li>插入数据时，为物化字段自动生成数据，并在 Partition Parameter 内保存物化关系。因此对插入数据的作业完全透明，表的维护方不需要修改已有作业。<br></li><li>查询时，检查所需查询的所有 Partition，如果都包含物化信息（people.age 到 age 的映射），直接将 select people.age 自动重写为 select age，从而实现对下游查询方的完全透明优化。同时兼容历史数据。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443240-98b45cc3-1d0f-4093-980d-265abb445a8a.webp#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&originHeight=683&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444777-84f75218-68e0-4929-909d-4c828ab64f33.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>下图展示了在某张核心表上使用物化列的收益：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443260-d3424ecd-67fe-41ed-a902-edf0d1ee45ab.webp#align=left&display=inline&height=423&margin=%5Bobject%20Object%5D&originHeight=423&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444729-1b39919b-8ea5-459e-bc2b-db24d8391b39.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>物化视图</strong><br>在 OLAP 领域，经常会对相同表的某些固定字段进行 Group By 和 Aggregate / Join 等耗时操作，造成大量重复性计算，浪费资源，且影响查询性能，不利于提升用户体验。<br>我们实现了基于物化视图的优化功能：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443261-8e8f3de2-937d-475b-aca0-d39fed39babb.webp#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>如上图所示，查询历史显示大量查询根据 user 进行 group by，然后对 num 进行 sum 或 count 计算。此时可创建一张物化视图，且对 user 进行 gorup by，对 num 进行 avg（avg 会自动转换为 count 和 sum）。用户对原始表进行 select user, sum(num) 查询时，Spark SQL 自动将查询重写为对物化视图的 select user, sum_num 查询。<br><strong><br></strong>Spark SQL 引擎上的其它优化<strong><br>下图展示了我们在 Spark SQL 上进行的其它部分优化工作：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443207-45af67c8-8989-4026-81f3-d646d8123845.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br></strong><br><a name="5UxSb"></a></p><h2 id="Spark-Shuffle稳定性提升与性能优化"><a href="#Spark-Shuffle稳定性提升与性能优化" class="headerlink" title="Spark Shuffle稳定性提升与性能优化"></a>Spark Shuffle稳定性提升与性能优化</h2><p><a name="5rTgp"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Ajtpe"></a></p><h3 id="Spark-Shuffle-存在的问题"><a href="#Spark-Shuffle-存在的问题" class="headerlink" title="Spark Shuffle 存在的问题"></a>Spark Shuffle 存在的问题</h3><p>Shuffle的原理，很多同学应该已经很熟悉了。鉴于时间关系，这里不介绍过多细节，只简单介绍下基本模型。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445126-5aa3b134-c904-42d9-ac04-09618bb32553.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443213-c2f4bcdb-24e6-4fdc-afc2-3397f6608fec.webp#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&originHeight=679&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，我们将 Shuffle 上游 Stage 称为 Mapper Stage，其中的 Task 称为 Mapper。Shuffle 下游 Stage 称为 Reducer Stage，其中的 Task 称为 Reducer。<br>每个 Mapper 会将自己的数据分为最多 N 个部分，N 为 Reducer 个数。每个 Reducer 需要去最多 M （Mapper 个数）个 Mapper 获取属于自己的那部分数据。<br><br><br>这个架构存在两个问题：</p><ul><li><strong>稳定性问题</strong>：Mapper 的 Shuffle Write 数据存于 Mapper 本地磁盘，只有一个副本。当该机器出现磁盘故障，或者 IO 满载，CPU 满载时，Reducer 无法读取该数据，从而引起 FetchFailedException，进而导致 Stage Retry。Stage Retry 会造成作业执行时间增长，直接影响 SLA。同时，执行时间越长，出现 Shuffle 数据无法读取的可能性越大，反过来又会造成更多 Stage Retry。如此循环，可能导致大型作业无法成功执行。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444392-8549057c-506b-4459-856d-0ce9cddf9910.webp#align=left&display=inline&height=628&margin=%5Bobject%20Object%5D&originHeight=628&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445074-9921e4da-02ca-438c-9b15-403f41fba339.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><ul><li><strong>性能问题</strong>：每个 Mapper 的数据会被大量 Reducer 读取，并且是随机读取不同部分。假设 Mapper 的 Shuffle 输出为 512MB，Reducer 有 10 万个，那平均每个 Reducer 读取数据 512MB / 100000 = 5.24KB。并且，不同 Reducer 并行读取数据。对于 Mapper 输出文件而言，存在大量的随机读取。而 HDD 的随机 IO 性能远低于顺序 IO。最终的现象是，Reducer 读取 Shuffle 数据非常慢，反映到 Metrics 上就是 Reducer Shuffle Read Blocked Time 较长，甚至占整个 Reducer 执行时间的一大半，如下图所示。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444444-a5a5eeea-5d00-473f-892c-bef8ff9e032f.webp#align=left&display=inline&height=545&margin=%5Bobject%20Object%5D&originHeight=545&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445093-1dba238c-18eb-4bc2-a637-4a5f0f069847.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>基于HDFS的Shuffle稳定性提升</strong><br>经观察，引起 Shuffle 失败的最大因素不是磁盘故障等硬件问题，而是 CPU 满载和磁盘 IO 满载。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445175-a4b309ba-f45b-4141-ac27-8771c622c313.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444483-bc3847a8-0664-4c0d-a4ed-ae5f1781331b.webp#align=left&display=inline&height=684&margin=%5Bobject%20Object%5D&originHeight=684&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，机器的 CPU 使用率接近 100%，使得 Mapper 侧的 Node Manager 内的 Spark External Shuffle Service 无法及时提供 Shuffle 服务。<br>下图中 Data Node 占用了整台机器 IO 资源的 84%，部分磁盘 IO 完全打满，这使得读取 Shuffle 数据非常慢，进而使得 Reducer 侧无法在超时时间内读取数据，造成 FetchFailedException。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444481-5779db89-9d8b-4b17-82e3-7b60b08eed02.webp#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&originHeight=627&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>无论是何种原因，问题的症结都是 Mapper 侧的 Shuffle Write 数据只保存在本地，一旦该节点出现问题，会造成该节点上所有 Shuffle Write 数据无法被 Reducer 读取。解决这个问题的一个通用方法是，通过多副本保证可用性。<br>最初始的一个简单方案是，Mapper 侧最终数据文件与索引文件不写在本地磁盘，而是直接写到 HDFS。Reducer 不再通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据，而是直接从 HDFS 上获取数据，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444436-0efdf8b6-0082-47c1-9218-36b6e6bbee4f.webp#align=left&display=inline&height=541&margin=%5Bobject%20Object%5D&originHeight=541&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>快速实现这个方案后，我们做了几组简单的测试。结果表明：</p><ul><li>Mapper 与 Reducer 不多时，Shuffle 读写性能与原始方案相比无差异。<br></li><li>Mapper 与 Reducer 较多时，Shuffle 读变得非常慢。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444384-9bb761c9-fbe1-43c8-9a82-a51e97c97f8e.webp#align=left&display=inline&height=540&margin=%5Bobject%20Object%5D&originHeight=540&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>在上面的实验过程中，HDFS 发出了报警信息。如下图所示，HDFS Name Node Proxy 的 QPS 峰值达到 60 万。（注：字节跳动自研了 Node Name Proxy，并在 Proxy 层实现了缓存，因此读 QPS 可以支撑到这个量级）。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444590-4c232e53-8507-472c-844c-18249a03f85f.webp#align=left&display=inline&height=698&margin=%5Bobject%20Object%5D&originHeight=698&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>原因在于，总共 10000 Reducer，需要从 10000 个 Mapper 处读取数据文件和索引文件，总共需要读取 HDFS 10000 * 1000 * 2 = 2 亿次。</p><p>如果只是 Name Node 的单点性能问题，还可以通过一些简单的方法解决。例如在 Spark Driver 侧保存所有 Mapper 的 Block Location，然后 Driver 将该信息广播至所有 Executor，每个 Reducer 可以直接从 Executor 处获取 Block Location，然后无须连接 Name Node，而是直接从 Data Node 读取数据。但鉴于 Data Node 的线程模型，这种方案会对 Data Node 造成较大冲击。<br>最后我们选择了一种比较简单可行的方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444471-aaff74d3-ec02-4014-8d71-1b201eafdbf1.webp#align=left&display=inline&height=582&margin=%5Bobject%20Object%5D&originHeight=582&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>Mapper 的 Shuffle 输出数据仍然按原方案写本地磁盘，写完后上传到 HDFS。Reducer 仍然按原始方案通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据。如果失败了，则从 HDFS 读取。这种方案极大减少了对 HDFS 的访问频率。<br>该方案上线近一年：</p><ul><li>覆盖 57% 以上的 Spark Shuffle 数据。<br></li><li>使得 Spark 作业整体性能提升 14%。<br></li><li>天级大作业性能提升 18%。<br></li><li>小时级作业性能提升 12%。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444424-472f0478-d4b8-47de-a9ea-557b4c201c05.webp#align=left&display=inline&height=670&margin=%5Bobject%20Object%5D&originHeight=670&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445065-2ed3ba00-5462-4f5f-9752-f2491fc7ea4d.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>该方案旨在提升 Spark Shuffle 稳定性从而提升作业稳定性，但最终没有使用方差等指标来衡量稳定性的提升。原因在于每天集群负载不一样，整体方差较大。Shuffle 稳定性提升后，Stage Retry 大幅减少，整体作业执行时间减少，也即性能提升。最终通过对比使用该方案前后的总的作业执行时间来对比性能的提升，用于衡量该方案的效果。<br><strong>Shuffle性能优化实践与探索</strong><br>如上文所分析，Shuffle 性能问题的原因在于，Shuffle Write 由 Mapper 完成，然后 Reducer 需要从所有 Mapper 处读取数据。这种模型，我们称之为以 Mapper 为中心的 Shuffle。它的问题在于：</p><ul><li>Mapper 侧会有 M 次顺序写 IO。<br></li><li>Mapper 侧会有 M * N * 2 次随机读 IO（这是最大的性能瓶颈）。<br></li><li>Mapper 侧的 External Shuffle Service 必须与 Mapper 位于同一台机器，无法做到有效的存储计算分离，Shuffle 服务无法独立扩展。<br></li></ul><p>针对上述问题，我们提出了以 Reducer 为中心的，存储计算分离的 Shuffle 方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444445-e51ea3d0-904b-48ff-af72-22657a86b3ba.webp#align=left&display=inline&height=730&margin=%5Bobject%20Object%5D&originHeight=730&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>该方案的原理是，Mapper 直接将属于不同 Reducer 的数据写到不同的 Shuffle Service。在上图中，总共 2 个 Mapper，5 个 Reducer，5 个 Shuffle Service。所有 Mapper 都将属于 Reducer 0 的数据远程流式发送给 Shuffle Service 0，并由它顺序写入磁盘。Reducer 0 只需要从 Shuffle Service 0 顺序读取所有数据即可，无需再从 M 个 Mapper 取数据。该方案的优势在于：</p><ul><li>将 M * N * 2 次随机 IO 变为 N 次顺序 IO。<br></li><li>Shuffle Service 可以独立于 Mapper 或者 Reducer 部署，从而做到独立扩展，做到存储计算分离。<br></li><li>Shuffle Service 可将数据直接存于 HDFS 等高可用存储，因此可同时解决 Shuffle 稳定性问题。<br></li></ul><p>我的分享就到这里，谢谢大家。<br><a name="23kL1"></a></p><h2 id="QA集锦"><a href="#QA集锦" class="headerlink" title="QA集锦"></a>QA集锦</h2><p><strong>- 提问：物化列新增一列，是否需要修改历史数据？</strong><br>回答：历史数据太多，不适合修改历史数据。</p><p><strong>- 提问：如果用户的请求同时包含新数据和历史数据，如何处理？</strong><br>回答：一般而言，用户修改数据都是以 Partition 为单位。所以我们在 Partition Parameter 上保存了物化列相关信息。如果用户的查询同时包含了新 Partition 与历史 Partition，我们会在新 Partition 上针对物化列进行 SQL Rewrite，历史 Partition 不 Rewrite，然后将新老 Partition 进行 Union，从而在保证数据正确性的前提下尽可能充分利用物化列的优势。</p><p><strong>- 提问：你好，你们针对用户的场景，做了很多挺有价值的优化。像物化列、物化视图，都需要根据用户的查询 Pattern 进行设置。目前你们是人工分析这些查询，还是有某种机制自动去分析并优化？</strong><br>回答：目前我们主要是通过一些审计信息辅助人工分析。同时我们也正在做物化列与物化视图的推荐服务，最终做到智能建设物化列与物化视图。</p><p><strong>- 提问：刚刚介绍的基于 HDFS 的 Spark Shuffle 稳定性提升方案，是否可以异步上传 Shuffle 数据至 HDFS？</strong><br>回答：这个想法挺好，我们之前也考虑过，但基于几点考虑，最终没有这样做。第一，单 Mapper 的 Shuffle 输出数据量一般很小，上传到 HDFS 耗时在 2 秒以内，这个时间开销可以忽略；第二，我们广泛使用 External Shuffle Service 和 Dynamic Allocation，Mapper 执行完成后可能 Executor 就回收了，如果要异步上传，就必须依赖其它组件，这会提升复杂度，ROI 较低。</p><p><a name="BtHKy"></a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>字节的这篇分享，我真的太喜欢了，所有的优化点，都是拿真实的业务场景进行举例，虽然上文有的技术点在我们的场景中还没必要去做到这种优化程度，但是如此实在的来源于线上的方案，非常容易理解。我们公司目前的数据量要比字节的量小很多，最近在新闻上看到抖音的日活已经达到了4亿，所以我们在数仓中的数据，还没有用到更细粒度的Bucket表，分区表就已经完全可以满足我们的需求。上文中的物化列方案，我觉得很新颖，但是从工程师的角度来讲，在物化列方案中，多维护一张表，添加了复杂度和运营成本，我们在数据的存储中，尽可量的回去避免复杂结构的数据类型，这样会降低存储端和计算端代码的复杂度。这篇文章是针对Spark SQL的优化方面，可以说基本上每个大数据公司都会用到Spark SQL，上述的优化方案肯定会帮助到更多的大数据团队 💪</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践&lt;br&gt;原文链接：&lt;a href
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hive 窗口函数</title>
    <link href="cpeixin.cn/2019/10/05/Hive-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"/>
    <id>cpeixin.cn/2019/10/05/Hive-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/</id>
    <published>2019-10-05T00:22:52.000Z</published>
    <updated>2020-06-15T00:47:29.706Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><a name="e05dce83"></a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文主要介绍hive中的窗口函数，hive中的窗口函数和sql中的窗口函数相类似,都是用来做一些数据分析类的工作,一般用于OLAP分析（在线分析处理）。<br><a name="b59c9e0f"></a></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>窗口函数是用于分析用的一类函数，要理解窗口函数要先从聚合函数说起，我们都知道在sql中有一类函数叫做聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的.但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数。<br><br><br>窗口函数可以在本行内做运算，得到多行的结果，即每一行对应一行的值。 通用的窗口函数可以用下面的语法来概括：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Function() Over (Partition By Column1，Column2，Order By Column3)</span><br></pre></td></tr></table></figure><p>窗口函数又分为以下三类：<br></p><ul><li>聚合型窗口函数</li><li>分析型窗口函数</li><li>取值型窗口函数</li></ul><p><a name="qC5oF"></a></p><h2 id="over-子句"><a href="#over-子句" class="headerlink" title="over()子句"></a>over()子句</h2><p>我们可以形象的把over()子句理解成开窗子句，即打开一个窗口，窗口内包含多条记录，over()会给每一行开一个窗口。如下图，总共有5条记录，每一行代表一条记录，over()在每一条记录的基础上打开一个窗口，给r1记录打开w1窗口，窗口内只包含自己，给r2打开w2窗口，窗口内包含r1、r2，给r3打开w3窗口，窗口内包含r1、r2、r3，以此类推…..<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1592154521700-b02fdcf0-79ee-4d4e-8bb2-59ebce5c4e55.png#align=left&display=inline&height=315&margin=%5Bobject%20Object%5D&name=20190930094823263.png&originHeight=315&originWidth=748&size=11034&status=done&style=none&width=748" alt="20190930094823263.png"><br></p><p><a name="WXNor"></a></p><h2 id="over-子句的开窗范围"><a href="#over-子句的开窗范围" class="headerlink" title="over()子句的开窗范围"></a>over()子句的开窗范围</h2><p>先看一张图：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1592154550224-95e40d42-9a67-4c6f-8bfe-0024c2ec7c6e.png#align=left&display=inline&height=303&margin=%5Bobject%20Object%5D&name=20190930103436749.png&originHeight=303&originWidth=697&size=17636&status=done&style=none&width=697" alt="20190930103436749.png"><br><br><br><br><br>current row代表查询的当前行，1 preceding代表前一行，1 following代表后一行，unbounded preceding代表第一行，unbounded following代表最后一行。（注意这里的第一行和最后一行并不是严格的第一行和最后一行，根据具体情况而定）<br><br><br>由上我们不难发现，在使用over()子句进行查询的时候， 不仅可以查询到每条记录的信息，还可以查询到这条记录对应窗口内的所有记录的聚合信息，所以我们通常结合聚合函数和over()子句一起使用。<br><br><br>那么over()是如何进行开窗的呢？即每条记录对应的窗口内应该包含哪些记录呢？这些都是在over()子句的括号内进行定义。<br></p><p><a name="uGpna"></a></p><h3 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h3><p>如果over()子句中接order by，例如：over(order by date)，则默认的开窗范围为根据date排序后的rows between unbounded preceding and current row，即第一行到当前行，意思是over(order by date)和over(order by date rows rows between unbounded preceding and current row)的效果是一样的。<br></p><p><a name="Qx7FK"></a></p><h3 id="partition-by"><a href="#partition-by" class="headerlink" title="partition by"></a>partition by</h3><p>如果over子句中接partition by（和group by类似，都是根据列值对行进行分组），例如over(partition by month(date))，则每一行的默认的开窗范围为当前行所在分组的所有记录。注意partition by子句不能单独和window clause子句一起使用，必须结合order by子句，下面会讨论。<br></p><p><a name="qEcpV"></a></p><h3 id="partition-by-order-by"><a href="#partition-by-order-by" class="headerlink" title="partition by + order by"></a>partition by + order by</h3><p>先分组，再排序，即组内排序。同样的，如果 order by后不接window clause，则每一行的默认的开窗范围为：当前行所在分组的第一行到当前行，即over(partition by (month(date)) order by orderdate)和over(partition by (month(date)) order by orderdate rows between undounded preceding and current row)是一样的。<br></p><p><a name="GxqqM"></a></p><h2 id="窗口大小"><a href="#窗口大小" class="headerlink" title="窗口大小"></a>窗口大小</h2><p>over()子句的开窗范围可以通过window 子句（window clause）在over()的括号中定义，window clause的规范如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br><span class="line"></span><br><span class="line">partition by …order by…rows between unbounded preceding and current row</span><br><span class="line">窗口大小为从起始行得到当前行。</span><br><span class="line">partition by …order by… rows between 3 preceding and current row</span><br><span class="line">窗口大小为从当前行到之前三行</span><br><span class="line">partition by …order by… rows between 3 preceding and 1 following</span><br><span class="line">窗口大小为当前行的前三行到之后的一行</span><br><span class="line">partition by …order by… rows between 3 preceding and unbounded following</span><br><span class="line">窗口大小为当前行的前三行到之后的所有行</span><br></pre></td></tr></table></figure><p>例如 select <em>,sum(column_name) over( rows between unbounded preceding and unbounded following) from table_name 表示查询每一行的所有列值，同时给每一行打开一个从第一行到最后一行的窗口，并统计窗口内所有记录的column_name列值的和。最后给每一行输出该行的所有属性以及该行对应窗口内记录的聚合值。<br><br><br>如果over()子句中什么都不写的话，默认开窗范围是：rows between unbounded preceding and unbounded following<br><br><br>*</em>在深入研究Over子句之前，一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于Order by字句之前。<strong><br></strong><br><a name="e4DLH"></a></p><h2 id="实践准备"><a href="#实践准备" class="headerlink" title="实践准备"></a>实践准备</h2><p><strong>创建Hive表</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE user_match_temp (</span><br><span class="line">user_name string,</span><br><span class="line">opponent string,</span><br><span class="line">result int,</span><br><span class="line">create_time timestamp);</span><br></pre></td></tr></table></figure><p><br>数据量较少，就直接手动插入了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO TABLE user_match_temp values</span><br><span class="line">(&#39;A&#39;,&#39;B&#39;,1,&#39;2019-07-18 23:19:00&#39;),</span><br><span class="line">(&#39;B&#39;,&#39;A&#39;,0,&#39;2019-07-18 23:19:00&#39;),</span><br><span class="line">(&#39;A&#39;,&#39;C&#39;,0,&#39;2019-07-18 23:20:00&#39;),</span><br><span class="line">(&#39;C&#39;,&#39;A&#39;,1,&#39;2019-07-18 23:20:00&#39;),</span><br><span class="line">(&#39;A&#39;,&#39;D&#39;,1,&#39;2019-07-19 22:19:00&#39;),</span><br><span class="line">(&#39;D&#39;,&#39;A&#39;,0,&#39;2019-07-19 22:19:00&#39;),</span><br><span class="line">(&#39;C&#39;,&#39;B&#39;,0,&#39;2019-07-19 23:19:00&#39;),</span><br><span class="line">(&#39;B&#39;,&#39;C&#39;,1,&#39;2019-07-19 23:19:00&#39;);</span><br></pre></td></tr></table></figure><p>数据包含4列，分别为 user_name，opponent，result，create_time。 我们将基于这些数据来介绍下窗口函数的一些使用场景。<br><br><br><strong>原始数据</strong><br>**<br>user_name opponent result create_time<br>A B 1 2019-07-18 23:19:00<br>B A 0 2019-07-18 23:19:00<br>A C 0 2019-07-18 23:20:00<br>C A 1 2019-07-18 23:20:00<br>A D 1 2019-07-19 22:19:00<br>D A 0 2019-07-19 22:19:00<br>C B 0 2019-07-19 23:19:00<br>B C 1 2019-07-19 23:19:00<br></p><p><a name="OOjek"></a></p><h2 id="聚合型窗口函数"><a href="#聚合型窗口函数" class="headerlink" title="聚合型窗口函数"></a>聚合型窗口函数</h2><p><br>聚合型即SUM(), MIN(),MAX(),AVG(),COUNT()这些常见的聚合函数。 聚合函数配合窗口函数使用可以使计算更加灵活，例如以下场景：<br></p><ol><li><p>至今累计分数<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select *, sum(result) over(partition by user_name order by create_time) result_sum from user_match_temp;</span><br><span class="line"></span><br><span class="line">user_name    opponent   result create_timeresult_sum</span><br><span class="line">AB12019-07-18 23:19:001</span><br><span class="line">AC02019-07-18 23:20:001</span><br><span class="line">AD12019-07-19 22:19:002</span><br><span class="line">BA02019-07-18 23:19:000</span><br><span class="line">BC12019-07-19 23:19:001</span><br><span class="line">CA12019-07-18 23:20:001</span><br><span class="line">CB02019-07-19 23:19:001</span><br><span class="line">DA02019-07-19 22:19:000</span><br></pre></td></tr></table></figure><br></li><li><p>之前3场平均胜场</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (test)&gt; SELECT *,avg(result) over (partition by user_name order by create_time rows between 3 preceding and current row) as recently_wins from user_match_temp;</span><br><span class="line"></span><br><span class="line">user_match_temp.user_nameuser_match_temp.opponentuser_match_temp.resultuser_match_temp.create_timerecently_wins</span><br><span class="line">AB12019-07-18 23:19:001.0</span><br><span class="line">AC02019-07-18 23:20:000.5</span><br><span class="line">AD12019-07-19 22:19:000.6666666666666666</span><br><span class="line">BA02019-07-18 23:19:000.0</span><br><span class="line">BC12019-07-19 23:19:000.5</span><br><span class="line">CA12019-07-18 23:20:001.0</span><br><span class="line">CB02019-07-19 23:19:000.5</span><br><span class="line">DA02019-07-19 22:19:000.0</span><br></pre></td></tr></table></figure></li></ol><p><br>我们通过rows between 即可定义窗口的范围，这里我们定义了窗口的范围为之前3行到该行。</p><ol start="3"><li>累计遇到的对手数量</li></ol><p>需要注意的是count(distinct xxx)在窗口函数里是不允许使用的，不过我们也可以用size(collect_set() over(partition by order by))来替代实现我们的需求</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (test)&gt; SELECT *,size(collect_set(opponent) over (partition by user_name order by create_time)) as recently_wins from user_match_temp;</span><br><span class="line"></span><br><span class="line">user_match_temp.user_nameuser_match_temp.opponentuser_match_temp.resultuser_match_temp.create_timerecently_wins</span><br><span class="line">AB12019-07-18 23:19:001</span><br><span class="line">AC02019-07-18 23:20:002</span><br><span class="line">AD12019-07-19 22:19:003</span><br><span class="line">BA02019-07-18 23:19:001</span><br><span class="line">BC12019-07-19 23:19:002</span><br><span class="line">CA12019-07-18 23:20:001</span><br><span class="line">CB02019-07-19 23:19:002</span><br><span class="line">DA02019-07-19 22:19:001</span><br></pre></td></tr></table></figure><p>collect_set()也是一个聚合函数，作用是将多行聚合进一行的某个set内，再用size()统计集合内的元素个数，即可实现我们的需求。<br></p><p><a name="gpmGg"></a></p><h2 id="分析型窗口函数"><a href="#分析型窗口函数" class="headerlink" title="分析型窗口函数"></a>分析型窗口函数</h2><p>分析型即RANk(),ROW_NUMBER(),DENSE_RANK()等常见的排序用的窗口函数，不过他们也是有区别的。<br><strong><br></strong>排名函数不支持window子句，即不支持自定义窗口大小**</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (test)&gt; SELECT *,rank() over (order by create_time) as user_rank,row_number() over (order by create_time) as user_row_number,dense_rank() over (order by create_time) as user_dense_rank FROM user_match_temp;</span><br><span class="line"></span><br><span class="line">user_name opponentresultcreate_timeuser_rankuser_row_numberuser_dense_rank</span><br><span class="line">BA02019-07-18 23:19:00111</span><br><span class="line">AB12019-07-18 23:19:00121</span><br><span class="line">CA12019-07-18 23:20:00332</span><br><span class="line">AC02019-07-18 23:20:00342</span><br><span class="line">DA02019-07-19 22:19:00553</span><br><span class="line">AD12019-07-19 22:19:00563</span><br><span class="line">BC12019-07-19 23:19:00774</span><br><span class="line">CB02019-07-19 23:19:00784</span><br></pre></td></tr></table></figure><p>如上所示： row_number函数：生成连续的序号（相同元素序号相同）；<br>rank函数：如两元素排序相同则序号相同，并且会跳过下一个序号；<br>dense_rank函数：如两元素排序相同则序号相同，不会跳过下一个序号；<br><br><br>除了这三个排序用的函数，还有 _CUME_DIST函数 ：小于等于当前值的行在所有行中的占比 _PERCENT_RANK() ：小于当前值的行在所有行中的占比 * NTILE() ：如果把数据按行数分为n份，那么该行所属的份数是第几份 这三种窗口函数 sql如下：<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT *,  CUME_DIST() over (order by create_time) as user_CUME_DIST, PERCENT_RANK() over (order by create_time) as user_PERCENT_RANK, NTILE(3) over (order by create_time) as user_NTILE FROM user_match_temp;</span><br></pre></td></tr></table></figure><br><a name="4yXY7"></a> ## 取值型窗口函数 这几个函数可以通过字面意思记得，LAG是迟滞的意思，也就是对某一列进行往后错行；LEAD是LAG的反义词，也就是对某一列进行提前几行；FIRST_VALUE是对该列到目前为止的首个值，而LAST_VALUE是到目前行为止的最后一个值。<br><br>LAG()和LEAD() 可以带3个参数，第一个是返回的值，第二个是前置或者后置的行数，第三个是默认值。<br>下一个对手，上一个对手，最近3局的第一个对手及最后一个对手，如下：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> SELECT *,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     lag(opponent,1) </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">         over (partition by user_name order by create_time) as lag_opponent,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     lead(opponent,1) over </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">         (partition by user_name order by create_time) as lead_opponent,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     first_value(opponent) over (partition by user_name order by create_time rows hive&gt;         between 3 preceding and 3 following) as first_opponent,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     last_value(opponent) over (partition by user_name order by create_time rows hive&gt;         between 3 preceding and 3 following) as last_opponent</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> From user_match_temp;</span></span><br></pre></td></tr></table></figure><br><br><br>参考文章：[https://blog.csdn.net/czr11616/article/details/101645693](https://blog.csdn.net/czr11616/article/details/101645693)<br>[https://zhuanlan.zhihu.com/p/77705681](https://zhuanlan.zhihu.com/p/77705681)<br>[https://blog.csdn.net/qq_37296285/article/details/90940591](https://blog.csdn.net/qq_37296285/article/details/90940591)<!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;e05dce83&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了（二）</title>
    <link href="cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2019-09-09T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:14.168Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。<br><br><br>由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。<br>这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><br>映入眼帘的是满屏的扫描和ssh尝试登陆<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">lines <span class="number">1105</span><span class="number">-1127</span>/<span class="number">1127</span> (END)</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">49</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><p><br>看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护<br>对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@*** ~]<span class="comment"># useradd one             #创建用户</span></span><br><span class="line">[root@*** ~]<span class="comment"># passwd one              #设置密码</span></span><br></pre></td></tr></table></figure><p><br>输入新用户密码<br>首先确保文件 /etc/sudoers 中<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%wheel    ALL=(ALL)    ALL</span><br><span class="line">```  </span><br><span class="line">没有被注释</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```linux</span><br><span class="line">usermod -g wheel onerocket</span><br></pre></td></tr></table></figure><p><br>设置只有指定用户组才能使用su命令切换到root用户<br><br><br>在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"><span class="comment"># Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限</span></span><br><span class="line">sed -i <span class="string">'/pam_wheel.so use_uid/c\auth            required        pam_wheel.so use_uid '</span> /etc/pam.d/su</span><br><span class="line">n=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`</span><br><span class="line"><span class="keyword">if</span> [ $n -eq <span class="number">0</span> ];then</span><br><span class="line">echo SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defs</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><br>打开SSHD的配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。<br><br><br>修改sshd默认端口<br>虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。<br>打开sshd配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>找到#Port 22 删掉注释<br><br><br><em>服务器端口最大可以开到65536</em><br><br><br>同时再添加一个Port 61024 （随意设置）<br><br><br>Port 22<br>Port 61024<br><br><br>重启sshd服务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service sshd restart      <span class="comment">#centos6系列</span></span><br><span class="line">systemctl restart sshd  <span class="comment">#centos7系列</span></span><br><span class="line">firewall-cmd --add-port=<span class="number">61024</span>/tcp</span><br></pre></td></tr></table></figure><p><br>测试，使用新用户，新端口进行登录<br><br><br>如果登陆成功后，再将Port22注释掉，重启sshd服务。<br>到这里，关于远程登录的防护工作，就做好了。<br>最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了</title>
    <link href="cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/"/>
    <id>cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/</id>
    <published>2019-08-24T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:09.871Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h1 id="服务器自述"><a href="#服务器自述" class="headerlink" title="服务器自述"></a>服务器自述</h1><p>我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。<br><br><br>而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢<br><a name="-3"></a></p><h1 id="为什么黑我的服务器"><a href="#为什么黑我的服务器" class="headerlink" title="为什么黑我的服务器"></a>为什么黑我的服务器</h1><p>这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。<br><br><br>但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。<br><br><br>那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…<br><br><br>那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404954445-f30a25a0-5939-4773-b5d9-8bb6a7c53b02.png#align=left&display=inline&height=892&name=1.png&originHeight=892&originWidth=1244&size=1778564&status=done&style=none&width=1244" alt="1.png"><br><br><br>曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器<br><a name="-4"></a></p><h1 id="如何发现被黑"><a href="#如何发现被黑" class="headerlink" title="如何发现被黑"></a>如何发现被黑</h1><p>回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？<br><br><br>最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作<br><br><br>1.关闭了Linux防火墙<br><br><br>2.没有设置redis访问密码<br><br><br>3.没有更改redis默认端口<br><br><br>4.开放了任意IP可以远程连接<br><br><br>以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。<br><br><br>当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。<br><br><br>但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i tcp:<span class="number">6379</span></span><br></pre></td></tr></table></figure><p><br>使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。<br><br><br>但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想<br>直到我运行：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404912911-97736d80-2ee3-455d-a948-6d134f4e2663.png#align=left&display=inline&height=534&name=2.png&originHeight=534&originWidth=3338&size=980508&status=done&style=none&width=3338" alt="2.png"><br>发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了<br><a name="-5"></a></p><h1 id="做了哪些急救工作"><a href="#做了哪些急救工作" class="headerlink" title="做了哪些急救工作"></a>做了哪些急救工作</h1><p>这时，感觉自己开始投入了一场对抗战<br><br><br>1.首先查找植入程序的位置。<br>在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。<br><br><br>2.删除清理可疑文件<br><br><br>杀死进程<br><br><br>删除了正在运行的程序文件还有安装包<br>3.查看所有用户的定时任务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd |cut -f <span class="number">1</span> -d:crontab -uXXX -l</span><br></pre></td></tr></table></figure><p><br>4.开启防火墙<br><br><br>仅开放会使用到的端口<br>5.修改redis默认端口<br><br><br>redis.conf中的port<br>6.添加redis授权密码<br><br><br>redis.conf中的requirepass<br>7.修改绑定远程绑定ip<br><br><br>redis.conf中的bind<br>最后重启redis服务！<br><a name="-6"></a></p><h1 id="从中学到了什么"><a href="#从中学到了什么" class="headerlink" title="从中学到了什么"></a>从中学到了什么</h1><p>明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～<br>最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;服务器自述&quot;&gt;&lt;a href=&quot;#服务器自述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink结合Kafka构建端到端的Exactly-Once处理</title>
    <link href="cpeixin.cn/2019/07/04/Apache-Flink%E7%BB%93%E5%90%88Kafka%E6%9E%84%E5%BB%BA%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84Exactly-Once%E5%A4%84%E7%90%86/"/>
    <id>cpeixin.cn/2019/07/04/Apache-Flink%E7%BB%93%E5%90%88Kafka%E6%9E%84%E5%BB%BA%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84Exactly-Once%E5%A4%84%E7%90%86/</id>
    <published>2019-07-03T17:06:47.000Z</published>
    <updated>2020-07-03T17:07:49.898Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>Apache Flink自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFunction（<a href="https://issues.apache.org/jira/browse/FLINK-7210" target="_blank" rel="external nofollow noopener noreferrer">相关的Jira</a>）。它提取了两阶段提交协议的通用逻辑，使得通过Flink来构建端到端的Exactly-Once程序成为可能。同时支持一些数据源（source）和输出端（sink），包括Apache Kafka 0.11及更高版本。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的Exactly-Once语义。<br><br><br>有关TwoPhaseCommitSinkFunction的使用详见文档: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html" target="_blank" rel="external nofollow noopener noreferrer">TwoPhaseCommitSinkFunction</a>。或者可以直接阅读Kafka 0.11 sink的文档: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="external nofollow noopener noreferrer">kafka</a>。<br>接下来会详细分析这个新功能以及Flink的实现逻辑，分为如下几点。</p><ul><li><p>描述Flink checkpoint机制是如何保证Flink程序结果的Exactly-Once的</p></li><li><p>显示Flink如何通过两阶段提交协议与数据源和数据输出端交互，以提供端到端的Exactly-Once保证</p></li><li><p>通过一个简单的示例，了解如何使用TwoPhaseCommitSinkFunction实现Exactly-Once的文件输出<br><a name="YwJZF"></a></p><h2 id="Apache-Flink应用程序中的Exactly-Once语义"><a href="#Apache-Flink应用程序中的Exactly-Once语义" class="headerlink" title="Apache Flink应用程序中的Exactly-Once语义"></a>Apache Flink应用程序中的Exactly-Once语义</h2><p>当我们说『Exactly-Once』时，指的是每个输入的事件只影响最终结果一次。即使机器或软件出现故障，既没有重复数据，也不会丢数据。<br><br><br><a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>很久之前就提供了Exactly-Once语义。在过去几年中，我们对Flink的<a href="https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink" target="_blank" rel="external nofollow noopener noreferrer">checkpoint机制</a>有过深入的描述，这是Flink有能力提供Exactly-Once语义的核心。Flink文档还提供了该功能的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html" target="_blank" rel="external nofollow noopener noreferrer">全面概述</a>。<br><br><br>在继续之前，先看下对checkpoint机制的简要介绍，这对理解后面的主题至关重要。<br><strong>一次checkpoint是以下内容的一致性快照</strong>：</p></li><li><p>应用程序的当前状态, eg: pv{‘app_1’:5000, ‘app_2’:6000}</p></li><li><p>输入流的位置, eg: offset:200</p></li></ul><p><br>Flink可以配置一个固定的时间点，定期产生checkpoint，将checkpoint的数据写入持久存储系统，例如S3或HDFS。将<strong>checkpoint数据写入持久存储是异步发生</strong>的，这意味着Flink应用程序在checkpoint过程中可以继续处理数据。<br><br><br>如果发生机器或软件故障，重新启动后，<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>应用程序将从最新的checkpoint点恢复处理； Flink会恢复应用程序状态，将输入流回滚到上次checkpoint保存的位置，然后重新开始运行。这意味着Flink可以像从未发生过故障一样计算结果。<br><br><br><strong>在Flink 1.4.0之前，Exactly-Once语义仅限于Flink应用程序内部</strong>，并没有扩展到Flink数据处理完后发送的大多数外部系统。Flink应用程序与各种数据输出端进行交互，开发人员需要有能力自己维护组件的上下文来保证Exactly-Once语义。<br><br><br>为了提供端到端的Exactly-Once语义，也就是说，除了Flink应用程序内部，Flink写入的外部系统也需要能满足Exactly-Once语义 ，这些外部系统必须提供提交或回滚的方法，然后通过Flink的checkpoint机制来协调。<br><br><br>分布式系统中，协调提交和回滚的常用方法是<a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external nofollow noopener noreferrer">两阶段提交协议</a>。在下一节中，我们将讨论Flink的<strong>TwoPhaseCommitSinkFunction</strong>是如何利用<strong>两阶段提交协议来提供端到端的Exactly-Once语义</strong>。<br><a name="JmXiE"></a></p><h2><a href="#" class="headerlink"></a></h2><p><a name="wiATx"></a></p><h2 id="Flink应用程序端到端的Exactly-Once语义"><a href="#Flink应用程序端到端的Exactly-Once语义" class="headerlink" title="Flink应用程序端到端的Exactly-Once语义"></a>Flink应用程序端到端的Exactly-Once语义</h2><p>我们将介绍两阶段提交协议，以及它如何在一个读写Kafka的Flink程序中实现端到端的Exactly-Once语义。<br><br><br>Kafka是一个流行的消息中间件，经常与Flink一起使用。Kafka在最近的0.11版本中添加了对事务的支持。这意味着现在通过Flink读写Kafaka，并提供<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="external nofollow noopener noreferrer">端到端的Exactly-Once语义有了必要的支持</a>。<br><br><br><a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>对端到端的Exactly-Once语义的支持不仅局限于Kafka，您可以将它与任何一个提供了必要的协调机制的源/输出端一起使用。例如<a href="http://pravega.io/" target="_blank" rel="external nofollow noopener noreferrer">Pravega</a>，来自DELL/EMC的开源流媒体存储系统，通过Flink的TwoPhaseCommitSinkFunction也能支持端到端的Exactly-Once语义。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122306.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122306.png#align=left&display=inline&height=1208&margin=%5Bobject%20Object%5D&originHeight=1208&originWidth=2154&status=done&style=none&width=2154" alt></a><br>在今天讨论的这个示例程序中，我们有：</p><ul><li>从Kafka读取的数据源（Flink内置的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-consumer" target="_blank" rel="external nofollow noopener noreferrer">KafkaConsumer</a>）</li><li>窗口聚合</li><li>将数据写回Kafka的数据输出端（Flink内置的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-producer" target="_blank" rel="external nofollow noopener noreferrer">KafkaProducer</a>）</li></ul><p><br>要使数据<strong>输出端</strong>提供Exactly-Once保证，它必须将所有数据通过一个事务提交给Kafka。提交捆绑了<strong>两个checkpoint</strong>之间的所有要写入的数据。这可确保在发生故障时能回滚写入的数据。但是在分布式系统中，通常会有多个并发运行的写入任务的，简单的提交或回滚是不够的，因为所有组件必须在提交或回滚时“一致”才能确保一致的结果。Flink使用两阶段提交协议及预提交阶段来解决这个问题。<br><br><br>在checkpoint开始的时候，即两阶段提交协议的“预提交”阶段。当checkpoint开始时，Flink的JobManager会将checkpoint barrier（将数据流中的记录分为进入当前checkpoint与进入下一个checkpoint）注入数据流。<br>brarrier在operator之间传递。对于每一个operator，它触发operator的状态快照写入到state backend。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122328.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122328.png#align=left&display=inline&height=1202&margin=%5Bobject%20Object%5D&originHeight=1202&originWidth=2146&status=done&style=none&width=2146" alt></a><br>数据源保存了消费Kafka的偏移量(offset)，之后将checkpoint barrier传递给下一个operator。<br>这种方式仅适用于operator具有『内部』状态。所谓内部状态，是指Flink state backend保存和管理的。<br><br><br>例如，第二个operator中window聚合算出来的sum值。当一个进程有它的内部状态的时候，除了在checkpoint之前需要将数据变更写入到state backend，不需要在预提交阶段执行任何其他操作。<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>负责在checkpoint成功的情况下正确提交这些写入，或者在出现故障时中止这些写入。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122346.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122346.png#align=left&display=inline&height=1210&margin=%5Bobject%20Object%5D&originHeight=1210&originWidth=2152&status=done&style=none&width=2152" alt></a><br><a name="zhApV"></a></p><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><p><a name="Q1EPD"></a></p><h2 id="Flink应用程序启动预提交阶段"><a href="#Flink应用程序启动预提交阶段" class="headerlink" title="Flink应用程序启动预提交阶段"></a>Flink应用程序启动预提交阶段</h2><p>但是，当进程具有『外部』状态时，需要作些额外的处理。外部状态通常以写入外部系统（如Kafka）的形式出现。在这种情况下，为了提供Exactly-Once保证，外部系统必须支持事务，这样才能和两阶段提交协议集成。<br><br><br>在本文示例中的数据需要写入Kafka，因此数据输出端（Data Sink）有外部状态。在这种情况下，在预提交阶段，<strong>除了将其状态写入state backend之外，数据输出端还必须预先提交其外部事务。</strong><br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122406.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122406.png#align=left&display=inline&height=1212&margin=%5Bobject%20Object%5D&originHeight=1212&originWidth=2154&status=done&style=none&width=2154" alt></a><br><br><br>当checkpoint barrier在所有operator都传递了一遍，并且触发的checkpoint回调成功完成时，预提交阶段就结束了。所有触发的状态快照都被视为该checkpoint的一部分。checkpoint是整个应用程序状态的快照，包括预先提交的外部状态。如果发生故障，我们可以回滚到上次成功完成快照的时间点。<br><br><br>下一步是通知所有operator，checkpoint已经成功了。这是两阶段提交协议的提交阶段，JobManager为应用程序中的每个operator发出checkpoint已完成的回调。<br><br><br>数据源和widnow operator没有外部状态，因此在提交阶段，这些operator不必执行任何操作。但是，数据输出端（Data Sink）拥有外部状态，此时应该提交外部事务。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122426.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122426.png#align=left&display=inline&height=1208&margin=%5Bobject%20Object%5D&originHeight=1208&originWidth=2156&status=done&style=none&width=2156" alt></a><br>我们对上述知识点总结下：</p><ul><li>一旦所有operator完成预提交，就提交一个commit。</li><li>如果至少有一个预提交失败，则所有其他提交都将中止，我们将回滚到上一个成功完成的checkpoint。</li><li>在预提交成功之后，提交的commit需要保证最终成功 ，operator和外部系统都需要保障这点。如果commit失败（例如，由于间歇性网络问题），整个<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>应用程序将失败，应用程序将根据用户的重启策略重新启动，还会尝试再提交。这个过程至关重要，因为如果commit最终没有成功，将会导致数据丢失。</li></ul><p>因此，我们可以确定所有operator都同意checkpoint的最终结果：所有operator都同意数据已提交，或提交被中止并回滚。<br></p><p><a name="8nafb"></a></p><h2 id="在Flink中实现两阶段提交Operator"><a href="#在Flink中实现两阶段提交Operator" class="headerlink" title="在Flink中实现两阶段提交Operator"></a>在Flink中实现两阶段提交Operator</h2><p>完整的实现两阶段提交协议可能有点复杂，这就是为什么<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>将它的通用逻辑提取到抽象类TwoPhaseCommitSinkFunction中的原因。<br><br><br>接下来基于输出到文件的简单示例，说明如何使用TwoPhaseCommitSinkFunction。用户只需要实现四个函数，就能为数据输出端实现Exactly-Once语义：</p><ul><li>beginTransaction - 在事务开始前，我们在目标文件系统的临时目录中创建一个临时文件。随后，我们可以在处理数据时将数据写入此文件。</li><li>preCommit - 在预提交阶段，我们刷新文件到存储，关闭文件，不再重新写入。我们还将为属于下一个checkpoint的任何后续文件写入启动一个新的事务。</li><li>commit - 在提交阶段，我们将预提交阶段的文件原子地移动到真正的目标目录。需要注意的是，这会增加输出数据可见性的延迟。</li><li>abort - 在中止阶段，我们删除临时文件。</li></ul><p><br>我们知道，如果发生任何故障，Flink会将应用程序的状态恢复到最新的一次checkpoint点。一种极端的情况是，预提交成功了，但在这次commit的通知到达operator之前发生了故障。在这种情况下，Flink会将operator的状态恢复到已经预提交，但尚未真正提交的状态。<br><br><br>我们需要在预提交阶段保存足够多的信息到checkpoint状态中，以便在重启后能正确的中止或提交事务。在这个例子中，这些信息是临时文件和目标目录的路径。<br><br><br>TwoPhaseCommitSinkFunction已经把这种情况考虑在内了，并且在从checkpoint点恢复状态时，会优先发出一个commit。我们需要以幂等方式实现提交，一般来说，这并不难。在这个示例中，我们可以识别出这样的情况：临时文件不在临时目录中，但已经移动到目标目录了。<br><br><br>在TwoPhaseCommitSinkFunction中，还有一些其他边界情况也会考虑在内，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html" target="_blank" rel="external nofollow noopener noreferrer">Flink文档</a>了解更多信息。<br><a name="FX1br"></a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总结下本文涉及的一些要点：</p><ul><li><a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>的checkpoint机制是支持两阶段提交协议并提供端到端的Exactly-Once语义的基础。</li><li>这个方案的优点是: Flink不像其他一些系统那样，通过网络传输存储数据 - 不需要像大多数批处理程序那样将计算的每个阶段写入磁盘。</li><li>Flink的TwoPhaseCommitSinkFunction提取了两阶段提交协议的通用逻辑，基于此将Flink和支持事务的外部系统结合，构建端到端的Exactly-Once成为可能。</li><li>从<a href="https://data-artisans.com/blog/announcing-the-apache-flink-1-4-0-release" target="_blank" rel="external nofollow noopener noreferrer">Flink 1.4.0</a>开始，Pravega和Kafka 0.11 producer都提供了Exactly-Once语义；Kafka在0.11版本首次引入了事务，为在Flink程序中使用Kafka producer提供Exactly-Once语义提供了可能性。</li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="external nofollow noopener noreferrer">Kafaka 0.11 producer</a>的事务是在TwoPhaseCommitSinkFunction基础上实现的，和at-least-once producer相比只增加了非常低的开销。</li></ul><p>这是个令人兴奋的功能，期待Flink TwoPhaseCommitSinkFunction在未来支持更多的数据接收端。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Apache Flink自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFu
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="cpeixin.cn/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>一文搞懂Flink内部的Exactly Once和At Least Once</title>
    <link href="cpeixin.cn/2019/07/03/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82Flink%E5%86%85%E9%83%A8%E7%9A%84Exactly-Once%E5%92%8CAt-Least-Once/"/>
    <id>cpeixin.cn/2019/07/03/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82Flink%E5%86%85%E9%83%A8%E7%9A%84Exactly-Once%E5%92%8CAt-Least-Once/</id>
    <published>2019-07-03T13:24:51.000Z</published>
    <updated>2020-07-03T13:26:50.945Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p><a name="gI6Sf"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Rg5ep"></a></p><h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><ul><li>介绍CheckPoint如何保障Flink任务的高可用</li><li>CheckPoint中的状态简介</li><li>如何实现全域一致的分布式快照？</li><li>什么是barrier？什么是barrier对齐？</li><li>为什么barrier对齐就是Exactly Once？为什么barrier不对齐就是 At Least Once？</li></ul><p><a name="pCF3J"></a></p><h2 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h2><p>Apache Flink® - Stateful Computations over Data Streams<br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fflink.apache.org%2Fzh%2F" target="_blank" rel="external nofollow noopener noreferrer">Apache Flink® - 数据流上的有状态计算</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fci.apache.org%2Fprojects%2Fflink%2Fflink-docs-release-1.8%2F" target="_blank" rel="external nofollow noopener noreferrer">Flink 1.8 Document</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fci.apache.org%2Fprojects%2Fflink%2Fflink-docs-release-1.8%2Fdev%2Fstream%2Fstate%2F" target="_blank" rel="external nofollow noopener noreferrer">State &amp; Fault Tolerance</a><br><br><br>有状态函数和运算符在各个元素/事件的处理中存储数据（状态数据可以修改和查询，可以自己维护，根据自己的业务场景，保存历史数据或者中间结果到状态中）</p><p>例如：</p><ul><li>当应用程序搜索某些事件模式时，状态将存储到目前为止遇到的事件序列。</li><li>在每分钟/小时/天聚合事件时，状态保存待处理的聚合。</li><li>当在数据点流上训练机器学习模型时，状态保持模型参数的当前版本。</li><li>当需要管理历史数据时，状态允许有效访问过去发生的事件。</li></ul><p><br>什么是状态？</p><ul><li>无状态计算的例子<ul><li>比如：我们只是进行一个字符串拼接，输入 a，输出 a_666,输入b，输出 b_666</li><li>输出的结果跟之前的状态没关系，符合幂等性。<ul><li>幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用</li></ul></li></ul></li><li>有状态计算的例子<ul><li>计算pv、uv</li><li>输出的结果跟之前的状态有关系，不符合幂等性，访问多次，pv会增加<br><a name="BFHz0"></a><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><a name="koz7u"></a><h2 id="Flink的CheckPoint功能简介"><a href="#Flink的CheckPoint功能简介" class="headerlink" title="Flink的CheckPoint功能简介"></a>Flink的CheckPoint功能简介</h2>Flink CheckPoint 的存在就是为了解决flink任务failover掉之后，能够正常恢复任务。那CheckPoint具体做了哪些功能，为什么任务挂掉之后，通过CheckPoint能使得任务恢复呢？</li></ul></li></ul><p>CheckPoint是通过给程序快照的方式使得将历史某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。问题来了，快照是什么鬼？能吃吗？<br><br><br>SnapShot翻译为快照，指将程序中某些信息存一份，后期可以用来恢复。对于一个Flink任务来讲，快照里面到底保存着什么信息呢？</p><p>晦涩难懂的概念怎么办？当然用案例来代替咯，用案例让大家理解快照里面到底存什么信息。选一个大家都比较清楚的指标，app的pv，flink该怎么统计呢？<br><br><br>我们从Kafka读取到一条条的日志，从日志中解析出app_id，然后将统计的结果放到内存中一个Map集合，app_id做为key，对应的pv做为value，每次只需要将相应app_id 的pv值+1后put到Map中即可<br><img src="//upload-images.jianshu.io/upload_images/19063731-073db38513bf8b50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=317&margin=%5Bobject%20Object%5D&originHeight=317&originWidth=1200&status=done&style=none&width=1200" alt><br>flink的Source task记录了当前消费到kafka <code>test</code> topic的所有partition的offset，为了方便理解CheckPoint的作用，这里先用一个partition进行讲解，假设名为 “test”的 topic只有一个partition0<br><br><br>例：（0，1000）,表示0号partition目前消费到offset为1000的数据<br><br><br>flink的pv task记录了当前计算的各app的pv值，为了方便讲解，我这里有两个app：app1、app2<br><br><br>例：（app1，50000）（app2，10000），表示app1当前pv值为50000 ，表示app2当前pv值为10000，每来一条数据，只需要确定相应app_id，将相应的value值+1后put到map中即可<br><br><br><strong>该案例中，CheckPoint到底记录了什么信息呢？</strong><br><br><br>记录的其实就是第n次CheckPoint消费的offset信息和各app的pv值信息，记录一下发生CheckPoint当前的状态信息，并将该状态信息保存到相应的状态后端。（注：<strong>状态后端是保存状态的地方</strong>，决定状态如何保存，如何保障状态高可用，我们只需要知道，我们能从状态后端拿到offset信息和pv信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过checkpoint来恢复我们的应用程序）<br><br><br>chk-100：{offset：（0，1000）pv：（app1，50000）（app2，10000）}<br><br><br>该状态信息表示第100次CheckPoint的时候， partition 0 offset消费到了1000，pv统计结果为（app1，50000）（app2，10000）<br><br><br><strong>任务挂了，如何恢复？</strong><br><br><br>假如我们设置了三分钟进行一次CheckPoint，保存了上述所说的 chk-100 的CheckPoint状态后，过了十秒钟，offset已经消费到 （0，1100），pv统计结果变成了（app1，50080）（app2，10020），但是突然任务挂了，怎么办？<br><br><br>莫慌，其实很简单，flink只需要从最近一次成功的CheckPoint保存的offset（0，1000）处接着消费即可，当然pv值也要按照状态里的pv值（app1，50000）（app2，10000）进行累加，不能从（app1，50080）（app2，10020）处进行累加，因为 <strong>partition 0 offset消费到 1000时，pv统计结果为（app1，50000）（app2，10000）</strong><br><br><br>当然如果你想从offset （0，1100）pv（app1，50080）（app2，10020）这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来。我们能做的最高效方式就是从最近一次成功的CheckPoint处恢复，也就是我一直所说的chk-100,以上讲解，基本就是CheckPoint承担的工作，描述的场景比较简单<br><br><br>疑问，计算pv的task在一直运行，它怎么知道什么时候去做这个快照？或者说计算pv的task怎么保障它自己计算的pv值（app1，50000）（app2，10000）就是offset（0，1000）那一刻的统计结果呢？<br><br><br>flink是在数据中加了一个叫做barrier的东西（barrier中文翻译：栅栏），下图中红圈处就是两个barrier<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1593765235849-e46de314-3eac-414e-af6c-753ae3e0a6d4.png#align=left&display=inline&height=534&margin=%5Bobject%20Object%5D&name=%E6%88%AA%E5%B1%8F2020-07-03%20%E4%B8%8B%E5%8D%884.33.18.png&originHeight=534&originWidth=1314&size=271951&status=done&style=none&width=1314" alt="截屏2020-07-03 下午4.33.18.png"><br><br><br><strong>barrier从Source Task处生成，一直流到Sink Task，期间所有的Task只要碰到barrier，就会触发自身进行快照</strong><br><strong>CheckPoin</strong>t , barrier n-1处做的快照就是指Job从开始处理到 barrier n-1所有的状态数据, barrier n 处做的快照就是指从Job开始到处理到 barrier n所有的状态数据<br><br><br>对应到pv案例中就是，<strong>Source Task接收到JobManager</strong>的编号为chk-100的CheckPoint触发请求后，发现自己恰好接收到kafka offset（0，1000）处的数据，<strong>所以会往offset（0，1000）数据之后offset（0，1001）数据之前安插一个barrier，然后自己开始做快照</strong>，也就是将offset（0，1000）保存到状态后端chk-100中。然后barrier接着往下游发送，当统计pv的task接收到barrier后，也会暂停处理数据，将自己内存中保存的pv信息（app1，50000）（app2，10000）保存到状态后端chk-100中。OK，flink大概就是通过这个原理来保存快照的。统计pv的task接收到barrier，就意味着barrier之前的数据都处理了，所以说，不会出现丢数据的情况<br><br><br>barrier的作用就是为了把数据区分开，CheckPoint过程中有一个同步做快照的环节不能处理barrier之后的数据，为什么呢？<br><br><br>如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据<br><br><br>结合案例来讲就是，统计pv的task想对（app1，50000）（app2，10000）做快照，但是如果数据还在处理，可能快照还没保存下来，状态已经变成了（app1，50001）（app2，10001），快照就不准确了，就不能保障Exactly Once了<br><br><br><strong>总结</strong><br><br><br>流式计算中状态交互<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1593765988323-23515639-93ef-4b8e-a6b9-e0de3c03dab7.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=%E6%88%AA%E5%B1%8F2020-07-03%20%E4%B8%8B%E5%8D%884.45.45.png&originHeight=720&originWidth=1426&size=313470&status=done&style=none&width=1426" alt="截屏2020-07-03 下午4.45.45.png"><br><br><br>简易场景精确一次的容错方法，周期性地对消费offset和统计的状态信息或统计结果进行快照<br><img src="//upload-images.jianshu.io/upload_images/19063731-76237293b1551a9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=395&margin=%5Bobject%20Object%5D&originHeight=395&originWidth=1200&status=done&style=none&width=1200" alt></p><p>消费到X位置的时候，将X对应的状态保存下来<br><img src="//upload-images.jianshu.io/upload_images/19063731-ad52ba2a04f78ae6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&originHeight=378&originWidth=1200&status=done&style=none&width=1200" alt></p><p>消费到Y位置的时候，将Y对应的状态保存下来<br><img src="//upload-images.jianshu.io/upload_images/19063731-b2a6809920a08ccf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=385&margin=%5Bobject%20Object%5D&originHeight=385&originWidth=1200&status=done&style=none&width=1200" alt></p><p><a name="ZQi6z"></a></p><h2 id="多并行度、多Operator情况下，CheckPoint过程"><a href="#多并行度、多Operator情况下，CheckPoint过程" class="headerlink" title="多并行度、多Operator情况下，CheckPoint过程"></a>多并行度、多Operator情况下，CheckPoint过程</h2><p><br><strong>分布式状态容错面临的问题与挑战</strong></p><ul><li>如何确保状态拥有<strong>精确一次</strong>的容错保证？</li><li>如何在分布式场景下替多个拥有本地状态的算子产生<strong>一个全域一致的快照</strong>？</li><li>如何在<strong>不中断运算</strong>的前提下产生快照？</li></ul><p><br><strong>多并行度、多Operator实例的情况下，如何做全域一致的快照</strong><br><br><br>所有的Operator运行过程中遇到barrier后，都对自身的状态进行一次快照，保存到相应状态后端<br><br><br>对应到pv案例：有的Operator计算的app1的pv，有的Operator计算的app2的pv，当他们碰到barrier时，都需要将目前统计的pv信息快照到状态后端<br><img src="//upload-images.jianshu.io/upload_images/19063731-958537f5f6d20593.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=520&margin=%5Bobject%20Object%5D&originHeight=520&originWidth=1200&status=done&style=none&width=1200" alt></p><p>多Operator状态恢复<br><img src="//upload-images.jianshu.io/upload_images/19063731-52083c801724bae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=541&margin=%5Bobject%20Object%5D&originHeight=541&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><strong>具体怎么做这个快照呢？</strong><br><br><br>利用之前所讲的barrier策略<br><img src="//upload-images.jianshu.io/upload_images/19063731-cc2a3e70cc211583.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=431&margin=%5Bobject%20Object%5D&originHeight=431&originWidth=1200&status=done&style=none&width=1200" alt></p><p>JobManager向Source Task发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier<br><img src="//upload-images.jianshu.io/upload_images/19063731-2907015ba67cf908.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&originHeight=489&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><br><br>Source Task自身做快照，并保存到状态后端<br><img src="//upload-images.jianshu.io/upload_images/19063731-70bdf95950e1fb07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=525&margin=%5Bobject%20Object%5D&originHeight=525&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><br><br>Source Task将barrier跟数据流一块往下游发送<br><img src="//upload-images.jianshu.io/upload_images/19063731-36fc49196f2592f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=513&margin=%5Bobject%20Object%5D&originHeight=513&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><br><br>当下游的Operator实例接收到CheckPoint barrier后，对自身做快照<br><img src="//upload-images.jianshu.io/upload_images/19063731-ccf2246af6bb22a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=528&margin=%5Bobject%20Object%5D&originHeight=528&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>多并行度快照详图<br><img src="//upload-images.jianshu.io/upload_images/19063731-f2edab06e990314e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=523&margin=%5Bobject%20Object%5D&originHeight=523&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>上述图中，有4个带状态的Operator实例，相应的状态后端就可以想象成填4个格子。整个CheckPoint 的过程可以当做Operator实例填自己格子的过程，Operator实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单的认为一次完整的CheckPoint做完了<br><br><br>上面只是快照的过程，整个CheckPoint执行过程如下<br><br><br>1、JobManager端的 CheckPointCoordinator向 所有SourceTask发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier</p><p>2、当task收到所有的barrier后，向自己的下游继续传递barrier，然后自身执行快照，并将自己的状态<strong>异步写入到持久化存储</strong>中</p><ul><li>增量CheckPoint只是把最新的一部分更新写入到外部存储</li><li>为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照</li></ul><p><br>3、当task完成备份后，会将备份数据的地址（state handle）通知给JobManager的CheckPointCoordinator</p><ul><li>如果CheckPoint的持续时长超过 了CheckPoint设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次CheckPoint失败，会把这次CheckPoint产生的所有 状态数据全部删除</li></ul><p><br>4、 最后 CheckPoint Coordinator 会把整个 StateHandle 封装成 completed CheckPoint Meta，写入到hdfs</p><p><a name="yrq0j"></a></p><h4 id="barrier对齐"><a href="#barrier对齐" class="headerlink" title="barrier对齐"></a>barrier对齐</h4><p><br><strong>什么是barrier对齐？</strong><img src="//upload-images.jianshu.io/upload_images/19063731-284548fb44b76c9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=216&margin=%5Bobject%20Object%5D&originHeight=216&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>一旦Operator从输入流接收到CheckPoint barrier n，它就不能处理来自该流的任何数据记录，直到它从其他所有输入接收到barrier n为止。否则，<strong>它会混合属于快照n的记录和属于快照n + 1的记录</strong><br><br><br>接收到barrier n的流暂时被搁置。从这些流接收的记录不会被处理，而是放入输入缓冲区。上图中第2个图，虽然数字流对应的barrier已经到达了，但是barrier之后的1、2、3这些数据只能放到buffer中，等待字母流的barrier到达<br><br><br>一旦最后所有输入流都接收到barrier n，Operator就会把缓冲区中pending 的输出数据发出去，然后把CheckPoint barrier n接着往下游发送<br><br><br>这里还会对自身进行快照，之后，Operator将继续处理来自所有输入流的记录，在处理来自流的记录之前先处理来自输入缓冲区的记录<br><br><br><strong>什么是barrier不对齐？</strong><br><br><br>上述图2中，当还有其他输入流的barrier还没有到达时，会把已到达的barrier之后的数据1、2、3搁置在缓冲区，等待其他流的barrier到达后才能处理<br><br><br>barrier不对齐就是指当还有其他流的barrier还没到达时，为了不影响性能，也不用理会，直接处理barrier之后的数据。等到所有流的barrier的都到达后，就可以对该Operator做CheckPoint了<br><br><br><strong>为什么要进行barrier对齐？不对齐到底行不行？</strong><br>答：<strong>Exactly Once时必须barrier对齐，如果barrier不对齐就变成了At Least Once</strong><br><br><br>后面的部分主要证明这句话<br><br><br>CheckPoint的目的就是为了保存快照，如果不对齐，那么在chk-100快照之前，已经处理了一些chk-100 对应的offset之后的数据，当程序从chk-100恢复任务时，chk-100对应的offset之后的数据还会被处理一次，所以就出现了重复消费。<br><br><br>如果听不懂没关系，后面有案例让您懂<br><br><br>结合pv案例来看，之前的案例为了简单，描述的kafka的topic只有1个partition，这里为了讲述barrier对齐，所以topic有2个partittion<img src="//upload-images.jianshu.io/upload_images/19063731-c5aca9a4bddb1317.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=668&margin=%5Bobject%20Object%5D&originHeight=668&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>结合业务，先介绍一下上述所有算子在业务中的功能</p><ul><li>Source的kafka的Consumer，从kakfa中读取数据到flink应用中</li><li>TaskA中的map将读取到的一条kafka日志转换为我们需要统计的app_id</li><li>keyBy 按照app_id进行keyBy，相同的app_id 会分到下游TaskB的同一个实例中</li><li>TaskB的map在状态中查出该app_id 对应的pv值，然后+1，存储到状态中</li><li>利用Sink将统计的pv值写入到外部存储介质中</li><li>我们从kafka的两个partition消费数据，TaskA和TaskB都有两个并行度，所以总共flink有4个Operator实例，这里我们称之为 TaskA0、TaskA1、TaskB0、TaskB1</li><li>假设已经成功做了99次CheckPoint，这里详细解释第100次CheckPoint过程<ul><li>JobManager内部有个定时调度，假如现在10点00分00秒到了第100次CheckPoint的时间了，<strong>JobManager的CheckPointCoordinator进程会向所有的Source Task发送CheckPointTrigger，也就是向TaskA0、TaskA1发送CheckPointTrigger</strong></li><li>TaskA0、TaskA1接收到CheckPointTrigger，会往数据流中安插barrier，将barrier发送到下游，在自己的状态中记录barrier安插的offset位置，然后自身做快照，将offset信息保存到状态后端<ul><li>这里假如TaskA0消费的partition0的offset为10000，TaskA1消费的partition1的offset为10005。那么状态中会保存 (0，10000)(1，10005)，表示0号partition消费到了offset为10000的位置，1号partition消费到了offset为10005的位置</li></ul></li><li>然后TaskA的map和keyBy算子中并没有状态，所以不需要进行快照</li><li>接着数据和barrier都向下游TaskB发送，相同的app_id 会发送到相同的TaskB实例上，这里假设有两个app：app0和app1，经过keyBy后，假设app0分到了TaskB0上，app1分到了TaskB1上。基于上面描述，TaskA0和TaskA1中的所有app0的数据都发送到TaskB0上，所有app1的数据都发送到TaskB1上</li><li>现在我们假设TaskB0做CheckPoint的时候barrier对齐了，TaskB1做CheckPoint的时候barrier不对齐，当然不能这么配置，我就是举这么个例子，带大家分析一下barrier对不对齐到底对统计结果有什么影响？</li><li>上面说了chk-100的这次CheckPoint，offset位置为(0，10000)(1，10005)，TaskB0使用barrier对齐，也就是说TaskB0不会处理barrier之后的数据，所以TaskB0在chk-100快照的时候，状态后端保存的app0的pv数据是从程序开始启动到kafka offset位置为(0，10000)(1，10005)的所有数据计算出来的pv值，一条不多（没处理barrier之后，所以不会重复），一条不少(barrier之前的所有数据都处理了，所以不会丢失)，假如保存的状态信息为(app0，8000)表示消费到(0，10000)(1，10005)offset的时候，app0的pv值为8000</li><li>TaskB1使用的barrier不对齐，假如TaskA0由于服务器的CPU或者网络等其他波动，导致TaskA0处理数据较慢，而TaskA1很稳定，所以处理数据比较快。导致的结果就是TaskB1先接收到了TaskA1的barrier，由于配置的barrier不对齐，所以TaskB1会接着处理TaskA1 barrier之后的数据，过了2秒后，TaskB1接收到了TaskA0的barrier，于是对状态中存储的app1的pv值开始做CheckPoint 快照，保存的状态信息为(app1，12050)，但是我们知道这个(app1，12050)实际上多处理了2秒TaskA1发来的barrier之后的数据，也就是kafka topic对应的partition1 offset 10005之后的数据，app1真实的pv数据肯定要小于这个12050，partition1的offset保存的offset虽然是10005，但是我们实际上可能已经处理到了offset 10200的数据，假设就是处理到了10200</li><li>虽然状态保存的pv值偏高了，但是不能说明重复处理，因为我的TaskA1并没有再次去消费partition1的offset 10005~10200的数据，所以相当于也没有重复消费，只是展示的结果更实时了</li><li>分析到这里，我们先梳理一下我们的状态保存了什么：<ul><li>chk-100<ul><li>offset：(0，10000)(1，10005)</li><li>pv：(app0，8000) (app1，12050)</li></ul></li></ul></li><li>接着程序在继续运行，过了10秒，由于某个服务器挂了，导致我们的四个Operator实例有一个Operator挂了，所以Flink会从最近一次的状态恢复，也就是我们刚刚详细讲的chk-100处恢复，那具体是怎么恢复的呢？<ul><li>Flink 同样会起四个Operator实例，我还称他们是 TaskA0、TaskA1、TaskB0、TaskB1。四个Operator会从状态后端读取保存的状态信息。</li><li>从offset：(0，10000)(1，10005) 开始消费，并且基于 pv：(app0，8000) (app1，12050)值进行累加统计</li><li>然后你就应该会发现这个app1的pv值12050实际上已经包含了partition1的offset 10005<del>10200的数据，所以partition1从offset 10005恢复任务时，partition1的offset 10005</del>10200的数据被消费了两次<ul><li>TaskB1设置的barrier不对齐，所以CheckPoint chk-100对应的状态中多消费了barrier之后的一些数据（TaskA1发送），重启后是从chk-100保存的offset恢复，这就是所说的At Least Once</li></ul></li><li>由于上面说TaskB0设置的barrier对齐，所以app0不会出现重复消费，因为app0没有消费offset：(0，10000)(1，10005) 之后的数据，也就是所谓的Exactly Once</li></ul></li></ul></li></ul><p><br>看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么barrier对齐就是Exactly Once，为什么barrier不对齐就是 At Least Once</p><p><a name="9Hprm"></a></p><h4 id="到底什么时候会出现barrier对齐？"><a href="#到底什么时候会出现barrier对齐？" class="headerlink" title="到底什么时候会出现barrier对齐？"></a>到底什么时候会出现barrier对齐？</h4><p><br>首先设置了Flink的CheckPoint语义是：Exactly Once<br><br><br>Operator实例必须有多个输入流才会出现barrier对齐</p><pre><code>- 对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以，必须有多个流才叫对齐。barrier对齐其实也就是上游多个流配合使得数据对齐的过程- 言外之意：如果Operator实例只有一个输入流，就根本不存在barrier对齐，自己跟自己默认永远都是对齐的</code></pre><p><a name="rnTha"></a></p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><p><strong>第一种场景计算PV，kafka只有一个partition，精确一次，至少一次就没有区别？</strong></p><p>答：如果只有一个partition，对应flink任务的Source Task并行度只能是1，确实没有区别，不会有至少一次的存在了，肯定是精确一次。因为<strong>只有barrier不对齐才会有可能重复处理，这里并行度都已经为1，默认就是对齐的</strong>，只有当上游有多个并行度的时候，多个并行度发到下游的barrier才需要对齐，单并行度不会出现barrier不对齐，所以必然精确一次。其实还是要理解barrier对齐就是Exactly Once不会重复消费，barrier不对齐就是 At Least Once可能重复消费，这里只有单个并行度根本不会存在barrier不对齐，所以不会存在至少一次语义</p><p><strong>为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照；这一步，如果向下发送barrier后，自己同步快照慢怎么办？下游已经同步好了，自己还没？</strong></p><p>答: 可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游快照的更及时了，我只要保障下游把barrier之前的数据都处理了，并且不处理barrier之后的数据，然后做快照，那么下游也同样支持精确一次。这个问题你不要从全局思考，你单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意一点，如果有一个Operator 的CheckPoint失败了或者因为CheckPoint超时也会导致失败，那么JobManager会认为整个CheckPoint失败。失败的CheckPoint是不能用来恢复任务的，必须所有的算子的CheckPoint都成功，那么这次CheckPoint才能认为是成功的，才能用来恢复任务</p><p><strong>我程序中Flink的CheckPoint语义设置了 Exactly Once，但是我的mysql中看到数据重复了？</strong><br><br><br>程序中设置了1分钟1次CheckPoint，但是5秒向mysql写一次数据，并commit<br>答：Flink要求end to end的精确一次都必须实现TwoPhaseCommitSinkFunction。如果你的chk-100成功了，过了30秒，由于5秒commit一次，所以实际上已经写入了6批数据进入mysql，但是突然程序挂了，从chk100处恢复，这样的话，之前提交的6批数据就会重复写入，所以出现了重复消费。<strong>Flink的精确一次有两种情况，一个是Flink内部的精确一次，一个是端对端的精确一次</strong>，这个博客所描述的都是关于Flink内部去的精确一次，我后期再发一个博客详细介绍一下Flink端对端的精确一次如何实现</p><p>转载自：<a href="https://www.jianshu.com/p/8d6569361999" target="_blank" rel="external nofollow noopener noreferrer">https://www.jianshu.com/p/8d6569361999</a><br><br><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;gI6Sf&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="cpeixin.cn/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫 - 动态爬取</title>
    <link href="cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
    <id>cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</id>
    <published>2019-06-12T15:26:15.000Z</published>
    <updated>2020-04-04T17:11:17.062Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。<br><br><br>到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。<br><br><br>那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。<br><br><br>那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843101-98e57df8-c124-4923-aa9f-7c42ce2288b6.png#align=left&display=inline&height=1300&originHeight=1300&originWidth=3356&size=0&status=done&style=none&width=3356" alt><br><br><br>注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843056-e12f0f79-905f-4420-abfd-fb85625767ac.png#align=left&display=inline&height=1150&originHeight=1150&originWidth=2266&size=0&status=done&style=none&width=2266" alt><br><br><br>点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：<br>Headers：请求和响应的详细信息<br>Preview：响应体格式化之后的显示<br>Response：响应体原始内容<br>Cookies：Cookies<br>Timing：时间开销<br><br><br>通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401842263-072689cf-5dea-4f8b-b103-0758d9c5e52e.png#align=left&display=inline&height=262&originHeight=262&originWidth=1276&size=0&status=done&style=none&width=1276" alt><br><br><br>上面的截图展示了这次请求的请求方式、请求地址等信息。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843887-cf8bff5f-4570-4ad4-8681-f0f3eae929b3.png#align=left&display=inline&height=884&originHeight=884&originWidth=2652&size=0&status=done&style=none&width=2652" alt><br><br><br>上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843009-4797a28d-3070-421e-a45c-86781d7f580d.png#align=left&display=inline&height=188&originHeight=188&originWidth=848&size=0&status=done&style=none&width=848" alt><br><br><br>上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。<br><br><br>那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。<br><br><br>我们回过头重新观察一下返回的json，格式化之后的层级关系如下：<br><br><br>很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">json_obj = json.loads(<span class="string">"&#123;'key': 'value'&#125;"</span>)  <span class="comment"># 字符串到对象</span></span><br><span class="line">json_str = json.dumps(json_obj)            <span class="comment"># 对象到字符串</span></span><br></pre></td></tr></table></figure><p><br>json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="爬虫" scheme="cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>【转载】谈谈流计算中的『Exactly Once』特性</title>
    <link href="cpeixin.cn/2019/06/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E8%B0%88%E8%B0%88%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E3%80%8EExactly-Once%E3%80%8F%E7%89%B9%E6%80%A7/"/>
    <id>cpeixin.cn/2019/06/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E8%B0%88%E8%B0%88%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E3%80%8EExactly-Once%E3%80%8F%E7%89%B9%E6%80%A7/</id>
    <published>2019-06-02T16:40:09.000Z</published>
    <updated>2020-07-01T16:41:41.011Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --><p>本文翻译自 streaml.io 网站上的一篇博文：“Exactly once is NOT exactly the same” ，分析了流计算系统中常说的『Exactly Once』特性，主要观点是：『精确一次』并不保证是完全一样。主要内容如下：<br></p><ol><li>背景</li></ol><p><br>– 1.1. 最多一次（At-most-once）<br><br><br>– 1.2. 至少一次（At-least-once）<br><br><br>– 1.3. 精确一次（Exactly-once）<br></p><ol start="2"><li>『精确一次』是真正的『精确一次』吗?</li><li>分布式快照与至少一次事件传递和重复数据删除的比较</li><li>结论</li><li>参考</li></ol><p><br>目前市面上使用较多的流计算系统有 Apache Storm，Apache Flink, Heron, Apache Kafka (Kafka Streams) 和 Apache Spark (Spark Streaming)。关于流计算系统有个被广泛讨论的特性是『exactly-once』语义，很多系统宣称已经支持了这一特性。但是，到底什么是『exactly-once』，怎么样才算是实现了『exactly-once』，人们存在很多误解和歧义。接下来我们做下分析。<br></p><p><a name="5x1Yy"></a></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>流处理（有时称为事件处理）可以简单地描述为是对无界数据或事件的连续处理。流或事件处理应用程序可以或多或少地被描述为有向图，并且通常被描述为有向无环图（DAG）。在这样的图中，每个边表示数据或事件流，每个顶点表示运算符，会使用程序中定义的逻辑处理来自相邻边的数据或事件。有两种特殊类型的顶点，通常称为 sources 和 sinks。<br><br><br>sources读取外部数据/事件到应用程序中，而 sinks 通常会收集应用程序生成的结果。下图是流式应用程序的示例。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1593619954126-f6392414-eda2-4785-8d13-222d40e52cb8.jpeg#align=left&display=inline&height=910&margin=%5Bobject%20Object%5D&name=01-3.jpeg&originHeight=910&originWidth=1080&size=49487&status=done&style=none&width=1080" alt="01-3.jpeg"><br>A typical stream processing topology<br><br><br>流处理引擎通常允许用户指定可靠性模式或处理语义，以指示它将为整个应用程序中的数据处理提供哪些保证。这些保证是有意义的，因为你始终会遇到由于网络，机器等可能导致数据丢失的故障。流处理引擎通常为应用程序提供了三种数据处理语义：<strong>最多一次、至少一次和精确一次。</strong><br><br><br>如下是对这些不同处理语义的宽松定义：<br></p><p><a name="L21lc"></a></p><h5 id="最多一次（At-most-once）"><a href="#最多一次（At-most-once）" class="headerlink" title="最多一次（At-most-once）"></a><strong>最多一次（At-most-once）</strong></h5><p><strong>这本质上是一『尽力而为』的方法。保证数据或事件最多由应用程序中的所有算子处理一次</strong>。 这意味着如果数据在被流应用程序完全处理之前发生丢失，则不会进行其他重试或者重新发送。下图中的例子说明了这种情况。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1593620204457-26766470-ba82-49e5-a6fa-fa1274ed7988.jpeg#align=left&display=inline&height=186&margin=%5Bobject%20Object%5D&name=02-3.jpeg&originHeight=186&originWidth=1080&size=19790&status=done&style=none&width=1080" alt="02-3.jpeg"><br></p><p><a name="XqnZ9"></a></p><h5 id="至少一次（At-least-once）"><a href="#至少一次（At-least-once）" class="headerlink" title="至少一次（At-least-once）"></a><strong>至少一次（At-least-once）</strong></h5><p><strong>应用程序中的所有算子都保证数据或事件至少被处理一次</strong>。这通常意味着如果事件在流应用程序完全处理之前丢失，则将从源头重放或重新传输事件。然而，由于事件是可以被重传的，因此一个事件有时会被处理多次，这就是所谓的至少一次。<br>下图的例子描述了这种情况：第一个算子最初未能成功处理事件，然后在重试时成功，接着在第二次重试时也成功了，其实是没有必要的。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1593620273328-ff68420b-1278-4afe-a06c-3c507717bc46.jpeg#align=left&display=inline&height=319&margin=%5Bobject%20Object%5D&name=03-2.jpeg&originHeight=319&originWidth=1080&size=31792&status=done&style=none&width=1080" alt="03-2.jpeg"><br><a name="TEyGB"></a></p><h5 id="精确一次（Exactly-once）"><a href="#精确一次（Exactly-once）" class="headerlink" title="精确一次（Exactly-once）"></a><strong>精确一次（Exactly-once）</strong></h5><p><strong>即使是在各种故障的情况下，流应用程序中的所有算子都保证事件只会被『精确一次』的处</strong>理。（也有文章将 Exactly-once 翻译为：完全一次，恰好一次）<br><br><br>通常使用两种流行的机制来实现『精确一次』处理语义。<br><strong>– 分布式快照 / 状态检查点</strong><br><strong>– 至少一次事件传递和对重复数据去重</strong><br><strong><br>实现『精确一次』的分布式快照/状态检查点方法受到 Chandy-Lamport 分布式快照算法的启发[1]。通过这种机制，流应用程序中每个算子的所有状态都会定期做 checkpoint。</strong>如果是在系统中的任何地方发生失败，每个算子的所有状态都回滚到最新的全局一致 checkpoint 点**。在回滚期间，将暂停所有处理。源也会重置为与最近 checkpoint 相对应的正确偏移量。整个流应用程序基本上是回到最近一次的一致状态，然后程序可以从该状态重新启动。下图描述了这种 checkpoint 机制的基础知识。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1593620573669-3146e6ae-5426-4757-9713-41dbf3301528.jpeg#align=left&display=inline&height=957&margin=%5Bobject%20Object%5D&name=04-2.jpeg&originHeight=957&originWidth=1080&size=61928&status=done&style=none&width=1080" alt="04-2.jpeg"><br>在上图中，流应用程序在 T1 时间处正常工作，并且做了checkpoint。然而，在时间 T2，算子未能处理输入的数据。此时，S=4 的状态值已保存到持久存储器中，而状态值 S=12 保存在算子的内存中。为了修复这种差异，在时间 T3，处理程序将状态回滚到 S=4 并“重放”流中的每个连续状态直到最近，并处理每个数据。最终结果是有些数据已被处理了多次，但这没关系，因为无论执行了多少次回滚，结果状态都是相同的。<br><br><br>另一种实现『精确一次』的方法是：在每个算子上实现至少一次事件传递和对重复数据去重。使用此方法的流处理引擎将重放失败事件，以便在事件进入算子中的用户定义逻辑之前，进一步尝试处理并移除每个算子的重复事件。此机制要求为每个算子维护一个事务日志，以跟踪它已处理的事件。利用这种机制的引擎有 Google 的 MillWheel[2] 和 Apache Kafka Streams。下图说明了这种机制的要点。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1593620777567-ed8af262-1e4b-4945-bc5f-24a2f326b78c.jpeg#align=left&display=inline&height=382&margin=%5Bobject%20Object%5D&name=05-1.jpeg&originHeight=382&originWidth=1080&size=31049&status=done&style=none&width=1080" alt="05-1.jpeg"><br></p><p><a name="Lgfgg"></a></p><h3 id="『精确一次』是真正的『精确一次』吗"><a href="#『精确一次』是真正的『精确一次』吗" class="headerlink" title="『精确一次』是真正的『精确一次』吗?"></a><strong>『精确一次』是真正的『精确一次』吗?</strong></h3><p>现在让我们重新审视『精确一次』处理语义真正对最终用户的保证。『精确一次』这个术语在描述正好处理一次时会让人产生误导。<br><br><br>有些人可能认为『精确一次』描述了事件处理的保证，其中流中的每个事件只被处理一次。<strong>实际上，没有引擎能够保证正好只处理一次</strong>。在面对任意故障时，不可能保证每个算子中的用户定义逻辑在每个事件中只执行一次，因为用户代码被部分执行的可能性是永远存在的。<br><br><br>考虑具有流处理运算符的场景，该运算符执行打印传入事件的 ID 的映射操作，然后返回事件不变。下面的伪代码说明了这个操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Map (Event event) &#123;</span><br><span class="line">    Print <span class="string">"Event ID: "</span> + event.getId()</span><br><span class="line">    Return event</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br>每个事件都有一个 GUID (全局惟一ID)。如果用户逻辑的精确执行一次得到保证，那么事件 ID 将只输出一次。但是，这是无法保证的，因为在用户定义的逻辑的执行过程中，随时都可能发生故障。<br><br><br>引擎无法自行确定执行用户定义的处理逻辑的时间点。因此，不能保证任意用户定义的逻辑只执行一次。<strong>这也意味着，在用户定义的逻辑中实现的外部操作(如写数据库)也不能保证只执行一次</strong>。此类操作仍然需要以幂等的方式执行。<br><br><br>那么，当引擎声明『精确一次』处理语义时，它们能保证什么呢？如果不能保证用户逻辑只执行一次，那么什么逻辑只执行一次？当引擎声明『精确一次』处理语义时，它们实际上是在说，它们可以保证引擎管理的状态更新只提交一次到持久的后端存储。<br><br><br>上面描述的两种机制都使用持久的后端存储作为真实性的来源，可以保存每个算子的状态并自动向其提交更新。<br><br><br>对于机制 1 (分布式快照 / 状态检查点)，此持久后端状态用于保存流应用程序的全局一致状态检查点(每个算子的检查点状态)。<br><br><br>对于机制 2 (至少一次事件传递加上重复数据删除)，持久后端状态用于存储每个算子的状态以及每个算子的事务日志，该日志跟踪它已经完全处理的所有事件。<br><br><br>提交状态或对作为真实来源的持久后端应用更新可以被描述为恰好发生一次。然而，如上所述，计算状态的更新 / 更改，即处理在事件上执行任意用户定义逻辑的事件，如果发生故障，则可能不止一次地发生。换句话说，<strong>事件的处理可以发生多次，但是该处理的效果只在持久后端状态存储中反映一次</strong>。因此，我们认为有效地描述这些处理语义最好的术语是『有效一次』（effectively once）。<br><a name="BiQgN"></a></p><h3><a href="#" class="headerlink" title="**"></a>**</h3><p><a name="wdS6q"></a></p><h3 id="分布式快照与至少一次事件传递和重复数据删除的比较"><a href="#分布式快照与至少一次事件传递和重复数据删除的比较" class="headerlink" title="分布式快照与至少一次事件传递和重复数据删除的比较"></a><strong>分布式快照与至少一次事件传递和重复数据删除的比较</strong></h3><p>从语义的角度来看，分布式快照和至少一次事件传递以及重复数据删除机制都提供了相同的保证。然而，由于两种机制之间的实现差异，存在显着的性能差异。<br><strong><br></strong>机制 1（分布式快照 / 状态检查点）的性能开销是最小的<strong>，因为引擎实际上是往流应用程序中的所有算子一起发送常规事件和特殊事件，而状态检查点可以在后台异步执行。但是，对于大型流应用程序，故障可能会更频繁地发生，导致引擎需要暂停应用程序并回滚所有算子的状态，这反过来又会影响性能。流式应用程序越大，故障发生的可能性就越大，因此也越频繁，反过来，流式应用程序的性能受到的影响也就越大。</strong>然而，这种机制是非侵入性的<strong>，运行时需要的额外资源影响很小。<br></strong><br><strong>机制 2（至少一次事件传递加重复数据删除）可能需要更多资源，尤其是存储</strong>。使用此机制，引擎需要能够跟踪每个算子实例已完全处理的每个元组，以执行重复数据删除，以及为每个事件执行重复数据删除本身。这意味着需要跟踪大量的数据，尤其是在流应用程序很大或者有许多应用程序在运行的情况下。执行重复数据删除的每个算子上的每个事件都会产生性能开销。但是，使用这种机制，流应用程序的性能不太可能受到应用程序大小的影响。<br><br><br>对于机制 1，如果任何算子发生故障，则需要发生全局暂停和状态回滚；对于机制 2，失败的影响更加局部性。当在算子中发生故障时，可能尚未完全处理的事件仅从上游源重放/重传。性能影响与流应用程序中发生故障的位置是隔离的，并且对流应用程序中其他算子的性能几乎没有影响。从性能角度来看，这两种机制的优缺点如下。<br><strong><br></strong>分布式快照 / 状态检查点的优缺点：<strong><br></strong>优点：<strong><br>– 较小的性能和资源开<br></strong>缺点：<strong><br>– 对性能的影响较大<br>– 拓扑越大，对性能的潜在影响越大<br></strong><br><strong>至少一次事件传递以及重复数据删除机制的优缺点：</strong><br><strong>优点： **<br>– 故障对性能的影响是局部的<br>– 故障的影响不一定会随着拓扑的大小而增加<br></strong>缺点：**<br>– 可能需要大量的存储和基础设施来支持<br>– 每个算子的每个事件的性能开销<br><br><br>虽然从理论上讲，分布式快照和至少一次事件传递加重复数据删除机制之间存在差异，但两者都可以简化为至少一次处理加幂等性。对于这两种机制，当发生故障时(至少实现一次)，事件将被重放/重传，并且通过状态回滚或事件重复数据删除，算子在更新内部管理状态时本质上是幂等的。<br><a name="9pba8"></a></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h3><p>在这篇博客文章中，我希望能够让你相信『精确一次』这个词是非常具有误导性的。提供『精确一次』的处理语义实际上意味着流处理引擎管理的算子状态的不同更新只反映一次。『精确一次』并不能保证事件的处理，即任意用户定义逻辑的执行，只会发生一次。我们更喜欢用『有效一次』（effectively once）这个术语来表示这种保证，因为处理不一定保证只发生一次，但是对引擎管理的状态的影响只反映一次。<br><br><br>两种流行的机制，分布式快照和重复数据删除，被用来实现精确/有效的一次性处理语义。这两种机制为消息处理和状态更新提供了相同的语义保证，但是在性能上存在差异。这篇文章并不是要让你相信任何一种机制都优于另一种，因为它们各有利弊。<strong>结</strong></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Jul 04 2020 01:08:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;本文翻译自 streaml.io 网站上的一篇博文：“Exactly once is NOT exactly the same” ，分析了流计算
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
</feed>
