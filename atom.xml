<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布兰特 | 不忘初心</title>
  
  <subtitle>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="cpeixin.cn/"/>
  <updated>2020-05-28T15:08:18.891Z</updated>
  <id>cpeixin.cn/</id>
  
  <author>
    <name>Brent</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux直接下载Google Drive文件</title>
    <link href="cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/"/>
    <id>cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/</id>
    <published>2020-05-27T17:03:54.000Z</published>
    <updated>2020-05-28T15:08:18.891Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p>在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测试。<br><br><br>首选方案是将整个项目上传到GitHub中，随后在服务器中直接wget，但是模型文件过大，GitHub单个文件的限制是100MB。<br><br><br>突然想到可不可以直接从Google Drive上进行下载模型文件到服务器😅<br><br><br><strong>下载小文件：</strong><br><br><br>选择要下载的文件右键<br><br><br>点击“共享”<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598566164-b176f9b4-62bf-41d6-82bf-36e8837816f6.png#align=left&display=inline&height=1066&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.51.36.png&originHeight=1066&originWidth=1712&size=193930&status=done&style=none&width=1712" alt="屏幕快照 2020-05-28 上午12.51.36.png"><br><br><br><br><br>点击“更改”，设置分享权限<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598584015-3b9be292-6dd3-4edc-86ef-de50b58be55b.png#align=left&display=inline&height=1088&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.52.08.png&originHeight=1088&originWidth=2036&size=141068&status=done&style=none&width=2036" alt="屏幕快照 2020-05-28 上午12.52.08.png"><br><br><br>这是复制图中选中部分的ID<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598604200-c2f98348-2daf-423b-9792-df03ad395236.png#align=left&display=inline&height=1078&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.53.24.png&originHeight=1078&originWidth=1858&size=172147&status=done&style=none&width=1858" alt="屏幕快照 2020-05-28 上午12.53.24.png"><br><br><br>拼接下载链接，进行下载<br><br><br>wget <a href="https://drive.google.com/uc?id=1sT6GvdtCG3AnV-62beWCP6LNdtFgmX-o" target="_blank" rel="external nofollow noopener noreferrer">https://drive.google.com/uc?id=</a>复制下来的共享id -O your_file_name</p><p><strong>下载大文件：</strong></p><p>上面的方法，适合下载一些小文件，大文件就不可以了。更换下面命令的id选项，并且准备好cookies.txt<br><br><br>关于cookies.txt，可以在Chrome浏览器中下载cookie.txt这个插件，点击下载，上传到服务器中/tmp目录下即可<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590678181832-6ba38a09-0ea7-4d48-a6a3-5cd116d19e1d.png#align=left&display=inline&height=1472&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8B%E5%8D%8810.59.19.png&originHeight=1472&originWidth=3094&size=893365&status=done&style=none&width=3094" alt="屏幕快照 2020-05-28 下午10.59.19.png"><br><br><br>关于文件id，和上面方法获取一致，接下来运行下面命令即可。<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZag' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZagARe" -O pytorch_model.bin</span><br></pre></td></tr></table></figure><br><br><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测
      
    
    </summary>
    
    
      <category term="Tools" scheme="cpeixin.cn/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink State 状态管理</title>
    <link href="cpeixin.cn/2020/04/29/Flink-State-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/"/>
    <id>cpeixin.cn/2020/04/29/Flink-State-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/</id>
    <published>2020-04-29T01:51:40.000Z</published>
    <updated>2020-07-07T01:53:46.284Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p>我们先回顾一下到底什么是 state，流式计算的数据往往是转瞬即逝， 当然，真实业务场景不可能说所有的数据都是进来之后就走掉，没有任何东西留下来，那么留下来的东西其实就是称之为 state，中文可以翻译成状态。<br>在下面这个图中，我们的所有的原始数据进入用户代码之后再输出到下游，如果中间涉及到 state 的读写，这些状态会存储在本地的 state backend（可以对标成嵌入式本地 kv 存储）当中。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057064519-9d422e4c-d12b-4194-9276-1352f2ef49fd.png#align=left&display=inline&height=445&margin=%5Bobject%20Object%5D&name=a6cd5ef2808d43da9fec1ae90a9bb8f3.png&originHeight=445&originWidth=899&size=74325&status=done&style=none&width=899" alt="a6cd5ef2808d43da9fec1ae90a9bb8f3.png"><br><a name="bylpp"></a></p><h3 id="一-状态管理的基本概念"><a href="#一-状态管理的基本概念" class="headerlink" title="一. 状态管理的基本概念"></a>一. 状态管理的基本概念</h3><p><a name="qtxng"></a></p><h4 id="1-什么是状态"><a href="#1-什么是状态" class="headerlink" title="1. 什么是状态"></a>1. 什么是状态</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056123816-435b7d81-14db-4a9b-8ea6-5969055263f2.png#align=left&display=inline&height=593&margin=%5Bobject%20Object%5D&originHeight=593&originWidth=1061&size=0&status=done&style=none&width=1061" alt><br>首先举一个无状态计算的例子：消费延迟计算。假设现在有一个消息队列，消息队列中有一个生产者持续往消费队列写入消息，多个消费者分别从消息队列中读取消息。从图上可以看出，生产者已经写入 16 条消息，Offset 停留在 15 ；有 3 个消费者，有的消费快，而有的消费慢。消费快的已经消费了 13 条数据，消费者慢的才消费了 7、8 条数据。<br><br><br>如何实时统计每个消费者落后多少条数据，如图给出了输入输出的示例。可以了解到输入的时间点有一个时间戳，生产者将消息写到了某个时间点的位置，每个消费者同一时间点分别读到了什么位置。刚才也提到了生产者写入了 15 条，消费者分别读取了 10、7、12 条。那么问题来了，怎么将生产者、消费者的进度转换为右侧示意图信息呢？<br><br><br>consumer 0 落后了 5 条，consumer 1 落后了 8 条，consumer 2 落后了 3 条，根据 Flink 的原理，此处需进行 Map 操作。Map 首先把消息读取进来，然后分别相减，即可知道每个 consumer 分别落后了几条。Map 一直往下发，则会得出最终结果。<br><br><br>大家会发现，在这种模式的计算中，无论这条输入进来多少次，输出的结果都是一样的，因为单条输入中已经包含了所需的所有信息。消费落后等于生产者减去消费者。生产者的消费在单条数据中可以得到，消费者的数据也可以在单条数据中得到，所以相同输入可以得到相同输出，这就是一个无状态的计算。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056123791-00ed5d81-07fc-419d-bed8-3e8a6d731a1a.png#align=left&display=inline&height=592&margin=%5Bobject%20Object%5D&originHeight=592&originWidth=1060&size=0&status=done&style=none&width=1060" alt><br>相应的什么是有状态的计算？<br><br><br>以访问日志统计量的例子进行说明，比如当前拿到一个 Nginx 访问日志，一条日志表示一个请求，记录该请求从哪里来，访问的哪个地址，需要实时统计每个地址总共被访问了多少次，也即每个 API 被调用了多少次。可以看到下面简化的输入和输出，输入第一条是在某个时间点请求 GET 了 /api/a；第二条日志记录了某个时间点 Post /api/b ; 第三条是在某个时间点 GET 了一个 /api/a，总共有 3 个 Nginx 日志。从这 3 条 Nginx 日志可以看出，第一条进来输出 /api/a 被访问了一次，第二条进来输出 /api/b 被访问了一次，紧接着又进来一条访问 api/a，所以 api/a 被访问了 2 次。不同的是，两条 /api/a 的 Nginx 日志进来的数据是一样的，但输出的时候结果可能不同，第一次输出 count=1 ，第二次输出 count=2，说明相同输入可能得到不同输出。输出的结果取决于当前请求的 API 地址之前累计被访问过多少次。第一条过来累计是 0 次，count = 1，第二条过来 API 的访问已经有一次了，所以 /api/a 访问累计次数 count=2。单条数据其实仅包含当前这次访问的信息，而不包含所有的信息。要得到这个结果，还需要依赖 API 累计访问的量，即状态。<br><br><br>这个计算模式是将数据输入算子中，用来进行各种复杂的计算并输出数据。这个过程中算子会去访问之前存储在里面的状态。另外一方面，它还会把现在的数据对状态的影响实时更新，如果输入 200 条数据，最后输出就是 200 条结果。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124024-fef7c859-c578-4251-b699-79678a8f6821.png#align=left&display=inline&height=592&margin=%5Bobject%20Object%5D&originHeight=592&originWidth=1056&size=0&status=done&style=none&width=1056" alt><br>什么场景会用到状态呢？下面列举了常见的 4 种：</p><ul><li><p>去重：比如上游的系统数据可能会有重复，落到下游系统时希望把重复的数据都去掉。去重需要先了解哪些数据来过，哪些数据还没有来，也就是把所有的主键都记录下来，当一条数据到来后，能够看到在主键当中是否存在。</p></li><li><p>窗口计算：比如统计每分钟 Nginx 日志 API 被访问了多少次。窗口是一分钟计算一次，在窗口触发前，如 08:00 ~ 08:01 这个窗口，前 59 秒的数据来了需要先放入内存，即需要把这个窗口之内的数据先保留下来，等到 8:01 时一分钟后，再将整个窗口内触发的数据输出。未触发的窗口数据也是一种状态。</p></li><li><p>机器学习 / 深度学习：如训练的模型以及当前模型的参数也是一种状态，机器学习可能每次都用有一个数据集，需要在数据集上进行学习，对模型进行一个反馈。</p></li><li><p>访问历史数据：比如与昨天的数据进行对比，需要访问一些历史数据。如果每次从外部去读，对资源的消耗可能比较大，所以也希望把这些历史数据也放入状态中做对比。<br><a name="GzjOf"></a></p><h4 id="2-为什么要管理状态"><a href="#2-为什么要管理状态" class="headerlink" title="2. 为什么要管理状态"></a>2. 为什么要管理状态</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124000-7c47840a-5a52-42cf-8c10-32c09505fbbd.png#align=left&display=inline&height=592&margin=%5Bobject%20Object%5D&originHeight=592&originWidth=1058&size=0&status=done&style=none&width=1058" alt><br>管理状态最直接的方式就是将数据都放到内存中，这也是很常见的做法。比如在做 WordCount 时，Word 作为输入，Count 作为输出。在计算的过程中把输入不断累加到 Count。<br><br><br>但对于流式作业有以下要求：</p></li><li><p>7*24 小时运行，高可靠；</p></li><li><p>数据不丢不重，恰好计算一次；</p></li><li><p>数据实时产出，不延迟；</p></li></ul><p><br>基于以上要求，内存的管理就会出现一些问题。由于内存的容量是有限制的。如果要做 24 小时的窗口计算，将 24 小时的数据都放到内存，可能会出现内存不足；另外，作业是 7*24，需要保障高可用，机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，保证运行的作业不受影响；此外，考虑横向扩展，假如网站的访问量不高，统计每个 API 访问次数的程序可以用单线程去运行，但如果网站访问量突然增加，单节点无法处理全部访问数据，此时需要增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点也问题之一。因此，将数据都放到内存中，并不是最合适的一种状态管理方式。<br><a name="p8Qc4"></a></p><h4 id="3-理想的状态管理"><a href="#3-理想的状态管理" class="headerlink" title="3. 理想的状态管理"></a>3. 理想的状态管理</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124032-35913185-c7e7-46ce-b404-737edfd54499.png#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1060&size=0&status=done&style=none&width=1060" alt><br>最理想的状态管理需要满足易用、高效、可靠三点需求：</p><ul><li><p>易用，Flink 提供了丰富的数据结构、多样的状态组织形式以及简洁的扩展接口，让状态管理更加易用；</p></li><li><p>高效，实时作业一般需要更低的延迟，一旦出现故障，恢复速度也需要更快；当处理能力不够时，可以横向扩展，同时在处理备份时，不影响作业本身处理性能；</p></li><li><p>可靠，Flink 提供了状态持久化，包括不丢不重的语义以及具备自动的容错能力，比如 HA，当节点挂掉后会自动拉起，不需要人工介入。<br><a name="qjnIe"></a></p><h3 id="二-Flink-状态的类型与使用示例"><a href="#二-Flink-状态的类型与使用示例" class="headerlink" title="二.Flink 状态的类型与使用示例"></a>二.Flink 状态的类型与使用示例</h3><p><a name="Q5y64"></a></p><h4 id="1-Managed-State-amp-Raw-State"><a href="#1-Managed-State-amp-Raw-State" class="headerlink" title="1.Managed State &amp; Raw State"></a>1.Managed State &amp; Raw State</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056123997-7096acee-f04c-4ee7-a407-ae3a6551229e.png#align=left&display=inline&height=595&margin=%5Bobject%20Object%5D&originHeight=595&originWidth=1060&size=0&status=done&style=none&width=1060" alt><br><strong>Managed State 是 Flink 自动管理的 State，而 Raw State 是原生态 State</strong>，两者的区别如下：</p></li><li><p>从状态管理方式的方式来说，Managed State 由 Flink Runtime 管理，自动存储，自动恢复，在内存管理上有优化；而 Raw State 需要用户自己管理，需要自己序列化，Flink 不知道 State 中存入的数据是什么结构，只有用户自己知道，需要最终序列化为可存储的数据结构。</p></li><li><p>从状态数据结构来说，Managed State 支持已知的数据结构，如 Value、List、Map 等。而 Raw State 只支持字节数组 ，所有状态都要转换为二进制字节数组才可以。</p></li><li><p>从推荐使用场景来说，Managed State 大多数情况下均可使用，而 Raw State 是当 Managed State 不够用时，比如需要自定义 Operator 时，推荐使用 Raw State。<br><a name="MRtIn"></a></p><h4 id="2-Keyed-State-amp-Operator-State"><a href="#2-Keyed-State-amp-Operator-State" class="headerlink" title="2.Keyed State &amp; Operator State"></a>2.Keyed State &amp; Operator State</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124012-aef1f515-3c6b-45f9-9813-f5714a7aa3ae.png#align=left&display=inline&height=593&margin=%5Bobject%20Object%5D&originHeight=593&originWidth=1057&size=0&status=done&style=none&width=1057" alt><br><strong>Managed State 分为两种，一种是 Keyed State；另外一种是 Operator State。</strong><br><br><br>在 Flink Stream 模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream 。每个 Key 对应一个 State，即一个 Operator 实例处理多个 Key，访问相应的多个 State，并由此就衍生了 Keyed State。<br><br><br>Keyed State 只能用在 KeyedStream 的算子中，即在整个程序中没有 keyBy 的过程就没有办法使用 KeyedStream。<br><br><br>相比较而言，Operator State 可以用于所有算子，相对于数据源有一个更好的匹配方式，常用于 Source，例如 FlinkKafkaConsumer。相比 Keyed State，一个 Operator 实例对应一个 State，随着并发的改变，Keyed State 中，State 随着 Key 在实例间迁移，比如原来有 1 个并发，对应的 API 请求过来，/api/a 和 /api/b 都存放在这个实例当中；如果请求量变大，需要扩容，就会把 /api/a 的状态和 /api/b 的状态分别放在不同的节点。由于 Operator State 没有 Key，并发改变时需要选择状态如何重新分配。其中内置了 2 种分配方式：一种是均匀分配，另外一种是将所有 State 合并为全量 State 再分发给每个实例。<br><br><br>在访问上，Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个 Rich Function。Operator State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。在数据结构上，Keyed State 支持的数据结构，比如 ValueState、ListState、ReducingState、AggregatingState 和 MapState；而 Operator State 支持的数据结构相对较少，如 ListState。<br></p></li></ul><p><a name="MfO5f"></a></p><h4 id="3-Keyed-State-amp-Operator-State-区别"><a href="#3-Keyed-State-amp-Operator-State-区别" class="headerlink" title="3.Keyed State &amp; Operator State 区别"></a>3.Keyed State &amp; Operator State 区别</h4><p>接下来我们会在四个维度来区分两种不同的 state：operator state 以及 keyed state。<br><strong>1. 是否存在当前处理的 key（current key）</strong>：operator state 是没有当前 key 的概念，而 keyed state 的数值总是与一个 current key 对应。<br><strong>2. 存储对象是否 on heap</strong>: 目前 operator state backend 仅有一种 on-heap 的实现；而 keyed state backend 有 on-heap 和 off-heap（RocksDB）的多种实现。<br><strong>3. 是否需要手动声明快照（snapshot）和恢复 (restore) 方法</strong>：operator state 需要手动实现 snapshot 和 restore 方法；而 keyed state 则由 backend 自行实现，对用户透明。<br><strong>4. 数据大小</strong>：一般而言，我们认为 operator state 的数据规模是比较小的；认为 keyed state 规模是相对比较大的。需要注意的是，这是一个经验判断，不是一个绝对的判断区分标准。<br><br><br></p><p><a name="7WN1V"></a></p><h4 id="4-Keyed-State-使用示例"><a href="#4-Keyed-State-使用示例" class="headerlink" title="4.Keyed State 使用示例"></a>4.Keyed State 使用示例</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124018-5cc36362-6b78-48a8-a53b-bde90d9bcdc4.png#align=left&display=inline&height=595&margin=%5Bobject%20Object%5D&originHeight=595&originWidth=1061&size=0&status=done&style=none&width=1061" alt><br>Keyed State 有很多种，如图为几种 Keyed State 之间的关系。首先 State 的子类中一级子类有 ValueState、MapState、AppendingState。AppendingState 又有一个子类 MergingState。MergingState 又分为 3 个子类分别是 ListState、ReducingState、AggregatingState。这个继承关系使它们的访问方式、数据结构也存在差异。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124045-cfa6640b-3c01-462c-9d67-3a3d34f74f52.png#align=left&display=inline&height=591&margin=%5Bobject%20Object%5D&originHeight=591&originWidth=1057&size=0&status=done&style=none&width=1057" alt><br>几种 Keyed State 的差异具体体现在：</p><ul><li>ValueState 存储单个值，比如 Wordcount，用 Word 当 Key，State 就是它的 Count。这里面的单个值可能是数值或者字符串，作为单个值，访问接口可能有两种，get 和 set。在 State 上体现的是 update(T) / T value()。</li><li>MapState 的状态数据类型是 Map，在 State 上有 put、remove 等。需要注意的是在 MapState 中的 key 和 Keyed state 中的 key 不是同一个。</li><li>ListState 状态数据类型是 List，访问接口如 add、update 等。</li><li>ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值，原因在于其中的 add 方法不是把当前的元素追加到列表中，而是把当前元素直接更新进了 Reducing 的结果中。</li><li>AggregatingState 的区别是在访问接口，ReducingState 中 add（T）和 T get() 进去和出来的元素都是同一个类型，但在 AggregatingState 输入的 IN，输出的是 OUT。</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124020-c155b729-8696-4b0b-ba68-99fda30b9612.png#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1059&size=0&status=done&style=none&width=1059" alt><br>下面以 ValueState 为例，来阐述一下具体如何使用，以状态机的案例来讲解 。<br><a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java" target="_blank" rel="external nofollow noopener noreferrer">源代码地址</a><br><br><br>感兴趣的同学可直接查看完整源代码，在此截取部分。如图为 Flink 作业的主方法与主函数中的内容，前面的输入、后面的输出以及一些个性化的配置项都已去掉，仅保留了主干。<br><br><br>首先 events 是一个 DataStream，通过 env.addSource 加载数据进来，接下来有一个 DataStream 叫 alerts，先 keyby 一个 sourceAddress，然后在 flatMap 一个 StateMachineMapper。StateMachineMapper 就是一个状态机，状态机指有不同的状态与状态间有不同的转换关系的结合，以买东西的过程简单举例。首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货。已付款，待发货的状态再来一个事件发货，订单状态将会变为配送中，配送中的状态再来一个事件签收，则该订单的状态就变为已签收。在整个过程中，随时都可以来一个事件，取消订单，无论哪个状态，一旦触发了取消订单事件最终就会将状态转移到已取消，至此状态就结束了。<br><br><br>Flink 写状态机是如何实现的？首先这是一个 RichFlatMapFunction，要用 Keyed State getRuntimeContext，getRuntimeContext 的过程中需要 RichFunction，所以需要在 open 方法中获取 currentState ，然后 getState，currentState 保存的是当前状态机上的状态。<br><br><br>如果刚下订单，那么 currentState 就是待付款状态，初始化后，currentState 就代表订单完成。订单来了后，就会走 flatMap 这个方法，在 flatMap 方法中，首先定义一个 State，从 currentState 取出，即 Value，Value 取值后先判断值是否为空，如果 sourceAddress state 是空，则说明没有被使用过，那么此状态应该为刚创建订单的初始状态，即待付款。然后赋值 state = State.Initial，注意此处的 State 是本地的变量，而不是 Flink 中管理的状态，将它的值从状态中取出。接下来在本地又会来一个变量，然后 transition，将事件对它的影响加上，刚才待付款的订单收到付款成功的事件，就会变成已付款，待发货，然后 nextState 即可算出。此外，还需要判断 State 是否合法，比如一个已签收的订单，又来一个状态叫取消订单，会发现已签收订单不能被取消，此时这个状态就会下发，订单状态为非法状态。<br><br><br>如果不是非法的状态，还要看该状态是否已经无法转换，比如这个状态变为已取消时，就不会在有其他的状态再发生了，此时就会从 state 中 clear。clear 是所有的 Flink 管理 keyed state 都有的公共方法，意味着将信息删除，如果既不是一个非法状态也不是一个结束状态，后面可能还会有更多的转换，此时需要将订单的当前状态 update ，这样就完成了 ValueState 的初始化、取值、更新以及清零，在整个过程中状态机的作用就是将非法的状态进行下发，方便下游进行处理。其他的状态也是类似的使用方式。<br><a name="YBpXO"></a></p><h3 id="三-容错机制与故障恢复"><a href="#三-容错机制与故障恢复" class="headerlink" title="三. 容错机制与故障恢复"></a>三. 容错机制与故障恢复</h3><p><a name="sBIcD"></a></p><h4 id="1-状态如何保存及恢复"><a href="#1-状态如何保存及恢复" class="headerlink" title="1. 状态如何保存及恢复"></a>1. 状态如何保存及恢复</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056124002-fbb53fb6-e25b-4651-87c4-ea2a384b6e52.png#align=left&display=inline&height=595&margin=%5Bobject%20Object%5D&originHeight=595&originWidth=1062&size=0&status=done&style=none&width=1062" alt><br>Flink 状态保存主要依靠 Checkpoint 机制，Checkpoint 会定时制作分布式快照，对程序中的状态进行备份，这里就不在阐述分布式快照具体是如何实现的。分布式快照 Checkpoint 完成后，当作业发生故障了如何去恢复？<br><br><br>假如作业分布跑在 3 台机器上，其中一台挂了。这个时候需要把进程或者线程移到 active 的 2 台机器上，此时还需要将整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态，然后从该点开始继续处理。<br><br><br>如果要从 Checkpoint 恢复，必要条件是数据源需要支持数据重新发送。Checkpoint 恢复后， Flink 提供两种一致性语义，一种是恰好一次，一种是至少一次。在做 Checkpoint 时，可根据 Barries 对齐来判断是恰好一次还是至少一次，如果对齐，则为恰好一次，否则没有对齐即为至少一次。如果作业是单线程处理，也就是说 Barries 是不需要对齐的；如果只有一个 Checkpoint 在做，不管什么时候从 Checkpoint 恢复，都会恢复到刚才的状态；如果有多个节点，假如一个数据的 Barries 到了，另一个 Barries 还没有来，内存中的状态如果已经存储。那么这 2 个流是不对齐的，恢复的时候其中一个流可能会有重复。<br><br><br>Checkpoint 通过代码的实现方法如下：</p><ul><li>首先从作业的运行环境 env.enableCheckpointing 传入 1000，意思是做 2 个 Checkpoint 的事件间隔为 1 秒。Checkpoint 做的越频繁，恢复时追数据就会相对减少，同时 Checkpoint 相应的也会有一些 IO 消耗。</li><li>接下来是设置 Checkpoint 的 model，即设置了 Exactly_Once 语义，并且需要 Barries 对齐，这样可以保证消息不会丢失也不会重复。</li><li>setMinPauseBetweenCheckpoints 是 2 个 Checkpoint 之间最少是要等 500ms，也就是刚做完一个 Checkpoint。比如某个 Checkpoint 做了 700ms，按照原则过 300ms 应该是做下一个 Checkpoint，因为设置了 1000ms 做一次 Checkpoint 的，但是中间的等待时间比较短，不足 500ms 了，需要多等 200ms，因此以这样的方式防止 Checkpoint 太过于频繁而导致业务处理的速度下降。</li><li>setCheckpointTimeout 表示做 Checkpoint 多久超时，如果 Checkpoint 在 1min 之内尚未完成，说明 Checkpoint 超时失败。<br>setMaxConcurrentCheckpoints 表示同时有多少个 Checkpoint 在做快照，这个可以根据具体需求去做设置。</li><li>enableExternalizedCheckpoints 表示下 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。</li></ul><p><br>上面讲过，除了故障恢复之外，还需要可以手动去调整并发重新分配这些状态。手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在，那么作业如何恢复数据？<br><br><br>一方面 Flink 在 Cancel 时允许在外部介质保留 Checkpoint ；另一方面，Flink 还有另外一个机制是 SavePoint。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056126709-c2281720-3121-45c4-8f5c-f0d82fcdd9a0.png#align=left&display=inline&height=592&margin=%5Bobject%20Object%5D&originHeight=592&originWidth=1058&size=0&status=done&style=none&width=1058" alt><br>Savepoint 与 Checkpoint 类似，同样是把状态存储到外部介质。当作业失败时，可以从外部恢复。Savepoint 与 Checkpoint 有什么区别呢？</p><ul><li><p>从触发管理方式来讲，Checkpoint 由 Flink 自动触发并管理，而 Savepoint 由用户手动触发并人肉管理；</p></li><li><p>从用途来讲，Checkpoint 在 Task 发生异常时快速恢复，例如网络抖动或超时异常，而 Savepoint 有计划地进行备份，使作业能停止后再恢复，例如修改代码、调整并发；</p></li><li><p>最后从特点来讲，Checkpoint 比较轻量级，作业出现问题会自动从故障中恢复，在作业停止后默认清除；而 Savepoint 比较持久，以标准格式存储，允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复。<br><a name="SRvWd"></a></p><h4 id="2-可选的状态存储方式"><a href="#2-可选的状态存储方式" class="headerlink" title="2. 可选的状态存储方式"></a>2. 可选的状态存储方式</h4><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056126684-3aaf0d05-24ea-4444-b3ab-0fce128465c4.png#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1060&size=0&status=done&style=none&width=1060" alt><br>Checkpoint 的存储，第一种是内存存储，即 MemoryStateBackend，构造方法是设置最大的 StateSize，选择是否做异步快照，这种存储状态本身存储在 TaskManager 节点也就是执行节点内存中的，因为内存有容量限制，所以单个 State maxStateSize 默认 5 M，且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M。Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。<strong>推荐使用的场景为：本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。</strong><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056126985-20e0d490-8a3b-4b15-b3d4-856d19b5e5d2.png#align=left&display=inline&height=592&margin=%5Bobject%20Object%5D&originHeight=592&originWidth=1059&size=0&status=done&style=none&width=1059" alt><br>另一种就是在文件系统上的 FsStateBackend ，构建方法是需要传一个文件路径和是否异步快照。State 依然在 TaskManager 内存中，但不会像 MemoryStateBackend 有 5 M 的设置上限，Checkpoint 存储在外部文件系统（本地或 HDFS），打破了总大小 Jobmanager 内存的限制。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。<strong>推荐使用的场景、常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启 HA 的作业。</strong><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594056126760-b3f5d051-6ee0-4e74-bf97-a67f770cba8d.png#align=left&display=inline&height=593&margin=%5Bobject%20Object%5D&originHeight=593&originWidth=1060&size=0&status=done&style=none&width=1060" alt><br>还有一种存储为 RocksDBStateBackend ，RocksDB 是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着每次用户不需要将所有状态都写进去，将增量的改变的状态写进去即可。它的 Checkpoint 存储在外部文件系统（本地或 HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存 + 磁盘，单 Key 最大 2G，总大小不超过配置的文件系统容量即可。<strong>推荐使用的场景为：超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。</strong><br>**<br><a name="Os3oT"></a></p><h4 id="3-StateBackend-的分类"><a href="#3-StateBackend-的分类" class="headerlink" title="3. StateBackend 的分类"></a>3. StateBackend 的分类</h4><p>下面这张图对目前广泛使用的三类 state backend 做了区分，其中绿色表示所创建的operator/keyed state backend 是 on-heap 的，黄色则表示是 off-heap 的。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497686-7867c701-5353-4e0e-837c-44ba5ecad74a.png#align=left&display=inline&height=433&margin=%5Bobject%20Object%5D&originHeight=433&originWidth=899&size=0&status=done&style=none&width=899" alt><br><br><br>一般而言，在生产中，我们会在 FsStateBackend 和 RocksDBStateBackend 间选择：</p></li><li><p><strong>FsStateBackend</strong>：性能更好；日常存储是在堆内存中，面临着 OOM 的风险，不支持增量 checkpoint。</p></li><li><p><strong>RocksDBStateBackend</strong>：无需担心 OOM 风险，是大部分时候的选择。<br><a name="DhXl8"></a></p><h4 id="4-RocksDB-StateBackend-概览和相关配置讨论"><a href="#4-RocksDB-StateBackend-概览和相关配置讨论" class="headerlink" title="4.RocksDB StateBackend 概览和相关配置讨论"></a>4.RocksDB StateBackend 概览和相关配置讨论</h4></li></ul><p><br>RocksDB 是 Facebook 开源的 LSM 的键值存储数据库，被广泛应用于大数据系统的单机组件中。Flink 的 keyed state 本质上来说就是一个键值对，所以与 RocksDB 的数据模型是吻合的。下图分别是 “window state” 和 “value state” 在 RocksDB 中的存储格式，所有存储的 key，value 均被序列化成 bytes 进行存储。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497724-6e1a4f76-caf1-4825-bebc-a3953a52bbe2.png#align=left&display=inline&height=352&margin=%5Bobject%20Object%5D&originHeight=352&originWidth=899&size=0&status=done&style=none&width=899" alt><br><br><br>在 RocksDB 中，每个 state 独享一个 Column Family，而每个 Column family 使用各自独享的 write buffer 和 block cache，上图中的 window state 和 value state实际上分属不同的 column family。<br><br><br>下面介绍一些对 RocksDB 性能比较有影响的参数，并整理了一些相关的推荐配置，至于其他配置项，可以参阅社区相关文档。</p><table><thead><tr><th align="left">状态</th><th align="left">建议</th></tr></thead><tbody><tr><td align="left">state.backend.rocksdb.thread.num</td><td align="left">后台 flush 和 compaction 的线程数. 默认值 ‘1‘. 建议调大</td></tr><tr><td align="left">state.backend.rocksdb.writebuffer.count</td><td align="left">每个 column family 的 write buffer 数目，默认值 ‘2‘. 如果有需要可以适当调大</td></tr><tr><td align="left">state.backend.rocksdb.writebuffer.size</td><td align="left">每个 write buffer 的 size，默认值‘64MB‘. 对于写频繁的场景，建议调大</td></tr><tr><td align="left">state.backend.rocksdb.block.cache-size</td><td align="left">每个 column family 的 block cache大小，默认值‘8MB’，如果存在重复读的场景，建议调大</td></tr></tbody></table><p><a name="vzbHq"></a></p><h4 id="5-State-best-practice：一些使用-state-的心得"><a href="#5-State-best-practice：一些使用-state-的心得" class="headerlink" title="5.State best practice：一些使用 state 的心得"></a>5.State best practice：一些使用 state 的心得</h4><p><strong><br></strong>Operator state 使用建议<strong><br></strong>■ 慎重使用长 list<strong><br>下图展示的是目前 task 端 operator state 在执行完 checkpoint 返回给 job master 端的 StateMetaInfo 的代码片段。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497758-3d0e807f-2d61-4a67-8832-a36b79df83e1.png#align=left&display=inline&height=360&margin=%5Bobject%20Object%5D&originHeight=360&originWidth=899&size=0&status=done&style=none&width=899" alt><br>由于 operator state 没有 key group 的概念，所以为了实现改并发恢复的功能，需要对 operator state 中的每一个序列化后的元素存储一个位置偏移 offset，也就是构成了上图红框中的 offset 数组。<br><br><br>那么如果你的 operator state 中的 list 长度达到一定规模时，这个 offset 数组就可能会有几十 MB 的规模，关键这个数组是会返回给 job master，当 operator 的并发数目很大时，很容易触发 job master 的内存超用问题。我们遇到过用户把 operator state 当做黑名单存储，结果这个黑名单规模很大，导致一旦开始执行 checkpoint，job master 就会因为收到 task 发来的“巨大”的 offset 数组，而内存不断增长直到超用无法正常响应。<br><br><br></strong>■ 正确使用 UnionListState**<br>union list state 目前被广泛使用在 kafka connector 中，不过可能用户日常开发中较少遇到，他的语义是从检查点恢复之后每个并发 task 内拿到的是原先所有operator 上的 state，如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497684-0428bc6e-1918-4fb8-8f60-a6e9250996f8.png#align=left&display=inline&height=327&margin=%5Bobject%20Object%5D&originHeight=327&originWidth=849&size=0&status=done&style=none&width=849" alt><br>kafka connector 使用该功能，为的是从检查点恢复时，可以拿到之前的全局信息，如果用户需要使用该功能，需要切记恢复的 task 只取其中的一部分进行处理和用于下一次 snapshot，否则有可能随着作业不断的重启而导致 state 规模不断增长。<br></p><p><a name="bTLj1"></a></p><h4 id="6-Keyed-state-使用建议"><a href="#6-Keyed-state-使用建议" class="headerlink" title="6. Keyed state 使用建议"></a>6. Keyed state 使用建议</h4><p><strong>■ 如何正确清空当前的 state</strong><br>state.clear() 实际上只能清理当前 key 对应的 value 值，如果想要清空整个 state，需要借助于 applyToAllKeys 方法，具体代码片段如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497725-7614e5f7-ed39-4112-84d8-9a6689875966.png#align=left&display=inline&height=168&margin=%5Bobject%20Object%5D&originHeight=168&originWidth=899&size=0&status=done&style=none&width=899" alt><br>如果你的需求中只是对 state 有过期需求，借助于 state TTL 功能来清理会是一个性能更好的方案。<br><strong>■ RocksDB 中考虑 value 值很大的极限场景</strong><br>受限于 JNI bridge API 的限制，单个 value 只支持 2^31 bytes 大小，如果存在很极限的情况，可以考虑使用 MapState 来替代 ListState 或者 ValueState，因为RocksDB 的 map state 并不是将整个 map 作为 value 进行存储，而是将 map 中的一个条目作为键值对进行存储。<br><strong>■ 如何知道当前 RocksDB 的运行情况</strong><br>比较直观的方式是打开 RocksDB 的 native metrics ，在默认使用 Flink managed memory 方式的情况下，state.backend.rocksdb.metrics.block-cache-usage ，state.backend.rocksdb.metrics.mem-table-flush-pending，state.backend.rocksdb.metrics.num-running-compactions 以及 state.backend.rocksdb.metrics.num-running-flushes 是比较重要的相关 metrics。<br>下面这张图是 Flink-1.10 之后，打开相关 metrics 的示例图：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497735-408964c0-efb8-4ad1-a162-ace3cc1ab7b5.png#align=left&display=inline&height=506&margin=%5Bobject%20Object%5D&originHeight=506&originWidth=899&size=0&status=done&style=none&width=899" alt><br><br><br>而下面这张是 Flink-1.10 之前或者关闭 state.backend.rocksdb.memory.managed 的效果：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1594057497739-56972272-4605-48e1-b24c-59f9b57c7117.png#align=left&display=inline&height=504&margin=%5Bobject%20Object%5D&originHeight=504&originWidth=899&size=0&status=done&style=none&width=899" alt><br><strong>■ 容器内运行的 RocksDB 的内存超用问题</strong><br>在 Flink-1.10 之前，由于一个 state 独占若干 write buffer 和一块 block cache，所以我们会建议用户不要在一个 operator 内创建过多的 state，否则需要考虑到相应的额外内存使用量，否则容易造成在容器内运行时，相关进程被容器环境所杀。对于用户来说，需要考虑一个 slot 内有多少 RocksDB 实例在运行，一个 RocksDB 中有多少 state，整体的计算规则就很复杂，很难真得落地实施。<br>Flink-1.10 之后，由于引入了 RocksDB 的内存托管机制，在绝大部分情况下， RocksDB 的这一部分 native 内存是可控的，不过受限于 RocksDB 的相关 cache 实现限制（这里暂不展开，后续会有文章讨论），在某些场景下，无法做到完美控制，这时候建议打开上文提到的 native metrics，观察相关 block cache 内存使用是否存在超用情况，可以将相关内存添加到 taskmanager.memory.task.off-heap.size 中，使得 Flink 有更多的空间给 native 内存使用。<br><a name="u4FoX"></a></p><h4 id="7-一些使用-checkpoint-的使用建议"><a href="#7-一些使用-checkpoint-的使用建议" class="headerlink" title="7.一些使用 checkpoint 的使用建议"></a>7.一些使用 checkpoint 的使用建议</h4><p><strong>■ Checkpoint 间隔不要太短</strong><br>虽然理论上 Flink 支持很短的 checkpoint 间隔，但是在实际生产中，过短的间隔对于底层分布式文件系统而言，会带来很大的压力。另一方面，由于检查点的语义，所以实际上 Flink 作业处理 record 与执行 checkpoint 存在互斥锁，过于频繁的 checkpoint，可能会影响整体的性能。当然，这个建议的出发点是底层分布式文件系统的压力考虑。<br><strong>■ 合理设置超时时间</strong><br>默认的超时时间是 10min，如果 state 规模大，则需要合理配置。最坏情况是分布式地创建速度大于单点（job master 端）的删除速度，导致整体存储集群可用空间压力较大。建议当检查点频繁因为超时而失败时，增大超时时间。<br>**<br><a name="tGfXq"></a></p><h3 id="四-总结"><a href="#四-总结" class="headerlink" title="四. 总结"></a>四. 总结</h3><p><a name="kI5n4"></a></p><h4 id="1-为什么要使用状态？"><a href="#1-为什么要使用状态？" class="headerlink" title="1. 为什么要使用状态？"></a>1. 为什么要使用状态？</h4><p>前面提到有状态的作业要有有状态的逻辑，有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。所以需要通过状态来满足业务逻辑。<br><a name="8jglw"></a></p><h4 id="2-为什么要管理状态？"><a href="#2-为什么要管理状态？" class="headerlink" title="2. 为什么要管理状态？"></a>2. 为什么要管理状态？</h4><p>使用了状态，为什么要管理状态？因为实时作业需要 7*24 不间断的运行，需要应对不可靠的因素而带来的影响。<br><a name="CK5Ia"></a></p><h4 id="3-如何选择状态的类型和存储方式？"><a href="#3-如何选择状态的类型和存储方式？" class="headerlink" title="3. 如何选择状态的类型和存储方式？"></a>3. 如何选择状态的类型和存储方式？</h4><p>那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。<br><br><br>转载自：<a href="https://developer.aliyun.com/article/756955" target="_blank" rel="external nofollow noopener noreferrer">Flink State 最佳实践</a><br><a href="https://www.infoq.cn/article/VGKZA-S9fMBgABP71Pgh" target="_blank" rel="external nofollow noopener noreferrer">Apache Flink 零基础入门（六）：状态管理及容错机制</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;我们先回顾一下到底什么是 state，流式计算的数据往往是转瞬即逝， 当然，真实业务场景不可能说所有的数据都是进来之后就走掉，没有任何东西留下来
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="cpeixin.cn/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>def neverGrowUp()</title>
    <link href="cpeixin.cn/2020/04/06/def-neverGrowUp/"/>
    <id>cpeixin.cn/2020/04/06/def-neverGrowUp/</id>
    <published>2020-04-05T16:00:00.000Z</published>
    <updated>2020-04-05T15:10:24.541Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neverGrowUp</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="title">while</span> <span class="title">true</span>:</span></span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗疫英雄</title>
    <link href="cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/"/>
    <id>cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/</id>
    <published>2020-04-04T14:45:15.000Z</published>
    <updated>2020-04-05T14:46:33.308Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p>致敬缅怀每一位抗疫英雄<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586098032229-b4c6c795-bf87-4105-8f82-87a86e48a89a.jpeg#align=left&display=inline&height=1796&name=WechatIMG86.jpeg&originHeight=1796&originWidth=1072&size=175464&status=done&style=none&width=1072" alt="WechatIMG86.jpeg"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;致敬缅怀每一位抗疫英雄&lt;br&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/jpeg/1072113
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python Flask &amp; Ajax 数据传输</title>
    <link href="cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/"/>
    <id>cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/</id>
    <published>2020-03-11T14:43:01.000Z</published>
    <updated>2020-04-04T17:13:00.080Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p>帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂</p><p>忙活三个小时，终于把前端和后端打通了～～</p><p>前端demo：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"&#123;&#123; url_for('send_message') &#125;&#125;"</span> <span class="attr">method</span>=<span class="string">"post"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span> =<span class="string">"domain"</span> <span class="attr">rows</span>=<span class="string">"30"</span> <span class="attr">cols</span>=<span class="string">"100"</span> <span class="attr">placeholder</span>=<span class="string">"请输入需要查询的域名,如cq5999.com"</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;input id="submit" type="submit" value="发送"&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">id</span>=<span class="string">"btn-bq"</span> <span class="attr">data-toggle</span>=<span class="string">"modal"</span> <span class="attr">data-target</span>=<span class="string">"#myModal"</span>&gt;</span>查询<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"send_content"</span>&gt;</span>向后台发送消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"send_content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send"</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"发送"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"recv_content"</span>&gt;</span>从后台接收消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"recv_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"recv_content"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 发送 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> message = $(<span class="string">"#send_content"</span>).val()</span></span><br><span class="line">        alert(message)</span><br><span class="line"><span class="javascript">        $.ajax(&#123;</span></span><br><span class="line"><span class="actionscript">            url:<span class="string">"/send_message"</span>,</span></span><br><span class="line"><span class="actionscript">            type:<span class="string">"POST"</span>,</span></span><br><span class="line">            data:&#123;</span><br><span class="line">                message:message</span><br><span class="line">            &#125;,</span><br><span class="line"><span class="actionscript">            dataType: <span class="string">'json'</span>,</span></span><br><span class="line"><span class="actionscript">            success:<span class="function"><span class="keyword">function</span> <span class="params">(data)</span> </span>&#123;</span></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 接收 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        $.getJSON(<span class="string">"/change_to_json"</span>,<span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#recv_content"</span>).val(data.message) <span class="comment">//将后端数据显示在前端</span></span></span><br><span class="line"><span class="javascript">            <span class="built_in">console</span>.log(<span class="string">"传到前端的数据的类型："</span> + <span class="keyword">typeof</span> (data.message))</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#send_content"</span>).val(<span class="string">""</span>)<span class="comment">//发送的输入框清空</span></span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>后端demo:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template, request, jsonify</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"index_v6.html"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/send_message', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_message</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_get = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    message_get = request.form[<span class="string">"domain"</span>].split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># message_get = request.form['message'] #input提交</span></span><br><span class="line">    print(<span class="string">"收到前端发过来的信息：%s"</span> % message_get)</span><br><span class="line">    print(<span class="string">"收到数据的类型为："</span> + str(type(message_get)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"收到消息"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/change_to_json', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_to_json</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_json = &#123;</span><br><span class="line">        <span class="string">"message"</span>: message_get</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jsonify(message_json)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>,debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂&lt;/p&gt;&lt;p&gt;忙活三个小时，终于把前端和后端打通了～～&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Python Flask接口设计-示例</title>
    <link href="cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/"/>
    <id>cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/</id>
    <published>2020-03-10T15:08:35.000Z</published>
    <updated>2020-04-04T17:12:52.356Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p><a name="LHF1q"></a></p><h3 id="Get-请求"><a href="#Get-请求" class="headerlink" title="Get 请求"></a>Get 请求</h3><p><strong><strong>开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。</strong></strong><br><strong><br>**</strong>方法 1:****</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["GET"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><p>此种方式对应的request请求方式：</p><ol><li>拼接请求链接, 直接请求：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0/test_1.0?name=ccc&amp;age=18</a></li><li>request 请求中带有参数，如下图</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583826674613-bc99538a-988e-4386-b8e6-9eb9fce1862f.png#align=left&display=inline&height=610&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%883.47.43.png&originHeight=610&originWidth=1424&size=98593&status=done&style=none&width=1424" alt="屏幕快照 2020-03-10 下午3.47.43.png"></p><p>方法 2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">banWordSingleStart</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> getWordStatus(word)</span><br></pre></td></tr></table></figure><p>此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字</p><p>请求链接：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0</a>/api/banWordSingle/输入词</p><p><a name="vJdOc"></a></p><h3 id="Post-请求"><a href="#Post-请求" class="headerlink" title="Post 请求"></a>Post 请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["POST"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">8080</span>)</span><br></pre></td></tr></table></figure><p>请求方式：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583831085097-3a858ae4-259d-408d-a162-6a4ed8c5e291.png#align=left&display=inline&height=692&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%885.00.28.png&originHeight=692&originWidth=1438&size=99272&status=done&style=none&width=1438" alt="屏幕快照 2020-03-10 下午5.00.28.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;LHF1q&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;Get-请求&quot;&gt;&lt;a href=&quot;#Get-请求&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.10版本发布</title>
    <link href="cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/"/>
    <id>cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</id>
    <published>2020-02-12T17:22:22.000Z</published>
    <updated>2020-06-03T14:38:01.179Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p><br>Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡献了200多个贡献者，此版本引入了对Flink作业的整体性能和稳定性的重大改进，原生Kubernetes集成的预览以及Python支持的巨大进步（PyFlink）。(Spark对Python的支持也越来越好😂)<br><br><br>Flink 1.10还标志着<a href="https://flink.apache.org/news/2019/08/22/release-1.9.0.html#preview-of-the-new-blink-sql-query-processor" target="_blank" rel="external nofollow noopener noreferrer">Blink集成</a>的完成，强化了流数据SQL并通过可用于生产的Hive集成和TPC-DS覆盖将成熟的批处理引入Flink。这篇博客文章描述了所有主要的新功能和改进，需要注意的重要更改以及预期的发展。<br><br><br>现在可以在Flink网站的更新的“ <a href="https://flink.apache.org/downloads.html" target="_blank" rel="external nofollow noopener noreferrer">下载”页面</a>上找到二进制分发文件和源工件。有关更多详细信息，请查看完整的<a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&version=12345845" target="_blank" rel="external nofollow noopener noreferrer">发行变更日志</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/" target="_blank" rel="external nofollow noopener noreferrer">更新的文档</a>。我们鼓励您下载发行版，并通过<a href="https://flink.apache.org/community.html#mailing-lists" target="_blank" rel="external nofollow noopener noreferrer">Flink邮件列表</a>或<a href="https://issues.apache.org/jira/projects/FLINK/summary" target="_blank" rel="external nofollow noopener noreferrer">JIRA</a>与社区分享您的反馈。</p><hr><p><a name="JxUa7"></a></p><h2 id="新功能和改进"><a href="#新功能和改进" class="headerlink" title="新功能和改进"></a>新功能和改进</h2><p><a name="improved-memory-management-and-configuration"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="ze04S"></a></p><h3 id="改进的内存管理和配置"><a href="#改进的内存管理和配置" class="headerlink" title="改进的内存管理和配置"></a>改进的内存管理和配置</h3><p><br>目前Flink中的TaskExecutor内存配置存在一些缺点，这些缺点使得难以推理或优化资源利用率，例如：</p><ul><li>流处理和批处理执行中用于内存占用的不同配置模型；<br></li><li>流处理执行中堆外状态后端（即RocksDB）的复杂且依赖用户的配置。<br></li></ul><p><br>为了使内存选项对用户更明确和直观，Flink 1.10对TaskExecutor内存模型和配置逻辑（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors" target="_blank" rel="external nofollow noopener noreferrer">FLIP-49</a>）进行了重大更改。这些更改使Flink更适合于各种部署环境（例如Kubernetes，Yarn，Mesos），从而使用户可以严格控制其内存消耗。<br><br><br><strong>托管内存扩展</strong><br><br><br>托管内存已经扩展扩展，当然还考虑了RocksDB StateBackend的内存使用情况。虽然批处理作业可以使用堆内（on-heap）或堆外（off-heap）内存，但具有这些功能的流作业RocksDBStateBackend只能使用堆内内存。因此，为了允许用户在流执行和批处理执行之间切换而不必修改群集配置，托管内存现在始终处于堆外状态。<br><br><br><strong>简化RocksDB配置</strong><br>**<br>曾经配置像RocksDB这样的off-heap (堆外)state backend涉及大量的手动调整，例如减小JVM堆大小或将Flink设置为使用堆外内存。现在可以通过Flink的现成配置来实现，并且调整RocksDBStateBackend内存预算就像调整内存大小一样简单。<br><br><br>另一个重要的改进是允许Flink绑定RocksDB本地内存使用情况（<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="external nofollow noopener noreferrer">FLINK-7289</a>），从而防止其超出总内存预算-这在Kubernetes等容器化环境中尤其重要。有关如何启用和调整此功能的详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/state/large_state_tuning.html#tuning-rocksdb" target="_blank" rel="external nofollow noopener noreferrer">Tuning RocksDB</a>。<br>注意 FLIP-49更改了群集资源配置的过程，这可能需要调整群集以从以前的Flink版本进行升级。有关所引入更改和调整指南的全面概述，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_setup.html" target="_blank" rel="external nofollow noopener noreferrer">此设置</a>。<br></p><p><a name="unified-logic-for-job-submission"></a></p><h3 id="提交作业的统一逻辑"><a href="#提交作业的统一逻辑" class="headerlink" title="提交作业的统一逻辑"></a>提交作业的统一逻辑</h3><p><br>在此版本之前，提交作业是执行环境的一部分职责，并且与不同的部署目标（例如，Yarn，Kubernetes，Mesos）紧密相关。这导致关注点分离不佳，并且随着时间的流逝，用户需要单独配置和管理的定制环境越来越多。<br><br><br>在Flink 1.10中，作业提交逻辑被抽象到通用Executor接口（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-73%3A+Introducing+Executors+for+job+submission" target="_blank" rel="external nofollow noopener noreferrer">FLIP-73</a>）中。另外ExecutorCLI（<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=133631524" target="_blank" rel="external nofollow noopener noreferrer">FLIP-81</a>）引入了一个统一的方式去指定配置参数对于任何 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/cli.html#deployment-targets" target="_blank" rel="external nofollow noopener noreferrer">执行对象</a>。为了完善这项工作，结果检索的过程也与作业提交分离，引入了JobClient（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API" target="_blank" rel="external nofollow noopener noreferrer">FLINK-74</a>）来负责获取JobExecutionResult。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590943394674-2fd89b17-b61e-4180-986d-72062c95f4d4.png#align=left&display=inline&height=334&margin=%5Bobject%20Object%5D&originHeight=789&originWidth=1999&size=0&status=done&style=none&width=847" alt><br><br><br>特别是，这些更改通过为用户提供Flink的统一入口点，使在下游框架（例如Apache Beam或Zeppelin交互式笔记本）中以编程方式使用Flink变得更加容易。对于跨多个目标环境使用Flink的用户，向基于配置的执行过程的过渡还可以显着减少样板代码和可维护性开销。<br></p><p><a name="native-kubernetes-integration-beta"></a></p><h3 id="原生Kubernetes集成（测试版）"><a href="#原生Kubernetes集成（测试版）" class="headerlink" title="原生Kubernetes集成（测试版）"></a>原生Kubernetes集成（测试版）</h3><p><br>对于希望在容器化环境上开始使用Flink的用户，在Kubernetes之上部署和管理独立集群需要有关容器，算子和环境工具kubectl的一些知识。<br><br><br>在Flink 1.10中，我们推出了Active Kubernetes集成（<a href="https://jira.apache.org/jira/browse/FLINK-9953" target="_blank" rel="external nofollow noopener noreferrer">FLINK-9953</a>）的第一阶段，其中，“主动”指 Flink ResourceManager (K8sResMngr) 原生地与 Kubernetes 通信，像 Flink 在 Yarn 和 Mesos 上一样按需申请 pod。用户可以利用 namespace，在多租户环境中以较少的资源开销启动 Flink。这需要用户提前配置好 RBAC 角色和有足够权限的服务账号。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590943394177-de66ac47-ba2d-4182-b5b4-585ff643cfc8.png#align=left&display=inline&height=322&margin=%5Bobject%20Object%5D&originHeight=870&originWidth=1714&size=0&status=done&style=none&width=635" alt><br><br><br>正如刚刚讲到的，Flink 1.10中的所有命令行选项都映射到统一配置。因此，用户可以简单地引用Kubernetes配置选项，然后使用以下命令在CLI中将作业提交到Kubernetes上的现有Flink会话：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run -d -e kubernetes-session -Dkubernetes.cluster-id&#x3D;&lt;ClusterId&gt; examples&#x2F;streaming&#x2F;WindowJoin.jar</span><br></pre></td></tr></table></figure><p><br>如果您想尝试使用此预览功能，我们建议您逐步完成本<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/native_kubernetes.html" target="_blank" rel="external nofollow noopener noreferrer">机Kubernetes的安装</a>，试用并与社区分享反馈。<br></p><p><a name="table-apisql-production-ready-hive-integration"></a></p><h3 id="Table-API-SQL：生产就绪的Hive集成"><a href="#Table-API-SQL：生产就绪的Hive集成" class="headerlink" title="Table API / SQL：生产就绪的Hive集成"></a>Table API / SQL：生产就绪的Hive集成</h3><p><br>Hive集成在Flink 1.9中宣布为预览功能。此预览允许用户使用SQL DDL将Flink-specific元数据（例如Kafka表）保留在Hive Metastore中，调用Hive中定义的UDF并使用Flink读取和写入Hive表。Flink 1.10通过进一步的开发使这项工作更加圆满，这些开发使可立即投入生产的Hive集成到Flink，并具有与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/#supported-hive-versions" target="_blank" rel="external nofollow noopener noreferrer">大多数Hive版本的</a>完全兼容性。<br><a name="native-partition-support-for-batch-sql"></a></p><h4 id="-1"><a href="#-1" class="headerlink"></a></h4><p><a name="IuODz"></a></p><h4 id="批处理SQL的本地分区支持"><a href="#批处理SQL的本地分区支持" class="headerlink" title="批处理SQL的本地分区支持"></a>批处理SQL的本地分区支持</h4><p><br>1.10版本以前，仅支持对未分区的Hive表进行写入。在Flink 1.10中，Flink SQL语法已通过<code>INSERT OVERWRITE</code>和<code>PARTITION</code>（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-63%3A+Rework+table+partition+support" target="_blank" rel="external nofollow noopener noreferrer">FLIP-63</a>）进行了扩展，使用户能够在Hive中写入静态和动态分区。<br><strong><br></strong>静态分区写入**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;</span><br></pre></td></tr></table></figure><p><strong><br></strong>动态分区编写**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 select_statement1 FROM from_statement;</span><br></pre></td></tr></table></figure><p><br>Flink对于分区表的全面支持，允许用户利用读取时的分区修剪功能，通过减少需要扫描的数据量来显着提高这些操作的性能。<br></p><p><a name="further-optimizations"></a></p><h4 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h4><p>除了分区修剪外，Flink 1.10还为Hive集成引入了更多<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/read_write_hive.html#optimizations" target="_blank" rel="external nofollow noopener noreferrer">读取优化</a>，例如：<br></p><ul><li><strong>投影下推：</strong> Flink通过省略表扫描中不必要的字段，利用投影下推来最大程度地减少Flink和Hive表之间的数据传输。这对于具有大量列的表尤其有利。</li></ul><ul><li><strong>LIMIT下推：</strong>对于带有<code>LIMIT</code>子句的查询，Flink将尽可能限制输出记录的数量，以最大程度地减少通过网络传输的数据量。</li></ul><ul><li><strong>读取时</strong>进行<strong>ORC矢量化：</strong>为了提高ORC文件的读取性能，Flink现在默认将本机ORC矢量化阅读器用于2.0.0以上的Hive版本以及具有非复杂数据类型的列。</li></ul><p><a name="pluggable-modules-as-flink-system-objects-beta"></a></p><h4 id="可插拔模块作为Flink系统对象（Beta）"><a href="#可插拔模块作为Flink系统对象（Beta）" class="headerlink" title="可插拔模块作为Flink系统对象（Beta）"></a>可插拔模块作为Flink系统对象（Beta）</h4><p>Flink 1.10引入了Flink Table核心中可插拔模块的通用机制，首先关注系统功能（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-68%3A+Extend+Core+Table+System+with+Pluggable+Modules" target="_blank" rel="external nofollow noopener noreferrer">FLIP-68</a>）。使用该模块，用户可以扩展Flink的系统对象，例如，使用行为类似于Flink系统功能的Hive内置函数。该版本附带一个预先实现的<code>HiveModule</code>，支持多个Hive版本的版本，但用户也可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/modules.html" target="_blank" rel="external nofollow noopener noreferrer">编写自己的可插拔模块</a>。<br><a name="other-improvements-to-the-table-apisql"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="rCuCf"></a></p><h3 id="Table-API-SQL的其他改进"><a href="#Table-API-SQL的其他改进" class="headerlink" title="Table API / SQL的其他改进"></a>Table API / SQL的其他改进</h3><p><a name="watermarks-and-computed-columns-in-sql-ddl"></a></p><h4 id="-3"><a href="#-3" class="headerlink"></a></h4><p><a name="FlcDT"></a></p><h4 id="SQL-DDL中的水印和计算列"><a href="#SQL-DDL中的水印和计算列" class="headerlink" title="SQL DDL中的水印和计算列"></a>SQL DDL中的水印和计算列</h4><p>Flink 1.10支持特定于流的语法扩展，以在Flink SQL DDL（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-66%3A+Support+Time+Attribute+in+SQL+DDL" target="_blank" rel="external nofollow noopener noreferrer">FLIP-66</a>）中定义时间属性和水印生成。这允许基于时间的操作（例如加窗），以及在使用DDL语句创建的表上定义<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html#create-table" target="_blank" rel="external nofollow noopener noreferrer">水印策略</a>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE table_name (</span><br><span class="line">  WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;</span><br><span class="line">) WITH (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><br>此版本还引入了对虚拟计算列（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-70%3A+Flink+SQL+Computed+Column+Design" target="_blank" rel="external nofollow noopener noreferrer">FLIP-70</a>）的支持，该列可基于同一表中的其他列或确定性表达式（即，文字值，UDF和内置函数）派生。在Flink中，计算列可用于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html#create-table" target="_blank" rel="external nofollow noopener noreferrer">在创建表时</a>定义时间属性。<br><a name="additional-extensions-to-sql-ddl"></a></p><h4 id="-4"><a href="#-4" class="headerlink"></a></h4><p><a name="Exz4X"></a></p><h4 id="SQL-DDL的其他扩展"><a href="#SQL-DDL的其他扩展" class="headerlink" title="SQL DDL的其他扩展"></a>SQL DDL的其他扩展</h4><p><br>现在，temporary/persistent 和 system/catalog（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-57%3A+Rework+FunctionCatalog" target="_blank" rel="external nofollow noopener noreferrer">FLIP-57</a>）之间有明显的区别。这不仅消除了函数引用中的歧义，而且允许确定性的函数解析顺序（即，在命名冲突的情况下，系统函数将优先于目录函数，而临时函数的优先级高于两个维度的持久性函数）。<br><br><br>遵循FLIP-57的基础知识，我们扩展了SQL DDL语法以支持目录功能，临时功能和临时系统功能（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-79+Flink+Function+DDL+Support" target="_blank" rel="external nofollow noopener noreferrer">FLIP-79</a>）的创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION </span><br><span class="line">  [IF NOT EXISTS] [catalog_name.][db_name.]function_name </span><br><span class="line">AS identifier [LANGUAGE JAVA|SCALA]</span><br></pre></td></tr></table></figure><p><br>有关Flink SQL中DDL支持的当前状态的完整概述，请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/" target="_blank" rel="external nofollow noopener noreferrer">更新的文档</a>。<br><strong><br></strong>注意**为了将来能正确处理和保证元对象（表，视图，函数）之间的行为一致，不建议使用Table API中的某些对象声明方法，而应使用更接近标准SQL DDL的方法（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-64%3A+Support+for+Temporary+Objects+in+Table+module" target="_blank" rel="external nofollow noopener noreferrer">FLIP -64</a>）。<br><a name="full-tpc-ds-coverage-for-batch"></a></p><h4 id="-5"><a href="#-5" class="headerlink"></a></h4><p><a name="eHBxG"></a></p><h4 id="TPC-DS的完整覆盖范围可批量处理"><a href="#TPC-DS的完整覆盖范围可批量处理" class="headerlink" title="TPC-DS的完整覆盖范围可批量处理"></a>TPC-DS的完整覆盖范围可批量处理</h4><p><br>TPC-DS是一种广泛使用的行业标准决策支持基准，用于评估和衡量基于SQL的数据处理引擎的性能。在Flink 1.10中，端到端（<a href="https://issues.apache.org/jira/browse/FLINK-11491" target="_blank" rel="external nofollow noopener noreferrer">FLINK-11491</a>）支持所有TPC-DS查询，这反映了它的SQL引擎已准备就绪，可以满足类似现代数据仓库的工作负载的需求。<br><a name="pyflink-support-for-native-user-defined-functions-udfs"></a></p><h3 id="-6"><a href="#-6" class="headerlink"></a></h3><p><a name="TgP6u"></a></p><h3 id="PyFlink：支持本机用户定义的函数（UDF）"><a href="#PyFlink：支持本机用户定义的函数（UDF）" class="headerlink" title="PyFlink：支持本机用户定义的函数（UDF）"></a>PyFlink：支持本机用户定义的函数（UDF）</h3><p><br>在以前的发行版中引入了PyFlink的预览版，朝着实现Flink中完全Python支持的目标迈进了一步。对于此发行版，重点是使用户能够在表API / SQL（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-58%3A+Flink+Python+User-Defined+Stateless+Function+for+Table" target="_blank" rel="external nofollow noopener noreferrer">FLIP-58</a>）中注册和使用Python用户定义函数（UDF，已计划UDTF / UDAF ）。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1590945929231-b78d0f66-a731-4359-8aa2-442b5f78bc77.gif#align=left&display=inline&height=405&margin=%5Bobject%20Object%5D&name=flink_1.10_pyflink.gif&originHeight=405&originWidth=779&size=2561122&status=done&style=none&width=779" alt="flink_1.10_pyflink.gif"><br><br><br>如果您对基础实现感兴趣（利用Apache Beam的<a href="https://beam.apache.org/roadmap/portability/" target="_blank" rel="external nofollow noopener noreferrer">可移植性框架）</a>，请参考FLIP-58的“架构”部分，也请参考<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-78%3A+Flink+Python+UDF+Environment+and+Dependency+Management" target="_blank" rel="external nofollow noopener noreferrer">FLIP-78</a>。这些数据结构为Pandas支持和PyFlink最终到达DataStream API奠定了必要的基础。<br><br><br>从Flink 1.10开始，用户还可以<code>pip</code>使用以下方法轻松安装PyFlink ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install apache-flink</span><br></pre></td></tr></table></figure><p><br>有关PyFlink计划进行的其他改进的预览，请查看<a href="https://issues.apache.org/jira/browse/FLINK-14500" target="_blank" rel="external nofollow noopener noreferrer">FLINK-14500</a>并参与有关所需用户功能的<a href="http://apache-flink.147419.n8.nabble.com/Re-DISCUSS-What-parts-of-the-Python-API-should-we-focus-on-next-td1285.html" target="_blank" rel="external nofollow noopener noreferrer">讨论</a>。<br><a name="important-changes"></a></p><h2 id="重要变化"><a href="#重要变化" class="headerlink" title="重要变化"></a>重要变化</h2><ul><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-10725" target="_blank" rel="external nofollow noopener noreferrer">FLINK-10725</a> ] Flink现在可以编译并在Java 11上运行。<br></li><li>[ <a href="https://jira.apache.org/jira/browse/FLINK-15495" target="_blank" rel="external nofollow noopener noreferrer">FLINK-15495</a> ] Blink计划程序现在是SQL Client中的默认设置，因此用户可以从所有最新功能和改进中受益。在下一个版本中，还计划从Table API中的旧计划程序进行切换，因此我们建议用户开始熟悉Blink计划程序。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-13025" target="_blank" rel="external nofollow noopener noreferrer">FLINK-13025</a> ]有一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/elasticsearch.html#elasticsearch-connector" target="_blank" rel="external nofollow noopener noreferrer">新的Elasticsearch接收器连接器</a>，完全支持Elasticsearch 7.x版本。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-15115" target="_blank" rel="external nofollow noopener noreferrer">FLINK-15115</a> ] Kafka 0.8和0.9的连接器已标记为不推荐使用，将不再得到积极支持。如果您仍在使用这些版本或有任何其他相关问题，请联系@dev邮件列表。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-14516" target="_blank" rel="external nofollow noopener noreferrer">FLINK-14516</a> ]删除了非基于信用的网络流控制代码以及配置选项<code>taskmanager.network.credit.model</code>。展望未来，Flink将始终使用基于信用的流量控制。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-12122" target="_blank" rel="external nofollow noopener noreferrer">FLINK-12122</a> ] <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external nofollow noopener noreferrer">FLIP-6</a>在Flink 1.5.0中推出，并引入了与从中分配插槽方式有关的代码回归<code>TaskManagers</code>。要使用更接近FLIP之前行为的调度策略（Flink尝试将工作负载分散到所有当前可用的行为中）<code>TaskManagers</code>，用户可以<code>cluster.evenly-spread-out-slots: true</code>在中设置<code>flink-conf.yaml</code>。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-11956" target="_blank" rel="external nofollow noopener noreferrer">FLINK-11956</a> ] <code>s3-hadoop</code>和<code>s3-presto</code>文件系统不再使用类重定位，而应通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/filesystems/#pluggable-file-systems" target="_blank" rel="external nofollow noopener noreferrer">插件</a>加载，但现在可以与所有凭据提供程序无缝集成。强烈建议将其他文件系统仅用作插件，因为我们将继续删除重定位。<br></li><li>Flink 1.9带有重构的Web UI，保留了旧版的UI作为备份，以防万一某些功能无法正常工作。到目前为止，尚未报告任何问题，因此<a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Remove-old-WebUI-td35218.html" target="_blank" rel="external nofollow noopener noreferrer">社区投票决定</a>在Flink 1.10中删除旧版Web UI。<br><br><a name="release-notes"></a><h2 id="发行说明"><a href="#发行说明" class="headerlink" title="发行说明"></a>发行说明</h2>如果您打算将设置升级到Flink 1.10，请仔细查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/flink-1.10.html" target="_blank" rel="external nofollow noopener noreferrer">发行说明</a>，以获取详细的更改和新功能列表。此版本与以前的1.x版本的API兼容，这些版本的API使用@Public注释进行了注释。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="cpeixin.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>IDEA install TabNine</title>
    <link href="cpeixin.cn/2020/01/22/IDEA-install-TabNine/"/>
    <id>cpeixin.cn/2020/01/22/IDEA-install-TabNine/</id>
    <published>2020-01-22T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:48.223Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p>TabNine是我目前遇到过最好的智能补全工具</p><p>TabNine基于GPT-2的插件</p><p>安装<br>IDEA编译器，找到plugins</p><p>Windows pycharm：File&gt;settings&gt;plugins;<br>Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins</p><p>查找 TabNine, 点击 install, 随后 restart</p><p>重启后：Help&gt;Edit Custom Properties…&gt;Create;</p><p>在跳出来的idea.properties中输入（注：英文字符） TabNine::config</p><p>随即会自动弹出TabNine激活页面；</p><p>激活<br>点击Activation Key下面的here；</p><p>输入你的邮箱号；</p><p>复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）</p><p>等待自动安装，观察页面（最下面有log可以看当前进度）；</p><p>激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；</p><p>确认激活完成，重启pycharm即可；</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;TabNine是我目前遇到过最好的智能补全工具&lt;/p&gt;&lt;p&gt;TabNine基于GPT-2的插件&lt;/p&gt;&lt;p&gt;安装&lt;br&gt;IDEA编译器，找到pl
      
    
    </summary>
    
    
      <category term="开发工具" scheme="cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="IDEA" scheme="cpeixin.cn/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动 EB 级 HDFS 实践</title>
    <link href="cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/</id>
    <published>2020-01-02T15:25:55.000Z</published>
    <updated>2020-05-31T15:28:54.936Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p>转载自<a href="https://juejin.im/post/5e0aac53e51d4575e82591e8" target="_blank" rel="external nofollow noopener noreferrer">字节跳动 EB 级 HDFS 实践</a>，学习学习字节跳动基础架构部门对上万节点的HDFS集群的管理方式<br><br><br>文章的最后，有写上自己的总结，工作这几年，确实没有遇到过这么庞大的数据量和集群，那么就不能先实战再总结了，目前是站在巨人的肩膀上看看远处的风景，当我到达的那一天，就会更从容的融入风景了。</p><hr><p><a name="MorZX"></a></p><h1 id="HDFS-简介"><a href="#HDFS-简介" class="headerlink" title="HDFS 简介"></a>HDFS 简介</h1><p>因为 HDFS 这样一个系统已经存在了非常长的时间，应用的场景已经非常成熟了，所以这部分我们会比较简单地介绍。<br>HDFS 全名 Hadoop Distributed File System，是业界使用最广泛的开源分布式文件系统。原理和架构与 Google 的 GFS 基本一致。它的特点主要有以下几项：</p><ul><li>和本地文件系统一样的目录树视图</li><li>Append Only 的写入（不支持随机写）</li><li>顺序和随机读</li><li>超大数据规模</li><li>易扩展，容错率高<br><a name="AUWUZ"></a><h1><a href="#" class="headerlink"></a></h1><a name="fQiBn"></a><h1 id="字节跳动特色的-HDFS"><a href="#字节跳动特色的-HDFS" class="headerlink" title="字节跳动特色的 HDFS"></a>字节跳动特色的 HDFS</h1></li></ul><p><br>字节跳动应用 HDFS 已经非常长的时间了，经历了 7 年的发展，目前已直接支持了十多种数据平台，间接支持了上百种业务发展。从集群规模和数据量来说，HDFS 平台在公司内部已经成长为总数几万台服务器的大平台，支持了 EB 级别的数据量。<br><br><br>在深入相关的技术细节之前，我们先看看字节跳动的 HDFS 架构。<br><a name="PdpVx"></a></p><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><p><a name="oK4fY"></a></p><h2 id="架构介绍"><a href="#架构介绍" class="headerlink" title="架构介绍"></a>架构介绍</h2><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590931343675-83b09747-e37a-438c-a445-2fdafdba9c83.webp#align=left&display=inline&height=749&margin=%5Bobject%20Object%5D&originHeight=749&originWidth=962&size=0&status=done&style=none&width=962" alt><br><a name="jC8CW"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="66HyQ"></a></p><h3 id="接入层"><a href="#接入层" class="headerlink" title="接入层"></a>接入层</h3><p><br>接入层是区别于社区版本最大的一层，社区版本中并无这一层定义。在字节跳动的落地实践中，由于集群的节点过于庞大，我们需要非常多的 NameNode 实现联邦机制来接入不同上层业务的数据服务。但当 NameNode 数量也变得非常多了以后，用户请求的统一接入及统一视图的管理也会有很大的问题。为了解决用户接入过于分散，我们需要一个独立的接入层来支持用户请求的统一接入，转发路由；同时也能结合业务提供用户权限和流量控制能力；另外，该接入层也需要提供对外的目录树统一视图。<br><br><br>该接入层从部署形态上来讲，依赖于一些外部组件如 Redis，MySQL 等，会有一批无状态的 NNProxy 组成，他们提供了请求路由，Quota 限制，Tracing 能力及流量限速等能力。<br><a name="gFV5T"></a></p><h3 id="-3"><a href="#-3" class="headerlink"></a></h3><p><a name="DTx6L"></a></p><h3 id="元数据层"><a href="#元数据层" class="headerlink" title="元数据层"></a>元数据层</h3><p><br>这一层主要模块有 Name Node，ZKFC，和 BookKeeper（不同于 QJM，BookKeeper 在大规模多节点数据同步上来讲会表现得更稳定可靠）。<br><br><br>Name Node 负责存储整个 HDFS 集群的元数据信息，是整个系统的大脑。一旦故障，整个集群都会陷入不可用状态。因此 Name Node 有一套基于 ZKFC 的主从热备的高可用方案。<br><br><br>Name Node 还面临着扩展性的问题，单机承载能力始终受限。于是 HDFS 引入了联邦（Federation）机制。一个集群中可以部署多组 Name Node，它们独立维护自己的元数据，共用 Data Node 存储资源。这样，一个 HDFS 集群就可以无限扩展了。但是这种 Federation 机制下，每一组 Name Node 的目录树都互相割裂的。于是又出现了一些解决方案，能够使整个 Federation 集群对外提供一个完整目录树的视图。<br></p><p><a name="OVDWh"></a></p><h3 id="数据层"><a href="#数据层" class="headerlink" title="数据层"></a>数据层</h3><p><br>相比元数据层，数据层主要节点是 Data Node。Data Node 负责实际的数据存储和读取。用户文件被切分成块复制成多副本，每个副本都存在不同的 Data Node 上，以达到容错容灾的效果。每个副本在 Data Node 上都以文件的形式存储，元信息在启动时被加载到内存中。<br><br><br>Data Node 会定时向 Name Node 做心跳汇报，并且周期性将自己所存储的副本信息汇报给 Name Node。这个过程对 Federation 中的每个集群都是独立完成的。在心跳汇报的返回结果中，会携带 Name Node 对 Data Node 下发的指令，例如，需要将某个副本拷贝到另外一台 Data Node 或者将某个副本删除等。<br></p><p><a name="NU3Ae"></a></p><h2 id="主要业务"><a href="#主要业务" class="headerlink" title="主要业务"></a>主要业务</h2><p>先来看一下当前在字节跳动 HDFS 承载的主要业务：</p><ul><li>Hive，HBase，日志服务，Kafka 数据存储</li><li>Yarn，Flink 的计算框架平台数据</li><li>Spark，MapReduce 的计算相关数据存储<br><a name="GHk55"></a><h2 id="发展阶段"><a href="#发展阶段" class="headerlink" title="发展阶段"></a>发展阶段</h2>在字节跳动，随着业务的快速发展，HDFS 的数据量和集群规模快速扩大，原来的 HDFS 的集群从几百台，迅速突破千台和万台的规模。这中间，踩了无数的坑，大的阶段归纳起来会有这样几个阶段。<br><a name="rzDSM"></a><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3>业务增长初期，集群规模增长趋势非常陡峭，单集群规模很快在元数据服务器 Name Node 侧遇到瓶颈。引入联邦机制（Federation）实现集群的横向扩展。<br><br><br>联邦又带来统一命名空间问题，因此，需要统一视图空间帮助业务构建统一接入。为了解决这个问题，我们引入了 Name Node Proxy 组件实现统一视图和多租户管理等功能，这部分会在下文的 NNProxy 章节中介绍。<br><a name="r78Fs"></a><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3></li></ul><p><br>数据量继续增大，Federation 方式下的目录树管理也存在瓶颈，主要体现在数据量增大后，Java 版本的 GC 变得更加频繁，跨子树迁移节点代价过大，节点启动时间太长等问题。因此我们通过重构的方式，解决了 GC，锁优化，启动加速等问题，将原 Name Node 的服务能力进一步提高。容纳更多的元数据信息。<br><br><br>为了解决这个问题，我们也实现了字节跳动特色的 DanceNN 组件，兼容了原有 Java 版本 NameNode 的全部功能基础上，大大增强了稳定性和性能。相关详细介绍会在下面的 DanceNN 章节中介绍。<br><a name="cKChL"></a></p><h3 id="-4"><a href="#-4" class="headerlink"></a></h3><p><a name="Ufy9s"></a></p><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p><br>当数据量跨过 EB，集群规模扩大到几万台的时候，慢节点问题，更细粒度服务分级问题，成本问题和元数据瓶颈进一步凸显。我们在架构上进一步在包括完善多租户体系构建，重构数据节点和元数据分层等方向进一步演进。这部分目前正在进行中，因为优化的点会非常多，本文会给出慢节点优化的落地实践。<br></p><p><a name="tEe39"></a></p><h2 id="关键改进"><a href="#关键改进" class="headerlink" title="关键改进"></a>关键改进</h2><p><br>在整个架构演进的过程中，我们做了非常多的探索和尝试。如上所述，结合之前提到的几个大的挑战和问题，我们就其中关键的 Name Node Proxy 和 Dance Name Node 这两个重点组件做一下介绍，同时，也会介绍一下我们在慢节点方面的优化和改进。<br><a name="TD5kf"></a></p><h3 id="-5"><a href="#-5" class="headerlink"></a></h3><p><a name="4I1Fk"></a></p><h3 id="NNProxy（Name-Node-Proxy）"><a href="#NNProxy（Name-Node-Proxy）" class="headerlink" title="NNProxy（Name Node Proxy）"></a>NNProxy（Name Node Proxy）</h3><p>作为系统的元数据操作接入端，NNProxy 提供了联邦模式下统一元数据视图，解决了用户请求的统一转发，业务流量的统一管控的问题。<br><br><br>先介绍一下 NNProxy 所处的系统上下游。<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590933295275-e12431d8-0bab-419e-bc29-4c3e03a5c6ff.webp#align=left&display=inline&height=572&margin=%5Bobject%20Object%5D&originHeight=572&originWidth=864&size=0&status=done&style=none&width=864" alt><br><br><br>我们先来看一下 NNProxy 都做了什么工作。<br><a name="rk9VU"></a></p><h4 id="-6"><a href="#-6" class="headerlink"></a></h4><p><a name="AYfYO"></a></p><h4 id="路由管理"><a href="#路由管理" class="headerlink" title="路由管理"></a>路由管理</h4><p><br>在上面 Federation 的介绍中提到，每个集群都维护自己独立的目录树，无法对外提供一个完整的目录树视图。NNProxy 中的路由管理就解决了这个问题。路由管理存储了一张 mount table，表中记录若干条路径到集群的映射关系。<br><br><br>例如 <strong>/user -&gt; hdfs://namenodeB</strong>，这条映射关系的含义就是 /user 及其子目录这个目录在 <strong>namenodeB</strong> 这个集群上，所有对 /user 及其子目录的访问都会由 NNProxy 转发给 <strong>namenodeB</strong>，获取结果后再返回给 Client。<br><br><br>匹配原则为最长匹配，例如我们还有另外一条映射 <strong>/user/tiger/dump -&gt; hdfs://namenodeC</strong>，那么 /user/tiger/dump 及其所有子目录都在 namenodeC，而 /user 目录下其他子目录都在 namenodeB 上。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590933295845-bb064f91-f470-4129-80b9-c49e55ddae75.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=864&size=0&status=done&style=none&width=864" alt><br><a name="kRRta"></a></p><h4 id="-7"><a href="#-7" class="headerlink"></a></h4><p><a name="DJKii"></a></p><h4 id="Quota-限制"><a href="#Quota-限制" class="headerlink" title="Quota 限制"></a>Quota 限制</h4><p>使用过 HDFS 的同学会知道 Quota 这个概念。我们给每个目录集合分配了额定的空间资源，一旦使用超过这个阈值，就会被禁止写入。这个工作就是由 NNProxy 完成的。NNProxy 会通过 Quota 实时监控系统获取最新 Quota 使用情况，当用户进行元数据操作的时候，NNProxy 就会根据用户的 Quota 情况作出判断，决定通过或者拒绝。<br><a name="e2E2o"></a></p><h4 id="-8"><a href="#-8" class="headerlink"></a></h4><p><a name="JG3Xt"></a></p><h4 id="Trace-支持"><a href="#Trace-支持" class="headerlink" title="Trace 支持"></a>Trace 支持</h4><p>ByteTrace 是一个 Trace 系统，记录追踪用户和系统以及系统之间的调用行为，以达到分析和运维的目的。其中的 Trace 信息会附在向 NNProxy 的请求 RPC 中。NNProxy 拿到 ByteTrace 以后就可以知道当前请求的上游模块，USER 及 Application ID 等信息。NNProxy 一方面将这些信息发到 Kafka 做一些离线分析，一方面实时聚合并打点，以便追溯线上流量。<br><a name="7lbZo"></a></p><h4 id="-9"><a href="#-9" class="headerlink"></a></h4><p><a name="9VGfn"></a></p><h4 id="流量限制"><a href="#流量限制" class="headerlink" title="流量限制"></a>流量限制</h4><p><br>虽然 NNProxy 非常轻量，可以承受很高的 QPS，但是后端的 Name Node 承载能力是有限的。因此突发的大作业造成高 QPS 的读写请求被全量转发到 Name Node 上时，会造成 Name Node 过载，延时变高，甚至出现 OOM，影响集群上所有用户。<br><br><br>因此 NNProxy 另一个非常重要的任务就是限流，以保护后端 Name Node。目前限流基于路径+RPC 以及 用户+RPC 维度，例如我们可以限制 /user/tiger/warhouse 路径的 create 请求为 100 QPS，或者某个用户的 delete 请求为 5 QPS。一旦该用户的访问量超过这个阈值，NNProxy 会返回一个可重试异常，Client 收到这个异常后会重试。因此被限流的路径或用户会感觉到访问 HDFS 变慢，但是并不会失败。<br><br><br></p><p><a name="a95em"></a></p><h3 id="Dance-NN（Dance-Name-Node）"><a href="#Dance-NN（Dance-Name-Node）" class="headerlink" title="Dance NN（Dance Name Node）"></a>Dance NN（Dance Name Node）</h3><p><a name="lhQQ9"></a></p><h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>如前所述，在数据量上到 EB 级别的场景后，原有的 Java 版本的 Name Node 存在了非常多的线上问题需要解决。以下是在实践过程中我们遇到的一些问题总结：<br></p><ul><li>Java 版本 Name Node 采用 Java 语言开发，在 INode 规模上亿时，不可避免的会带来严重的 GC 问题；</li><li>Java 版本 Name Node 将 INode meta 信息完全放置于内存，10 亿 INode 大约占用 800GB 内存（包含 JVM 自身占用的部分 native memory），更进一步加重了 GC；</li><li>我们目前的集群规模下，Name Node 从重启到恢复服务需要 6 个小时，在主备同时发生故障的情况下，严重影响上层业务；</li><li>Java 版本 Name Node 全局一把读写锁，任何对目录树的修改操作都会阻塞其他的读写操作，并发度较低；</li></ul><p>从上可以看出，在大数据量场景下，我们亟需一个新架构版本的 Name Node 来承载我们的海量元数据。除了 C++语言重写来规避 Java 带来的 GC 问题以外，我们还在一些场景下做了特殊的优化。<br></p><p><a name="1ILjM"></a></p><h4 id="目录树锁设计"><a href="#目录树锁设计" class="headerlink" title="目录树锁设计"></a>目录树锁设计</h4><p><br>HDFS 对内是一个分布式集群，对外提供的是一个 unified 的文件系统，因此对文件及目录的操作需要像操作 Linux 本地文件系统一样。这就要求 HDFS 满足类似于数据库系统中 ACID 特性一样的原子性，一致性、隔离性和持久性。因此 DanceNN 在面对多个用户同时操作同一个文件或者同一个目录时，需要保证不会破坏掉 ACID 属性，需要对操作做锁保护。<br><br><br>不同于传统的 KV 存储和数据库表结构，DanceNN 上维护的是一棵树状的数据结构，因此单纯的 key 锁或者行锁在 DanceNN 下不适用。而像数据库的表锁或者原生 NN 的做法，对整棵目录树加单独一把锁又会严重的影响整体吞吐和延迟，因此 DanceNN 重新设计了树状锁结构，做到保证 ACID 的情况下，读吞吐能够到 8w，写吞吐能够到 2w，是原生 NN 性能的 10 倍以上。<br><br><br>这里，我们会重新对 RPC 做分类，像 <code>createFile</code>，<code>getFileInfo</code>，<code>setXAttr</code> 这类 RPC 依然是简单的对某一个 INode 进行 CURD 操作；像 <code>delete</code> RPC，有可能删除一个文件，也有可能会删除目录，后者会影响整棵子树下的所有文件；像 <code>rename</code> RPC，则是更复杂的另外一类操作，可能会涉及到多个 INode，甚至是多棵子树下的所有 INode。<br></p><p><a name="FOHq3"></a></p><h4 id="DanceNN-启动优化"><a href="#DanceNN-启动优化" class="headerlink" title="DanceNN 启动优化"></a>DanceNN 启动优化</h4><p><br>由于我们的 DanceNN 底层元数据实现了本地目录树管理结构，因此我们 DanceNN 的启动优化都是围绕着这样的设计来做的。<br><a name="cwcfb"></a></p><h5 id="-10"><a href="#-10" class="headerlink"></a></h5><p><a name="ygdYf"></a></p><h5 id="多线程扫描和填充-BlockMap"><a href="#多线程扫描和填充-BlockMap" class="headerlink" title="多线程扫描和填充 BlockMap"></a>多线程扫描和填充 BlockMap</h5><p>在系统启动过程中，第一步就是读取目录树中保存的信息并且填入 BlockMap 中，类似 Java 版 NN 读取 FSImage 的操作。在具体实现过程中，首先起多个线程并行扫描静态目录树结构。将扫描的结果放入一个加锁的 Buffer 中。当 Buffer 中的元素个数达到设定的数量以后，重新生成一个新的 Buffer 接收请求，并在老 Buffer 上起一个线程将数据填入 BlockMap。<br><a name="wPkVT"></a></p><h5 id="-11"><a href="#-11" class="headerlink"></a></h5><p><a name="GvGcm"></a></p><h5 id="接收块上报优化"><a href="#接收块上报优化" class="headerlink" title="接收块上报优化"></a>接收块上报优化</h5><p>DanceNN 启动以后会首先进入安全模式，接收所有 Date Node 的块上报，完善 BlockMap 中保存的信息。当上报的 Date Node 达到一定比例以后，才会退出安全模式，这时候才能正式接收 client 的请求。所以接收块上报的速度也会影响 Date Node 的启动时长。DanceNN 这里做了一个优化，根据 BlockID 将不同请求分配给不同的线程处理，每个线程负责固定的 Slice，线程之间无竞争，这样就极大的加快了接收块上报的速度。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935398857-421715f0-1f68-40e2-a738-5d53fc518e03.webp#align=left&display=inline&height=412&margin=%5Bobject%20Object%5D&originHeight=412&originWidth=864&size=0&status=done&style=none&width=864" alt><br><br><br></p><p><a name="tCZ1r"></a></p><h3 id="慢节点优化"><a href="#慢节点优化" class="headerlink" title="慢节点优化"></a>慢节点优化</h3><p><br>慢节点问题在很多分布式系统中都存在。其产生的原因通常为上层业务的热点或者底层资源故障。上层业务热点，会导致一些数据在较短的时间段内被集中访问。而底层资源故障，如出现慢盘或者盘损坏，更多的请求就会集中到某一个副本节点上从而导致慢节点。<br><br><br>通常来说，慢节点问题的优化和上层业务需求及底层资源量有很大的关系，极端情况，上层请求很小，下层资源充分富裕的情况下，慢节点问题将会非常少，反之则会变得非常严重。在字节跳动的 HDFS 集群中，慢节点问题一度非常严重，尤其是磁盘占用百分比非常高以后，各种慢节点问题层出不穷。其根本原因就是资源的平衡滞后，许多机器的磁盘占用已经触及红线导致写降级；新增热资源则会集中到少量机器上，这种情况下，当上层业务的每秒请求数升高后，对于 P999 时延要求比较高的一些大数据分析查询业务就容易出现一大批数据访问（&gt;10000 请求）被卡在某个慢请求的处理上。<br><br><br>我们优化的方向会分为读慢节点和写慢节点两个方面。<br></p><p><a name="PwyJV"></a></p><h4 id="读慢节点优化"><a href="#读慢节点优化" class="headerlink" title="读慢节点优化"></a>读慢节点优化</h4><p>我们经历了几个阶段：</p><ul><li>最早，使用社区版本，其 Switch Read 以读取一个 packet 的时长为统计单位，当读取一个 packet 的时间超过阈值时，认为读取当前 packet 超时。如果一定时间窗口内超时 packet 的数量过多，则认为当前节点是慢节点。但这个问题在于以 packet 作为统计单位使得算法不够敏感，这样使得每次读慢节点发生的时候，对于小 IO 场景（字节跳动的一些业务是以大量随机小 IO 为典型使用场景的），这些个积攒的 Packet 已经造成了问题。</li></ul><ul><li>后续，我们研发了 Hedged Read 的读优化。Hedged Read 对每一次读取设置一个超时时间。如果读取超时，那么会另开一个线程，在新的线程中向第二个副本发起读请求，最后取第一第二个副本上优先返回的 response 作为读取的结果。但这种情况下，在慢节点集中发生的时候，会导致读流量放大。严重的时候甚至导致小范围带宽短时间内不可用。</li></ul><ul><li>基于之前的经验，我们进一步优化，开启了 Fast Switch Read 的优化，该优化方式使用吞吐量作为判断慢节点的标准，当一段时间窗口内的吞吐量小于阈值时，认为当前节点是慢节点。并且根据当前的读取状况动态地调整阈值，动态改变时间窗口的长度以及吞吐量阈值的大小。<br>下表是当时线上某业务测试的值：</li></ul><table><thead><tr><th>Host:X.X.X.X</th><th>3 副本 Switch Read</th><th>2 副本 Hedged Read</th><th>3 副本 Hedged Read</th><th>3 副本 Fast Switch Read（优化后算法）</th></tr></thead><tbody><tr><td>读取时长 p999</td><td>977 ms</td><td>549 ms</td><td>192 ms</td><td><strong>128 ms</strong></td></tr><tr><td>最长读取时间</td><td>300 s</td><td>125 s</td><td>60 s</td><td><strong>15.5 s</strong></td></tr><tr><td>长尾出现次数（大于 500ms）</td><td>238 次/天</td><td>75 次/天</td><td>15 次/天</td><td><strong>3 次/天</strong></td></tr><tr><td>长尾出现次数（大于 1000ms）</td><td>196 次/天</td><td>64 次/天</td><td>6 次/天</td><td><strong>3 次/天</strong></td></tr></tbody></table><p><br>进一步的相关测试数据：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935682709-6dceebc5-46fb-459b-a559-c238e04d66be.webp#align=left&display=inline&height=211&margin=%5Bobject%20Object%5D&originHeight=211&originWidth=746&size=0&status=done&style=none&width=746" alt><br><a name="aI0wv"></a></p><h4 id="-12"><a href="#-12" class="headerlink"></a></h4><p><a name="hBmkC"></a></p><h4 id="写慢节点优化"><a href="#写慢节点优化" class="headerlink" title="写慢节点优化"></a>写慢节点优化</h4><p><br>写慢节点优化的适用场景会相对简单一些。主要解决的是写过程中，Pipeline 的中间节点变慢的情况。为了解决这个问题，我们也发展了 Fast Failover 和 Fast Failover+两种算法。<br><a name="2ceD5"></a></p><h5 id="-13"><a href="#-13" class="headerlink"></a></h5><p><a name="yHKlv"></a></p><h5 id="Fast-Failover"><a href="#Fast-Failover" class="headerlink" title="Fast Failover"></a>Fast Failover</h5><p>Fast Failover 会维护一段时间内 ACK 时间过长的 packet 数目，当超时 ACK 的数量超过阈值后，会结束当前的 block，向 namenode 申请新块继续写入。<br><br><br>Fast Failover 的问题在于，随意结束当前的 block 会造成系统的小 block 数目增加，给之后的读取速度以及 namenode 的元数据维护都带来负面影响。所以 Fast Failover 维护了一个切换阈值，如果已写入的数据量（block 的大小）大于这个阈值，才会进行 block 切换。<br><br><br>但是往往为了达到这个写入数据大小阈值，就会造成用户难以接收的延迟，因此当数据量小于阈时需要进额外的优化。<br><a name="Ohd15"></a></p><h5 id="-14"><a href="#-14" class="headerlink"></a></h5><p><a name="Hq46w"></a></p><h5 id="Fast-Failover-1"><a href="#Fast-Failover-1" class="headerlink" title="Fast Failover+"></a>Fast Failover+</h5><p>为了解决上述的问题，当已写入的数据量（block 的大小）小于阈值时，我们引入了新的优化手段——Fast Failover+。该算法首先从 pipeline 中筛选出速度较慢的 datanode，将慢节点从当前 pipeline 中剔除，并进入 Pipeline Recovery 阶段。Pipeline Recovery 会向 namenode 申请一个新的 datanode，与剩下的 datanode 组成一个新的 pipeline，并将已写入的数据同步到新的 datanode 上（该步骤称为 transfer block）。由于已经写入的数据量较小，transfer block 的耗时并不高。统计 p999 平均耗时只有 150ms。由 Pipeline Recovery 所带来的额外消耗是可接受的。<br><br><br>下表是当时线上某业务测试的值：</p><table><thead><tr><th>Host:X.X.X.X</th><th>Fast Failover p99</th><th>Fast Failover+ p99 <strong>(优化后算法)</strong></th><th>Fast Failover p95</th><th>Fast Failover+ p95 <strong>(优化后算法)</strong></th></tr></thead><tbody><tr><td>平均 Flush 时长</td><td>1.49 s</td><td><strong>1.23 s</strong></td><td>182 ms</td><td><strong>147 ms</strong></td></tr><tr><td>最长 Flush 时间</td><td>80 s</td><td><strong>66 s</strong></td><td>9.7 s</td><td><strong>6.5 s</strong></td></tr><tr><td>长尾出现次数（p99 大于 10s, p95 大于 1s）</td><td>63 次/天</td><td><strong>38 次/天</strong></td><td>94 次/天</td><td><strong>55 次/天</strong></td></tr><tr><td>长尾出现次数（p99 大于 5s, p95 大于 0.5s）</td><td>133 次/天</td><td><strong>101 次/天</strong></td><td>173 次/天</td><td><strong>156 次/天</strong></td></tr></tbody></table><p>一些进一步的实际效果对比：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935682666-c4712d74-258e-44be-b16d-a6a773f9fcbb.webp#align=left&display=inline&height=232&margin=%5Bobject%20Object%5D&originHeight=232&originWidth=734&size=0&status=done&style=none&width=734" alt><br><a name="d4UFq"></a></p><h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><p>HDFS 在字节跳动的发展历程已经非常长了。从最初的几百台的集群规模支持 PB 级别的数据量，到现在几万台级别多集群的平台支持 EB 级别的数据量，我们经历了 7 年的发展。伴随着业务的快速上量，我们团队也经历了野蛮式爆发，规模化发展，平台化运营的阶段。这过程中我们踩了不少坑，也积累了相当丰富的经验。当然，最重要的，公司还在持续高速发展，而我们仍旧不忘初心，坚持“DAY ONE”，继续在路上。<br><br><br></p><p><a name="mZaR7"></a></p><h1 id="学习总结"><a href="#学习总结" class="headerlink" title="学习总结"></a>学习总结</h1><p><br>接入层：接入层是字节设计假如的一层，在上万节点的HDFS集群中，必然要使用多NameNode模式，那么对于用户大量的请求统一管理，字节引入了接入层，具体实现借用Redis，Mysql以及NNProxy转发路由等外界组件实现。<br><br><br>元数据层：这里面有一点，在字节的HDFS集群中，并没有使用社区版的QJM HA高可用方案，而是使用了BookKeeper。Apache bookkeeper是一个分布式，可扩展，容错（多副本），低延迟的存储系统，其提供了高性能，高吞吐的存储能力。而QJM/Qurom Journal Manager是Clouera提出的，这是一个基于Paxos算法实现的HDFS HA方案<br><br><br>数据层倒是没什么特别的改善<br><br><br>可以看出，字节的数据暴涨阶段，首先遇到的问题是Name Node的瓶颈，而此时字节的集群环境为单集群，此时的解决方案是采用Federation。</p><p>数据持续高速增长，Federation 方式下的目录树管理也存在瓶颈，主要原因是Java频繁GC，那么字节的解决方案就显得有些硬核了，重写了NameNode，在字节中叫做DanceNN 🐂🍺。</p><p>在数据超过EB级别之后，遇到的问题就更多了。不同粒度服务分级，元数据存储瓶颈，慢节点等问题。那么字节的解决方案则是考虑到一方面从存储方面的数据节点进行重构，另一方面对于大块的元数据进行分级。</p><p>这里要说一下上面提到的NNProxy，好用！！主要有两个功能很吸引我，在Hadoop集群原有的基础上，字节添加了NNProxy，一个是根据用户请求的路径转发到不同的HDFS空间，二呢，对多租户的场景下，对每个用户的请求做判断，如果某个请求量过大，则会对其限流。</p><p>在这里，我也领略到了一个场景，那就是在字节EB级别的集群规模下，集群重启到全部服务恢复，需要6个小时左右。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;转载自&lt;a href=&quot;https://juejin.im/post/5e0aac53e51d4575e82591e8&quot; target=&quot;_bl
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hdfs" scheme="cpeixin.cn/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 Chinese 自动生成文章 - 环境准备</title>
    <link href="cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <id>cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</id>
    <published>2020-01-01T14:28:43.000Z</published>
    <updated>2020-04-13T09:28:23.224Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p><a name="R14AA"></a></p><h2 id="Google-Colab"><a href="#Google-Colab" class="headerlink" title="Google Colab"></a>Google Colab</h2><p><br>Colaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。<br><br><br>Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。<br><br><br>缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。<br><br><br><strong>优点 免费！ 免费！免费！</strong><br>**<br><a name="dpofS"></a></p><h3 id="谷歌云盘"><a href="#谷歌云盘" class="headerlink" title="谷歌云盘"></a>谷歌云盘</h3><p><br>当登录账号进入<a href="https://drive.google.com/drive/my-drive" target="_blank" rel="external nofollow noopener noreferrer">谷歌云盘</a>时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723531238-28bbbd81-69e8-472d-b048-1ac67166a201.png#align=left&display=inline&height=612&name=image.png&originHeight=612&originWidth=1268&size=104029&status=done&style=none&width=1268" alt="image.png"><br>可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：</p><p><a name="EHdj9"></a></p><h3 id="引入Colab"><a href="#引入Colab" class="headerlink" title="引入Colab"></a>引入Colab</h3><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723706098-527d9fff-e46e-4dd1-b92a-0640b0d61555.png#align=left&display=inline&height=674&name=image.png&originHeight=674&originWidth=1125&size=104056&status=done&style=none&width=1125" alt="image.png"><br><br><br><br><br></p><p><a name="kykCO"></a></p><h3 id="设置GPU环境"><a href="#设置GPU环境" class="headerlink" title="设置GPU环境"></a>设置GPU环境</h3><p><br>打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723911273-f07371f9-e982-44b2-af34-b3781f294879.png#align=left&display=inline&height=739&name=image.png&originHeight=739&originWidth=1191&size=94677&status=done&style=none&width=1191" alt="image.png"><br><br><br></p><p><a name="f4U2h"></a></p><h3 id="挂载和切换工作目录"><a href="#挂载和切换工作目录" class="headerlink" title="挂载和切换工作目录"></a>挂载和切换工作目录</h3><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录</span></span><br><span class="line"></span><br><span class="line">os.chdir(<span class="string">'/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese'</span>) <span class="comment">## 共享云盘的目录</span></span><br></pre></td></tr></table></figure><p>其中： My Drive 代表你的google网盘根目录</p><pre><code>code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录</code></pre><p><a name="MyewB"></a></p><h3 id="在Colab中运行任务"><a href="#在Colab中运行任务" class="headerlink" title="在Colab中运行任务"></a>在Colab中运行任务</h3><p>下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769633567-4e4118a0-5c52-4517-9233-71d897e7fd68.png#align=left&display=inline&height=1748&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.15.46.png&originHeight=1748&originWidth=3096&size=378104&status=done&style=none&width=3096" alt="屏幕快照 2020-04-13 下午5.15.46.png"></p><p><a name="GZDbL"></a></p><h3 id="ipynb文件内容"><a href="#ipynb文件内容" class="headerlink" title=".ipynb文件内容"></a>.ipynb文件内容</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769997876-4536842e-6bb3-4d6f-8df9-e220a66026a0.png#align=left&display=inline&height=1702&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.23.22.png&originHeight=1702&originWidth=3154&size=396138&status=done&style=none&width=3154" alt="屏幕快照 2020-04-13 下午5.23.22.png"><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;R14AA&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;Google-Colab&quot;&gt;&lt;a href=&quot;#Google-Colab&quot; cl
      
    
    </summary>
    
    
      <category term="NLP" scheme="cpeixin.cn/categories/NLP/"/>
    
    
      <category term="GPT-2" scheme="cpeixin.cn/tags/GPT-2/"/>
    
  </entry>
  
  <entry>
    <title>架构思想</title>
    <link href="cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/"/>
    <id>cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/</id>
    <published>2019-12-20T02:26:15.000Z</published>
    <updated>2020-04-04T11:23:45.206Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h2><a href="#" class="headerlink"></a></h2><p>关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。<br><br><br>从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。<br><br><br>具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。<br><br><br>架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。<br><a name="-3"></a></p><h4 id="架构思想"><a href="#架构思想" class="headerlink" title="架构思想"></a>架构思想</h4><p>CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度</p><ul><li><p>满足业务发展需求是最高准则</p></li><li><p>业务建模，抽象和枚举是两种方式，需要平衡，不能走极端</p></li><li><p>模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计</p></li><li><p>基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。</p></li><li><p>分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程</p></li><li><p>分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中</p></li><li><p>不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术</p></li><li><p>好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难</p></li><li><p>业务架构、系统架构、数据模型<br><a name="-4"></a></p><h4 id="面对一块新业务，如何系统架构？"><a href="#面对一块新业务，如何系统架构？" class="headerlink" title="面对一块新业务，如何系统架构？"></a>面对一块新业务，如何系统架构？</h4></li><li><p>业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。</p></li><li><p>系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？</p></li><li><p>系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？</p></li><li><p>模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。<br><a name="-5"></a></p><h4 id="如何模块化"><a href="#如何模块化" class="headerlink" title="如何模块化"></a>如何模块化</h4></li><li><p>基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。</p></li><li><p>基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。</p></li><li><p>基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。</p></li><li><p>基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。<br><a name="-6"></a></p><h4 id="提升系统的稳定性"><a href="#提升系统的稳定性" class="headerlink" title="提升系统的稳定性"></a>提升系统的稳定性</h4></li><li><p>流控</p></li></ul><p>双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。<br>另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。</p><ul><li>容灾</li></ul><p>最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。<br>另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。</p><ul><li>稳定性</li></ul><p>第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。</p><ul><li>容量规划</li></ul><p>做好容量规划、系统间强弱依赖关系梳理。<br>如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。<br><a name="-7"></a></p><h4 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h4><ul><li><p>分布式系统</p></li><li><p>分布式缓存</p></li><li><p>分布式数据<br><a name="api"></a></p><h4 id="API-和乐高积木有什么相似之处？"><a href="#API-和乐高积木有什么相似之处？" class="headerlink" title="API 和乐高积木有什么相似之处？"></a>API 和乐高积木有什么相似之处？</h4><p>相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。<br>对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。<br>你可以发现很多 API 和乐高积木的相似之处：</p></li><li><p>标准化：通用、标准化的组件，作为基本的构建块（building blocks）；<br></p></li><li><p>可用性：强调可用性，附有文档或使用说明；<br></p></li><li><p>可定制：为不同功能使用不同的API；<br></p></li><li><p>创造性：能够组合不同的 API 来创造混搭的结果；</p></li></ul><p><br>乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。<br>虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。<br><br><br>让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;关于什么
      
    
    </summary>
    
    
      <category term="架构" scheme="cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>kali中文设置</title>
    <link href="cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/"/>
    <id>cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/</id>
    <published>2019-12-01T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:21.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p>更新源</p><p><a href="https://blog.csdn.net/qq_38333291/article/details/89764967" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_38333291/article/details/89764967</a></p><p>设置编码和中文字体安装</p><p><a href="http://www.linuxdiyf.com/linux/20701.html" target="_blank" rel="external nofollow noopener noreferrer">http://www.linuxdiyf.com/linux/20701.html</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;更新源&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_38333291/article/details/897
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="kali" scheme="cpeixin.cn/tags/kali/"/>
    
  </entry>
  
  <entry>
    <title>分布式下的数据hash分布</title>
    <link href="cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/"/>
    <id>cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/</id>
    <published>2019-11-19T15:05:08.000Z</published>
    <updated>2020-04-04T11:24:04.737Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动在Spark SQL上的核心优化实践</title>
    <link href="cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</id>
    <published>2019-11-12T10:57:27.000Z</published>
    <updated>2020-05-16T10:59:03.835Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p><br>本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践<br>原文链接：<a href="https://juejin.im/post/5dc3ed336fb9a04a7847f25c" target="_blank" rel="external nofollow noopener noreferrer">https://juejin.im/post/5dc3ed336fb9a04a7847f25c</a><br><a name="KLy4v"></a></p><h2 id="Spark-SQL-架构简介"><a href="#Spark-SQL-架构简介" class="headerlink" title="Spark SQL 架构简介"></a>Spark SQL 架构简介</h2><p>我们先简单聊一下Spark SQL 的架构。下面这张图描述了一条 SQL 提交之后需要经历的几个阶段，结合这些阶段就可以看到在哪些环节可以做优化。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589617132571-55a23e83-cc96-437b-93a9-871d41c6724e.webp#align=left&display=inline&height=448&margin=%5Bobject%20Object%5D&originHeight=448&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>很多时候，做数据仓库建模的同学更倾向于直接写 SQL 而非使用 Spark 的 DSL。一条 SQL 提交之后会被 Parser 解析并转化为 Unresolved Logical Plan。它的重点是 Logical Plan 也即逻辑计划，它描述了希望做什么样的查询。Unresolved 是指该查询相关的一些信息未知，比如不知道查询的目标表的 Schema 以及数据位置。<br>上述信息存于 Catalog 内。在生产环境中，一般由 Hive Metastore 提供 Catalog 服务。Analyzer 会结合 Catalog 将 Unresolved Logical Plan 转换为 Resolved Logical Plan。</p><p>到这里还不够。不同的人写出来的 SQL 不一样，生成的 Resolved Logical Plan 也就不一样，执行效率也不一样。为了保证无论用户如何写 SQL 都可以高效的执行，Spark SQL 需要对 Resolved Logical Plan 进行优化，这个优化由 Optimizer 完成。Optimizer 包含了一系列规则，对 Resolved Logical Plan 进行等价转换，最终生成 Optimized Logical Plan。<strong>该 Optimized Logical Plan 不能保证是全局最优的，但至少是接近最优的。</strong><br>上述过程只与 SQL 有关，与查询有关，但是与 Spark 无关，因此无法直接提交给 Spark 执行。Query Planner 负责将 Optimized Logical Plan 转换为 Physical Plan，进而可以直接由 Spark 执行。<br><br><br>由于同一种逻辑算子可以有多种物理实现。如 Join 有多种实现，ShuffledHashJoin、BroadcastHashJoin、BroadcastNestedLoopJoin、SortMergeJoin 等。因此 Optimized Logical Plan 可被 Query Planner 转换为多个 Physical Plan。如何选择最优的 Physical Plan 成为一件非常影响最终执行性能的事情。一种比较好的方式是，<strong>构建一个 Cost Model，并对所有候选的 Physical Plan 应用该 Model 并挑选 Cost 最小的 Physical Plan 作为最终的 Selected Physical Plan。</strong></p><p>Physical Plan 可直接转换成 RDD 由 Spark 执行。我们经常说“计划赶不上变化”，在执行过程中，可能发现原计划不是最优的，后续执行计划如果能根据运行时的统计信息进行调整可能提升整体执行效率。这部分<strong>动态调整由 Adaptive Execution</strong> 完成。<br><br><br>后面介绍字节跳动在 Spark SQL 上做的一些优化，主要围绕这一节介绍的逻辑计划优化与物理计划优化展开。<br><a name="Z68Tc"></a></p><h2 id="Spark-SQL引擎优化"><a href="#Spark-SQL引擎优化" class="headerlink" title="Spark SQL引擎优化"></a>Spark SQL引擎优化</h2><p><a name="L4CUN"></a></p><h3 id="Bucket-Join改进"><a href="#Bucket-Join改进" class="headerlink" title="Bucket Join改进"></a>Bucket Join改进</h3><p>在 Spark 里，实际并没有 Bucket Join 算子。这里说的 Bucket Join 泛指不需要 Shuffle 的 Sort Merge Join。<br>下图展示了 Sort Merge Join 的基本原理。用虚线框代表的 Table 1 和 Table 2 是两张需要按某字段进行 Join 的表。虚线框内的 partition 0 到 partition m 是该表转换成 RDD 后的 Partition，而非表的分区。</p><p>假设 Table 1 与 Table 2 转换为 RDD 后分别包含 m 和 k 个 Partition。为了进行 Join，需要通过 Shuffle 保证相同 Join Key 的数据在同一个 Partition 内且 Partition 内按 Key 排序，同时保证 Table 1 与 Table 2 经过 Shuffle 后的 RDD 的 Partition 数相同。<br><br><br>如下图所示，经过 Shuffle 后只需要启动 n 个 Task，每个 Task 处理 Table 1 与 Table 2 中对应 Partition 的数据进行 Join 即可。如 Task 0 只需要顺序扫描 Shuffle 后的左右两边的 partition 0 即可完成 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443202-f22b73ca-17f1-450c-86ca-e47c9b934bca.webp#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&originHeight=629&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>该方法的优势是适用场景广，几乎可用于任意大小的数据集。劣势是每次 Join 都需要对全量数据进行 Shuffle，而 Shuffle 是最影响 Spark SQL 性能的环节。如果能避免 Shuffle 往往能大幅提升 Spark SQL 性能。</p><p>对于大数据的场景来讲，数据一般是一次写入多次查询。如果经常对两张表按相同或类似的方式进行 Join，每次都需要付出 Shuffle 的代价。与其这样，<strong>不如让数据在写的时候，就让数据按照利于 Join 的方式分布，从而使得 Join 时无需进行 Shuffle。</strong>如下图所示，Table 1 与 Table 2 内的数据按照相同的 Key 进行分桶且桶数都为 n，同时桶内按该 Key 排序。对这两张表进行 Join 时，可以避免 Shuffle，直接启动 n 个 Task 进行 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443345-00aac217-cf9d-4ca2-beea-3a27868e9709.webp#align=left&display=inline&height=697&margin=%5Bobject%20Object%5D&originHeight=697&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>字节跳动对 Spark SQL 的 BucketJoin 做了四项比较大的改进。<br><strong><br></strong>改进一：支持与 Hive 兼容**<br>在过去一段时间，字节跳动把大量的 Hive 作业迁移到了 SparkSQL。而 Hive 与 Spark SQL 的 Bucket 表不兼容。对于使用 Bucket 表的场景，如果直接更新计算引擎，会造成 Spark SQL 写入 Hive Bucket 表的数据无法被下游的 Hive 作业当成 Bucket 表进行 Bucket Join，从而造成作业执行时间变长，可能影响 SLA。</p><p>为了解决这个问题，我们让 Spark SQL 支持 Hive 兼容模式，从而保证 Spark SQL 写入的 Bucket 表与 Hive 写入的 Bucket 表效果一致，并且这种表可以被 Hive 和 Spark SQL 当成 Bucket 表进行 Bucket Join 而不需要 Shuffle。通过这种方式保证 Hive 向 Spark SQL 的透明迁移。</p><p>第一个需要解决的问题是，Hive 的一个 Bucket 一般只包含一个文件，而 Spark SQL 的一个 Bucket 可能包含多个文件。<strong>解决办法是动态增加一次以 Bucket Key 为 Key 并且并行度与 Bucket 个数相同的 Shuffle。</strong></p><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443237-9db92d7d-db57-44a7-8c38-d02bece98cc8.webp#align=left&display=inline&height=609&margin=%5Bobject%20Object%5D&originHeight=609&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br><br><br>第二个需要解决的问题是，Hive 1.x 的哈希方式与 Spark SQL 2.x 的哈希方式（Murmur3Hash）不同，使得相同的数据在 Hive 中的 Bucket ID 与 Spark SQL 中的 Bucket ID 不同而无法直接 Join。在 Hive 兼容模式下，我们让上述动态增加的 Shuffle 使用 Hive 相同的哈希方式，从而解决该问题。<br><strong><br></strong>改进二：支持倍数关系Bucket Join**<br>Spark SQL 要求只有 Bucket 相同的表才能（必要非充分条件）进行 Bucket Join。对于两张大小相差很大的表，比如几百 GB 的维度表与几十 TB （单分区）的事实表，它们的 Bucket 个数往往不同，并且个数相差很多，默认无法进行 Bucket Join。因此我们通过两种方式支持了倍数关系的 Bucket Join，即当两张 Bucket 表的 Bucket 数是倍数关系时支持 Bucket Join。</p><p>第一种方式，Task 个数与小表 Bucket 个数相同。如下图所示，Table A 包含 3 个 Bucket，Table B 包含 6 个 Bucket。此时 Table B 的 bucket 0 与 bucket 3 的数据合集应该与 Table A 的 bucket 0 进行 Join。这种情况下，可以启动 3 个 Task。其中 Task 0 对 Table A 的 bucket 0 与 Table B 的 bucket 0 + bucket 3 进行 Join。在这里，需要对 Table B 的 bucket 0 与 bucket 3 的数据再做一次 merge sort 从而保证合集有序。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443244-bfe94284-63b6-4abe-8e6f-1df98c09fd08.webp#align=left&display=inline&height=616&margin=%5Bobject%20Object%5D&originHeight=616&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444353-c4e9ece6-e014-4583-ba9d-28bda6a5fe62.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><p>如果 Table A 与 Table B 的 Bucket 个数相差不大，可以使用上述方式。如果 Table B 的 Bucket 个数是 Bucket A Bucket 个数的 10 倍，那上述方式虽然避免了 Shuffle，但可能因为并行度不够反而比包含 Shuffle 的 SortMergeJoin 速度慢。此时可以使用另外一种方式，即 Task 个数与大表 Bucket 个数相等，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443251-d514de79-4fd1-4960-9e27-2a4e408c88ba.webp#align=left&display=inline&height=555&margin=%5Bobject%20Object%5D&originHeight=555&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>在该方案下，可将 Table A 的 3 个 Bucket 读多次。在上图中，直接将 Table A 与 Table A 进行 Bucket Union （新的算子，与 Union 类似，但保留了 Bucket 特性），结果相当于 6 个 Bucket，与 Table B 的 Bucket 个数相同，从而可以进行 Bucket Join。<br><strong><br></strong>改进三：支持BucketJoin 降级**<br>公司内部过去使用 Bucket 的表较少，在我们对 Bucket 做了一系列改进后，大量用户希望将表转换为 Bucket 表。转换后，表的元信息显示该表为 Bucket 表，而历史分区内的数据并未按 Bucket 表要求分布，在查询历史数据时会出现无法识别 Bucket 的问题。</p><p>同时，由于数据量上涨快，平均 Bucket 大小也快速增长。这会造成单 Task 需要处理的数据量过大进而引起使用 Bucket 后的效果可能不如直接使用基于 Shuffle 的 Join。<br><br><br>为了解决上述问题，我们实现了支持降级的 Bucket 表。基本原理是，每次修改 Bucket 信息（包含上述两种情况——将非 Bucket 表转为 Bucket 表，以及修改 Bucket 个数）时，记录修改日期。并且在决定使用哪种 Join 方式时，对于 Bucket 表先检查所查询的数据是否只包含该日期之后的分区。如果是，则当成 Bucket 表处理，支持 Bucket Join；否则当成普通无 Bucket 的表。<br><strong><br></strong>改进四：支持超集<strong><br>对于一张常用表，可能会与另外一张表按 User 字段做 Join，也可能会与另外一张表按 User 和 App 字段做 Join，与其它表按 User 与 Item 字段进行 Join。而 Spark SQL 原生的 Bucket Join 要求 Join Key Set 与表的 Bucket Key Set 完全相同才能进行 Bucket Join。在该场景中，不同 Join 的 Key Set 不同，因此无法同时使用 Bucket Join。这极大的限制了 Bucket Join 的适用场景。<br><br><br>针对此问题，我们支持了超集场景下的 Bucket Join。只要 Join Key Set 包含了 Bucket Key Set，即可进行 Bucket Join。<br><br><br>如下图所示，Table X 与 Table Y，都按字段 A 分 Bucket。而查询需要对 Table X 与 Table Y 进行 Join，且 Join Key Set 为 A 与 B。此时，由于 A 相等的数据，在两表中的 Bucket ID 相同，那 A 与 B 各自相等的数据在两表中的 Bucket ID 肯定也相同，所以数据分布是满足 Join 要求的，不需要 Shuffle。同时，Bucket Join 还需要保证两表按 Join Key Set 即 A 和 B 排序，此时只需要对 Table X 与 Table Y 进行分区内排序即可。由于两边已经按字段 A 排序了，此时再按 A 与 B 排序，代价相对较低。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443338-9f06a217-9078-4734-84ce-860707b4612e.webp#align=left&display=inline&height=661&margin=%5Bobject%20Object%5D&originHeight=661&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444719-a5289c2c-0dea-4316-8924-1e4377bce1ea.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br></strong>物化列**<br>Spark SQL 处理嵌套类型数据时，存在以下问题：</p><ul><li><strong>读取大量不必要的数据</strong>：对于 Parquet / ORC 等列式存储格式，可只读取需要的字段，而直接跳过其它字段，从而极大节省 IO。而对于嵌套数据类型的字段，如下图中的 Map 类型的 people 字段，往往只需要读取其中的子字段，如 people.age。却需要将整个 Map 类型的 people 字段全部读取出来然后抽取出 people.age 字段。这会引入大量的无意义的 IO 开销。在我们的场景中，存在不少 Map 类型的字段，而且很多包含几十至几百个 Key，这也就意味着 IO 被放大了几十至几百倍。<br></li><li><strong>无法进行向量化读取</strong>：而向量化读能极大的提升性能。但截止到目前（2019年10月26日），Spark 不支持包含嵌套数据类型的向量化读取。这极大的影响了包含嵌套数据类型的查询性能<br></li><li><strong>不支持 Filter 下推</strong>：目前（2019年10月26日）的 Spark 不支持嵌套类型字段上的 Filter 的下推<br></li><li><strong>重复计算</strong>：JSON 字段，在 Spark SQL 中以 String 类型存在，严格来说不算嵌套数据类型。不过实践中也常用于保存不固定的多个字段，在查询时通过 JSON Path 抽取目标子字段，而大型 JSON 字符串的字段抽取非常消耗 CPU。对于热点表，频繁重复抽取相同子字段非常浪费资源。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443241-a0b575d3-4b99-451f-802f-991fd41107f6.webp#align=left&display=inline&height=668&margin=%5Bobject%20Object%5D&originHeight=668&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444769-6635f36d-d873-430c-8223-adcb532766d6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>对于这个问题，做数仓的同学也想了一些解决方案。如下图所示，<strong>在名为 base_table 的表之外创建了一张名为 sub_table 的表，并且将高频使用的子字段 people.age 设置为一个额外的 Integer 类型的字段</strong>。下游不再通过 base_table 查询 people.age，而是使用 sub_table 上的 age 字段代替。通过这种方式，将嵌套类型字段上的查询转为了 Primitive 类型字段的查询，同时解决了上述问题。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444716-a4b11d0a-899e-430c-af62-8d0151c63af6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443482-11bc4e9f-a7bb-4d7d-9026-746ea16d0d27.webp#align=left&display=inline&height=611&margin=%5Bobject%20Object%5D&originHeight=611&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>这种方案存在明显缺陷：</p><ul><li><strong>额外维护了一张表</strong>，引入了大量的额外存储/计算开销。<br></li><li>无法在新表上查询新增字段的历史数据（如要支持对历史数据的查询，需要重跑历史作业，开销过大，无法接受）。<br></li><li>表的维护方需要在修改表结构后修改插入数据的作业。<br></li><li>需要下游查询方修改查询语句，推广成本较大。<br></li><li><strong>运营成本高</strong>：如果高频子字段变化，需要删除不再需要的独立子字段，并添加新子字段为独立字段。删除前，需要确保下游无业务使用该字段。而新增字段需要通知并推进下游业务方使用新字段。<br></li></ul><p>为解决上述所有问题，我们设计并实现了物化列。它的原理是：</p><ul><li>新增一个 Primitive 类型字段，比如 Integer 类型的 age 字段，并且指定它是 people.age 的物化字段。<br></li><li>插入数据时，为物化字段自动生成数据，并在 Partition Parameter 内保存物化关系。因此对插入数据的作业完全透明，表的维护方不需要修改已有作业。<br></li><li>查询时，检查所需查询的所有 Partition，如果都包含物化信息（people.age 到 age 的映射），直接将 select people.age 自动重写为 select age，从而实现对下游查询方的完全透明优化。同时兼容历史数据。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443240-98b45cc3-1d0f-4093-980d-265abb445a8a.webp#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&originHeight=683&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444777-84f75218-68e0-4929-909d-4c828ab64f33.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>下图展示了在某张核心表上使用物化列的收益：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443260-d3424ecd-67fe-41ed-a902-edf0d1ee45ab.webp#align=left&display=inline&height=423&margin=%5Bobject%20Object%5D&originHeight=423&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444729-1b39919b-8ea5-459e-bc2b-db24d8391b39.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>物化视图</strong><br>在 OLAP 领域，经常会对相同表的某些固定字段进行 Group By 和 Aggregate / Join 等耗时操作，造成大量重复性计算，浪费资源，且影响查询性能，不利于提升用户体验。<br>我们实现了基于物化视图的优化功能：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443261-8e8f3de2-937d-475b-aca0-d39fed39babb.webp#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>如上图所示，查询历史显示大量查询根据 user 进行 group by，然后对 num 进行 sum 或 count 计算。此时可创建一张物化视图，且对 user 进行 gorup by，对 num 进行 avg（avg 会自动转换为 count 和 sum）。用户对原始表进行 select user, sum(num) 查询时，Spark SQL 自动将查询重写为对物化视图的 select user, sum_num 查询。<br><strong><br></strong>Spark SQL 引擎上的其它优化<strong><br>下图展示了我们在 Spark SQL 上进行的其它部分优化工作：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443207-45af67c8-8989-4026-81f3-d646d8123845.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br></strong><br><a name="5UxSb"></a></p><h2 id="Spark-Shuffle稳定性提升与性能优化"><a href="#Spark-Shuffle稳定性提升与性能优化" class="headerlink" title="Spark Shuffle稳定性提升与性能优化"></a>Spark Shuffle稳定性提升与性能优化</h2><p><a name="5rTgp"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Ajtpe"></a></p><h3 id="Spark-Shuffle-存在的问题"><a href="#Spark-Shuffle-存在的问题" class="headerlink" title="Spark Shuffle 存在的问题"></a>Spark Shuffle 存在的问题</h3><p>Shuffle的原理，很多同学应该已经很熟悉了。鉴于时间关系，这里不介绍过多细节，只简单介绍下基本模型。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445126-5aa3b134-c904-42d9-ac04-09618bb32553.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443213-c2f4bcdb-24e6-4fdc-afc2-3397f6608fec.webp#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&originHeight=679&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，我们将 Shuffle 上游 Stage 称为 Mapper Stage，其中的 Task 称为 Mapper。Shuffle 下游 Stage 称为 Reducer Stage，其中的 Task 称为 Reducer。<br>每个 Mapper 会将自己的数据分为最多 N 个部分，N 为 Reducer 个数。每个 Reducer 需要去最多 M （Mapper 个数）个 Mapper 获取属于自己的那部分数据。<br><br><br>这个架构存在两个问题：</p><ul><li><strong>稳定性问题</strong>：Mapper 的 Shuffle Write 数据存于 Mapper 本地磁盘，只有一个副本。当该机器出现磁盘故障，或者 IO 满载，CPU 满载时，Reducer 无法读取该数据，从而引起 FetchFailedException，进而导致 Stage Retry。Stage Retry 会造成作业执行时间增长，直接影响 SLA。同时，执行时间越长，出现 Shuffle 数据无法读取的可能性越大，反过来又会造成更多 Stage Retry。如此循环，可能导致大型作业无法成功执行。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444392-8549057c-506b-4459-856d-0ce9cddf9910.webp#align=left&display=inline&height=628&margin=%5Bobject%20Object%5D&originHeight=628&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445074-9921e4da-02ca-438c-9b15-403f41fba339.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><ul><li><strong>性能问题</strong>：每个 Mapper 的数据会被大量 Reducer 读取，并且是随机读取不同部分。假设 Mapper 的 Shuffle 输出为 512MB，Reducer 有 10 万个，那平均每个 Reducer 读取数据 512MB / 100000 = 5.24KB。并且，不同 Reducer 并行读取数据。对于 Mapper 输出文件而言，存在大量的随机读取。而 HDD 的随机 IO 性能远低于顺序 IO。最终的现象是，Reducer 读取 Shuffle 数据非常慢，反映到 Metrics 上就是 Reducer Shuffle Read Blocked Time 较长，甚至占整个 Reducer 执行时间的一大半，如下图所示。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444444-a5a5eeea-5d00-473f-892c-bef8ff9e032f.webp#align=left&display=inline&height=545&margin=%5Bobject%20Object%5D&originHeight=545&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445093-1dba238c-18eb-4bc2-a637-4a5f0f069847.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>基于HDFS的Shuffle稳定性提升</strong><br>经观察，引起 Shuffle 失败的最大因素不是磁盘故障等硬件问题，而是 CPU 满载和磁盘 IO 满载。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445175-a4b309ba-f45b-4141-ac27-8771c622c313.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444483-bc3847a8-0664-4c0d-a4ed-ae5f1781331b.webp#align=left&display=inline&height=684&margin=%5Bobject%20Object%5D&originHeight=684&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，机器的 CPU 使用率接近 100%，使得 Mapper 侧的 Node Manager 内的 Spark External Shuffle Service 无法及时提供 Shuffle 服务。<br>下图中 Data Node 占用了整台机器 IO 资源的 84%，部分磁盘 IO 完全打满，这使得读取 Shuffle 数据非常慢，进而使得 Reducer 侧无法在超时时间内读取数据，造成 FetchFailedException。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444481-5779db89-9d8b-4b17-82e3-7b60b08eed02.webp#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&originHeight=627&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>无论是何种原因，问题的症结都是 Mapper 侧的 Shuffle Write 数据只保存在本地，一旦该节点出现问题，会造成该节点上所有 Shuffle Write 数据无法被 Reducer 读取。解决这个问题的一个通用方法是，通过多副本保证可用性。<br>最初始的一个简单方案是，Mapper 侧最终数据文件与索引文件不写在本地磁盘，而是直接写到 HDFS。Reducer 不再通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据，而是直接从 HDFS 上获取数据，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444436-0efdf8b6-0082-47c1-9218-36b6e6bbee4f.webp#align=left&display=inline&height=541&margin=%5Bobject%20Object%5D&originHeight=541&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>快速实现这个方案后，我们做了几组简单的测试。结果表明：</p><ul><li>Mapper 与 Reducer 不多时，Shuffle 读写性能与原始方案相比无差异。<br></li><li>Mapper 与 Reducer 较多时，Shuffle 读变得非常慢。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444384-9bb761c9-fbe1-43c8-9a82-a51e97c97f8e.webp#align=left&display=inline&height=540&margin=%5Bobject%20Object%5D&originHeight=540&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>在上面的实验过程中，HDFS 发出了报警信息。如下图所示，HDFS Name Node Proxy 的 QPS 峰值达到 60 万。（注：字节跳动自研了 Node Name Proxy，并在 Proxy 层实现了缓存，因此读 QPS 可以支撑到这个量级）。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444590-4c232e53-8507-472c-844c-18249a03f85f.webp#align=left&display=inline&height=698&margin=%5Bobject%20Object%5D&originHeight=698&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>原因在于，总共 10000 Reducer，需要从 10000 个 Mapper 处读取数据文件和索引文件，总共需要读取 HDFS 10000 * 1000 * 2 = 2 亿次。</p><p>如果只是 Name Node 的单点性能问题，还可以通过一些简单的方法解决。例如在 Spark Driver 侧保存所有 Mapper 的 Block Location，然后 Driver 将该信息广播至所有 Executor，每个 Reducer 可以直接从 Executor 处获取 Block Location，然后无须连接 Name Node，而是直接从 Data Node 读取数据。但鉴于 Data Node 的线程模型，这种方案会对 Data Node 造成较大冲击。<br>最后我们选择了一种比较简单可行的方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444471-aaff74d3-ec02-4014-8d71-1b201eafdbf1.webp#align=left&display=inline&height=582&margin=%5Bobject%20Object%5D&originHeight=582&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>Mapper 的 Shuffle 输出数据仍然按原方案写本地磁盘，写完后上传到 HDFS。Reducer 仍然按原始方案通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据。如果失败了，则从 HDFS 读取。这种方案极大减少了对 HDFS 的访问频率。<br>该方案上线近一年：</p><ul><li>覆盖 57% 以上的 Spark Shuffle 数据。<br></li><li>使得 Spark 作业整体性能提升 14%。<br></li><li>天级大作业性能提升 18%。<br></li><li>小时级作业性能提升 12%。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444424-472f0478-d4b8-47de-a9ea-557b4c201c05.webp#align=left&display=inline&height=670&margin=%5Bobject%20Object%5D&originHeight=670&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445065-2ed3ba00-5462-4f5f-9752-f2491fc7ea4d.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>该方案旨在提升 Spark Shuffle 稳定性从而提升作业稳定性，但最终没有使用方差等指标来衡量稳定性的提升。原因在于每天集群负载不一样，整体方差较大。Shuffle 稳定性提升后，Stage Retry 大幅减少，整体作业执行时间减少，也即性能提升。最终通过对比使用该方案前后的总的作业执行时间来对比性能的提升，用于衡量该方案的效果。<br><strong>Shuffle性能优化实践与探索</strong><br>如上文所分析，Shuffle 性能问题的原因在于，Shuffle Write 由 Mapper 完成，然后 Reducer 需要从所有 Mapper 处读取数据。这种模型，我们称之为以 Mapper 为中心的 Shuffle。它的问题在于：</p><ul><li>Mapper 侧会有 M 次顺序写 IO。<br></li><li>Mapper 侧会有 M * N * 2 次随机读 IO（这是最大的性能瓶颈）。<br></li><li>Mapper 侧的 External Shuffle Service 必须与 Mapper 位于同一台机器，无法做到有效的存储计算分离，Shuffle 服务无法独立扩展。<br></li></ul><p>针对上述问题，我们提出了以 Reducer 为中心的，存储计算分离的 Shuffle 方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444445-e51ea3d0-904b-48ff-af72-22657a86b3ba.webp#align=left&display=inline&height=730&margin=%5Bobject%20Object%5D&originHeight=730&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>该方案的原理是，Mapper 直接将属于不同 Reducer 的数据写到不同的 Shuffle Service。在上图中，总共 2 个 Mapper，5 个 Reducer，5 个 Shuffle Service。所有 Mapper 都将属于 Reducer 0 的数据远程流式发送给 Shuffle Service 0，并由它顺序写入磁盘。Reducer 0 只需要从 Shuffle Service 0 顺序读取所有数据即可，无需再从 M 个 Mapper 取数据。该方案的优势在于：</p><ul><li>将 M * N * 2 次随机 IO 变为 N 次顺序 IO。<br></li><li>Shuffle Service 可以独立于 Mapper 或者 Reducer 部署，从而做到独立扩展，做到存储计算分离。<br></li><li>Shuffle Service 可将数据直接存于 HDFS 等高可用存储，因此可同时解决 Shuffle 稳定性问题。<br></li></ul><p>我的分享就到这里，谢谢大家。<br><a name="23kL1"></a></p><h2 id="QA集锦"><a href="#QA集锦" class="headerlink" title="QA集锦"></a>QA集锦</h2><p><strong>- 提问：物化列新增一列，是否需要修改历史数据？</strong><br>回答：历史数据太多，不适合修改历史数据。</p><p><strong>- 提问：如果用户的请求同时包含新数据和历史数据，如何处理？</strong><br>回答：一般而言，用户修改数据都是以 Partition 为单位。所以我们在 Partition Parameter 上保存了物化列相关信息。如果用户的查询同时包含了新 Partition 与历史 Partition，我们会在新 Partition 上针对物化列进行 SQL Rewrite，历史 Partition 不 Rewrite，然后将新老 Partition 进行 Union，从而在保证数据正确性的前提下尽可能充分利用物化列的优势。</p><p><strong>- 提问：你好，你们针对用户的场景，做了很多挺有价值的优化。像物化列、物化视图，都需要根据用户的查询 Pattern 进行设置。目前你们是人工分析这些查询，还是有某种机制自动去分析并优化？</strong><br>回答：目前我们主要是通过一些审计信息辅助人工分析。同时我们也正在做物化列与物化视图的推荐服务，最终做到智能建设物化列与物化视图。</p><p><strong>- 提问：刚刚介绍的基于 HDFS 的 Spark Shuffle 稳定性提升方案，是否可以异步上传 Shuffle 数据至 HDFS？</strong><br>回答：这个想法挺好，我们之前也考虑过，但基于几点考虑，最终没有这样做。第一，单 Mapper 的 Shuffle 输出数据量一般很小，上传到 HDFS 耗时在 2 秒以内，这个时间开销可以忽略；第二，我们广泛使用 External Shuffle Service 和 Dynamic Allocation，Mapper 执行完成后可能 Executor 就回收了，如果要异步上传，就必须依赖其它组件，这会提升复杂度，ROI 较低。</p><p><a name="BtHKy"></a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>字节的这篇分享，我真的太喜欢了，所有的优化点，都是拿真实的业务场景进行举例，虽然上文有的技术点在我们的场景中还没必要去做到这种优化程度，但是如此实在的来源于线上的方案，非常容易理解。我们公司目前的数据量要比字节的量小很多，最近在新闻上看到抖音的日活已经达到了4亿，所以我们在数仓中的数据，还没有用到更细粒度的Bucket表，分区表就已经完全可以满足我们的需求。上文中的物化列方案，我觉得很新颖，但是从工程师的角度来讲，在物化列方案中，多维护一张表，添加了复杂度和运营成本，我们在数据的存储中，尽可量的回去避免复杂结构的数据类型，这样会降低存储端和计算端代码的复杂度。这篇文章是针对Spark SQL的优化方面，可以说基本上每个大数据公司都会用到Spark SQL，上述的优化方案肯定会帮助到更多的大数据团队 💪</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践&lt;br&gt;原文链接：&lt;a href
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hive 窗口函数</title>
    <link href="cpeixin.cn/2019/10/05/Hive-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"/>
    <id>cpeixin.cn/2019/10/05/Hive-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/</id>
    <published>2019-10-05T00:22:52.000Z</published>
    <updated>2020-06-15T00:47:29.706Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p><a name="e05dce83"></a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文主要介绍hive中的窗口函数，hive中的窗口函数和sql中的窗口函数相类似,都是用来做一些数据分析类的工作,一般用于OLAP分析（在线分析处理）。<br><a name="b59c9e0f"></a></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>窗口函数是用于分析用的一类函数，要理解窗口函数要先从聚合函数说起，我们都知道在sql中有一类函数叫做聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的.但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数。<br><br><br>窗口函数可以在本行内做运算，得到多行的结果，即每一行对应一行的值。 通用的窗口函数可以用下面的语法来概括：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Function() Over (Partition By Column1，Column2，Order By Column3)</span><br></pre></td></tr></table></figure><p>窗口函数又分为以下三类：<br></p><ul><li>聚合型窗口函数</li><li>分析型窗口函数</li><li>取值型窗口函数</li></ul><p><a name="qC5oF"></a></p><h2 id="over-子句"><a href="#over-子句" class="headerlink" title="over()子句"></a>over()子句</h2><p>我们可以形象的把over()子句理解成开窗子句，即打开一个窗口，窗口内包含多条记录，over()会给每一行开一个窗口。如下图，总共有5条记录，每一行代表一条记录，over()在每一条记录的基础上打开一个窗口，给r1记录打开w1窗口，窗口内只包含自己，给r2打开w2窗口，窗口内包含r1、r2，给r3打开w3窗口，窗口内包含r1、r2、r3，以此类推…..<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1592154521700-b02fdcf0-79ee-4d4e-8bb2-59ebce5c4e55.png#align=left&display=inline&height=315&margin=%5Bobject%20Object%5D&name=20190930094823263.png&originHeight=315&originWidth=748&size=11034&status=done&style=none&width=748" alt="20190930094823263.png"><br></p><p><a name="WXNor"></a></p><h2 id="over-子句的开窗范围"><a href="#over-子句的开窗范围" class="headerlink" title="over()子句的开窗范围"></a>over()子句的开窗范围</h2><p>先看一张图：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1592154550224-95e40d42-9a67-4c6f-8bfe-0024c2ec7c6e.png#align=left&display=inline&height=303&margin=%5Bobject%20Object%5D&name=20190930103436749.png&originHeight=303&originWidth=697&size=17636&status=done&style=none&width=697" alt="20190930103436749.png"><br><br><br><br><br>current row代表查询的当前行，1 preceding代表前一行，1 following代表后一行，unbounded preceding代表第一行，unbounded following代表最后一行。（注意这里的第一行和最后一行并不是严格的第一行和最后一行，根据具体情况而定）<br><br><br>由上我们不难发现，在使用over()子句进行查询的时候， 不仅可以查询到每条记录的信息，还可以查询到这条记录对应窗口内的所有记录的聚合信息，所以我们通常结合聚合函数和over()子句一起使用。<br><br><br>那么over()是如何进行开窗的呢？即每条记录对应的窗口内应该包含哪些记录呢？这些都是在over()子句的括号内进行定义。<br></p><p><a name="uGpna"></a></p><h3 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h3><p>如果over()子句中接order by，例如：over(order by date)，则默认的开窗范围为根据date排序后的rows between unbounded preceding and current row，即第一行到当前行，意思是over(order by date)和over(order by date rows rows between unbounded preceding and current row)的效果是一样的。<br></p><p><a name="Qx7FK"></a></p><h3 id="partition-by"><a href="#partition-by" class="headerlink" title="partition by"></a>partition by</h3><p>如果over子句中接partition by（和group by类似，都是根据列值对行进行分组），例如over(partition by month(date))，则每一行的默认的开窗范围为当前行所在分组的所有记录。注意partition by子句不能单独和window clause子句一起使用，必须结合order by子句，下面会讨论。<br></p><p><a name="qEcpV"></a></p><h3 id="partition-by-order-by"><a href="#partition-by-order-by" class="headerlink" title="partition by + order by"></a>partition by + order by</h3><p>先分组，再排序，即组内排序。同样的，如果 order by后不接window clause，则每一行的默认的开窗范围为：当前行所在分组的第一行到当前行，即over(partition by (month(date)) order by orderdate)和over(partition by (month(date)) order by orderdate rows between undounded preceding and current row)是一样的。<br></p><p><a name="GxqqM"></a></p><h2 id="窗口大小"><a href="#窗口大小" class="headerlink" title="窗口大小"></a>窗口大小</h2><p>over()子句的开窗范围可以通过window 子句（window clause）在over()的括号中定义，window clause的规范如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br><span class="line"></span><br><span class="line">partition by …order by…rows between unbounded preceding and current row</span><br><span class="line">窗口大小为从起始行得到当前行。</span><br><span class="line">partition by …order by… rows between 3 preceding and current row</span><br><span class="line">窗口大小为从当前行到之前三行</span><br><span class="line">partition by …order by… rows between 3 preceding and 1 following</span><br><span class="line">窗口大小为当前行的前三行到之后的一行</span><br><span class="line">partition by …order by… rows between 3 preceding and unbounded following</span><br><span class="line">窗口大小为当前行的前三行到之后的所有行</span><br></pre></td></tr></table></figure><p>例如 select <em>,sum(column_name) over( rows between unbounded preceding and unbounded following) from table_name 表示查询每一行的所有列值，同时给每一行打开一个从第一行到最后一行的窗口，并统计窗口内所有记录的column_name列值的和。最后给每一行输出该行的所有属性以及该行对应窗口内记录的聚合值。<br><br><br>如果over()子句中什么都不写的话，默认开窗范围是：rows between unbounded preceding and unbounded following<br><br><br>*</em>在深入研究Over子句之前，一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于Order by字句之前。<strong><br></strong><br><a name="e4DLH"></a></p><h2 id="实践准备"><a href="#实践准备" class="headerlink" title="实践准备"></a>实践准备</h2><p><strong>创建Hive表</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE user_match_temp (</span><br><span class="line">user_name string,</span><br><span class="line">opponent string,</span><br><span class="line">result int,</span><br><span class="line">create_time timestamp);</span><br></pre></td></tr></table></figure><p><br>数据量较少，就直接手动插入了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO TABLE user_match_temp values</span><br><span class="line">(&#39;A&#39;,&#39;B&#39;,1,&#39;2019-07-18 23:19:00&#39;),</span><br><span class="line">(&#39;B&#39;,&#39;A&#39;,0,&#39;2019-07-18 23:19:00&#39;),</span><br><span class="line">(&#39;A&#39;,&#39;C&#39;,0,&#39;2019-07-18 23:20:00&#39;),</span><br><span class="line">(&#39;C&#39;,&#39;A&#39;,1,&#39;2019-07-18 23:20:00&#39;),</span><br><span class="line">(&#39;A&#39;,&#39;D&#39;,1,&#39;2019-07-19 22:19:00&#39;),</span><br><span class="line">(&#39;D&#39;,&#39;A&#39;,0,&#39;2019-07-19 22:19:00&#39;),</span><br><span class="line">(&#39;C&#39;,&#39;B&#39;,0,&#39;2019-07-19 23:19:00&#39;),</span><br><span class="line">(&#39;B&#39;,&#39;C&#39;,1,&#39;2019-07-19 23:19:00&#39;);</span><br></pre></td></tr></table></figure><p>数据包含4列，分别为 user_name，opponent，result，create_time。 我们将基于这些数据来介绍下窗口函数的一些使用场景。<br><br><br><strong>原始数据</strong><br>**<br>user_name opponent result create_time<br>A B 1 2019-07-18 23:19:00<br>B A 0 2019-07-18 23:19:00<br>A C 0 2019-07-18 23:20:00<br>C A 1 2019-07-18 23:20:00<br>A D 1 2019-07-19 22:19:00<br>D A 0 2019-07-19 22:19:00<br>C B 0 2019-07-19 23:19:00<br>B C 1 2019-07-19 23:19:00<br></p><p><a name="OOjek"></a></p><h2 id="聚合型窗口函数"><a href="#聚合型窗口函数" class="headerlink" title="聚合型窗口函数"></a>聚合型窗口函数</h2><p><br>聚合型即SUM(), MIN(),MAX(),AVG(),COUNT()这些常见的聚合函数。 聚合函数配合窗口函数使用可以使计算更加灵活，例如以下场景：<br></p><ol><li><p>至今累计分数<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select *, sum(result) over(partition by user_name order by create_time) result_sum from user_match_temp;</span><br><span class="line"></span><br><span class="line">user_name    opponent   result create_timeresult_sum</span><br><span class="line">AB12019-07-18 23:19:001</span><br><span class="line">AC02019-07-18 23:20:001</span><br><span class="line">AD12019-07-19 22:19:002</span><br><span class="line">BA02019-07-18 23:19:000</span><br><span class="line">BC12019-07-19 23:19:001</span><br><span class="line">CA12019-07-18 23:20:001</span><br><span class="line">CB02019-07-19 23:19:001</span><br><span class="line">DA02019-07-19 22:19:000</span><br></pre></td></tr></table></figure><br></li><li><p>之前3场平均胜场</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (test)&gt; SELECT *,avg(result) over (partition by user_name order by create_time rows between 3 preceding and current row) as recently_wins from user_match_temp;</span><br><span class="line"></span><br><span class="line">user_match_temp.user_nameuser_match_temp.opponentuser_match_temp.resultuser_match_temp.create_timerecently_wins</span><br><span class="line">AB12019-07-18 23:19:001.0</span><br><span class="line">AC02019-07-18 23:20:000.5</span><br><span class="line">AD12019-07-19 22:19:000.6666666666666666</span><br><span class="line">BA02019-07-18 23:19:000.0</span><br><span class="line">BC12019-07-19 23:19:000.5</span><br><span class="line">CA12019-07-18 23:20:001.0</span><br><span class="line">CB02019-07-19 23:19:000.5</span><br><span class="line">DA02019-07-19 22:19:000.0</span><br></pre></td></tr></table></figure></li></ol><p><br>我们通过rows between 即可定义窗口的范围，这里我们定义了窗口的范围为之前3行到该行。</p><ol start="3"><li>累计遇到的对手数量</li></ol><p>需要注意的是count(distinct xxx)在窗口函数里是不允许使用的，不过我们也可以用size(collect_set() over(partition by order by))来替代实现我们的需求</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (test)&gt; SELECT *,size(collect_set(opponent) over (partition by user_name order by create_time)) as recently_wins from user_match_temp;</span><br><span class="line"></span><br><span class="line">user_match_temp.user_nameuser_match_temp.opponentuser_match_temp.resultuser_match_temp.create_timerecently_wins</span><br><span class="line">AB12019-07-18 23:19:001</span><br><span class="line">AC02019-07-18 23:20:002</span><br><span class="line">AD12019-07-19 22:19:003</span><br><span class="line">BA02019-07-18 23:19:001</span><br><span class="line">BC12019-07-19 23:19:002</span><br><span class="line">CA12019-07-18 23:20:001</span><br><span class="line">CB02019-07-19 23:19:002</span><br><span class="line">DA02019-07-19 22:19:001</span><br></pre></td></tr></table></figure><p>collect_set()也是一个聚合函数，作用是将多行聚合进一行的某个set内，再用size()统计集合内的元素个数，即可实现我们的需求。<br></p><p><a name="gpmGg"></a></p><h2 id="分析型窗口函数"><a href="#分析型窗口函数" class="headerlink" title="分析型窗口函数"></a>分析型窗口函数</h2><p>分析型即RANk(),ROW_NUMBER(),DENSE_RANK()等常见的排序用的窗口函数，不过他们也是有区别的。<br><strong><br></strong>排名函数不支持window子句，即不支持自定义窗口大小**</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (test)&gt; SELECT *,rank() over (order by create_time) as user_rank,row_number() over (order by create_time) as user_row_number,dense_rank() over (order by create_time) as user_dense_rank FROM user_match_temp;</span><br><span class="line"></span><br><span class="line">user_name opponentresultcreate_timeuser_rankuser_row_numberuser_dense_rank</span><br><span class="line">BA02019-07-18 23:19:00111</span><br><span class="line">AB12019-07-18 23:19:00121</span><br><span class="line">CA12019-07-18 23:20:00332</span><br><span class="line">AC02019-07-18 23:20:00342</span><br><span class="line">DA02019-07-19 22:19:00553</span><br><span class="line">AD12019-07-19 22:19:00563</span><br><span class="line">BC12019-07-19 23:19:00774</span><br><span class="line">CB02019-07-19 23:19:00784</span><br></pre></td></tr></table></figure><p>如上所示： row_number函数：生成连续的序号（相同元素序号相同）；<br>rank函数：如两元素排序相同则序号相同，并且会跳过下一个序号；<br>dense_rank函数：如两元素排序相同则序号相同，不会跳过下一个序号；<br><br><br>除了这三个排序用的函数，还有 _CUME_DIST函数 ：小于等于当前值的行在所有行中的占比 _PERCENT_RANK() ：小于当前值的行在所有行中的占比 * NTILE() ：如果把数据按行数分为n份，那么该行所属的份数是第几份 这三种窗口函数 sql如下：<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT *,  CUME_DIST() over (order by create_time) as user_CUME_DIST, PERCENT_RANK() over (order by create_time) as user_PERCENT_RANK, NTILE(3) over (order by create_time) as user_NTILE FROM user_match_temp;</span><br></pre></td></tr></table></figure><br><a name="4yXY7"></a> ## 取值型窗口函数 这几个函数可以通过字面意思记得，LAG是迟滞的意思，也就是对某一列进行往后错行；LEAD是LAG的反义词，也就是对某一列进行提前几行；FIRST_VALUE是对该列到目前为止的首个值，而LAST_VALUE是到目前行为止的最后一个值。<br><br>LAG()和LEAD() 可以带3个参数，第一个是返回的值，第二个是前置或者后置的行数，第三个是默认值。<br>下一个对手，上一个对手，最近3局的第一个对手及最后一个对手，如下：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> SELECT *,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     lag(opponent,1) </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">         over (partition by user_name order by create_time) as lag_opponent,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     lead(opponent,1) over </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">         (partition by user_name order by create_time) as lead_opponent,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     first_value(opponent) over (partition by user_name order by create_time rows hive&gt;         between 3 preceding and 3 following) as first_opponent,</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">     last_value(opponent) over (partition by user_name order by create_time rows hive&gt;         between 3 preceding and 3 following) as last_opponent</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> From user_match_temp;</span></span><br></pre></td></tr></table></figure><br><br><br>参考文章：[https://blog.csdn.net/czr11616/article/details/101645693](https://blog.csdn.net/czr11616/article/details/101645693)<br>[https://zhuanlan.zhihu.com/p/77705681](https://zhuanlan.zhihu.com/p/77705681)<br>[https://blog.csdn.net/qq_37296285/article/details/90940591](https://blog.csdn.net/qq_37296285/article/details/90940591)<!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;e05dce83&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了（二）</title>
    <link href="cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2019-09-09T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:14.168Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p>苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。<br><br><br>由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。<br>这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><br>映入眼帘的是满屏的扫描和ssh尝试登陆<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">lines <span class="number">1105</span><span class="number">-1127</span>/<span class="number">1127</span> (END)</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">49</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><p><br>看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护<br>对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@*** ~]<span class="comment"># useradd one             #创建用户</span></span><br><span class="line">[root@*** ~]<span class="comment"># passwd one              #设置密码</span></span><br></pre></td></tr></table></figure><p><br>输入新用户密码<br>首先确保文件 /etc/sudoers 中<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%wheel    ALL=(ALL)    ALL</span><br><span class="line">```  </span><br><span class="line">没有被注释</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```linux</span><br><span class="line">usermod -g wheel onerocket</span><br></pre></td></tr></table></figure><p><br>设置只有指定用户组才能使用su命令切换到root用户<br><br><br>在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"><span class="comment"># Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限</span></span><br><span class="line">sed -i <span class="string">'/pam_wheel.so use_uid/c\auth            required        pam_wheel.so use_uid '</span> /etc/pam.d/su</span><br><span class="line">n=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`</span><br><span class="line"><span class="keyword">if</span> [ $n -eq <span class="number">0</span> ];then</span><br><span class="line">echo SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defs</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><br>打开SSHD的配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。<br><br><br>修改sshd默认端口<br>虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。<br>打开sshd配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>找到#Port 22 删掉注释<br><br><br><em>服务器端口最大可以开到65536</em><br><br><br>同时再添加一个Port 61024 （随意设置）<br><br><br>Port 22<br>Port 61024<br><br><br>重启sshd服务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service sshd restart      <span class="comment">#centos6系列</span></span><br><span class="line">systemctl restart sshd  <span class="comment">#centos7系列</span></span><br><span class="line">firewall-cmd --add-port=<span class="number">61024</span>/tcp</span><br></pre></td></tr></table></figure><p><br>测试，使用新用户，新端口进行登录<br><br><br>如果登陆成功后，再将Port22注释掉，重启sshd服务。<br>到这里，关于远程登录的防护工作，就做好了。<br>最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了</title>
    <link href="cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/"/>
    <id>cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/</id>
    <published>2019-08-24T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:09.871Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h1 id="服务器自述"><a href="#服务器自述" class="headerlink" title="服务器自述"></a>服务器自述</h1><p>我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。<br><br><br>而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢<br><a name="-3"></a></p><h1 id="为什么黑我的服务器"><a href="#为什么黑我的服务器" class="headerlink" title="为什么黑我的服务器"></a>为什么黑我的服务器</h1><p>这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。<br><br><br>但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。<br><br><br>那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…<br><br><br>那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404954445-f30a25a0-5939-4773-b5d9-8bb6a7c53b02.png#align=left&display=inline&height=892&name=1.png&originHeight=892&originWidth=1244&size=1778564&status=done&style=none&width=1244" alt="1.png"><br><br><br>曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器<br><a name="-4"></a></p><h1 id="如何发现被黑"><a href="#如何发现被黑" class="headerlink" title="如何发现被黑"></a>如何发现被黑</h1><p>回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？<br><br><br>最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作<br><br><br>1.关闭了Linux防火墙<br><br><br>2.没有设置redis访问密码<br><br><br>3.没有更改redis默认端口<br><br><br>4.开放了任意IP可以远程连接<br><br><br>以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。<br><br><br>当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。<br><br><br>但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i tcp:<span class="number">6379</span></span><br></pre></td></tr></table></figure><p><br>使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。<br><br><br>但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想<br>直到我运行：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404912911-97736d80-2ee3-455d-a948-6d134f4e2663.png#align=left&display=inline&height=534&name=2.png&originHeight=534&originWidth=3338&size=980508&status=done&style=none&width=3338" alt="2.png"><br>发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了<br><a name="-5"></a></p><h1 id="做了哪些急救工作"><a href="#做了哪些急救工作" class="headerlink" title="做了哪些急救工作"></a>做了哪些急救工作</h1><p>这时，感觉自己开始投入了一场对抗战<br><br><br>1.首先查找植入程序的位置。<br>在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。<br><br><br>2.删除清理可疑文件<br><br><br>杀死进程<br><br><br>删除了正在运行的程序文件还有安装包<br>3.查看所有用户的定时任务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd |cut -f <span class="number">1</span> -d:crontab -uXXX -l</span><br></pre></td></tr></table></figure><p><br>4.开启防火墙<br><br><br>仅开放会使用到的端口<br>5.修改redis默认端口<br><br><br>redis.conf中的port<br>6.添加redis授权密码<br><br><br>redis.conf中的requirepass<br>7.修改绑定远程绑定ip<br><br><br>redis.conf中的bind<br>最后重启redis服务！<br><a name="-6"></a></p><h1 id="从中学到了什么"><a href="#从中学到了什么" class="headerlink" title="从中学到了什么"></a>从中学到了什么</h1><p>明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～<br>最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;服务器自述&quot;&gt;&lt;a href=&quot;#服务器自述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>团灭 LeetCode 股票买卖问题</title>
    <link href="cpeixin.cn/2019/08/05/%E5%9B%A2%E7%81%AD-LeetCode-%E8%82%A1%E7%A5%A8%E4%B9%B0%E5%8D%96%E9%97%AE%E9%A2%98/"/>
    <id>cpeixin.cn/2019/08/05/%E5%9B%A2%E7%81%AD-LeetCode-%E8%82%A1%E7%A5%A8%E4%B9%B0%E5%8D%96%E9%97%AE%E9%A2%98/</id>
    <published>2019-08-04T17:00:08.000Z</published>
    <updated>2020-07-04T17:01:29.086Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p><a name="JJDTz"></a></p><h3 id="思路介绍"><a href="#思路介绍" class="headerlink" title="思路介绍"></a>思路介绍</h3><p>很多读者抱怨 LeetCode 的股票系列问题奇技淫巧太多，如果面试真的遇到这类问题，基本不会想到那些巧妙的办法，怎么办？<strong>所以本文拒绝奇技淫巧，而是稳扎稳打，只用一种通用方法解决所用问题，以不变应万变</strong>。<br>这篇文章用状态机的技巧来解决，可以全部提交通过。不要觉得这个名词高大上，文学词汇而已，实际上就是 DP table，看一眼就明白了。<br><br><br>PS：本文参考自<a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock/discuss/39038" target="_blank" rel="external nofollow noopener noreferrer">英文版 LeetCode 的一篇题解</a>。<br><br><br>先随便抽出一道题，看看别人的解法：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">int maxProfit(vector&lt;int&gt;&amp; prices) &#123;</span><br><span class="line">    <span class="keyword">if</span>(prices.empty()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    int s1=-prices[<span class="number">0</span>],s2=INT_MIN,s3=INT_MIN,s4=INT_MIN;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span>(int i=<span class="number">1</span>;i&lt;prices.size();++i) &#123;            </span><br><span class="line">        s1 = max(s1, -prices[i]);</span><br><span class="line">        s2 = max(s2, s1+prices[i]);</span><br><span class="line">        s3 = max(s3, s2-prices[i]);</span><br><span class="line">        s4 = max(s4, s3+prices[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> max(<span class="number">0</span>,s4);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>能看懂吧？会做了吗？不可能的，你看不懂，这才正常。就算你勉强看懂了，下一个问题你还是做不出来。为什么别人能写出这么诡异却又高效的解法呢？因为这类问题是有框架的，但是人家不会告诉你的，因为一旦告诉你，你五分钟就学会了，该算法题就不再神秘，变得不堪一击了。<br><br><br>本文就来告诉你这个框架，然后带着你一道一道秒杀。这篇文章用状态机的技巧来解决，可以全部提交通过。不要觉得这个名词高大上，文学词汇而已，实际上就是 DP table，看一眼就明白了。<br>这 6 道题目是有共性的，我就抽出来第 4 道题目，因为这道题是一个最泛化的形式，其他的问题都是这个形式的简化，看下题目：<br><a href="https://github.com/labuladong/fucking-algorithm/blob/master/pictures/%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98/title.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://github.com/labuladong/fucking-algorithm/raw/master/pictures/%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98/title.png#align=left&display=inline&height=622&margin=%5Bobject%20Object%5D&originHeight=622&originWidth=798&status=done&style=none&width=798" alt></a><br><br><br>第一题是只进行一次交易，相当于 k = 1；<br>第二题是不限交易次数，相当于 k = +infinity（正无穷）；<br>第三题是只进行 2 次交易，相当于 k = 2；<br>剩下两道也是不限次数，但是加了交易「冷冻期」和「手续费」的额外条件，其实就是第二题的变种，都很容易处理。<br></p><p><a name="KEGVj"></a></p><h4 id="一、穷举框架"><a href="#一、穷举框架" class="headerlink" title="一、穷举框架"></a>一、穷举框架</h4><p>首先，还是一样的思路：如何穷举？这里的穷举思路和上篇文章递归的思想不太一样。<br>递归其实是符合我们思考的逻辑的，一步步推进，遇到无法解决的就丢给递归，一不小心就做出来了，可读性还很好。缺点就是一旦出错，你也不容易找到错误出现的原因。比如上篇文章的递归解法，肯定还有计算冗余，但确实不容易找到。<br><br><br>而这里，我们不用递归思想进行穷举，而是利用「状态」进行穷举。我们具体到每一天，看看总共有几种可能的「状态」，再找出每个「状态」对应的「选择」。我们要穷举所有「状态」，穷举的目的是根据对应的「选择」更新状态。听起来抽象，你只要记住「状态」和「选择」两个词就行，下面实操一下就很容易明白了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> 状态<span class="number">1</span> <span class="keyword">in</span> 状态<span class="number">1</span>的所有取值：</span><br><span class="line">    <span class="keyword">for</span> 状态<span class="number">2</span> <span class="keyword">in</span> 状态<span class="number">2</span>的所有取值：</span><br><span class="line">        <span class="keyword">for</span> ...</span><br><span class="line">            dp[状态<span class="number">1</span>][状态<span class="number">2</span>][...] = 择优(选择<span class="number">1</span>，选择<span class="number">2.</span>..)</span><br></pre></td></tr></table></figure><p>比如说这个问题，<strong>每天都有三种「选择」</strong>：买入、卖出、无操作，我们用 buy, sell, rest 表示这三种选择。<br><br><br>但问题是，并不是每天都可以任意选择这三种选择的，因为 sell 必须在 buy 之后，buy 必须在 sell 之后。那么 rest 操作还应该分两种状态，一种是 buy 之后的 rest（持有了股票），一种是 sell 之后的 rest（没有持有股票）。<br><br><br>而且别忘了，我们还有交易次数 k 的限制，就是说你 buy 还只能在 k &gt; 0 的前提下操作。<br><br><br>很复杂对吧，不要怕，我们现在的目的只是穷举，你有再多的状态，老夫要做的就是一把梭全部列举出来。<strong>这个问题的「状态」有三个</strong>，第一个是天数，第二个是允许交易的最大次数，第三个是当前的持有状态（即之前说的 rest 的状态，我们不妨用 1 表示持有，0 表示没有持有）。然后我们用一个三维数组就可以装下这几种状态的全部组合：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dp[i][k][<span class="number">0</span> <span class="keyword">or</span> <span class="number">1</span>]</span><br><span class="line"><span class="number">0</span> &lt;= i &lt;= n<span class="number">-1</span>, <span class="number">1</span> &lt;= k &lt;= K</span><br><span class="line">n 为天数，大 K 为最多交易数</span><br><span class="line">此问题共 n × K × <span class="number">2</span> 种状态，全部穷举就能搞定。</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>,K+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> &#123;<span class="number">0</span>, <span class="number">1</span>&#125;:</span><br><span class="line">            dp[i][k][s] = max(buy, sell, rest)</span><br></pre></td></tr></table></figure><p><br>而且我们可以用自然语言描述出每一个状态的含义，比如说 <code>dp[3][2][1]</code> 的含义就是：今天是第三天，我现在手上持有着股票，至今最多进行 2 次交易。再比如 <code>dp[2][3][0]</code> 的含义：今天是第二天，我现在手上没有持有股票，至今最多进行 3 次交易。很容易理解，对吧？<br><br><br>我们想求的最终答案是 dp[n - 1][K][0]，即最后一天，最多允许 K 次交易，最多获得多少利润。读者可能问为什么不是 dp[n - 1][K][1]？因为 [1] 代表手上还持有股票，[0] 表示手上的股票已经卖出去了，很显然后者得到的利润一定大于前者。<br><br><br>记住如何解释「状态」，一旦你觉得哪里不好理解，把它翻译成自然语言就容易理解了。<br><a name="6pIbJ"></a></p><h4><a href="#" class="headerlink"></a></h4><p><a name="4aY7l"></a></p><h4 id="二、状态转移框架"><a href="#二、状态转移框架" class="headerlink" title="二、状态转移框架"></a>二、状态转移框架</h4><p>现在，我们完成了「状态」的穷举，我们开始思考每种「状态」有哪些「选择」，应该如何更新「状态」。只看「持有状态」，可以画个状态转移图。<br><a href="https://github.com/labuladong/fucking-algorithm/blob/master/pictures/%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98/1.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://github.com/labuladong/fucking-algorithm/raw/master/pictures/%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98/1.png#align=left&display=inline&height=519&margin=%5Bobject%20Object%5D&originHeight=519&originWidth=794&status=done&style=none&width=794" alt></a><br>通过这个图可以很清楚地看到，每种状态（0 和 1）是如何转移而来的。根据这个图，我们来写一下状态转移方程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp[i][k][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">0</span>], dp[i<span class="number">-1</span>][k][<span class="number">1</span>] + prices[i])</span><br><span class="line">              <span class="comment">#￥max(   选择 rest  ,             选择 sell      )</span></span><br></pre></td></tr></table></figure><p>解释：今天我没有持有股票，有两种可能：<br>要么是我昨天就没有持有，然后今天选择 rest，所以我今天还是没有持有；<br>要么是我昨天持有股票，但是今天我 sell 了，所以我今天没有持有股票了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp[i][k][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">1</span>], dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>] - prices[i])</span><br><span class="line">              <span class="comment">#max(   选择 rest  ,           选择 buy         )</span></span><br></pre></td></tr></table></figure><p>解释：今天我持有着股票，有两种可能：<br>要么我昨天就持有着股票，然后今天选择 rest，所以我今天还持有着股票；<br>要么我昨天本没有持有，但今天我选择 buy，所以今天我就持有股票了。<br><br><br>这个解释应该很清楚了，如果 buy，就要从利润中减去 prices[i]，如果 sell，就要给利润增加 prices[i]。今天的最大利润就是这两种可能选择中较大的那个。<strong>而且注意 k 的限制和变化，根据题意，**</strong>允许完成一笔交易（即买入和卖出一支股票一次），<strong>**我们在选择 buy 的时候，把 k 减小了 1，所以在卖出的时候就不用减小1了，很好理解吧，当然你也可以在 sell 的时候减 1，一样的。</strong><br>**<br>现在，我们已经完成了动态规划中最困难的一步：状态转移方程。如果之前的内容你都可以理解，那么你已经可以秒杀所有问题了，只要套这个框架就行了。不过还差最后一点点，就是定义 base case，即最简单的情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dp[<span class="number">-1</span>][k][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">解释：因为 i 是从 <span class="number">0</span> 开始的，所以 i = <span class="number">-1</span> 意味着还没有开始，这时候的利润当然是 <span class="number">0</span> 。</span><br><span class="line">dp[<span class="number">-1</span>][k][<span class="number">1</span>] = -infinity</span><br><span class="line">解释：还没开始的时候，是不可能持有股票的，用负无穷表示这种不可能。</span><br><span class="line">dp[i][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">解释：因为 k 是从 <span class="number">1</span> 开始的，所以 k = <span class="number">0</span> 意味着根本不允许交易，这时候利润当然是 <span class="number">0</span> 。</span><br><span class="line">dp[i][<span class="number">0</span>][<span class="number">1</span>] = -infinity</span><br><span class="line">解释：不允许交易的情况下，是不可能持有股票的，用负无穷表示这种不可能。</span><br></pre></td></tr></table></figure><p><br>把上面的状态转移方程总结一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">base case：</span><br><span class="line">dp[<span class="number">-1</span>][k][<span class="number">0</span>] = dp[i][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">dp[<span class="number">-1</span>][k][<span class="number">1</span>] = dp[i][<span class="number">0</span>][<span class="number">1</span>] = -infinity</span><br><span class="line">状态转移方程：</span><br><span class="line">dp[i][k][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">0</span>], dp[i<span class="number">-1</span>][k][<span class="number">1</span>] + prices[i])</span><br><span class="line">dp[i][k][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">1</span>], dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>] - prices[i])</span><br></pre></td></tr></table></figure><p>读者可能会问，这个数组索引是 -1 怎么编程表示出来呢，负无穷怎么表示呢？这都是细节问题，有很多方法实现。现在完整的框架已经完成，下面开始具体化。<br><br><br><strong>以上是labuladong讲解的状态方程变化，我觉得讲的很不错，至少我从不知道状态方程是什么，到已经理解了。</strong><br><strong>下面的题解部分，我有修改成python，并且根据自己能更好理解的方式进行了修改，labuladong的题解是采用了空间复杂度O(1)的方法，使用固定的两个变量来转换。而我下面的方式是dp[i][0]这种方式，这种方式我写的时候思路更清晰。但是这种方式的空间复杂度貌似是O(n)的。</strong></p><hr><p><a name="GVn6V"></a></p><h4 id="三、秒杀题目"><a href="#三、秒杀题目" class="headerlink" title="三、秒杀题目"></a>三、秒杀题目</h4><p><strong>第一题，k = 1</strong><br>直接套状态转移方程，根据 base case，可以做一些化简：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dp[i][1][0] &#x3D; max(dp[i-1][1][0], dp[i-1][1][1] + prices[i])</span><br><span class="line">dp[i][1][1] &#x3D; max(dp[i-1][1][1], dp[i-1][0][0] - prices[i]) </span><br><span class="line">&#x3D; max(dp[i-1][1][1], -prices[i])</span><br></pre></td></tr></table></figure><p>解释：k = 0 的 base case，所以 dp[i-1][0][0] = 0。<br>现在发现 k 都是 1，不会改变，即 k 对状态转移已经没有影响了。<br>可以进行进一步化简去掉所有 k：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp[i][0] &#x3D; max(dp[i-1][0], dp[i-1][1] + prices[i])</span><br><span class="line">dp[i][1] &#x3D; max(dp[i-1][1], -prices[i])</span><br></pre></td></tr></table></figure><p><br>直接写出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n = len(prices)</span><br><span class="line">dp = [[]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">    dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], -prices[i])</span><br><span class="line"><span class="keyword">return</span> dp[n - <span class="number">1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><br>显然 i = 0 时 dp[i-1] 是不合法的。这是因为我们没有对 i 的 base case 进行处理。可以这样处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    <span class="keyword">if</span> i - <span class="number">1</span> == <span class="number">-1</span>:</span><br><span class="line">        dp[i][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="comment"># 解释：</span></span><br><span class="line">        <span class="comment">#   dp[i][0] = max(dp[-1][0], dp[-1][1] + prices[i])</span></span><br><span class="line">        <span class="comment">#            = max(0, -infinity + prices[i]) = 0</span></span><br><span class="line">        dp[i][<span class="number">1</span>] = -prices[i]</span><br><span class="line">        <span class="comment">#解释：</span></span><br><span class="line">        <span class="comment">#   dp[i][1] = max(dp[-1][1], dp[-1][0] - prices[i])</span></span><br><span class="line">        <span class="comment">#            = max(-infinity, 0 - prices[i]) </span></span><br><span class="line">        <span class="comment">#            = -prices[i]</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    dp[i][<span class="number">0</span>] = Math.max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">    dp[i][<span class="number">1</span>] = Math.max(dp[i<span class="number">-1</span>][<span class="number">1</span>], -prices[i])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> dp[n - <span class="number">1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>第一题就解决了，但是这样处理 base case 很麻烦，而且注意一下状态转移方程，新状态只和相邻的一个状态有关，其实不用整个 dp 数组，只需要一个变量储存相邻的那个状态就足够了，这样可以把空间复杂度降到 O(1):<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// k == <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices)</span>:</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    // base case: dp[<span class="number">-1</span>][<span class="number">0</span>] = <span class="number">0</span>, dp[<span class="number">-1</span>][<span class="number">1</span>] = -infinity</span><br><span class="line">    dp_i_0 = <span class="number">0</span></span><br><span class="line">    dp_i_1 = float(<span class="string">'-inf'</span>);</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        // dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">        dp_i_0 = Math.max(dp_i_0, dp_i_1 + prices[i]);</span><br><span class="line">        // dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], -prices[i])</span><br><span class="line">        dp_i_1 = Math.max(dp_i_1, -prices[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp_i_0;</span><br></pre></td></tr></table></figure><p><br>两种方式都是一样的，不过这种编程方法简洁很多。但是如果没有前面状态转移方程的引导，是肯定看不懂的。后续的题目，我主要写这种空间复杂度 O(1) 的解法。<br><br><br>没有使用变量的python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type prices: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n&lt;=<span class="number">1</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        dp = [[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">        <span class="comment"># base case:</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], -prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong><br></strong>第二题，k = +infinity<strong><br>每天都有三种动作：买入（buy）、卖出（sell）、无操作（rest）。<br>因为不限制交易次数，因此交易次数这个因素不影响题目，不必考虑。DP Table 是二维的，两个维度分别是天数（0,1,…,n-1）和是否持有股票（1 表持有，0 表不持有）。<br><br><br>状态转移方程<br></strong>Case 1，今天我没有股票，有两种可能：**</p><ul><li>昨天我手上就没有股票，今天不做任何操作（rest）；</li><li>昨天我手上有一只股票，今天按照时价卖掉了（sell），收获了一笔钱</li></ul><p><strong>Case 2，今天持有一只股票，有两种可能：</strong></p><ul><li>昨天我手上就有这只股票，今天不做任何操作（rest）；</li><li>昨天我没有股票，今天按照时价买入一只（sell），花掉了一笔钱</li></ul><p><br>综上，第 i 天的状态转移方程为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">0</span>] - prices[i])</span><br></pre></td></tr></table></figure><p><br>注意上面的转移方程只是对某一天而言的，要求出整个 DP Table 的状态，需要对 i 进行遍历。<br><br><br><strong>边界状态</strong><br>观察状态转移方程，第 i 天的状态只由第 i-1 天状态推导而来，因此边界状态只需要定义 i=0（也就是第一天）即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span>        <span class="comment"># 第一天没有股票，说明没买没卖，获利为0</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]   <span class="comment"># 第一天持有股票，说明买入了，花掉一笔钱</span></span><br></pre></td></tr></table></figure><p><br><strong>代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices)</span>:</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n&lt;=<span class="number">1</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># dp table</span></span><br><span class="line">        dp = [[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        边界条件，初始条件</span></span><br><span class="line"><span class="string">        第 i 天的状态只由第 i-1 天状态推导而来，因此边界状态只需要定义 i=0（也就是第一天）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">0</span>] - prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]<span class="comment"># 返回最后一天且手上没有股票时的获利情况</span></span><br></pre></td></tr></table></figure><p><br><strong>第三题，k = +infinity with cooldown</strong><br>题解：<br>这道题的在 <a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii/" target="_blank" rel="external nofollow noopener noreferrer">买卖股票的最佳时机 II</a> 的基础上添加了冷冻期的要求，即每次 sell 之后要等一天才能继续交易。状态转移方程要做修改，如果第 i 天选择买入股票，状态要从第 i-2 的转移，而不是 i-1 (因为第 i-1 天是冷冻期)。另外，由于状态转移方程中出现了 dp[i-2] 推导 dp[i-1]，因此状态边界除了考虑 i=0 天，<strong>还要加上 i=1 天的状态。</strong>Solution 如下<br><br><br>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type prices: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">1</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        如果第 i 天选择买入股票，状态要从第 i-2 的转移，而不是 i-1 (因为第 i-1 天是冷冻期)。</span></span><br><span class="line"><span class="string">        另外，由于状态转移方程中出现了 dp[i-2] 推导 dp[i-1]，因此状态边界除了考虑 i=0 天，还要加上 i=1 天的状态。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">        dp[<span class="number">1</span>][<span class="number">0</span>] = max(dp[<span class="number">0</span>][<span class="number">0</span>], dp[<span class="number">0</span>][<span class="number">1</span>] + prices[<span class="number">1</span>])</span><br><span class="line">        dp[<span class="number">1</span>][<span class="number">1</span>] = max(dp[<span class="number">0</span>][<span class="number">1</span>], dp[<span class="number">0</span>][<span class="number">0</span>] - prices[<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">            dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-2</span>][<span class="number">0</span>]-prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>第四题，k = +infinity with fee</strong><br>每次交易要支付手续费，只要把手续费从利润中减去即可。改写方程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dp[i][0] &#x3D; max(dp[i-1][0], dp[i-1][1] + prices[i])</span><br><span class="line">dp[i][1] &#x3D; max(dp[i-1][1], dp[i-1][0] - prices[i] - fee)</span><br><span class="line">解释：相当于买入股票的价格升高了。</span><br><span class="line">在第一个式子里减也是一样的，相当于卖出股票的价格减小了。</span><br></pre></td></tr></table></figure><p>直接翻译成代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int], fee: int)</span> -&gt; int:</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n&lt;=<span class="number">1</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i]-fee)  <span class="comment"># 卖出股票时注意要缴手续费</span></span><br><span class="line">            dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">0</span>]-prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>第五题，k = 2</strong><br>题目约定最多交易次数 k=2，因此交易次数必须作为一个新的维度考虑进 DP Table 里，也就是说，这道题需要三维 DP 来解决。三个维度分别为：天数 i（i=0,1,…,n-1），买入股票的次数 j（j=1,2,…,k）和是否持有股票（1 表持有，0 表不持有）. 特别注意买入股票的次数为 j 时，其实隐含了另一个信息，就是卖出股票的次数为 j-1 或 j 次。<br><br><br>状态转移方程，这里还是比较难懂，读了几遍，我的理解是，下面第一行代码，右边表示的是，昨天持有股票，但是今天没有持有，所以卖掉，卖掉没有影响k的次数。但是第二行代码的右边，昨天没有持有股票而今天有持有，则是今天🈶购买股票，昨天没有购买的情况下，则对应的k为k-1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][j][<span class="number">0</span>], dp[i<span class="number">-1</span>][j][<span class="number">1</span>]+prices[i]) </span><br><span class="line"><span class="comment"># 右边:今天卖了昨天持有的股票，所以两天买入股票的次数都是j</span></span><br><span class="line">dp[i][j][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][j][<span class="number">1</span>], dp[i<span class="number">-1</span>][j<span class="number">-1</span>][<span class="number">0</span>]-prices[i]) </span><br><span class="line"><span class="comment"># 右边:昨天没有持股而今天买入一只，故昨天买入的次数是j-1</span></span><br></pre></td></tr></table></figure><p>注意上面的转移方程只是穷举了第三个维度，要求出整个 DP Table 的状态，需要对 i 和 j 进行遍历。<br><br><br>边界状态<br>观察状态转移方程知，边界状态需要考虑两个方面：i=0 和 j=0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># j=0 </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    dp[i][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 没有买入过股票，且手头没有持股，则获取的利润为0</span></span><br><span class="line">    dp[i][<span class="number">0</span>][<span class="number">1</span>] = -float(<span class="string">'inf'</span>)<span class="comment"># 没有买入过股票，不可能持股，用利润负无穷表示这种不可能</span></span><br><span class="line"><span class="comment"># i=0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, k+<span class="number">1</span>):<span class="comment"># 前面j=0已经赋值了，这里j从1开始</span></span><br><span class="line">    dp[<span class="number">0</span>][k][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    dp[<span class="number">0</span>][k][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><br>特别注意，上述两轮边界定义有交集——dp[0][0][0] 和 dp[0][0][1] ，后者会得到不同的结果，应以 j=0 时赋值结果为准。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n&lt;=<span class="number">1</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># dp table</span></span><br><span class="line">        dp = [[[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">        <span class="string">"""边界条件，分别为i=0, k=0"""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            dp[i][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            dp[i][<span class="number">0</span>][<span class="number">1</span>] = float(<span class="string">'-inf'</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            dp[<span class="number">0</span>][k][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            dp[<span class="number">0</span>][k][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 这里注意：i=0， k=0在上面已经计算过了。所以这里的下标从1开始</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">                dp[i][k][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">0</span>], dp[i<span class="number">-1</span>][k][<span class="number">1</span>] + prices[i])</span><br><span class="line">                dp[i][k][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">1</span>], dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>] - prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><br><strong><br></strong>第六题，k = any integer**<br>有了上一题 k = 2 的铺垫，这题应该和上一题的第一个解法没啥区别。但是出现了一个超内存的错误，原来是传入的 k 值会非常大，dp 数组太大了。现在想想，交易次数 k 最多有多大呢？<br><br><br>一次交易由买入和卖出构成，至少需要两天。所以说有效的限制 k 应该不超过 n/2，如果超过，就没有约束作用了，相当于 k = +infinity。这种情况是之前解决过的。<br>直接把之前的代码重用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, k, prices)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string">        :type prices: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n&lt;=<span class="number">1</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        正常交易的情况下，完成一次交易最少也需要两天的时间。所以有效的交易次数应该小于等于 n//2</span></span><br><span class="line"><span class="string">        如果大于 n//2 ，则此种情况退化为可交易任意次的情况</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> k &gt; n // <span class="number">2</span>:</span><br><span class="line">            dp_1 = [[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">            <span class="comment">#           # 边界条件</span></span><br><span class="line">            dp_1[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            dp_1[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">                dp_1[i][<span class="number">0</span>] = max(dp_1[i - <span class="number">1</span>][<span class="number">0</span>], dp_1[i - <span class="number">1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">                dp_1[i][<span class="number">1</span>] = max(dp_1[i - <span class="number">1</span>][<span class="number">1</span>], dp_1[i - <span class="number">1</span>][<span class="number">0</span>] - prices[i])</span><br><span class="line">            <span class="keyword">return</span> dp_1[<span class="number">-1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp_2 = [[[<span class="literal">None</span>, <span class="literal">None</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> range(k+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">            <span class="string">"""边界条件，分别为i=0, k=0"""</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">                dp_2[i][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                dp_2[i][<span class="number">0</span>][<span class="number">1</span>] = float(<span class="string">'-inf'</span>)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(k+<span class="number">1</span>):</span><br><span class="line">                dp_2[<span class="number">0</span>][k][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                dp_2[<span class="number">0</span>][k][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, k+<span class="number">1</span>):</span><br><span class="line">                    dp_2[i][k][<span class="number">0</span>] = max(dp_2[i<span class="number">-1</span>][k][<span class="number">0</span>], dp_2[i<span class="number">-1</span>][k][<span class="number">1</span>] + prices[i])</span><br><span class="line">                    dp_2[i][k][<span class="number">1</span>] = max(dp_2[i<span class="number">-1</span>][k][<span class="number">1</span>], dp_2[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>] - prices[i])</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> dp_2[<span class="number">-1</span>][<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>至此，6 道题目通过一个状态转移方程全部解决。<br><strong>四、最后总结</strong><br>本文给大家讲了如何通过状态转移的方法解决复杂的问题，用一个状态转移方程秒杀了 6 道股票买卖问题，现在想想，其实也不算难对吧？这已经属于动态规划问题中较困难的了。<br>关键就在于列举出所有可能的「状态」，然后想想怎么穷举更新这些「状态」。一般用一个多维 dp 数组储存这些状态，从 base case 开始向后推进，推进到最后的状态，就是我们想要的答案。想想这个过程，你是不是有点理解「动态规划」这个名词的意义了呢？<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;JJDTz&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;思路介绍&quot;&gt;&lt;a href=&quot;#思路介绍&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
      <category term="算法" scheme="cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="动态规划" scheme="cpeixin.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink结合Kafka构建端到端的Exactly-Once处理</title>
    <link href="cpeixin.cn/2019/07/04/Apache-Flink%E7%BB%93%E5%90%88Kafka%E6%9E%84%E5%BB%BA%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84Exactly-Once%E5%A4%84%E7%90%86/"/>
    <id>cpeixin.cn/2019/07/04/Apache-Flink%E7%BB%93%E5%90%88Kafka%E6%9E%84%E5%BB%BA%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84Exactly-Once%E5%A4%84%E7%90%86/</id>
    <published>2019-07-03T17:06:47.000Z</published>
    <updated>2020-07-03T17:07:49.898Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --><p>Apache Flink自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFunction（<a href="https://issues.apache.org/jira/browse/FLINK-7210" target="_blank" rel="external nofollow noopener noreferrer">相关的Jira</a>）。它提取了两阶段提交协议的通用逻辑，使得通过Flink来构建端到端的Exactly-Once程序成为可能。同时支持一些数据源（source）和输出端（sink），包括Apache Kafka 0.11及更高版本。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的Exactly-Once语义。<br><br><br>有关TwoPhaseCommitSinkFunction的使用详见文档: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html" target="_blank" rel="external nofollow noopener noreferrer">TwoPhaseCommitSinkFunction</a>。或者可以直接阅读Kafka 0.11 sink的文档: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="external nofollow noopener noreferrer">kafka</a>。<br>接下来会详细分析这个新功能以及Flink的实现逻辑，分为如下几点。</p><ul><li><p>描述Flink checkpoint机制是如何保证Flink程序结果的Exactly-Once的</p></li><li><p>显示Flink如何通过两阶段提交协议与数据源和数据输出端交互，以提供端到端的Exactly-Once保证</p></li><li><p>通过一个简单的示例，了解如何使用TwoPhaseCommitSinkFunction实现Exactly-Once的文件输出<br><a name="YwJZF"></a></p><h2 id="Apache-Flink应用程序中的Exactly-Once语义"><a href="#Apache-Flink应用程序中的Exactly-Once语义" class="headerlink" title="Apache Flink应用程序中的Exactly-Once语义"></a>Apache Flink应用程序中的Exactly-Once语义</h2><p>当我们说『Exactly-Once』时，指的是每个输入的事件只影响最终结果一次。即使机器或软件出现故障，既没有重复数据，也不会丢数据。<br><br><br><a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>很久之前就提供了Exactly-Once语义。在过去几年中，我们对Flink的<a href="https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink" target="_blank" rel="external nofollow noopener noreferrer">checkpoint机制</a>有过深入的描述，这是Flink有能力提供Exactly-Once语义的核心。Flink文档还提供了该功能的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html" target="_blank" rel="external nofollow noopener noreferrer">全面概述</a>。<br><br><br>在继续之前，先看下对checkpoint机制的简要介绍，这对理解后面的主题至关重要。<br><strong>一次checkpoint是以下内容的一致性快照</strong>：</p></li><li><p>应用程序的当前状态, eg: pv{‘app_1’:5000, ‘app_2’:6000}</p></li><li><p>输入流的位置, eg: offset:200</p></li></ul><p><br>Flink可以配置一个固定的时间点，定期产生checkpoint，将checkpoint的数据写入持久存储系统，例如S3或HDFS。将<strong>checkpoint数据写入持久存储是异步发生</strong>的，这意味着Flink应用程序在checkpoint过程中可以继续处理数据。<br><br><br>如果发生机器或软件故障，重新启动后，<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>应用程序将从最新的checkpoint点恢复处理； Flink会恢复应用程序状态，将输入流回滚到上次checkpoint保存的位置，然后重新开始运行。这意味着Flink可以像从未发生过故障一样计算结果。<br><br><br><strong>在Flink 1.4.0之前，Exactly-Once语义仅限于Flink应用程序内部</strong>，并没有扩展到Flink数据处理完后发送的大多数外部系统。Flink应用程序与各种数据输出端进行交互，开发人员需要有能力自己维护组件的上下文来保证Exactly-Once语义。<br><br><br>为了提供端到端的Exactly-Once语义，也就是说，除了Flink应用程序内部，Flink写入的外部系统也需要能满足Exactly-Once语义 ，这些外部系统必须提供提交或回滚的方法，然后通过Flink的checkpoint机制来协调。<br><br><br>分布式系统中，协调提交和回滚的常用方法是<a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external nofollow noopener noreferrer">两阶段提交协议</a>。在下一节中，我们将讨论Flink的<strong>TwoPhaseCommitSinkFunction</strong>是如何利用<strong>两阶段提交协议来提供端到端的Exactly-Once语义</strong>。<br><a name="JmXiE"></a></p><h2><a href="#" class="headerlink"></a></h2><p><a name="wiATx"></a></p><h2 id="Flink应用程序端到端的Exactly-Once语义"><a href="#Flink应用程序端到端的Exactly-Once语义" class="headerlink" title="Flink应用程序端到端的Exactly-Once语义"></a>Flink应用程序端到端的Exactly-Once语义</h2><p>我们将介绍两阶段提交协议，以及它如何在一个读写Kafka的Flink程序中实现端到端的Exactly-Once语义。<br><br><br>Kafka是一个流行的消息中间件，经常与Flink一起使用。Kafka在最近的0.11版本中添加了对事务的支持。这意味着现在通过Flink读写Kafaka，并提供<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="external nofollow noopener noreferrer">端到端的Exactly-Once语义有了必要的支持</a>。<br><br><br><a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>对端到端的Exactly-Once语义的支持不仅局限于Kafka，您可以将它与任何一个提供了必要的协调机制的源/输出端一起使用。例如<a href="http://pravega.io/" target="_blank" rel="external nofollow noopener noreferrer">Pravega</a>，来自DELL/EMC的开源流媒体存储系统，通过Flink的TwoPhaseCommitSinkFunction也能支持端到端的Exactly-Once语义。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122306.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122306.png#align=left&display=inline&height=1208&margin=%5Bobject%20Object%5D&originHeight=1208&originWidth=2154&status=done&style=none&width=2154" alt></a><br>在今天讨论的这个示例程序中，我们有：</p><ul><li>从Kafka读取的数据源（Flink内置的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-consumer" target="_blank" rel="external nofollow noopener noreferrer">KafkaConsumer</a>）</li><li>窗口聚合</li><li>将数据写回Kafka的数据输出端（Flink内置的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-producer" target="_blank" rel="external nofollow noopener noreferrer">KafkaProducer</a>）</li></ul><p><br>要使数据<strong>输出端</strong>提供Exactly-Once保证，它必须将所有数据通过一个事务提交给Kafka。提交捆绑了<strong>两个checkpoint</strong>之间的所有要写入的数据。这可确保在发生故障时能回滚写入的数据。但是在分布式系统中，通常会有多个并发运行的写入任务的，简单的提交或回滚是不够的，因为所有组件必须在提交或回滚时“一致”才能确保一致的结果。Flink使用两阶段提交协议及预提交阶段来解决这个问题。<br><br><br>在checkpoint开始的时候，即两阶段提交协议的“预提交”阶段。当checkpoint开始时，Flink的JobManager会将checkpoint barrier（将数据流中的记录分为进入当前checkpoint与进入下一个checkpoint）注入数据流。<br>brarrier在operator之间传递。对于每一个operator，它触发operator的状态快照写入到state backend。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122328.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122328.png#align=left&display=inline&height=1202&margin=%5Bobject%20Object%5D&originHeight=1202&originWidth=2146&status=done&style=none&width=2146" alt></a><br>数据源保存了消费Kafka的偏移量(offset)，之后将checkpoint barrier传递给下一个operator。<br>这种方式仅适用于operator具有『内部』状态。所谓内部状态，是指Flink state backend保存和管理的。<br><br><br>例如，第二个operator中window聚合算出来的sum值。当一个进程有它的内部状态的时候，除了在checkpoint之前需要将数据变更写入到state backend，不需要在预提交阶段执行任何其他操作。<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>负责在checkpoint成功的情况下正确提交这些写入，或者在出现故障时中止这些写入。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122346.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122346.png#align=left&display=inline&height=1210&margin=%5Bobject%20Object%5D&originHeight=1210&originWidth=2152&status=done&style=none&width=2152" alt></a><br><a name="zhApV"></a></p><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><p><a name="Q1EPD"></a></p><h2 id="Flink应用程序启动预提交阶段"><a href="#Flink应用程序启动预提交阶段" class="headerlink" title="Flink应用程序启动预提交阶段"></a>Flink应用程序启动预提交阶段</h2><p>但是，当进程具有『外部』状态时，需要作些额外的处理。外部状态通常以写入外部系统（如Kafka）的形式出现。在这种情况下，为了提供Exactly-Once保证，外部系统必须支持事务，这样才能和两阶段提交协议集成。<br><br><br>在本文示例中的数据需要写入Kafka，因此数据输出端（Data Sink）有外部状态。在这种情况下，在预提交阶段，<strong>除了将其状态写入state backend之外，数据输出端还必须预先提交其外部事务。</strong><br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122406.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122406.png#align=left&display=inline&height=1212&margin=%5Bobject%20Object%5D&originHeight=1212&originWidth=2154&status=done&style=none&width=2154" alt></a><br><br><br>当checkpoint barrier在所有operator都传递了一遍，并且触发的checkpoint回调成功完成时，预提交阶段就结束了。所有触发的状态快照都被视为该checkpoint的一部分。checkpoint是整个应用程序状态的快照，包括预先提交的外部状态。如果发生故障，我们可以回滚到上次成功完成快照的时间点。<br><br><br>下一步是通知所有operator，checkpoint已经成功了。这是两阶段提交协议的提交阶段，JobManager为应用程序中的每个operator发出checkpoint已完成的回调。<br><br><br>数据源和widnow operator没有外部状态，因此在提交阶段，这些operator不必执行任何操作。但是，数据输出端（Data Sink）拥有外部状态，此时应该提交外部事务。<br><a href="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122426.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122426.png#align=left&display=inline&height=1208&margin=%5Bobject%20Object%5D&originHeight=1208&originWidth=2156&status=done&style=none&width=2156" alt></a><br>我们对上述知识点总结下：</p><ul><li>一旦所有operator完成预提交，就提交一个commit。</li><li>如果至少有一个预提交失败，则所有其他提交都将中止，我们将回滚到上一个成功完成的checkpoint。</li><li>在预提交成功之后，提交的commit需要保证最终成功 ，operator和外部系统都需要保障这点。如果commit失败（例如，由于间歇性网络问题），整个<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>应用程序将失败，应用程序将根据用户的重启策略重新启动，还会尝试再提交。这个过程至关重要，因为如果commit最终没有成功，将会导致数据丢失。</li></ul><p>因此，我们可以确定所有operator都同意checkpoint的最终结果：所有operator都同意数据已提交，或提交被中止并回滚。<br></p><p><a name="8nafb"></a></p><h2 id="在Flink中实现两阶段提交Operator"><a href="#在Flink中实现两阶段提交Operator" class="headerlink" title="在Flink中实现两阶段提交Operator"></a>在Flink中实现两阶段提交Operator</h2><p>完整的实现两阶段提交协议可能有点复杂，这就是为什么<a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>将它的通用逻辑提取到抽象类TwoPhaseCommitSinkFunction中的原因。<br><br><br>接下来基于输出到文件的简单示例，说明如何使用TwoPhaseCommitSinkFunction。用户只需要实现四个函数，就能为数据输出端实现Exactly-Once语义：</p><ul><li>beginTransaction - 在事务开始前，我们在目标文件系统的临时目录中创建一个临时文件。随后，我们可以在处理数据时将数据写入此文件。</li><li>preCommit - 在预提交阶段，我们刷新文件到存储，关闭文件，不再重新写入。我们还将为属于下一个checkpoint的任何后续文件写入启动一个新的事务。</li><li>commit - 在提交阶段，我们将预提交阶段的文件原子地移动到真正的目标目录。需要注意的是，这会增加输出数据可见性的延迟。</li><li>abort - 在中止阶段，我们删除临时文件。</li></ul><p><br>我们知道，如果发生任何故障，Flink会将应用程序的状态恢复到最新的一次checkpoint点。一种极端的情况是，预提交成功了，但在这次commit的通知到达operator之前发生了故障。在这种情况下，Flink会将operator的状态恢复到已经预提交，但尚未真正提交的状态。<br><br><br>我们需要在预提交阶段保存足够多的信息到checkpoint状态中，以便在重启后能正确的中止或提交事务。在这个例子中，这些信息是临时文件和目标目录的路径。<br><br><br>TwoPhaseCommitSinkFunction已经把这种情况考虑在内了，并且在从checkpoint点恢复状态时，会优先发出一个commit。我们需要以幂等方式实现提交，一般来说，这并不难。在这个示例中，我们可以识别出这样的情况：临时文件不在临时目录中，但已经移动到目标目录了。<br><br><br>在TwoPhaseCommitSinkFunction中，还有一些其他边界情况也会考虑在内，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html" target="_blank" rel="external nofollow noopener noreferrer">Flink文档</a>了解更多信息。<br><a name="FX1br"></a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总结下本文涉及的一些要点：</p><ul><li><a href="https://zhoukaibo.com/tags/flink/" target="_blank" rel="external nofollow noopener noreferrer">Flink</a>的checkpoint机制是支持两阶段提交协议并提供端到端的Exactly-Once语义的基础。</li><li>这个方案的优点是: Flink不像其他一些系统那样，通过网络传输存储数据 - 不需要像大多数批处理程序那样将计算的每个阶段写入磁盘。</li><li>Flink的TwoPhaseCommitSinkFunction提取了两阶段提交协议的通用逻辑，基于此将Flink和支持事务的外部系统结合，构建端到端的Exactly-Once成为可能。</li><li>从<a href="https://data-artisans.com/blog/announcing-the-apache-flink-1-4-0-release" target="_blank" rel="external nofollow noopener noreferrer">Flink 1.4.0</a>开始，Pravega和Kafka 0.11 producer都提供了Exactly-Once语义；Kafka在0.11版本首次引入了事务，为在Flink程序中使用Kafka producer提供Exactly-Once语义提供了可能性。</li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="external nofollow noopener noreferrer">Kafaka 0.11 producer</a>的事务是在TwoPhaseCommitSinkFunction基础上实现的，和at-least-once producer相比只增加了非常低的开销。</li></ul><p>这是个令人兴奋的功能，期待Flink TwoPhaseCommitSinkFunction在未来支持更多的数据接收端。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Apache Flink自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFu
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="cpeixin.cn/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>一文搞懂Flink内部的Exactly Once和At Least Once</title>
    <link href="cpeixin.cn/2019/07/03/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82Flink%E5%86%85%E9%83%A8%E7%9A%84Exactly-Once%E5%92%8CAt-Least-Once/"/>
    <id>cpeixin.cn/2019/07/03/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82Flink%E5%86%85%E9%83%A8%E7%9A%84Exactly-Once%E5%92%8CAt-Least-Once/</id>
    <published>2019-07-03T13:24:51.000Z</published>
    <updated>2020-07-03T13:26:50.945Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --><p><a name="gI6Sf"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Rg5ep"></a></p><h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><ul><li>介绍CheckPoint如何保障Flink任务的高可用</li><li>CheckPoint中的状态简介</li><li>如何实现全域一致的分布式快照？</li><li>什么是barrier？什么是barrier对齐？</li><li>为什么barrier对齐就是Exactly Once？为什么barrier不对齐就是 At Least Once？</li></ul><p><a name="pCF3J"></a></p><h2 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h2><p>Apache Flink® - Stateful Computations over Data Streams<br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fflink.apache.org%2Fzh%2F" target="_blank" rel="external nofollow noopener noreferrer">Apache Flink® - 数据流上的有状态计算</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fci.apache.org%2Fprojects%2Fflink%2Fflink-docs-release-1.8%2F" target="_blank" rel="external nofollow noopener noreferrer">Flink 1.8 Document</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fci.apache.org%2Fprojects%2Fflink%2Fflink-docs-release-1.8%2Fdev%2Fstream%2Fstate%2F" target="_blank" rel="external nofollow noopener noreferrer">State &amp; Fault Tolerance</a><br><br><br>有状态函数和运算符在各个元素/事件的处理中存储数据（状态数据可以修改和查询，可以自己维护，根据自己的业务场景，保存历史数据或者中间结果到状态中）</p><p>例如：</p><ul><li>当应用程序搜索某些事件模式时，状态将存储到目前为止遇到的事件序列。</li><li>在每分钟/小时/天聚合事件时，状态保存待处理的聚合。</li><li>当在数据点流上训练机器学习模型时，状态保持模型参数的当前版本。</li><li>当需要管理历史数据时，状态允许有效访问过去发生的事件。</li></ul><p><br>什么是状态？</p><ul><li>无状态计算的例子<ul><li>比如：我们只是进行一个字符串拼接，输入 a，输出 a_666,输入b，输出 b_666</li><li>输出的结果跟之前的状态没关系，符合幂等性。<ul><li>幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用</li></ul></li></ul></li><li>有状态计算的例子<ul><li>计算pv、uv</li><li>输出的结果跟之前的状态有关系，不符合幂等性，访问多次，pv会增加<br><a name="BFHz0"></a><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><a name="koz7u"></a><h2 id="Flink的CheckPoint功能简介"><a href="#Flink的CheckPoint功能简介" class="headerlink" title="Flink的CheckPoint功能简介"></a>Flink的CheckPoint功能简介</h2>Flink CheckPoint 的存在就是为了解决flink任务failover掉之后，能够正常恢复任务。那CheckPoint具体做了哪些功能，为什么任务挂掉之后，通过CheckPoint能使得任务恢复呢？</li></ul></li></ul><p>CheckPoint是通过给程序快照的方式使得将历史某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。问题来了，快照是什么鬼？能吃吗？<br><br><br>SnapShot翻译为快照，指将程序中某些信息存一份，后期可以用来恢复。对于一个Flink任务来讲，快照里面到底保存着什么信息呢？</p><p>晦涩难懂的概念怎么办？当然用案例来代替咯，用案例让大家理解快照里面到底存什么信息。选一个大家都比较清楚的指标，app的pv，flink该怎么统计呢？<br><br><br>我们从Kafka读取到一条条的日志，从日志中解析出app_id，然后将统计的结果放到内存中一个Map集合，app_id做为key，对应的pv做为value，每次只需要将相应app_id 的pv值+1后put到Map中即可<br><img src="//upload-images.jianshu.io/upload_images/19063731-073db38513bf8b50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=317&margin=%5Bobject%20Object%5D&originHeight=317&originWidth=1200&status=done&style=none&width=1200" alt><br>flink的Source task记录了当前消费到kafka <code>test</code> topic的所有partition的offset，为了方便理解CheckPoint的作用，这里先用一个partition进行讲解，假设名为 “test”的 topic只有一个partition0<br><br><br>例：（0，1000）,表示0号partition目前消费到offset为1000的数据<br><br><br>flink的pv task记录了当前计算的各app的pv值，为了方便讲解，我这里有两个app：app1、app2<br><br><br>例：（app1，50000）（app2，10000），表示app1当前pv值为50000 ，表示app2当前pv值为10000，每来一条数据，只需要确定相应app_id，将相应的value值+1后put到map中即可<br><br><br><strong>该案例中，CheckPoint到底记录了什么信息呢？</strong><br><br><br>记录的其实就是第n次CheckPoint消费的offset信息和各app的pv值信息，记录一下发生CheckPoint当前的状态信息，并将该状态信息保存到相应的状态后端。（注：<strong>状态后端是保存状态的地方</strong>，决定状态如何保存，如何保障状态高可用，我们只需要知道，我们能从状态后端拿到offset信息和pv信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过checkpoint来恢复我们的应用程序）<br><br><br>chk-100：{offset：（0，1000）pv：（app1，50000）（app2，10000）}<br><br><br>该状态信息表示第100次CheckPoint的时候， partition 0 offset消费到了1000，pv统计结果为（app1，50000）（app2，10000）<br><br><br><strong>任务挂了，如何恢复？</strong><br><br><br>假如我们设置了三分钟进行一次CheckPoint，保存了上述所说的 chk-100 的CheckPoint状态后，过了十秒钟，offset已经消费到 （0，1100），pv统计结果变成了（app1，50080）（app2，10020），但是突然任务挂了，怎么办？<br><br><br>莫慌，其实很简单，flink只需要从最近一次成功的CheckPoint保存的offset（0，1000）处接着消费即可，当然pv值也要按照状态里的pv值（app1，50000）（app2，10000）进行累加，不能从（app1，50080）（app2，10020）处进行累加，因为 <strong>partition 0 offset消费到 1000时，pv统计结果为（app1，50000）（app2，10000）</strong><br><br><br>当然如果你想从offset （0，1100）pv（app1，50080）（app2，10020）这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来。我们能做的最高效方式就是从最近一次成功的CheckPoint处恢复，也就是我一直所说的chk-100,以上讲解，基本就是CheckPoint承担的工作，描述的场景比较简单<br><br><br>疑问，计算pv的task在一直运行，它怎么知道什么时候去做这个快照？或者说计算pv的task怎么保障它自己计算的pv值（app1，50000）（app2，10000）就是offset（0，1000）那一刻的统计结果呢？<br><br><br>flink是在数据中加了一个叫做barrier的东西（barrier中文翻译：栅栏），下图中红圈处就是两个barrier<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1593765235849-e46de314-3eac-414e-af6c-753ae3e0a6d4.png#align=left&display=inline&height=534&margin=%5Bobject%20Object%5D&name=%E6%88%AA%E5%B1%8F2020-07-03%20%E4%B8%8B%E5%8D%884.33.18.png&originHeight=534&originWidth=1314&size=271951&status=done&style=none&width=1314" alt="截屏2020-07-03 下午4.33.18.png"><br><br><br><strong>barrier从Source Task处生成，一直流到Sink Task，期间所有的Task只要碰到barrier，就会触发自身进行快照</strong><br><strong>CheckPoin</strong>t , barrier n-1处做的快照就是指Job从开始处理到 barrier n-1所有的状态数据, barrier n 处做的快照就是指从Job开始到处理到 barrier n所有的状态数据<br><br><br>对应到pv案例中就是，<strong>Source Task接收到JobManager</strong>的编号为chk-100的CheckPoint触发请求后，发现自己恰好接收到kafka offset（0，1000）处的数据，<strong>所以会往offset（0，1000）数据之后offset（0，1001）数据之前安插一个barrier，然后自己开始做快照</strong>，也就是将offset（0，1000）保存到状态后端chk-100中。然后barrier接着往下游发送，当统计pv的task接收到barrier后，也会暂停处理数据，将自己内存中保存的pv信息（app1，50000）（app2，10000）保存到状态后端chk-100中。OK，flink大概就是通过这个原理来保存快照的。统计pv的task接收到barrier，就意味着barrier之前的数据都处理了，所以说，不会出现丢数据的情况<br><br><br>barrier的作用就是为了把数据区分开，CheckPoint过程中有一个同步做快照的环节不能处理barrier之后的数据，为什么呢？<br><br><br>如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据<br><br><br>结合案例来讲就是，统计pv的task想对（app1，50000）（app2，10000）做快照，但是如果数据还在处理，可能快照还没保存下来，状态已经变成了（app1，50001）（app2，10001），快照就不准确了，就不能保障Exactly Once了<br><br><br><strong>总结</strong><br><br><br>流式计算中状态交互<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1593765988323-23515639-93ef-4b8e-a6b9-e0de3c03dab7.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=%E6%88%AA%E5%B1%8F2020-07-03%20%E4%B8%8B%E5%8D%884.45.45.png&originHeight=720&originWidth=1426&size=313470&status=done&style=none&width=1426" alt="截屏2020-07-03 下午4.45.45.png"><br><br><br>简易场景精确一次的容错方法，周期性地对消费offset和统计的状态信息或统计结果进行快照<br><img src="//upload-images.jianshu.io/upload_images/19063731-76237293b1551a9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=395&margin=%5Bobject%20Object%5D&originHeight=395&originWidth=1200&status=done&style=none&width=1200" alt></p><p>消费到X位置的时候，将X对应的状态保存下来<br><img src="//upload-images.jianshu.io/upload_images/19063731-ad52ba2a04f78ae6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&originHeight=378&originWidth=1200&status=done&style=none&width=1200" alt></p><p>消费到Y位置的时候，将Y对应的状态保存下来<br><img src="//upload-images.jianshu.io/upload_images/19063731-b2a6809920a08ccf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=385&margin=%5Bobject%20Object%5D&originHeight=385&originWidth=1200&status=done&style=none&width=1200" alt></p><p><a name="ZQi6z"></a></p><h2 id="多并行度、多Operator情况下，CheckPoint过程"><a href="#多并行度、多Operator情况下，CheckPoint过程" class="headerlink" title="多并行度、多Operator情况下，CheckPoint过程"></a>多并行度、多Operator情况下，CheckPoint过程</h2><p><br><strong>分布式状态容错面临的问题与挑战</strong></p><ul><li>如何确保状态拥有<strong>精确一次</strong>的容错保证？</li><li>如何在分布式场景下替多个拥有本地状态的算子产生<strong>一个全域一致的快照</strong>？</li><li>如何在<strong>不中断运算</strong>的前提下产生快照？</li></ul><p><br><strong>多并行度、多Operator实例的情况下，如何做全域一致的快照</strong><br><br><br>所有的Operator运行过程中遇到barrier后，都对自身的状态进行一次快照，保存到相应状态后端<br><br><br>对应到pv案例：有的Operator计算的app1的pv，有的Operator计算的app2的pv，当他们碰到barrier时，都需要将目前统计的pv信息快照到状态后端<br><img src="//upload-images.jianshu.io/upload_images/19063731-958537f5f6d20593.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=520&margin=%5Bobject%20Object%5D&originHeight=520&originWidth=1200&status=done&style=none&width=1200" alt></p><p>多Operator状态恢复<br><img src="//upload-images.jianshu.io/upload_images/19063731-52083c801724bae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=541&margin=%5Bobject%20Object%5D&originHeight=541&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><strong>具体怎么做这个快照呢？</strong><br><br><br>利用之前所讲的barrier策略<br><img src="//upload-images.jianshu.io/upload_images/19063731-cc2a3e70cc211583.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=431&margin=%5Bobject%20Object%5D&originHeight=431&originWidth=1200&status=done&style=none&width=1200" alt></p><p>JobManager向Source Task发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier<br><img src="//upload-images.jianshu.io/upload_images/19063731-2907015ba67cf908.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&originHeight=489&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><br><br>Source Task自身做快照，并保存到状态后端<br><img src="//upload-images.jianshu.io/upload_images/19063731-70bdf95950e1fb07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=525&margin=%5Bobject%20Object%5D&originHeight=525&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><br><br>Source Task将barrier跟数据流一块往下游发送<br><img src="//upload-images.jianshu.io/upload_images/19063731-36fc49196f2592f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=513&margin=%5Bobject%20Object%5D&originHeight=513&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br><br><br>当下游的Operator实例接收到CheckPoint barrier后，对自身做快照<br><img src="//upload-images.jianshu.io/upload_images/19063731-ccf2246af6bb22a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=528&margin=%5Bobject%20Object%5D&originHeight=528&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>多并行度快照详图<br><img src="//upload-images.jianshu.io/upload_images/19063731-f2edab06e990314e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=523&margin=%5Bobject%20Object%5D&originHeight=523&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>上述图中，有4个带状态的Operator实例，相应的状态后端就可以想象成填4个格子。整个CheckPoint 的过程可以当做Operator实例填自己格子的过程，Operator实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单的认为一次完整的CheckPoint做完了<br><br><br>上面只是快照的过程，整个CheckPoint执行过程如下<br><br><br>1、JobManager端的 CheckPointCoordinator向 所有SourceTask发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier</p><p>2、当task收到所有的barrier后，向自己的下游继续传递barrier，然后自身执行快照，并将自己的状态<strong>异步写入到持久化存储</strong>中</p><ul><li>增量CheckPoint只是把最新的一部分更新写入到外部存储</li><li>为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照</li></ul><p><br>3、当task完成备份后，会将备份数据的地址（state handle）通知给JobManager的CheckPointCoordinator</p><ul><li>如果CheckPoint的持续时长超过 了CheckPoint设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次CheckPoint失败，会把这次CheckPoint产生的所有 状态数据全部删除</li></ul><p><br>4、 最后 CheckPoint Coordinator 会把整个 StateHandle 封装成 completed CheckPoint Meta，写入到hdfs</p><p><a name="yrq0j"></a></p><h4 id="barrier对齐"><a href="#barrier对齐" class="headerlink" title="barrier对齐"></a>barrier对齐</h4><p><br><strong>什么是barrier对齐？</strong><img src="//upload-images.jianshu.io/upload_images/19063731-284548fb44b76c9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=216&margin=%5Bobject%20Object%5D&originHeight=216&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>一旦Operator从输入流接收到CheckPoint barrier n，它就不能处理来自该流的任何数据记录，直到它从其他所有输入接收到barrier n为止。否则，<strong>它会混合属于快照n的记录和属于快照n + 1的记录</strong><br><br><br>接收到barrier n的流暂时被搁置。从这些流接收的记录不会被处理，而是放入输入缓冲区。上图中第2个图，虽然数字流对应的barrier已经到达了，但是barrier之后的1、2、3这些数据只能放到buffer中，等待字母流的barrier到达<br><br><br>一旦最后所有输入流都接收到barrier n，Operator就会把缓冲区中pending 的输出数据发出去，然后把CheckPoint barrier n接着往下游发送<br><br><br>这里还会对自身进行快照，之后，Operator将继续处理来自所有输入流的记录，在处理来自流的记录之前先处理来自输入缓冲区的记录<br><br><br><strong>什么是barrier不对齐？</strong><br><br><br>上述图2中，当还有其他输入流的barrier还没有到达时，会把已到达的barrier之后的数据1、2、3搁置在缓冲区，等待其他流的barrier到达后才能处理<br><br><br>barrier不对齐就是指当还有其他流的barrier还没到达时，为了不影响性能，也不用理会，直接处理barrier之后的数据。等到所有流的barrier的都到达后，就可以对该Operator做CheckPoint了<br><br><br><strong>为什么要进行barrier对齐？不对齐到底行不行？</strong><br>答：<strong>Exactly Once时必须barrier对齐，如果barrier不对齐就变成了At Least Once</strong><br><br><br>后面的部分主要证明这句话<br><br><br>CheckPoint的目的就是为了保存快照，如果不对齐，那么在chk-100快照之前，已经处理了一些chk-100 对应的offset之后的数据，当程序从chk-100恢复任务时，chk-100对应的offset之后的数据还会被处理一次，所以就出现了重复消费。<br><br><br>如果听不懂没关系，后面有案例让您懂<br><br><br>结合pv案例来看，之前的案例为了简单，描述的kafka的topic只有1个partition，这里为了讲述barrier对齐，所以topic有2个partittion<img src="//upload-images.jianshu.io/upload_images/19063731-c5aca9a4bddb1317.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp#align=left&display=inline&height=668&margin=%5Bobject%20Object%5D&originHeight=668&originWidth=1200&status=done&style=none&width=1200" alt><br><br><br>结合业务，先介绍一下上述所有算子在业务中的功能</p><ul><li>Source的kafka的Consumer，从kakfa中读取数据到flink应用中</li><li>TaskA中的map将读取到的一条kafka日志转换为我们需要统计的app_id</li><li>keyBy 按照app_id进行keyBy，相同的app_id 会分到下游TaskB的同一个实例中</li><li>TaskB的map在状态中查出该app_id 对应的pv值，然后+1，存储到状态中</li><li>利用Sink将统计的pv值写入到外部存储介质中</li><li>我们从kafka的两个partition消费数据，TaskA和TaskB都有两个并行度，所以总共flink有4个Operator实例，这里我们称之为 TaskA0、TaskA1、TaskB0、TaskB1</li><li>假设已经成功做了99次CheckPoint，这里详细解释第100次CheckPoint过程<ul><li>JobManager内部有个定时调度，假如现在10点00分00秒到了第100次CheckPoint的时间了，<strong>JobManager的CheckPointCoordinator进程会向所有的Source Task发送CheckPointTrigger，也就是向TaskA0、TaskA1发送CheckPointTrigger</strong></li><li>TaskA0、TaskA1接收到CheckPointTrigger，会往数据流中安插barrier，将barrier发送到下游，在自己的状态中记录barrier安插的offset位置，然后自身做快照，将offset信息保存到状态后端<ul><li>这里假如TaskA0消费的partition0的offset为10000，TaskA1消费的partition1的offset为10005。那么状态中会保存 (0，10000)(1，10005)，表示0号partition消费到了offset为10000的位置，1号partition消费到了offset为10005的位置</li></ul></li><li>然后TaskA的map和keyBy算子中并没有状态，所以不需要进行快照</li><li>接着数据和barrier都向下游TaskB发送，相同的app_id 会发送到相同的TaskB实例上，这里假设有两个app：app0和app1，经过keyBy后，假设app0分到了TaskB0上，app1分到了TaskB1上。基于上面描述，TaskA0和TaskA1中的所有app0的数据都发送到TaskB0上，所有app1的数据都发送到TaskB1上</li><li>现在我们假设TaskB0做CheckPoint的时候barrier对齐了，TaskB1做CheckPoint的时候barrier不对齐，当然不能这么配置，我就是举这么个例子，带大家分析一下barrier对不对齐到底对统计结果有什么影响？</li><li>上面说了chk-100的这次CheckPoint，offset位置为(0，10000)(1，10005)，TaskB0使用barrier对齐，也就是说TaskB0不会处理barrier之后的数据，所以TaskB0在chk-100快照的时候，状态后端保存的app0的pv数据是从程序开始启动到kafka offset位置为(0，10000)(1，10005)的所有数据计算出来的pv值，一条不多（没处理barrier之后，所以不会重复），一条不少(barrier之前的所有数据都处理了，所以不会丢失)，假如保存的状态信息为(app0，8000)表示消费到(0，10000)(1，10005)offset的时候，app0的pv值为8000</li><li>TaskB1使用的barrier不对齐，假如TaskA0由于服务器的CPU或者网络等其他波动，导致TaskA0处理数据较慢，而TaskA1很稳定，所以处理数据比较快。导致的结果就是TaskB1先接收到了TaskA1的barrier，由于配置的barrier不对齐，所以TaskB1会接着处理TaskA1 barrier之后的数据，过了2秒后，TaskB1接收到了TaskA0的barrier，于是对状态中存储的app1的pv值开始做CheckPoint 快照，保存的状态信息为(app1，12050)，但是我们知道这个(app1，12050)实际上多处理了2秒TaskA1发来的barrier之后的数据，也就是kafka topic对应的partition1 offset 10005之后的数据，app1真实的pv数据肯定要小于这个12050，partition1的offset保存的offset虽然是10005，但是我们实际上可能已经处理到了offset 10200的数据，假设就是处理到了10200</li><li>虽然状态保存的pv值偏高了，但是不能说明重复处理，因为我的TaskA1并没有再次去消费partition1的offset 10005~10200的数据，所以相当于也没有重复消费，只是展示的结果更实时了</li><li>分析到这里，我们先梳理一下我们的状态保存了什么：<ul><li>chk-100<ul><li>offset：(0，10000)(1，10005)</li><li>pv：(app0，8000) (app1，12050)</li></ul></li></ul></li><li>接着程序在继续运行，过了10秒，由于某个服务器挂了，导致我们的四个Operator实例有一个Operator挂了，所以Flink会从最近一次的状态恢复，也就是我们刚刚详细讲的chk-100处恢复，那具体是怎么恢复的呢？<ul><li>Flink 同样会起四个Operator实例，我还称他们是 TaskA0、TaskA1、TaskB0、TaskB1。四个Operator会从状态后端读取保存的状态信息。</li><li>从offset：(0，10000)(1，10005) 开始消费，并且基于 pv：(app0，8000) (app1，12050)值进行累加统计</li><li>然后你就应该会发现这个app1的pv值12050实际上已经包含了partition1的offset 10005<del>10200的数据，所以partition1从offset 10005恢复任务时，partition1的offset 10005</del>10200的数据被消费了两次<ul><li>TaskB1设置的barrier不对齐，所以CheckPoint chk-100对应的状态中多消费了barrier之后的一些数据（TaskA1发送），重启后是从chk-100保存的offset恢复，这就是所说的At Least Once</li></ul></li><li>由于上面说TaskB0设置的barrier对齐，所以app0不会出现重复消费，因为app0没有消费offset：(0，10000)(1，10005) 之后的数据，也就是所谓的Exactly Once</li></ul></li></ul></li></ul><p><br>看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么barrier对齐就是Exactly Once，为什么barrier不对齐就是 At Least Once</p><p><a name="9Hprm"></a></p><h4 id="到底什么时候会出现barrier对齐？"><a href="#到底什么时候会出现barrier对齐？" class="headerlink" title="到底什么时候会出现barrier对齐？"></a>到底什么时候会出现barrier对齐？</h4><p><br>首先设置了Flink的CheckPoint语义是：Exactly Once<br><br><br>Operator实例必须有多个输入流才会出现barrier对齐</p><pre><code>- 对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以，必须有多个流才叫对齐。barrier对齐其实也就是上游多个流配合使得数据对齐的过程- 言外之意：如果Operator实例只有一个输入流，就根本不存在barrier对齐，自己跟自己默认永远都是对齐的</code></pre><p><a name="rnTha"></a></p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><p><strong>第一种场景计算PV，kafka只有一个partition，精确一次，至少一次就没有区别？</strong></p><p>答：如果只有一个partition，对应flink任务的Source Task并行度只能是1，确实没有区别，不会有至少一次的存在了，肯定是精确一次。因为<strong>只有barrier不对齐才会有可能重复处理，这里并行度都已经为1，默认就是对齐的</strong>，只有当上游有多个并行度的时候，多个并行度发到下游的barrier才需要对齐，单并行度不会出现barrier不对齐，所以必然精确一次。其实还是要理解barrier对齐就是Exactly Once不会重复消费，barrier不对齐就是 At Least Once可能重复消费，这里只有单个并行度根本不会存在barrier不对齐，所以不会存在至少一次语义</p><p><strong>为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照；这一步，如果向下发送barrier后，自己同步快照慢怎么办？下游已经同步好了，自己还没？</strong></p><p>答: 可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游快照的更及时了，我只要保障下游把barrier之前的数据都处理了，并且不处理barrier之后的数据，然后做快照，那么下游也同样支持精确一次。这个问题你不要从全局思考，你单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意一点，如果有一个Operator 的CheckPoint失败了或者因为CheckPoint超时也会导致失败，那么JobManager会认为整个CheckPoint失败。失败的CheckPoint是不能用来恢复任务的，必须所有的算子的CheckPoint都成功，那么这次CheckPoint才能认为是成功的，才能用来恢复任务</p><p><strong>我程序中Flink的CheckPoint语义设置了 Exactly Once，但是我的mysql中看到数据重复了？</strong><br><br><br>程序中设置了1分钟1次CheckPoint，但是5秒向mysql写一次数据，并commit<br>答：Flink要求end to end的精确一次都必须实现TwoPhaseCommitSinkFunction。如果你的chk-100成功了，过了30秒，由于5秒commit一次，所以实际上已经写入了6批数据进入mysql，但是突然程序挂了，从chk100处恢复，这样的话，之前提交的6批数据就会重复写入，所以出现了重复消费。<strong>Flink的精确一次有两种情况，一个是Flink内部的精确一次，一个是端对端的精确一次</strong>，这个博客所描述的都是关于Flink内部去的精确一次，我后期再发一个博客详细介绍一下Flink端对端的精确一次如何实现</p><p>转载自：<a href="https://www.jianshu.com/p/8d6569361999" target="_blank" rel="external nofollow noopener noreferrer">https://www.jianshu.com/p/8d6569361999</a><br><br><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jul 20 2020 23:12:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;gI6Sf&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="cpeixin.cn/tags/flink/"/>
    
  </entry>
  
</feed>
