<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布兰特 | 不忘初心</title>
  
  <subtitle>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="cpeixin.cn/"/>
  <updated>2020-05-28T15:08:18.891Z</updated>
  <id>cpeixin.cn/</id>
  
  <author>
    <name>Brent</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux直接下载Google Drive文件</title>
    <link href="cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/"/>
    <id>cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/</id>
    <published>2020-05-27T17:03:54.000Z</published>
    <updated>2020-05-28T15:08:18.891Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测试。<br><br><br>首选方案是将整个项目上传到GitHub中，随后在服务器中直接wget，但是模型文件过大，GitHub单个文件的限制是100MB。<br><br><br>突然想到可不可以直接从Google Drive上进行下载模型文件到服务器😅<br><br><br><strong>下载小文件：</strong><br><br><br>选择要下载的文件右键<br><br><br>点击“共享”<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598566164-b176f9b4-62bf-41d6-82bf-36e8837816f6.png#align=left&display=inline&height=1066&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.51.36.png&originHeight=1066&originWidth=1712&size=193930&status=done&style=none&width=1712" alt="屏幕快照 2020-05-28 上午12.51.36.png"><br><br><br><br><br>点击“更改”，设置分享权限<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598584015-3b9be292-6dd3-4edc-86ef-de50b58be55b.png#align=left&display=inline&height=1088&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.52.08.png&originHeight=1088&originWidth=2036&size=141068&status=done&style=none&width=2036" alt="屏幕快照 2020-05-28 上午12.52.08.png"><br><br><br>这是复制图中选中部分的ID<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590598604200-c2f98348-2daf-423b-9792-df03ad395236.png#align=left&display=inline&height=1078&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8A%E5%8D%8812.53.24.png&originHeight=1078&originWidth=1858&size=172147&status=done&style=none&width=1858" alt="屏幕快照 2020-05-28 上午12.53.24.png"><br><br><br>拼接下载链接，进行下载<br><br><br>wget <a href="https://drive.google.com/uc?id=1sT6GvdtCG3AnV-62beWCP6LNdtFgmX-o" target="_blank" rel="external nofollow noopener noreferrer">https://drive.google.com/uc?id=</a>复制下来的共享id -O your_file_name</p><p><strong>下载大文件：</strong></p><p>上面的方法，适合下载一些小文件，大文件就不可以了。更换下面命令的id选项，并且准备好cookies.txt<br><br><br>关于cookies.txt，可以在Chrome浏览器中下载cookie.txt这个插件，点击下载，上传到服务器中/tmp目录下即可<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590678181832-6ba38a09-0ea7-4d48-a6a3-5cd116d19e1d.png#align=left&display=inline&height=1472&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-28%20%E4%B8%8B%E5%8D%8810.59.19.png&originHeight=1472&originWidth=3094&size=893365&status=done&style=none&width=3094" alt="屏幕快照 2020-05-28 下午10.59.19.png"><br><br><br>关于文件id，和上面方法获取一致，接下来运行下面命令即可。<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZag' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZagARe" -O pytorch_model.bin</span><br></pre></td></tr></table></figure><br><br><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测
      
    
    </summary>
    
    
      <category term="Tools" scheme="cpeixin.cn/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>def neverGrowUp()</title>
    <link href="cpeixin.cn/2020/04/06/def-neverGrowUp/"/>
    <id>cpeixin.cn/2020/04/06/def-neverGrowUp/</id>
    <published>2020-04-05T16:00:00.000Z</published>
    <updated>2020-04-05T15:10:24.541Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neverGrowUp</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="title">while</span> <span class="title">true</span>:</span></span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br><span class="line">开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗疫英雄</title>
    <link href="cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/"/>
    <id>cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/</id>
    <published>2020-04-04T14:45:15.000Z</published>
    <updated>2020-04-05T14:46:33.308Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>致敬缅怀每一位抗疫英雄<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586098032229-b4c6c795-bf87-4105-8f82-87a86e48a89a.jpeg#align=left&display=inline&height=1796&name=WechatIMG86.jpeg&originHeight=1796&originWidth=1072&size=175464&status=done&style=none&width=1072" alt="WechatIMG86.jpeg"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;致敬缅怀每一位抗疫英雄&lt;br&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/jpeg/1072113
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python Flask &amp; Ajax 数据传输</title>
    <link href="cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/"/>
    <id>cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/</id>
    <published>2020-03-11T14:43:01.000Z</published>
    <updated>2020-04-04T17:13:00.080Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂</p><p>忙活三个小时，终于把前端和后端打通了～～</p><p>前端demo：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"&#123;&#123; url_for('send_message') &#125;&#125;"</span> <span class="attr">method</span>=<span class="string">"post"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span> =<span class="string">"domain"</span> <span class="attr">rows</span>=<span class="string">"30"</span> <span class="attr">cols</span>=<span class="string">"100"</span> <span class="attr">placeholder</span>=<span class="string">"请输入需要查询的域名,如cq5999.com"</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;input id="submit" type="submit" value="发送"&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">id</span>=<span class="string">"btn-bq"</span> <span class="attr">data-toggle</span>=<span class="string">"modal"</span> <span class="attr">data-target</span>=<span class="string">"#myModal"</span>&gt;</span>查询<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"send_content"</span>&gt;</span>向后台发送消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"send_content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send"</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"发送"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"recv_content"</span>&gt;</span>从后台接收消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"recv_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"recv_content"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 发送 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> message = $(<span class="string">"#send_content"</span>).val()</span></span><br><span class="line">        alert(message)</span><br><span class="line"><span class="javascript">        $.ajax(&#123;</span></span><br><span class="line"><span class="actionscript">            url:<span class="string">"/send_message"</span>,</span></span><br><span class="line"><span class="actionscript">            type:<span class="string">"POST"</span>,</span></span><br><span class="line">            data:&#123;</span><br><span class="line">                message:message</span><br><span class="line">            &#125;,</span><br><span class="line"><span class="actionscript">            dataType: <span class="string">'json'</span>,</span></span><br><span class="line"><span class="actionscript">            success:<span class="function"><span class="keyword">function</span> <span class="params">(data)</span> </span>&#123;</span></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 接收 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        $.getJSON(<span class="string">"/change_to_json"</span>,<span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#recv_content"</span>).val(data.message) <span class="comment">//将后端数据显示在前端</span></span></span><br><span class="line"><span class="javascript">            <span class="built_in">console</span>.log(<span class="string">"传到前端的数据的类型："</span> + <span class="keyword">typeof</span> (data.message))</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#send_content"</span>).val(<span class="string">""</span>)<span class="comment">//发送的输入框清空</span></span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>后端demo:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template, request, jsonify</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"index_v6.html"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/send_message', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_message</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_get = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    message_get = request.form[<span class="string">"domain"</span>].split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># message_get = request.form['message'] #input提交</span></span><br><span class="line">    print(<span class="string">"收到前端发过来的信息：%s"</span> % message_get)</span><br><span class="line">    print(<span class="string">"收到数据的类型为："</span> + str(type(message_get)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"收到消息"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/change_to_json', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_to_json</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_json = &#123;</span><br><span class="line">        <span class="string">"message"</span>: message_get</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jsonify(message_json)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>,debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂&lt;/p&gt;&lt;p&gt;忙活三个小时，终于把前端和后端打通了～～&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Python Flask接口设计-示例</title>
    <link href="cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/"/>
    <id>cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/</id>
    <published>2020-03-10T15:08:35.000Z</published>
    <updated>2020-04-04T17:12:52.356Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><a name="LHF1q"></a></p><h3 id="Get-请求"><a href="#Get-请求" class="headerlink" title="Get 请求"></a>Get 请求</h3><p><strong><strong>开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。</strong></strong><br><strong><br>**</strong>方法 1:****</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["GET"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><p>此种方式对应的request请求方式：</p><ol><li>拼接请求链接, 直接请求：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0/test_1.0?name=ccc&amp;age=18</a></li><li>request 请求中带有参数，如下图</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583826674613-bc99538a-988e-4386-b8e6-9eb9fce1862f.png#align=left&display=inline&height=610&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%883.47.43.png&originHeight=610&originWidth=1424&size=98593&status=done&style=none&width=1424" alt="屏幕快照 2020-03-10 下午3.47.43.png"></p><p>方法 2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">banWordSingleStart</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> getWordStatus(word)</span><br></pre></td></tr></table></figure><p>此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字</p><p>请求链接：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0</a>/api/banWordSingle/输入词</p><p><a name="vJdOc"></a></p><h3 id="Post-请求"><a href="#Post-请求" class="headerlink" title="Post 请求"></a>Post 请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["POST"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">8080</span>)</span><br></pre></td></tr></table></figure><p>请求方式：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583831085097-3a858ae4-259d-408d-a162-6a4ed8c5e291.png#align=left&display=inline&height=692&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%885.00.28.png&originHeight=692&originWidth=1438&size=99272&status=done&style=none&width=1438" alt="屏幕快照 2020-03-10 下午5.00.28.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;LHF1q&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;Get-请求&quot;&gt;&lt;a href=&quot;#Get-请求&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="flask" scheme="cpeixin.cn/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.10版本发布</title>
    <link href="cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/"/>
    <id>cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</id>
    <published>2020-02-12T17:22:22.000Z</published>
    <updated>2020-06-03T14:38:01.179Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><br>Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡献了200多个贡献者，此版本引入了对Flink作业的整体性能和稳定性的重大改进，原生Kubernetes集成的预览以及Python支持的巨大进步（PyFlink）。(Spark对Python的支持也越来越好😂)<br><br><br>Flink 1.10还标志着<a href="https://flink.apache.org/news/2019/08/22/release-1.9.0.html#preview-of-the-new-blink-sql-query-processor" target="_blank" rel="external nofollow noopener noreferrer">Blink集成</a>的完成，强化了流数据SQL并通过可用于生产的Hive集成和TPC-DS覆盖将成熟的批处理引入Flink。这篇博客文章描述了所有主要的新功能和改进，需要注意的重要更改以及预期的发展。<br><br><br>现在可以在Flink网站的更新的“ <a href="https://flink.apache.org/downloads.html" target="_blank" rel="external nofollow noopener noreferrer">下载”页面</a>上找到二进制分发文件和源工件。有关更多详细信息，请查看完整的<a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&version=12345845" target="_blank" rel="external nofollow noopener noreferrer">发行变更日志</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/" target="_blank" rel="external nofollow noopener noreferrer">更新的文档</a>。我们鼓励您下载发行版，并通过<a href="https://flink.apache.org/community.html#mailing-lists" target="_blank" rel="external nofollow noopener noreferrer">Flink邮件列表</a>或<a href="https://issues.apache.org/jira/projects/FLINK/summary" target="_blank" rel="external nofollow noopener noreferrer">JIRA</a>与社区分享您的反馈。</p><hr><p><a name="JxUa7"></a></p><h2 id="新功能和改进"><a href="#新功能和改进" class="headerlink" title="新功能和改进"></a>新功能和改进</h2><p><a name="improved-memory-management-and-configuration"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="ze04S"></a></p><h3 id="改进的内存管理和配置"><a href="#改进的内存管理和配置" class="headerlink" title="改进的内存管理和配置"></a>改进的内存管理和配置</h3><p><br>目前Flink中的TaskExecutor内存配置存在一些缺点，这些缺点使得难以推理或优化资源利用率，例如：</p><ul><li>流处理和批处理执行中用于内存占用的不同配置模型；<br></li><li>流处理执行中堆外状态后端（即RocksDB）的复杂且依赖用户的配置。<br></li></ul><p><br>为了使内存选项对用户更明确和直观，Flink 1.10对TaskExecutor内存模型和配置逻辑（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors" target="_blank" rel="external nofollow noopener noreferrer">FLIP-49</a>）进行了重大更改。这些更改使Flink更适合于各种部署环境（例如Kubernetes，Yarn，Mesos），从而使用户可以严格控制其内存消耗。<br><br><br><strong>托管内存扩展</strong><br><br><br>托管内存已经扩展扩展，当然还考虑了RocksDB StateBackend的内存使用情况。虽然批处理作业可以使用堆内（on-heap）或堆外（off-heap）内存，但具有这些功能的流作业RocksDBStateBackend只能使用堆内内存。因此，为了允许用户在流执行和批处理执行之间切换而不必修改群集配置，托管内存现在始终处于堆外状态。<br><br><br><strong>简化RocksDB配置</strong><br>**<br>曾经配置像RocksDB这样的off-heap (堆外)state backend涉及大量的手动调整，例如减小JVM堆大小或将Flink设置为使用堆外内存。现在可以通过Flink的现成配置来实现，并且调整RocksDBStateBackend内存预算就像调整内存大小一样简单。<br><br><br>另一个重要的改进是允许Flink绑定RocksDB本地内存使用情况（<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="external nofollow noopener noreferrer">FLINK-7289</a>），从而防止其超出总内存预算-这在Kubernetes等容器化环境中尤其重要。有关如何启用和调整此功能的详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/state/large_state_tuning.html#tuning-rocksdb" target="_blank" rel="external nofollow noopener noreferrer">Tuning RocksDB</a>。<br>注意 FLIP-49更改了群集资源配置的过程，这可能需要调整群集以从以前的Flink版本进行升级。有关所引入更改和调整指南的全面概述，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_setup.html" target="_blank" rel="external nofollow noopener noreferrer">此设置</a>。<br></p><p><a name="unified-logic-for-job-submission"></a></p><h3 id="提交作业的统一逻辑"><a href="#提交作业的统一逻辑" class="headerlink" title="提交作业的统一逻辑"></a>提交作业的统一逻辑</h3><p><br>在此版本之前，提交作业是执行环境的一部分职责，并且与不同的部署目标（例如，Yarn，Kubernetes，Mesos）紧密相关。这导致关注点分离不佳，并且随着时间的流逝，用户需要单独配置和管理的定制环境越来越多。<br><br><br>在Flink 1.10中，作业提交逻辑被抽象到通用Executor接口（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-73%3A+Introducing+Executors+for+job+submission" target="_blank" rel="external nofollow noopener noreferrer">FLIP-73</a>）中。另外ExecutorCLI（<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=133631524" target="_blank" rel="external nofollow noopener noreferrer">FLIP-81</a>）引入了一个统一的方式去指定配置参数对于任何 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/cli.html#deployment-targets" target="_blank" rel="external nofollow noopener noreferrer">执行对象</a>。为了完善这项工作，结果检索的过程也与作业提交分离，引入了JobClient（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API" target="_blank" rel="external nofollow noopener noreferrer">FLINK-74</a>）来负责获取JobExecutionResult。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590943394674-2fd89b17-b61e-4180-986d-72062c95f4d4.png#align=left&display=inline&height=334&margin=%5Bobject%20Object%5D&originHeight=789&originWidth=1999&size=0&status=done&style=none&width=847" alt><br><br><br>特别是，这些更改通过为用户提供Flink的统一入口点，使在下游框架（例如Apache Beam或Zeppelin交互式笔记本）中以编程方式使用Flink变得更加容易。对于跨多个目标环境使用Flink的用户，向基于配置的执行过程的过渡还可以显着减少样板代码和可维护性开销。<br></p><p><a name="native-kubernetes-integration-beta"></a></p><h3 id="原生Kubernetes集成（测试版）"><a href="#原生Kubernetes集成（测试版）" class="headerlink" title="原生Kubernetes集成（测试版）"></a>原生Kubernetes集成（测试版）</h3><p><br>对于希望在容器化环境上开始使用Flink的用户，在Kubernetes之上部署和管理独立集群需要有关容器，算子和环境工具kubectl的一些知识。<br><br><br>在Flink 1.10中，我们推出了Active Kubernetes集成（<a href="https://jira.apache.org/jira/browse/FLINK-9953" target="_blank" rel="external nofollow noopener noreferrer">FLINK-9953</a>）的第一阶段，其中，“主动”指 Flink ResourceManager (K8sResMngr) 原生地与 Kubernetes 通信，像 Flink 在 Yarn 和 Mesos 上一样按需申请 pod。用户可以利用 namespace，在多租户环境中以较少的资源开销启动 Flink。这需要用户提前配置好 RBAC 角色和有足够权限的服务账号。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1590943394177-de66ac47-ba2d-4182-b5b4-585ff643cfc8.png#align=left&display=inline&height=322&margin=%5Bobject%20Object%5D&originHeight=870&originWidth=1714&size=0&status=done&style=none&width=635" alt><br><br><br>正如刚刚讲到的，Flink 1.10中的所有命令行选项都映射到统一配置。因此，用户可以简单地引用Kubernetes配置选项，然后使用以下命令在CLI中将作业提交到Kubernetes上的现有Flink会话：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run -d -e kubernetes-session -Dkubernetes.cluster-id&#x3D;&lt;ClusterId&gt; examples&#x2F;streaming&#x2F;WindowJoin.jar</span><br></pre></td></tr></table></figure><p><br>如果您想尝试使用此预览功能，我们建议您逐步完成本<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/native_kubernetes.html" target="_blank" rel="external nofollow noopener noreferrer">机Kubernetes的安装</a>，试用并与社区分享反馈。<br></p><p><a name="table-apisql-production-ready-hive-integration"></a></p><h3 id="Table-API-SQL：生产就绪的Hive集成"><a href="#Table-API-SQL：生产就绪的Hive集成" class="headerlink" title="Table API / SQL：生产就绪的Hive集成"></a>Table API / SQL：生产就绪的Hive集成</h3><p><br>Hive集成在Flink 1.9中宣布为预览功能。此预览允许用户使用SQL DDL将Flink-specific元数据（例如Kafka表）保留在Hive Metastore中，调用Hive中定义的UDF并使用Flink读取和写入Hive表。Flink 1.10通过进一步的开发使这项工作更加圆满，这些开发使可立即投入生产的Hive集成到Flink，并具有与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/#supported-hive-versions" target="_blank" rel="external nofollow noopener noreferrer">大多数Hive版本的</a>完全兼容性。<br><a name="native-partition-support-for-batch-sql"></a></p><h4 id="-1"><a href="#-1" class="headerlink"></a></h4><p><a name="IuODz"></a></p><h4 id="批处理SQL的本地分区支持"><a href="#批处理SQL的本地分区支持" class="headerlink" title="批处理SQL的本地分区支持"></a>批处理SQL的本地分区支持</h4><p><br>1.10版本以前，仅支持对未分区的Hive表进行写入。在Flink 1.10中，Flink SQL语法已通过<code>INSERT OVERWRITE</code>和<code>PARTITION</code>（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-63%3A+Rework+table+partition+support" target="_blank" rel="external nofollow noopener noreferrer">FLIP-63</a>）进行了扩展，使用户能够在Hive中写入静态和动态分区。<br><strong><br></strong>静态分区写入**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;</span><br></pre></td></tr></table></figure><p><strong><br></strong>动态分区编写**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 select_statement1 FROM from_statement;</span><br></pre></td></tr></table></figure><p><br>Flink对于分区表的全面支持，允许用户利用读取时的分区修剪功能，通过减少需要扫描的数据量来显着提高这些操作的性能。<br></p><p><a name="further-optimizations"></a></p><h4 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h4><p>除了分区修剪外，Flink 1.10还为Hive集成引入了更多<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/read_write_hive.html#optimizations" target="_blank" rel="external nofollow noopener noreferrer">读取优化</a>，例如：<br></p><ul><li><strong>投影下推：</strong> Flink通过省略表扫描中不必要的字段，利用投影下推来最大程度地减少Flink和Hive表之间的数据传输。这对于具有大量列的表尤其有利。</li></ul><ul><li><strong>LIMIT下推：</strong>对于带有<code>LIMIT</code>子句的查询，Flink将尽可能限制输出记录的数量，以最大程度地减少通过网络传输的数据量。</li></ul><ul><li><strong>读取时</strong>进行<strong>ORC矢量化：</strong>为了提高ORC文件的读取性能，Flink现在默认将本机ORC矢量化阅读器用于2.0.0以上的Hive版本以及具有非复杂数据类型的列。</li></ul><p><a name="pluggable-modules-as-flink-system-objects-beta"></a></p><h4 id="可插拔模块作为Flink系统对象（Beta）"><a href="#可插拔模块作为Flink系统对象（Beta）" class="headerlink" title="可插拔模块作为Flink系统对象（Beta）"></a>可插拔模块作为Flink系统对象（Beta）</h4><p>Flink 1.10引入了Flink Table核心中可插拔模块的通用机制，首先关注系统功能（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-68%3A+Extend+Core+Table+System+with+Pluggable+Modules" target="_blank" rel="external nofollow noopener noreferrer">FLIP-68</a>）。使用该模块，用户可以扩展Flink的系统对象，例如，使用行为类似于Flink系统功能的Hive内置函数。该版本附带一个预先实现的<code>HiveModule</code>，支持多个Hive版本的版本，但用户也可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/modules.html" target="_blank" rel="external nofollow noopener noreferrer">编写自己的可插拔模块</a>。<br><a name="other-improvements-to-the-table-apisql"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="rCuCf"></a></p><h3 id="Table-API-SQL的其他改进"><a href="#Table-API-SQL的其他改进" class="headerlink" title="Table API / SQL的其他改进"></a>Table API / SQL的其他改进</h3><p><a name="watermarks-and-computed-columns-in-sql-ddl"></a></p><h4 id="-3"><a href="#-3" class="headerlink"></a></h4><p><a name="FlcDT"></a></p><h4 id="SQL-DDL中的水印和计算列"><a href="#SQL-DDL中的水印和计算列" class="headerlink" title="SQL DDL中的水印和计算列"></a>SQL DDL中的水印和计算列</h4><p>Flink 1.10支持特定于流的语法扩展，以在Flink SQL DDL（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-66%3A+Support+Time+Attribute+in+SQL+DDL" target="_blank" rel="external nofollow noopener noreferrer">FLIP-66</a>）中定义时间属性和水印生成。这允许基于时间的操作（例如加窗），以及在使用DDL语句创建的表上定义<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html#create-table" target="_blank" rel="external nofollow noopener noreferrer">水印策略</a>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE table_name (</span><br><span class="line">  WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;</span><br><span class="line">) WITH (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><br>此版本还引入了对虚拟计算列（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-70%3A+Flink+SQL+Computed+Column+Design" target="_blank" rel="external nofollow noopener noreferrer">FLIP-70</a>）的支持，该列可基于同一表中的其他列或确定性表达式（即，文字值，UDF和内置函数）派生。在Flink中，计算列可用于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html#create-table" target="_blank" rel="external nofollow noopener noreferrer">在创建表时</a>定义时间属性。<br><a name="additional-extensions-to-sql-ddl"></a></p><h4 id="-4"><a href="#-4" class="headerlink"></a></h4><p><a name="Exz4X"></a></p><h4 id="SQL-DDL的其他扩展"><a href="#SQL-DDL的其他扩展" class="headerlink" title="SQL DDL的其他扩展"></a>SQL DDL的其他扩展</h4><p><br>现在，temporary/persistent 和 system/catalog（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-57%3A+Rework+FunctionCatalog" target="_blank" rel="external nofollow noopener noreferrer">FLIP-57</a>）之间有明显的区别。这不仅消除了函数引用中的歧义，而且允许确定性的函数解析顺序（即，在命名冲突的情况下，系统函数将优先于目录函数，而临时函数的优先级高于两个维度的持久性函数）。<br><br><br>遵循FLIP-57的基础知识，我们扩展了SQL DDL语法以支持目录功能，临时功能和临时系统功能（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-79+Flink+Function+DDL+Support" target="_blank" rel="external nofollow noopener noreferrer">FLIP-79</a>）的创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION </span><br><span class="line">  [IF NOT EXISTS] [catalog_name.][db_name.]function_name </span><br><span class="line">AS identifier [LANGUAGE JAVA|SCALA]</span><br></pre></td></tr></table></figure><p><br>有关Flink SQL中DDL支持的当前状态的完整概述，请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/" target="_blank" rel="external nofollow noopener noreferrer">更新的文档</a>。<br><strong><br></strong>注意**为了将来能正确处理和保证元对象（表，视图，函数）之间的行为一致，不建议使用Table API中的某些对象声明方法，而应使用更接近标准SQL DDL的方法（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-64%3A+Support+for+Temporary+Objects+in+Table+module" target="_blank" rel="external nofollow noopener noreferrer">FLIP -64</a>）。<br><a name="full-tpc-ds-coverage-for-batch"></a></p><h4 id="-5"><a href="#-5" class="headerlink"></a></h4><p><a name="eHBxG"></a></p><h4 id="TPC-DS的完整覆盖范围可批量处理"><a href="#TPC-DS的完整覆盖范围可批量处理" class="headerlink" title="TPC-DS的完整覆盖范围可批量处理"></a>TPC-DS的完整覆盖范围可批量处理</h4><p><br>TPC-DS是一种广泛使用的行业标准决策支持基准，用于评估和衡量基于SQL的数据处理引擎的性能。在Flink 1.10中，端到端（<a href="https://issues.apache.org/jira/browse/FLINK-11491" target="_blank" rel="external nofollow noopener noreferrer">FLINK-11491</a>）支持所有TPC-DS查询，这反映了它的SQL引擎已准备就绪，可以满足类似现代数据仓库的工作负载的需求。<br><a name="pyflink-support-for-native-user-defined-functions-udfs"></a></p><h3 id="-6"><a href="#-6" class="headerlink"></a></h3><p><a name="TgP6u"></a></p><h3 id="PyFlink：支持本机用户定义的函数（UDF）"><a href="#PyFlink：支持本机用户定义的函数（UDF）" class="headerlink" title="PyFlink：支持本机用户定义的函数（UDF）"></a>PyFlink：支持本机用户定义的函数（UDF）</h3><p><br>在以前的发行版中引入了PyFlink的预览版，朝着实现Flink中完全Python支持的目标迈进了一步。对于此发行版，重点是使用户能够在表API / SQL（<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-58%3A+Flink+Python+User-Defined+Stateless+Function+for+Table" target="_blank" rel="external nofollow noopener noreferrer">FLIP-58</a>）中注册和使用Python用户定义函数（UDF，已计划UDTF / UDAF ）。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1590945929231-b78d0f66-a731-4359-8aa2-442b5f78bc77.gif#align=left&display=inline&height=405&margin=%5Bobject%20Object%5D&name=flink_1.10_pyflink.gif&originHeight=405&originWidth=779&size=2561122&status=done&style=none&width=779" alt="flink_1.10_pyflink.gif"><br><br><br>如果您对基础实现感兴趣（利用Apache Beam的<a href="https://beam.apache.org/roadmap/portability/" target="_blank" rel="external nofollow noopener noreferrer">可移植性框架）</a>，请参考FLIP-58的“架构”部分，也请参考<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-78%3A+Flink+Python+UDF+Environment+and+Dependency+Management" target="_blank" rel="external nofollow noopener noreferrer">FLIP-78</a>。这些数据结构为Pandas支持和PyFlink最终到达DataStream API奠定了必要的基础。<br><br><br>从Flink 1.10开始，用户还可以<code>pip</code>使用以下方法轻松安装PyFlink ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install apache-flink</span><br></pre></td></tr></table></figure><p><br>有关PyFlink计划进行的其他改进的预览，请查看<a href="https://issues.apache.org/jira/browse/FLINK-14500" target="_blank" rel="external nofollow noopener noreferrer">FLINK-14500</a>并参与有关所需用户功能的<a href="http://apache-flink.147419.n8.nabble.com/Re-DISCUSS-What-parts-of-the-Python-API-should-we-focus-on-next-td1285.html" target="_blank" rel="external nofollow noopener noreferrer">讨论</a>。<br><a name="important-changes"></a></p><h2 id="重要变化"><a href="#重要变化" class="headerlink" title="重要变化"></a>重要变化</h2><ul><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-10725" target="_blank" rel="external nofollow noopener noreferrer">FLINK-10725</a> ] Flink现在可以编译并在Java 11上运行。<br></li><li>[ <a href="https://jira.apache.org/jira/browse/FLINK-15495" target="_blank" rel="external nofollow noopener noreferrer">FLINK-15495</a> ] Blink计划程序现在是SQL Client中的默认设置，因此用户可以从所有最新功能和改进中受益。在下一个版本中，还计划从Table API中的旧计划程序进行切换，因此我们建议用户开始熟悉Blink计划程序。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-13025" target="_blank" rel="external nofollow noopener noreferrer">FLINK-13025</a> ]有一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/elasticsearch.html#elasticsearch-connector" target="_blank" rel="external nofollow noopener noreferrer">新的Elasticsearch接收器连接器</a>，完全支持Elasticsearch 7.x版本。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-15115" target="_blank" rel="external nofollow noopener noreferrer">FLINK-15115</a> ] Kafka 0.8和0.9的连接器已标记为不推荐使用，将不再得到积极支持。如果您仍在使用这些版本或有任何其他相关问题，请联系@dev邮件列表。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-14516" target="_blank" rel="external nofollow noopener noreferrer">FLINK-14516</a> ]删除了非基于信用的网络流控制代码以及配置选项<code>taskmanager.network.credit.model</code>。展望未来，Flink将始终使用基于信用的流量控制。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-12122" target="_blank" rel="external nofollow noopener noreferrer">FLINK-12122</a> ] <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external nofollow noopener noreferrer">FLIP-6</a>在Flink 1.5.0中推出，并引入了与从中分配插槽方式有关的代码回归<code>TaskManagers</code>。要使用更接近FLIP之前行为的调度策略（Flink尝试将工作负载分散到所有当前可用的行为中）<code>TaskManagers</code>，用户可以<code>cluster.evenly-spread-out-slots: true</code>在中设置<code>flink-conf.yaml</code>。<br></li><li>[ <a href="https://issues.apache.org/jira/browse/FLINK-11956" target="_blank" rel="external nofollow noopener noreferrer">FLINK-11956</a> ] <code>s3-hadoop</code>和<code>s3-presto</code>文件系统不再使用类重定位，而应通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/filesystems/#pluggable-file-systems" target="_blank" rel="external nofollow noopener noreferrer">插件</a>加载，但现在可以与所有凭据提供程序无缝集成。强烈建议将其他文件系统仅用作插件，因为我们将继续删除重定位。<br></li><li>Flink 1.9带有重构的Web UI，保留了旧版的UI作为备份，以防万一某些功能无法正常工作。到目前为止，尚未报告任何问题，因此<a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Remove-old-WebUI-td35218.html" target="_blank" rel="external nofollow noopener noreferrer">社区投票决定</a>在Flink 1.10中删除旧版Web UI。<br><br><a name="release-notes"></a><h2 id="发行说明"><a href="#发行说明" class="headerlink" title="发行说明"></a>发行说明</h2>如果您打算将设置升级到Flink 1.10，请仔细查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/flink-1.10.html" target="_blank" rel="external nofollow noopener noreferrer">发行说明</a>，以获取详细的更改和新功能列表。此版本与以前的1.x版本的API兼容，这些版本的API使用@Public注释进行了注释。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="cpeixin.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>IDEA install TabNine</title>
    <link href="cpeixin.cn/2020/01/22/IDEA-install-TabNine/"/>
    <id>cpeixin.cn/2020/01/22/IDEA-install-TabNine/</id>
    <published>2020-01-22T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:48.223Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>TabNine是我目前遇到过最好的智能补全工具</p><p>TabNine基于GPT-2的插件</p><p>安装<br>IDEA编译器，找到plugins</p><p>Windows pycharm：File&gt;settings&gt;plugins;<br>Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins</p><p>查找 TabNine, 点击 install, 随后 restart</p><p>重启后：Help&gt;Edit Custom Properties…&gt;Create;</p><p>在跳出来的idea.properties中输入（注：英文字符） TabNine::config</p><p>随即会自动弹出TabNine激活页面；</p><p>激活<br>点击Activation Key下面的here；</p><p>输入你的邮箱号；</p><p>复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）</p><p>等待自动安装，观察页面（最下面有log可以看当前进度）；</p><p>激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；</p><p>确认激活完成，重启pycharm即可；</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;TabNine是我目前遇到过最好的智能补全工具&lt;/p&gt;&lt;p&gt;TabNine基于GPT-2的插件&lt;/p&gt;&lt;p&gt;安装&lt;br&gt;IDEA编译器，找到pl
      
    
    </summary>
    
    
      <category term="开发工具" scheme="cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="IDEA" scheme="cpeixin.cn/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动 EB 级 HDFS 实践</title>
    <link href="cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/</id>
    <published>2020-01-02T15:25:55.000Z</published>
    <updated>2020-05-31T15:28:54.936Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:12 GMT+0800 (GMT+08:00) --><p>转载自<a href="https://juejin.im/post/5e0aac53e51d4575e82591e8" target="_blank" rel="external nofollow noopener noreferrer">字节跳动 EB 级 HDFS 实践</a>，学习学习字节跳动基础架构部门对上万节点的HDFS集群的管理方式<br><br><br>文章的最后，有写上自己的总结，工作这几年，确实没有遇到过这么庞大的数据量和集群，那么就不能先实战再总结了，目前是站在巨人的肩膀上看看远处的风景，当我到达的那一天，就会更从容的融入风景了。</p><hr><p><a name="MorZX"></a></p><h1 id="HDFS-简介"><a href="#HDFS-简介" class="headerlink" title="HDFS 简介"></a>HDFS 简介</h1><p>因为 HDFS 这样一个系统已经存在了非常长的时间，应用的场景已经非常成熟了，所以这部分我们会比较简单地介绍。<br>HDFS 全名 Hadoop Distributed File System，是业界使用最广泛的开源分布式文件系统。原理和架构与 Google 的 GFS 基本一致。它的特点主要有以下几项：</p><ul><li>和本地文件系统一样的目录树视图</li><li>Append Only 的写入（不支持随机写）</li><li>顺序和随机读</li><li>超大数据规模</li><li>易扩展，容错率高<br><a name="AUWUZ"></a><h1><a href="#" class="headerlink"></a></h1><a name="fQiBn"></a><h1 id="字节跳动特色的-HDFS"><a href="#字节跳动特色的-HDFS" class="headerlink" title="字节跳动特色的 HDFS"></a>字节跳动特色的 HDFS</h1></li></ul><p><br>字节跳动应用 HDFS 已经非常长的时间了，经历了 7 年的发展，目前已直接支持了十多种数据平台，间接支持了上百种业务发展。从集群规模和数据量来说，HDFS 平台在公司内部已经成长为总数几万台服务器的大平台，支持了 EB 级别的数据量。<br><br><br>在深入相关的技术细节之前，我们先看看字节跳动的 HDFS 架构。<br><a name="PdpVx"></a></p><h2 id="-1"><a href="#-1" class="headerlink"></a></h2><p><a name="oK4fY"></a></p><h2 id="架构介绍"><a href="#架构介绍" class="headerlink" title="架构介绍"></a>架构介绍</h2><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590931343675-83b09747-e37a-438c-a445-2fdafdba9c83.webp#align=left&display=inline&height=749&margin=%5Bobject%20Object%5D&originHeight=749&originWidth=962&size=0&status=done&style=none&width=962" alt><br><a name="jC8CW"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="66HyQ"></a></p><h3 id="接入层"><a href="#接入层" class="headerlink" title="接入层"></a>接入层</h3><p><br>接入层是区别于社区版本最大的一层，社区版本中并无这一层定义。在字节跳动的落地实践中，由于集群的节点过于庞大，我们需要非常多的 NameNode 实现联邦机制来接入不同上层业务的数据服务。但当 NameNode 数量也变得非常多了以后，用户请求的统一接入及统一视图的管理也会有很大的问题。为了解决用户接入过于分散，我们需要一个独立的接入层来支持用户请求的统一接入，转发路由；同时也能结合业务提供用户权限和流量控制能力；另外，该接入层也需要提供对外的目录树统一视图。<br><br><br>该接入层从部署形态上来讲，依赖于一些外部组件如 Redis，MySQL 等，会有一批无状态的 NNProxy 组成，他们提供了请求路由，Quota 限制，Tracing 能力及流量限速等能力。<br><a name="gFV5T"></a></p><h3 id="-3"><a href="#-3" class="headerlink"></a></h3><p><a name="DTx6L"></a></p><h3 id="元数据层"><a href="#元数据层" class="headerlink" title="元数据层"></a>元数据层</h3><p><br>这一层主要模块有 Name Node，ZKFC，和 BookKeeper（不同于 QJM，BookKeeper 在大规模多节点数据同步上来讲会表现得更稳定可靠）。<br><br><br>Name Node 负责存储整个 HDFS 集群的元数据信息，是整个系统的大脑。一旦故障，整个集群都会陷入不可用状态。因此 Name Node 有一套基于 ZKFC 的主从热备的高可用方案。<br><br><br>Name Node 还面临着扩展性的问题，单机承载能力始终受限。于是 HDFS 引入了联邦（Federation）机制。一个集群中可以部署多组 Name Node，它们独立维护自己的元数据，共用 Data Node 存储资源。这样，一个 HDFS 集群就可以无限扩展了。但是这种 Federation 机制下，每一组 Name Node 的目录树都互相割裂的。于是又出现了一些解决方案，能够使整个 Federation 集群对外提供一个完整目录树的视图。<br></p><p><a name="OVDWh"></a></p><h3 id="数据层"><a href="#数据层" class="headerlink" title="数据层"></a>数据层</h3><p><br>相比元数据层，数据层主要节点是 Data Node。Data Node 负责实际的数据存储和读取。用户文件被切分成块复制成多副本，每个副本都存在不同的 Data Node 上，以达到容错容灾的效果。每个副本在 Data Node 上都以文件的形式存储，元信息在启动时被加载到内存中。<br><br><br>Data Node 会定时向 Name Node 做心跳汇报，并且周期性将自己所存储的副本信息汇报给 Name Node。这个过程对 Federation 中的每个集群都是独立完成的。在心跳汇报的返回结果中，会携带 Name Node 对 Data Node 下发的指令，例如，需要将某个副本拷贝到另外一台 Data Node 或者将某个副本删除等。<br></p><p><a name="NU3Ae"></a></p><h2 id="主要业务"><a href="#主要业务" class="headerlink" title="主要业务"></a>主要业务</h2><p>先来看一下当前在字节跳动 HDFS 承载的主要业务：</p><ul><li>Hive，HBase，日志服务，Kafka 数据存储</li><li>Yarn，Flink 的计算框架平台数据</li><li>Spark，MapReduce 的计算相关数据存储<br><a name="GHk55"></a><h2 id="发展阶段"><a href="#发展阶段" class="headerlink" title="发展阶段"></a>发展阶段</h2>在字节跳动，随着业务的快速发展，HDFS 的数据量和集群规模快速扩大，原来的 HDFS 的集群从几百台，迅速突破千台和万台的规模。这中间，踩了无数的坑，大的阶段归纳起来会有这样几个阶段。<br><a name="rzDSM"></a><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3>业务增长初期，集群规模增长趋势非常陡峭，单集群规模很快在元数据服务器 Name Node 侧遇到瓶颈。引入联邦机制（Federation）实现集群的横向扩展。<br><br><br>联邦又带来统一命名空间问题，因此，需要统一视图空间帮助业务构建统一接入。为了解决这个问题，我们引入了 Name Node Proxy 组件实现统一视图和多租户管理等功能，这部分会在下文的 NNProxy 章节中介绍。<br><a name="r78Fs"></a><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3></li></ul><p><br>数据量继续增大，Federation 方式下的目录树管理也存在瓶颈，主要体现在数据量增大后，Java 版本的 GC 变得更加频繁，跨子树迁移节点代价过大，节点启动时间太长等问题。因此我们通过重构的方式，解决了 GC，锁优化，启动加速等问题，将原 Name Node 的服务能力进一步提高。容纳更多的元数据信息。<br><br><br>为了解决这个问题，我们也实现了字节跳动特色的 DanceNN 组件，兼容了原有 Java 版本 NameNode 的全部功能基础上，大大增强了稳定性和性能。相关详细介绍会在下面的 DanceNN 章节中介绍。<br><a name="cKChL"></a></p><h3 id="-4"><a href="#-4" class="headerlink"></a></h3><p><a name="Ufy9s"></a></p><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p><br>当数据量跨过 EB，集群规模扩大到几万台的时候，慢节点问题，更细粒度服务分级问题，成本问题和元数据瓶颈进一步凸显。我们在架构上进一步在包括完善多租户体系构建，重构数据节点和元数据分层等方向进一步演进。这部分目前正在进行中，因为优化的点会非常多，本文会给出慢节点优化的落地实践。<br></p><p><a name="tEe39"></a></p><h2 id="关键改进"><a href="#关键改进" class="headerlink" title="关键改进"></a>关键改进</h2><p><br>在整个架构演进的过程中，我们做了非常多的探索和尝试。如上所述，结合之前提到的几个大的挑战和问题，我们就其中关键的 Name Node Proxy 和 Dance Name Node 这两个重点组件做一下介绍，同时，也会介绍一下我们在慢节点方面的优化和改进。<br><a name="TD5kf"></a></p><h3 id="-5"><a href="#-5" class="headerlink"></a></h3><p><a name="4I1Fk"></a></p><h3 id="NNProxy（Name-Node-Proxy）"><a href="#NNProxy（Name-Node-Proxy）" class="headerlink" title="NNProxy（Name Node Proxy）"></a>NNProxy（Name Node Proxy）</h3><p>作为系统的元数据操作接入端，NNProxy 提供了联邦模式下统一元数据视图，解决了用户请求的统一转发，业务流量的统一管控的问题。<br><br><br>先介绍一下 NNProxy 所处的系统上下游。<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590933295275-e12431d8-0bab-419e-bc29-4c3e03a5c6ff.webp#align=left&display=inline&height=572&margin=%5Bobject%20Object%5D&originHeight=572&originWidth=864&size=0&status=done&style=none&width=864" alt><br><br><br>我们先来看一下 NNProxy 都做了什么工作。<br><a name="rk9VU"></a></p><h4 id="-6"><a href="#-6" class="headerlink"></a></h4><p><a name="AYfYO"></a></p><h4 id="路由管理"><a href="#路由管理" class="headerlink" title="路由管理"></a>路由管理</h4><p><br>在上面 Federation 的介绍中提到，每个集群都维护自己独立的目录树，无法对外提供一个完整的目录树视图。NNProxy 中的路由管理就解决了这个问题。路由管理存储了一张 mount table，表中记录若干条路径到集群的映射关系。<br><br><br>例如 <strong>/user -&gt; hdfs://namenodeB</strong>，这条映射关系的含义就是 /user 及其子目录这个目录在 <strong>namenodeB</strong> 这个集群上，所有对 /user 及其子目录的访问都会由 NNProxy 转发给 <strong>namenodeB</strong>，获取结果后再返回给 Client。<br><br><br>匹配原则为最长匹配，例如我们还有另外一条映射 <strong>/user/tiger/dump -&gt; hdfs://namenodeC</strong>，那么 /user/tiger/dump 及其所有子目录都在 namenodeC，而 /user 目录下其他子目录都在 namenodeB 上。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590933295845-bb064f91-f470-4129-80b9-c49e55ddae75.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=864&size=0&status=done&style=none&width=864" alt><br><a name="kRRta"></a></p><h4 id="-7"><a href="#-7" class="headerlink"></a></h4><p><a name="DJKii"></a></p><h4 id="Quota-限制"><a href="#Quota-限制" class="headerlink" title="Quota 限制"></a>Quota 限制</h4><p>使用过 HDFS 的同学会知道 Quota 这个概念。我们给每个目录集合分配了额定的空间资源，一旦使用超过这个阈值，就会被禁止写入。这个工作就是由 NNProxy 完成的。NNProxy 会通过 Quota 实时监控系统获取最新 Quota 使用情况，当用户进行元数据操作的时候，NNProxy 就会根据用户的 Quota 情况作出判断，决定通过或者拒绝。<br><a name="e2E2o"></a></p><h4 id="-8"><a href="#-8" class="headerlink"></a></h4><p><a name="JG3Xt"></a></p><h4 id="Trace-支持"><a href="#Trace-支持" class="headerlink" title="Trace 支持"></a>Trace 支持</h4><p>ByteTrace 是一个 Trace 系统，记录追踪用户和系统以及系统之间的调用行为，以达到分析和运维的目的。其中的 Trace 信息会附在向 NNProxy 的请求 RPC 中。NNProxy 拿到 ByteTrace 以后就可以知道当前请求的上游模块，USER 及 Application ID 等信息。NNProxy 一方面将这些信息发到 Kafka 做一些离线分析，一方面实时聚合并打点，以便追溯线上流量。<br><a name="7lbZo"></a></p><h4 id="-9"><a href="#-9" class="headerlink"></a></h4><p><a name="9VGfn"></a></p><h4 id="流量限制"><a href="#流量限制" class="headerlink" title="流量限制"></a>流量限制</h4><p><br>虽然 NNProxy 非常轻量，可以承受很高的 QPS，但是后端的 Name Node 承载能力是有限的。因此突发的大作业造成高 QPS 的读写请求被全量转发到 Name Node 上时，会造成 Name Node 过载，延时变高，甚至出现 OOM，影响集群上所有用户。<br><br><br>因此 NNProxy 另一个非常重要的任务就是限流，以保护后端 Name Node。目前限流基于路径+RPC 以及 用户+RPC 维度，例如我们可以限制 /user/tiger/warhouse 路径的 create 请求为 100 QPS，或者某个用户的 delete 请求为 5 QPS。一旦该用户的访问量超过这个阈值，NNProxy 会返回一个可重试异常，Client 收到这个异常后会重试。因此被限流的路径或用户会感觉到访问 HDFS 变慢，但是并不会失败。<br><br><br></p><p><a name="a95em"></a></p><h3 id="Dance-NN（Dance-Name-Node）"><a href="#Dance-NN（Dance-Name-Node）" class="headerlink" title="Dance NN（Dance Name Node）"></a>Dance NN（Dance Name Node）</h3><p><a name="lhQQ9"></a></p><h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>如前所述，在数据量上到 EB 级别的场景后，原有的 Java 版本的 Name Node 存在了非常多的线上问题需要解决。以下是在实践过程中我们遇到的一些问题总结：<br></p><ul><li>Java 版本 Name Node 采用 Java 语言开发，在 INode 规模上亿时，不可避免的会带来严重的 GC 问题；</li><li>Java 版本 Name Node 将 INode meta 信息完全放置于内存，10 亿 INode 大约占用 800GB 内存（包含 JVM 自身占用的部分 native memory），更进一步加重了 GC；</li><li>我们目前的集群规模下，Name Node 从重启到恢复服务需要 6 个小时，在主备同时发生故障的情况下，严重影响上层业务；</li><li>Java 版本 Name Node 全局一把读写锁，任何对目录树的修改操作都会阻塞其他的读写操作，并发度较低；</li></ul><p>从上可以看出，在大数据量场景下，我们亟需一个新架构版本的 Name Node 来承载我们的海量元数据。除了 C++语言重写来规避 Java 带来的 GC 问题以外，我们还在一些场景下做了特殊的优化。<br></p><p><a name="1ILjM"></a></p><h4 id="目录树锁设计"><a href="#目录树锁设计" class="headerlink" title="目录树锁设计"></a>目录树锁设计</h4><p><br>HDFS 对内是一个分布式集群，对外提供的是一个 unified 的文件系统，因此对文件及目录的操作需要像操作 Linux 本地文件系统一样。这就要求 HDFS 满足类似于数据库系统中 ACID 特性一样的原子性，一致性、隔离性和持久性。因此 DanceNN 在面对多个用户同时操作同一个文件或者同一个目录时，需要保证不会破坏掉 ACID 属性，需要对操作做锁保护。<br><br><br>不同于传统的 KV 存储和数据库表结构，DanceNN 上维护的是一棵树状的数据结构，因此单纯的 key 锁或者行锁在 DanceNN 下不适用。而像数据库的表锁或者原生 NN 的做法，对整棵目录树加单独一把锁又会严重的影响整体吞吐和延迟，因此 DanceNN 重新设计了树状锁结构，做到保证 ACID 的情况下，读吞吐能够到 8w，写吞吐能够到 2w，是原生 NN 性能的 10 倍以上。<br><br><br>这里，我们会重新对 RPC 做分类，像 <code>createFile</code>，<code>getFileInfo</code>，<code>setXAttr</code> 这类 RPC 依然是简单的对某一个 INode 进行 CURD 操作；像 <code>delete</code> RPC，有可能删除一个文件，也有可能会删除目录，后者会影响整棵子树下的所有文件；像 <code>rename</code> RPC，则是更复杂的另外一类操作，可能会涉及到多个 INode，甚至是多棵子树下的所有 INode。<br></p><p><a name="FOHq3"></a></p><h4 id="DanceNN-启动优化"><a href="#DanceNN-启动优化" class="headerlink" title="DanceNN 启动优化"></a>DanceNN 启动优化</h4><p><br>由于我们的 DanceNN 底层元数据实现了本地目录树管理结构，因此我们 DanceNN 的启动优化都是围绕着这样的设计来做的。<br><a name="cwcfb"></a></p><h5 id="-10"><a href="#-10" class="headerlink"></a></h5><p><a name="ygdYf"></a></p><h5 id="多线程扫描和填充-BlockMap"><a href="#多线程扫描和填充-BlockMap" class="headerlink" title="多线程扫描和填充 BlockMap"></a>多线程扫描和填充 BlockMap</h5><p>在系统启动过程中，第一步就是读取目录树中保存的信息并且填入 BlockMap 中，类似 Java 版 NN 读取 FSImage 的操作。在具体实现过程中，首先起多个线程并行扫描静态目录树结构。将扫描的结果放入一个加锁的 Buffer 中。当 Buffer 中的元素个数达到设定的数量以后，重新生成一个新的 Buffer 接收请求，并在老 Buffer 上起一个线程将数据填入 BlockMap。<br><a name="wPkVT"></a></p><h5 id="-11"><a href="#-11" class="headerlink"></a></h5><p><a name="GvGcm"></a></p><h5 id="接收块上报优化"><a href="#接收块上报优化" class="headerlink" title="接收块上报优化"></a>接收块上报优化</h5><p>DanceNN 启动以后会首先进入安全模式，接收所有 Date Node 的块上报，完善 BlockMap 中保存的信息。当上报的 Date Node 达到一定比例以后，才会退出安全模式，这时候才能正式接收 client 的请求。所以接收块上报的速度也会影响 Date Node 的启动时长。DanceNN 这里做了一个优化，根据 BlockID 将不同请求分配给不同的线程处理，每个线程负责固定的 Slice，线程之间无竞争，这样就极大的加快了接收块上报的速度。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935398857-421715f0-1f68-40e2-a738-5d53fc518e03.webp#align=left&display=inline&height=412&margin=%5Bobject%20Object%5D&originHeight=412&originWidth=864&size=0&status=done&style=none&width=864" alt><br><br><br></p><p><a name="tCZ1r"></a></p><h3 id="慢节点优化"><a href="#慢节点优化" class="headerlink" title="慢节点优化"></a>慢节点优化</h3><p><br>慢节点问题在很多分布式系统中都存在。其产生的原因通常为上层业务的热点或者底层资源故障。上层业务热点，会导致一些数据在较短的时间段内被集中访问。而底层资源故障，如出现慢盘或者盘损坏，更多的请求就会集中到某一个副本节点上从而导致慢节点。<br><br><br>通常来说，慢节点问题的优化和上层业务需求及底层资源量有很大的关系，极端情况，上层请求很小，下层资源充分富裕的情况下，慢节点问题将会非常少，反之则会变得非常严重。在字节跳动的 HDFS 集群中，慢节点问题一度非常严重，尤其是磁盘占用百分比非常高以后，各种慢节点问题层出不穷。其根本原因就是资源的平衡滞后，许多机器的磁盘占用已经触及红线导致写降级；新增热资源则会集中到少量机器上，这种情况下，当上层业务的每秒请求数升高后，对于 P999 时延要求比较高的一些大数据分析查询业务就容易出现一大批数据访问（&gt;10000 请求）被卡在某个慢请求的处理上。<br><br><br>我们优化的方向会分为读慢节点和写慢节点两个方面。<br></p><p><a name="PwyJV"></a></p><h4 id="读慢节点优化"><a href="#读慢节点优化" class="headerlink" title="读慢节点优化"></a>读慢节点优化</h4><p>我们经历了几个阶段：</p><ul><li>最早，使用社区版本，其 Switch Read 以读取一个 packet 的时长为统计单位，当读取一个 packet 的时间超过阈值时，认为读取当前 packet 超时。如果一定时间窗口内超时 packet 的数量过多，则认为当前节点是慢节点。但这个问题在于以 packet 作为统计单位使得算法不够敏感，这样使得每次读慢节点发生的时候，对于小 IO 场景（字节跳动的一些业务是以大量随机小 IO 为典型使用场景的），这些个积攒的 Packet 已经造成了问题。</li></ul><ul><li>后续，我们研发了 Hedged Read 的读优化。Hedged Read 对每一次读取设置一个超时时间。如果读取超时，那么会另开一个线程，在新的线程中向第二个副本发起读请求，最后取第一第二个副本上优先返回的 response 作为读取的结果。但这种情况下，在慢节点集中发生的时候，会导致读流量放大。严重的时候甚至导致小范围带宽短时间内不可用。</li></ul><ul><li>基于之前的经验，我们进一步优化，开启了 Fast Switch Read 的优化，该优化方式使用吞吐量作为判断慢节点的标准，当一段时间窗口内的吞吐量小于阈值时，认为当前节点是慢节点。并且根据当前的读取状况动态地调整阈值，动态改变时间窗口的长度以及吞吐量阈值的大小。<br>下表是当时线上某业务测试的值：</li></ul><table><thead><tr><th>Host:X.X.X.X</th><th>3 副本 Switch Read</th><th>2 副本 Hedged Read</th><th>3 副本 Hedged Read</th><th>3 副本 Fast Switch Read（优化后算法）</th></tr></thead><tbody><tr><td>读取时长 p999</td><td>977 ms</td><td>549 ms</td><td>192 ms</td><td><strong>128 ms</strong></td></tr><tr><td>最长读取时间</td><td>300 s</td><td>125 s</td><td>60 s</td><td><strong>15.5 s</strong></td></tr><tr><td>长尾出现次数（大于 500ms）</td><td>238 次/天</td><td>75 次/天</td><td>15 次/天</td><td><strong>3 次/天</strong></td></tr><tr><td>长尾出现次数（大于 1000ms）</td><td>196 次/天</td><td>64 次/天</td><td>6 次/天</td><td><strong>3 次/天</strong></td></tr></tbody></table><p><br>进一步的相关测试数据：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935682709-6dceebc5-46fb-459b-a559-c238e04d66be.webp#align=left&display=inline&height=211&margin=%5Bobject%20Object%5D&originHeight=211&originWidth=746&size=0&status=done&style=none&width=746" alt><br><a name="aI0wv"></a></p><h4 id="-12"><a href="#-12" class="headerlink"></a></h4><p><a name="hBmkC"></a></p><h4 id="写慢节点优化"><a href="#写慢节点优化" class="headerlink" title="写慢节点优化"></a>写慢节点优化</h4><p><br>写慢节点优化的适用场景会相对简单一些。主要解决的是写过程中，Pipeline 的中间节点变慢的情况。为了解决这个问题，我们也发展了 Fast Failover 和 Fast Failover+两种算法。<br><a name="2ceD5"></a></p><h5 id="-13"><a href="#-13" class="headerlink"></a></h5><p><a name="yHKlv"></a></p><h5 id="Fast-Failover"><a href="#Fast-Failover" class="headerlink" title="Fast Failover"></a>Fast Failover</h5><p>Fast Failover 会维护一段时间内 ACK 时间过长的 packet 数目，当超时 ACK 的数量超过阈值后，会结束当前的 block，向 namenode 申请新块继续写入。<br><br><br>Fast Failover 的问题在于，随意结束当前的 block 会造成系统的小 block 数目增加，给之后的读取速度以及 namenode 的元数据维护都带来负面影响。所以 Fast Failover 维护了一个切换阈值，如果已写入的数据量（block 的大小）大于这个阈值，才会进行 block 切换。<br><br><br>但是往往为了达到这个写入数据大小阈值，就会造成用户难以接收的延迟，因此当数据量小于阈时需要进额外的优化。<br><a name="Ohd15"></a></p><h5 id="-14"><a href="#-14" class="headerlink"></a></h5><p><a name="Hq46w"></a></p><h5 id="Fast-Failover-1"><a href="#Fast-Failover-1" class="headerlink" title="Fast Failover+"></a>Fast Failover+</h5><p>为了解决上述的问题，当已写入的数据量（block 的大小）小于阈值时，我们引入了新的优化手段——Fast Failover+。该算法首先从 pipeline 中筛选出速度较慢的 datanode，将慢节点从当前 pipeline 中剔除，并进入 Pipeline Recovery 阶段。Pipeline Recovery 会向 namenode 申请一个新的 datanode，与剩下的 datanode 组成一个新的 pipeline，并将已写入的数据同步到新的 datanode 上（该步骤称为 transfer block）。由于已经写入的数据量较小，transfer block 的耗时并不高。统计 p999 平均耗时只有 150ms。由 Pipeline Recovery 所带来的额外消耗是可接受的。<br><br><br>下表是当时线上某业务测试的值：</p><table><thead><tr><th>Host:X.X.X.X</th><th>Fast Failover p99</th><th>Fast Failover+ p99 <strong>(优化后算法)</strong></th><th>Fast Failover p95</th><th>Fast Failover+ p95 <strong>(优化后算法)</strong></th></tr></thead><tbody><tr><td>平均 Flush 时长</td><td>1.49 s</td><td><strong>1.23 s</strong></td><td>182 ms</td><td><strong>147 ms</strong></td></tr><tr><td>最长 Flush 时间</td><td>80 s</td><td><strong>66 s</strong></td><td>9.7 s</td><td><strong>6.5 s</strong></td></tr><tr><td>长尾出现次数（p99 大于 10s, p95 大于 1s）</td><td>63 次/天</td><td><strong>38 次/天</strong></td><td>94 次/天</td><td><strong>55 次/天</strong></td></tr><tr><td>长尾出现次数（p99 大于 5s, p95 大于 0.5s）</td><td>133 次/天</td><td><strong>101 次/天</strong></td><td>173 次/天</td><td><strong>156 次/天</strong></td></tr></tbody></table><p>一些进一步的实际效果对比：<br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1590935682666-c4712d74-258e-44be-b16d-a6a773f9fcbb.webp#align=left&display=inline&height=232&margin=%5Bobject%20Object%5D&originHeight=232&originWidth=734&size=0&status=done&style=none&width=734" alt><br><a name="d4UFq"></a></p><h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><p>HDFS 在字节跳动的发展历程已经非常长了。从最初的几百台的集群规模支持 PB 级别的数据量，到现在几万台级别多集群的平台支持 EB 级别的数据量，我们经历了 7 年的发展。伴随着业务的快速上量，我们团队也经历了野蛮式爆发，规模化发展，平台化运营的阶段。这过程中我们踩了不少坑，也积累了相当丰富的经验。当然，最重要的，公司还在持续高速发展，而我们仍旧不忘初心，坚持“DAY ONE”，继续在路上。<br><br><br></p><p><a name="mZaR7"></a></p><h1 id="学习总结"><a href="#学习总结" class="headerlink" title="学习总结"></a>学习总结</h1><p><br>接入层：接入层是字节设计假如的一层，在上万节点的HDFS集群中，必然要使用多NameNode模式，那么对于用户大量的请求统一管理，字节引入了接入层，具体实现借用Redis，Mysql以及NNProxy转发路由等外界组件实现。<br><br><br>元数据层：这里面有一点，在字节的HDFS集群中，并没有使用社区版的QJM HA高可用方案，而是使用了BookKeeper。Apache bookkeeper是一个分布式，可扩展，容错（多副本），低延迟的存储系统，其提供了高性能，高吞吐的存储能力。而QJM/Qurom Journal Manager是Clouera提出的，这是一个基于Paxos算法实现的HDFS HA方案<br><br><br>数据层倒是没什么特别的改善<br><br><br>可以看出，字节的数据暴涨阶段，首先遇到的问题是Name Node的瓶颈，而此时字节的集群环境为单集群，此时的解决方案是采用Federation。</p><p>数据持续高速增长，Federation 方式下的目录树管理也存在瓶颈，主要原因是Java频繁GC，那么字节的解决方案就显得有些硬核了，重写了NameNode，在字节中叫做DanceNN 🐂🍺。</p><p>在数据超过EB级别之后，遇到的问题就更多了。不同粒度服务分级，元数据存储瓶颈，慢节点等问题。那么字节的解决方案则是考虑到一方面从存储方面的数据节点进行重构，另一方面对于大块的元数据进行分级。</p><p>这里要说一下上面提到的NNProxy，好用！！主要有两个功能很吸引我，在Hadoop集群原有的基础上，字节添加了NNProxy，一个是根据用户请求的路径转发到不同的HDFS空间，二呢，对多租户的场景下，对每个用户的请求做判断，如果某个请求量过大，则会对其限流。</p><p>在这里，我也领略到了一个场景，那就是在字节EB级别的集群规模下，集群重启到全部服务恢复，需要6个小时左右。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:12 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;转载自&lt;a href=&quot;https://juejin.im/post/5e0aac53e51d4575e82591e8&quot; target=&quot;_bl
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hdfs" scheme="cpeixin.cn/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 Chinese 自动生成文章 - 环境准备</title>
    <link href="cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <id>cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</id>
    <published>2020-01-01T14:28:43.000Z</published>
    <updated>2020-04-13T09:28:23.224Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><a name="R14AA"></a></p><h2 id="Google-Colab"><a href="#Google-Colab" class="headerlink" title="Google Colab"></a>Google Colab</h2><p><br>Colaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。<br><br><br>Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。<br><br><br>缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。<br><br><br><strong>优点 免费！ 免费！免费！</strong><br>**<br><a name="dpofS"></a></p><h3 id="谷歌云盘"><a href="#谷歌云盘" class="headerlink" title="谷歌云盘"></a>谷歌云盘</h3><p><br>当登录账号进入<a href="https://drive.google.com/drive/my-drive" target="_blank" rel="external nofollow noopener noreferrer">谷歌云盘</a>时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723531238-28bbbd81-69e8-472d-b048-1ac67166a201.png#align=left&display=inline&height=612&name=image.png&originHeight=612&originWidth=1268&size=104029&status=done&style=none&width=1268" alt="image.png"><br>可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：</p><p><a name="EHdj9"></a></p><h3 id="引入Colab"><a href="#引入Colab" class="headerlink" title="引入Colab"></a>引入Colab</h3><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723706098-527d9fff-e46e-4dd1-b92a-0640b0d61555.png#align=left&display=inline&height=674&name=image.png&originHeight=674&originWidth=1125&size=104056&status=done&style=none&width=1125" alt="image.png"><br><br><br><br><br></p><p><a name="kykCO"></a></p><h3 id="设置GPU环境"><a href="#设置GPU环境" class="headerlink" title="设置GPU环境"></a>设置GPU环境</h3><p><br>打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583723911273-f07371f9-e982-44b2-af34-b3781f294879.png#align=left&display=inline&height=739&name=image.png&originHeight=739&originWidth=1191&size=94677&status=done&style=none&width=1191" alt="image.png"><br><br><br></p><p><a name="f4U2h"></a></p><h3 id="挂载和切换工作目录"><a href="#挂载和切换工作目录" class="headerlink" title="挂载和切换工作目录"></a>挂载和切换工作目录</h3><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录</span></span><br><span class="line"></span><br><span class="line">os.chdir(<span class="string">'/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese'</span>) <span class="comment">## 共享云盘的目录</span></span><br></pre></td></tr></table></figure><p>其中： My Drive 代表你的google网盘根目录</p><pre><code>code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录</code></pre><p><a name="MyewB"></a></p><h3 id="在Colab中运行任务"><a href="#在Colab中运行任务" class="headerlink" title="在Colab中运行任务"></a>在Colab中运行任务</h3><p>下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769633567-4e4118a0-5c52-4517-9233-71d897e7fd68.png#align=left&display=inline&height=1748&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.15.46.png&originHeight=1748&originWidth=3096&size=378104&status=done&style=none&width=3096" alt="屏幕快照 2020-04-13 下午5.15.46.png"></p><p><a name="GZDbL"></a></p><h3 id="ipynb文件内容"><a href="#ipynb文件内容" class="headerlink" title=".ipynb文件内容"></a>.ipynb文件内容</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586769997876-4536842e-6bb3-4d6f-8df9-e220a66026a0.png#align=left&display=inline&height=1702&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-13%20%E4%B8%8B%E5%8D%885.23.22.png&originHeight=1702&originWidth=3154&size=396138&status=done&style=none&width=3154" alt="屏幕快照 2020-04-13 下午5.23.22.png"><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;R14AA&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;Google-Colab&quot;&gt;&lt;a href=&quot;#Google-Colab&quot; cl
      
    
    </summary>
    
    
      <category term="NLP" scheme="cpeixin.cn/categories/NLP/"/>
    
    
      <category term="GPT-2" scheme="cpeixin.cn/tags/GPT-2/"/>
    
  </entry>
  
  <entry>
    <title>架构思想</title>
    <link href="cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/"/>
    <id>cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/</id>
    <published>2019-12-20T02:26:15.000Z</published>
    <updated>2020-04-04T11:23:45.206Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h2><a href="#" class="headerlink"></a></h2><p>关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。<br><br><br>从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。<br><br><br>具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。<br><br><br>架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。<br><a name="-3"></a></p><h4 id="架构思想"><a href="#架构思想" class="headerlink" title="架构思想"></a>架构思想</h4><p>CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度</p><ul><li><p>满足业务发展需求是最高准则</p></li><li><p>业务建模，抽象和枚举是两种方式，需要平衡，不能走极端</p></li><li><p>模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计</p></li><li><p>基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。</p></li><li><p>分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程</p></li><li><p>分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中</p></li><li><p>不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术</p></li><li><p>好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难</p></li><li><p>业务架构、系统架构、数据模型<br><a name="-4"></a></p><h4 id="面对一块新业务，如何系统架构？"><a href="#面对一块新业务，如何系统架构？" class="headerlink" title="面对一块新业务，如何系统架构？"></a>面对一块新业务，如何系统架构？</h4></li><li><p>业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。</p></li><li><p>系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？</p></li><li><p>系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？</p></li><li><p>模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。<br><a name="-5"></a></p><h4 id="如何模块化"><a href="#如何模块化" class="headerlink" title="如何模块化"></a>如何模块化</h4></li><li><p>基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。</p></li><li><p>基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。</p></li><li><p>基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。</p></li><li><p>基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。<br><a name="-6"></a></p><h4 id="提升系统的稳定性"><a href="#提升系统的稳定性" class="headerlink" title="提升系统的稳定性"></a>提升系统的稳定性</h4></li><li><p>流控</p></li></ul><p>双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。<br>另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。</p><ul><li>容灾</li></ul><p>最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。<br>另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。</p><ul><li>稳定性</li></ul><p>第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。</p><ul><li>容量规划</li></ul><p>做好容量规划、系统间强弱依赖关系梳理。<br>如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。<br><a name="-7"></a></p><h4 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h4><ul><li><p>分布式系统</p></li><li><p>分布式缓存</p></li><li><p>分布式数据<br><a name="api"></a></p><h4 id="API-和乐高积木有什么相似之处？"><a href="#API-和乐高积木有什么相似之处？" class="headerlink" title="API 和乐高积木有什么相似之处？"></a>API 和乐高积木有什么相似之处？</h4><p>相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。<br>对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。<br>你可以发现很多 API 和乐高积木的相似之处：</p></li><li><p>标准化：通用、标准化的组件，作为基本的构建块（building blocks）；<br></p></li><li><p>可用性：强调可用性，附有文档或使用说明；<br></p></li><li><p>可定制：为不同功能使用不同的API；<br></p></li><li><p>创造性：能够组合不同的 API 来创造混搭的结果；</p></li></ul><p><br>乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。<br>虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。<br><br><br>让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;关于什么
      
    
    </summary>
    
    
      <category term="架构" scheme="cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>kali中文设置</title>
    <link href="cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/"/>
    <id>cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/</id>
    <published>2019-12-01T02:26:15.000Z</published>
    <updated>2020-04-04T11:06:21.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>更新源</p><p><a href="https://blog.csdn.net/qq_38333291/article/details/89764967" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_38333291/article/details/89764967</a></p><p>设置编码和中文字体安装</p><p><a href="http://www.linuxdiyf.com/linux/20701.html" target="_blank" rel="external nofollow noopener noreferrer">http://www.linuxdiyf.com/linux/20701.html</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;更新源&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_38333291/article/details/897
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="kali" scheme="cpeixin.cn/tags/kali/"/>
    
  </entry>
  
  <entry>
    <title>分布式下的数据hash分布</title>
    <link href="cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/"/>
    <id>cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/</id>
    <published>2019-11-19T15:05:08.000Z</published>
    <updated>2020-04-04T11:24:04.737Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>【转载】字节跳动在Spark SQL上的核心优化实践</title>
    <link href="cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</id>
    <published>2019-11-12T10:57:27.000Z</published>
    <updated>2020-05-16T10:59:03.835Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:12 GMT+0800 (GMT+08:00) --><p><br>本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践<br>原文链接：<a href="https://juejin.im/post/5dc3ed336fb9a04a7847f25c" target="_blank" rel="external nofollow noopener noreferrer">https://juejin.im/post/5dc3ed336fb9a04a7847f25c</a><br><a name="KLy4v"></a></p><h2 id="Spark-SQL-架构简介"><a href="#Spark-SQL-架构简介" class="headerlink" title="Spark SQL 架构简介"></a>Spark SQL 架构简介</h2><p>我们先简单聊一下Spark SQL 的架构。下面这张图描述了一条 SQL 提交之后需要经历的几个阶段，结合这些阶段就可以看到在哪些环节可以做优化。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589617132571-55a23e83-cc96-437b-93a9-871d41c6724e.webp#align=left&display=inline&height=448&margin=%5Bobject%20Object%5D&originHeight=448&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>很多时候，做数据仓库建模的同学更倾向于直接写 SQL 而非使用 Spark 的 DSL。一条 SQL 提交之后会被 Parser 解析并转化为 Unresolved Logical Plan。它的重点是 Logical Plan 也即逻辑计划，它描述了希望做什么样的查询。Unresolved 是指该查询相关的一些信息未知，比如不知道查询的目标表的 Schema 以及数据位置。<br>上述信息存于 Catalog 内。在生产环境中，一般由 Hive Metastore 提供 Catalog 服务。Analyzer 会结合 Catalog 将 Unresolved Logical Plan 转换为 Resolved Logical Plan。</p><p>到这里还不够。不同的人写出来的 SQL 不一样，生成的 Resolved Logical Plan 也就不一样，执行效率也不一样。为了保证无论用户如何写 SQL 都可以高效的执行，Spark SQL 需要对 Resolved Logical Plan 进行优化，这个优化由 Optimizer 完成。Optimizer 包含了一系列规则，对 Resolved Logical Plan 进行等价转换，最终生成 Optimized Logical Plan。<strong>该 Optimized Logical Plan 不能保证是全局最优的，但至少是接近最优的。</strong><br>上述过程只与 SQL 有关，与查询有关，但是与 Spark 无关，因此无法直接提交给 Spark 执行。Query Planner 负责将 Optimized Logical Plan 转换为 Physical Plan，进而可以直接由 Spark 执行。<br><br><br>由于同一种逻辑算子可以有多种物理实现。如 Join 有多种实现，ShuffledHashJoin、BroadcastHashJoin、BroadcastNestedLoopJoin、SortMergeJoin 等。因此 Optimized Logical Plan 可被 Query Planner 转换为多个 Physical Plan。如何选择最优的 Physical Plan 成为一件非常影响最终执行性能的事情。一种比较好的方式是，<strong>构建一个 Cost Model，并对所有候选的 Physical Plan 应用该 Model 并挑选 Cost 最小的 Physical Plan 作为最终的 Selected Physical Plan。</strong></p><p>Physical Plan 可直接转换成 RDD 由 Spark 执行。我们经常说“计划赶不上变化”，在执行过程中，可能发现原计划不是最优的，后续执行计划如果能根据运行时的统计信息进行调整可能提升整体执行效率。这部分<strong>动态调整由 Adaptive Execution</strong> 完成。<br><br><br>后面介绍字节跳动在 Spark SQL 上做的一些优化，主要围绕这一节介绍的逻辑计划优化与物理计划优化展开。<br><a name="Z68Tc"></a></p><h2 id="Spark-SQL引擎优化"><a href="#Spark-SQL引擎优化" class="headerlink" title="Spark SQL引擎优化"></a>Spark SQL引擎优化</h2><p><a name="L4CUN"></a></p><h3 id="Bucket-Join改进"><a href="#Bucket-Join改进" class="headerlink" title="Bucket Join改进"></a>Bucket Join改进</h3><p>在 Spark 里，实际并没有 Bucket Join 算子。这里说的 Bucket Join 泛指不需要 Shuffle 的 Sort Merge Join。<br>下图展示了 Sort Merge Join 的基本原理。用虚线框代表的 Table 1 和 Table 2 是两张需要按某字段进行 Join 的表。虚线框内的 partition 0 到 partition m 是该表转换成 RDD 后的 Partition，而非表的分区。</p><p>假设 Table 1 与 Table 2 转换为 RDD 后分别包含 m 和 k 个 Partition。为了进行 Join，需要通过 Shuffle 保证相同 Join Key 的数据在同一个 Partition 内且 Partition 内按 Key 排序，同时保证 Table 1 与 Table 2 经过 Shuffle 后的 RDD 的 Partition 数相同。<br><br><br>如下图所示，经过 Shuffle 后只需要启动 n 个 Task，每个 Task 处理 Table 1 与 Table 2 中对应 Partition 的数据进行 Join 即可。如 Task 0 只需要顺序扫描 Shuffle 后的左右两边的 partition 0 即可完成 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443202-f22b73ca-17f1-450c-86ca-e47c9b934bca.webp#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&originHeight=629&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>该方法的优势是适用场景广，几乎可用于任意大小的数据集。劣势是每次 Join 都需要对全量数据进行 Shuffle，而 Shuffle 是最影响 Spark SQL 性能的环节。如果能避免 Shuffle 往往能大幅提升 Spark SQL 性能。</p><p>对于大数据的场景来讲，数据一般是一次写入多次查询。如果经常对两张表按相同或类似的方式进行 Join，每次都需要付出 Shuffle 的代价。与其这样，<strong>不如让数据在写的时候，就让数据按照利于 Join 的方式分布，从而使得 Join 时无需进行 Shuffle。</strong>如下图所示，Table 1 与 Table 2 内的数据按照相同的 Key 进行分桶且桶数都为 n，同时桶内按该 Key 排序。对这两张表进行 Join 时，可以避免 Shuffle，直接启动 n 个 Task 进行 Join。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443345-00aac217-cf9d-4ca2-beea-3a27868e9709.webp#align=left&display=inline&height=697&margin=%5Bobject%20Object%5D&originHeight=697&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>字节跳动对 Spark SQL 的 BucketJoin 做了四项比较大的改进。<br><strong><br></strong>改进一：支持与 Hive 兼容**<br>在过去一段时间，字节跳动把大量的 Hive 作业迁移到了 SparkSQL。而 Hive 与 Spark SQL 的 Bucket 表不兼容。对于使用 Bucket 表的场景，如果直接更新计算引擎，会造成 Spark SQL 写入 Hive Bucket 表的数据无法被下游的 Hive 作业当成 Bucket 表进行 Bucket Join，从而造成作业执行时间变长，可能影响 SLA。</p><p>为了解决这个问题，我们让 Spark SQL 支持 Hive 兼容模式，从而保证 Spark SQL 写入的 Bucket 表与 Hive 写入的 Bucket 表效果一致，并且这种表可以被 Hive 和 Spark SQL 当成 Bucket 表进行 Bucket Join 而不需要 Shuffle。通过这种方式保证 Hive 向 Spark SQL 的透明迁移。</p><p>第一个需要解决的问题是，Hive 的一个 Bucket 一般只包含一个文件，而 Spark SQL 的一个 Bucket 可能包含多个文件。<strong>解决办法是动态增加一次以 Bucket Key 为 Key 并且并行度与 Bucket 个数相同的 Shuffle。</strong></p><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443237-9db92d7d-db57-44a7-8c38-d02bece98cc8.webp#align=left&display=inline&height=609&margin=%5Bobject%20Object%5D&originHeight=609&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br><br><br>第二个需要解决的问题是，Hive 1.x 的哈希方式与 Spark SQL 2.x 的哈希方式（Murmur3Hash）不同，使得相同的数据在 Hive 中的 Bucket ID 与 Spark SQL 中的 Bucket ID 不同而无法直接 Join。在 Hive 兼容模式下，我们让上述动态增加的 Shuffle 使用 Hive 相同的哈希方式，从而解决该问题。<br><strong><br></strong>改进二：支持倍数关系Bucket Join**<br>Spark SQL 要求只有 Bucket 相同的表才能（必要非充分条件）进行 Bucket Join。对于两张大小相差很大的表，比如几百 GB 的维度表与几十 TB （单分区）的事实表，它们的 Bucket 个数往往不同，并且个数相差很多，默认无法进行 Bucket Join。因此我们通过两种方式支持了倍数关系的 Bucket Join，即当两张 Bucket 表的 Bucket 数是倍数关系时支持 Bucket Join。</p><p>第一种方式，Task 个数与小表 Bucket 个数相同。如下图所示，Table A 包含 3 个 Bucket，Table B 包含 6 个 Bucket。此时 Table B 的 bucket 0 与 bucket 3 的数据合集应该与 Table A 的 bucket 0 进行 Join。这种情况下，可以启动 3 个 Task。其中 Task 0 对 Table A 的 bucket 0 与 Table B 的 bucket 0 + bucket 3 进行 Join。在这里，需要对 Table B 的 bucket 0 与 bucket 3 的数据再做一次 merge sort 从而保证合集有序。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443244-bfe94284-63b6-4abe-8e6f-1df98c09fd08.webp#align=left&display=inline&height=616&margin=%5Bobject%20Object%5D&originHeight=616&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444353-c4e9ece6-e014-4583-ba9d-28bda6a5fe62.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><p>如果 Table A 与 Table B 的 Bucket 个数相差不大，可以使用上述方式。如果 Table B 的 Bucket 个数是 Bucket A Bucket 个数的 10 倍，那上述方式虽然避免了 Shuffle，但可能因为并行度不够反而比包含 Shuffle 的 SortMergeJoin 速度慢。此时可以使用另外一种方式，即 Task 个数与大表 Bucket 个数相等，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443251-d514de79-4fd1-4960-9e27-2a4e408c88ba.webp#align=left&display=inline&height=555&margin=%5Bobject%20Object%5D&originHeight=555&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>在该方案下，可将 Table A 的 3 个 Bucket 读多次。在上图中，直接将 Table A 与 Table A 进行 Bucket Union （新的算子，与 Union 类似，但保留了 Bucket 特性），结果相当于 6 个 Bucket，与 Table B 的 Bucket 个数相同，从而可以进行 Bucket Join。<br><strong><br></strong>改进三：支持BucketJoin 降级**<br>公司内部过去使用 Bucket 的表较少，在我们对 Bucket 做了一系列改进后，大量用户希望将表转换为 Bucket 表。转换后，表的元信息显示该表为 Bucket 表，而历史分区内的数据并未按 Bucket 表要求分布，在查询历史数据时会出现无法识别 Bucket 的问题。</p><p>同时，由于数据量上涨快，平均 Bucket 大小也快速增长。这会造成单 Task 需要处理的数据量过大进而引起使用 Bucket 后的效果可能不如直接使用基于 Shuffle 的 Join。<br><br><br>为了解决上述问题，我们实现了支持降级的 Bucket 表。基本原理是，每次修改 Bucket 信息（包含上述两种情况——将非 Bucket 表转为 Bucket 表，以及修改 Bucket 个数）时，记录修改日期。并且在决定使用哪种 Join 方式时，对于 Bucket 表先检查所查询的数据是否只包含该日期之后的分区。如果是，则当成 Bucket 表处理，支持 Bucket Join；否则当成普通无 Bucket 的表。<br><strong><br></strong>改进四：支持超集<strong><br>对于一张常用表，可能会与另外一张表按 User 字段做 Join，也可能会与另外一张表按 User 和 App 字段做 Join，与其它表按 User 与 Item 字段进行 Join。而 Spark SQL 原生的 Bucket Join 要求 Join Key Set 与表的 Bucket Key Set 完全相同才能进行 Bucket Join。在该场景中，不同 Join 的 Key Set 不同，因此无法同时使用 Bucket Join。这极大的限制了 Bucket Join 的适用场景。<br><br><br>针对此问题，我们支持了超集场景下的 Bucket Join。只要 Join Key Set 包含了 Bucket Key Set，即可进行 Bucket Join。<br><br><br>如下图所示，Table X 与 Table Y，都按字段 A 分 Bucket。而查询需要对 Table X 与 Table Y 进行 Join，且 Join Key Set 为 A 与 B。此时，由于 A 相等的数据，在两表中的 Bucket ID 相同，那 A 与 B 各自相等的数据在两表中的 Bucket ID 肯定也相同，所以数据分布是满足 Join 要求的，不需要 Shuffle。同时，Bucket Join 还需要保证两表按 Join Key Set 即 A 和 B 排序，此时只需要对 Table X 与 Table Y 进行分区内排序即可。由于两边已经按字段 A 排序了，此时再按 A 与 B 排序，代价相对较低。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443338-9f06a217-9078-4734-84ce-860707b4612e.webp#align=left&display=inline&height=661&margin=%5Bobject%20Object%5D&originHeight=661&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444719-a5289c2c-0dea-4316-8924-1e4377bce1ea.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br></strong>物化列**<br>Spark SQL 处理嵌套类型数据时，存在以下问题：</p><ul><li><strong>读取大量不必要的数据</strong>：对于 Parquet / ORC 等列式存储格式，可只读取需要的字段，而直接跳过其它字段，从而极大节省 IO。而对于嵌套数据类型的字段，如下图中的 Map 类型的 people 字段，往往只需要读取其中的子字段，如 people.age。却需要将整个 Map 类型的 people 字段全部读取出来然后抽取出 people.age 字段。这会引入大量的无意义的 IO 开销。在我们的场景中，存在不少 Map 类型的字段，而且很多包含几十至几百个 Key，这也就意味着 IO 被放大了几十至几百倍。<br></li><li><strong>无法进行向量化读取</strong>：而向量化读能极大的提升性能。但截止到目前（2019年10月26日），Spark 不支持包含嵌套数据类型的向量化读取。这极大的影响了包含嵌套数据类型的查询性能<br></li><li><strong>不支持 Filter 下推</strong>：目前（2019年10月26日）的 Spark 不支持嵌套类型字段上的 Filter 的下推<br></li><li><strong>重复计算</strong>：JSON 字段，在 Spark SQL 中以 String 类型存在，严格来说不算嵌套数据类型。不过实践中也常用于保存不固定的多个字段，在查询时通过 JSON Path 抽取目标子字段，而大型 JSON 字符串的字段抽取非常消耗 CPU。对于热点表，频繁重复抽取相同子字段非常浪费资源。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443241-a0b575d3-4b99-451f-802f-991fd41107f6.webp#align=left&display=inline&height=668&margin=%5Bobject%20Object%5D&originHeight=668&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444769-6635f36d-d873-430c-8223-adcb532766d6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>对于这个问题，做数仓的同学也想了一些解决方案。如下图所示，<strong>在名为 base_table 的表之外创建了一张名为 sub_table 的表，并且将高频使用的子字段 people.age 设置为一个额外的 Integer 类型的字段</strong>。下游不再通过 base_table 查询 people.age，而是使用 sub_table 上的 age 字段代替。通过这种方式，将嵌套类型字段上的查询转为了 Primitive 类型字段的查询，同时解决了上述问题。<br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444716-a4b11d0a-899e-430c-af62-8d0151c63af6.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443482-11bc4e9f-a7bb-4d7d-9026-746ea16d0d27.webp#align=left&display=inline&height=611&margin=%5Bobject%20Object%5D&originHeight=611&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>这种方案存在明显缺陷：</p><ul><li><strong>额外维护了一张表</strong>，引入了大量的额外存储/计算开销。<br></li><li>无法在新表上查询新增字段的历史数据（如要支持对历史数据的查询，需要重跑历史作业，开销过大，无法接受）。<br></li><li>表的维护方需要在修改表结构后修改插入数据的作业。<br></li><li>需要下游查询方修改查询语句，推广成本较大。<br></li><li><strong>运营成本高</strong>：如果高频子字段变化，需要删除不再需要的独立子字段，并添加新子字段为独立字段。删除前，需要确保下游无业务使用该字段。而新增字段需要通知并推进下游业务方使用新字段。<br></li></ul><p>为解决上述所有问题，我们设计并实现了物化列。它的原理是：</p><ul><li>新增一个 Primitive 类型字段，比如 Integer 类型的 age 字段，并且指定它是 people.age 的物化字段。<br></li><li>插入数据时，为物化字段自动生成数据，并在 Partition Parameter 内保存物化关系。因此对插入数据的作业完全透明，表的维护方不需要修改已有作业。<br></li><li>查询时，检查所需查询的所有 Partition，如果都包含物化信息（people.age 到 age 的映射），直接将 select people.age 自动重写为 select age，从而实现对下游查询方的完全透明优化。同时兼容历史数据。<br></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443240-98b45cc3-1d0f-4093-980d-265abb445a8a.webp#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&originHeight=683&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444777-84f75218-68e0-4929-909d-4c828ab64f33.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>下图展示了在某张核心表上使用物化列的收益：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443260-d3424ecd-67fe-41ed-a902-edf0d1ee45ab.webp#align=left&display=inline&height=423&margin=%5Bobject%20Object%5D&originHeight=423&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621444729-1b39919b-8ea5-459e-bc2b-db24d8391b39.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>物化视图</strong><br>在 OLAP 领域，经常会对相同表的某些固定字段进行 Group By 和 Aggregate / Join 等耗时操作，造成大量重复性计算，浪费资源，且影响查询性能，不利于提升用户体验。<br>我们实现了基于物化视图的优化功能：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443261-8e8f3de2-937d-475b-aca0-d39fed39babb.webp#align=left&display=inline&height=594&margin=%5Bobject%20Object%5D&originHeight=594&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>如上图所示，查询历史显示大量查询根据 user 进行 group by，然后对 num 进行 sum 或 count 计算。此时可创建一张物化视图，且对 user 进行 gorup by，对 num 进行 avg（avg 会自动转换为 count 和 sum）。用户对原始表进行 select user, sum(num) 查询时，Spark SQL 自动将查询重写为对物化视图的 select user, sum_num 查询。<br><strong><br></strong>Spark SQL 引擎上的其它优化<strong><br>下图展示了我们在 Spark SQL 上进行的其它部分优化工作：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443207-45af67c8-8989-4026-81f3-d646d8123845.webp#align=left&display=inline&height=750&margin=%5Bobject%20Object%5D&originHeight=750&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br></strong><br><a name="5UxSb"></a></p><h2 id="Spark-Shuffle稳定性提升与性能优化"><a href="#Spark-Shuffle稳定性提升与性能优化" class="headerlink" title="Spark Shuffle稳定性提升与性能优化"></a>Spark Shuffle稳定性提升与性能优化</h2><p><a name="5rTgp"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="Ajtpe"></a></p><h3 id="Spark-Shuffle-存在的问题"><a href="#Spark-Shuffle-存在的问题" class="headerlink" title="Spark Shuffle 存在的问题"></a>Spark Shuffle 存在的问题</h3><p>Shuffle的原理，很多同学应该已经很熟悉了。鉴于时间关系，这里不介绍过多细节，只简单介绍下基本模型。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445126-5aa3b134-c904-42d9-ac04-09618bb32553.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621443213-c2f4bcdb-24e6-4fdc-afc2-3397f6608fec.webp#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&originHeight=679&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，我们将 Shuffle 上游 Stage 称为 Mapper Stage，其中的 Task 称为 Mapper。Shuffle 下游 Stage 称为 Reducer Stage，其中的 Task 称为 Reducer。<br>每个 Mapper 会将自己的数据分为最多 N 个部分，N 为 Reducer 个数。每个 Reducer 需要去最多 M （Mapper 个数）个 Mapper 获取属于自己的那部分数据。<br><br><br>这个架构存在两个问题：</p><ul><li><strong>稳定性问题</strong>：Mapper 的 Shuffle Write 数据存于 Mapper 本地磁盘，只有一个副本。当该机器出现磁盘故障，或者 IO 满载，CPU 满载时，Reducer 无法读取该数据，从而引起 FetchFailedException，进而导致 Stage Retry。Stage Retry 会造成作业执行时间增长，直接影响 SLA。同时，执行时间越长，出现 Shuffle 数据无法读取的可能性越大，反过来又会造成更多 Stage Retry。如此循环，可能导致大型作业无法成功执行。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444392-8549057c-506b-4459-856d-0ce9cddf9910.webp#align=left&display=inline&height=628&margin=%5Bobject%20Object%5D&originHeight=628&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445074-9921e4da-02ca-438c-9b15-403f41fba339.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"></p><ul><li><strong>性能问题</strong>：每个 Mapper 的数据会被大量 Reducer 读取，并且是随机读取不同部分。假设 Mapper 的 Shuffle 输出为 512MB，Reducer 有 10 万个，那平均每个 Reducer 读取数据 512MB / 100000 = 5.24KB。并且，不同 Reducer 并行读取数据。对于 Mapper 输出文件而言，存在大量的随机读取。而 HDD 的随机 IO 性能远低于顺序 IO。最终的现象是，Reducer 读取 Shuffle 数据非常慢，反映到 Metrics 上就是 Reducer Shuffle Read Blocked Time 较长，甚至占整个 Reducer 执行时间的一大半，如下图所示。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444444-a5a5eeea-5d00-473f-892c-bef8ff9e032f.webp#align=left&display=inline&height=545&margin=%5Bobject%20Object%5D&originHeight=545&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445093-1dba238c-18eb-4bc2-a637-4a5f0f069847.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br><strong>基于HDFS的Shuffle稳定性提升</strong><br>经观察，引起 Shuffle 失败的最大因素不是磁盘故障等硬件问题，而是 CPU 满载和磁盘 IO 满载。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445175-a4b309ba-f45b-4141-ac27-8771c622c313.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif">)<img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444483-bc3847a8-0664-4c0d-a4ed-ae5f1781331b.webp#align=left&display=inline&height=684&margin=%5Bobject%20Object%5D&originHeight=684&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>如上图所示，机器的 CPU 使用率接近 100%，使得 Mapper 侧的 Node Manager 内的 Spark External Shuffle Service 无法及时提供 Shuffle 服务。<br>下图中 Data Node 占用了整台机器 IO 资源的 84%，部分磁盘 IO 完全打满，这使得读取 Shuffle 数据非常慢，进而使得 Reducer 侧无法在超时时间内读取数据，造成 FetchFailedException。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444481-5779db89-9d8b-4b17-82e3-7b60b08eed02.webp#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&originHeight=627&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>无论是何种原因，问题的症结都是 Mapper 侧的 Shuffle Write 数据只保存在本地，一旦该节点出现问题，会造成该节点上所有 Shuffle Write 数据无法被 Reducer 读取。解决这个问题的一个通用方法是，通过多副本保证可用性。<br>最初始的一个简单方案是，Mapper 侧最终数据文件与索引文件不写在本地磁盘，而是直接写到 HDFS。Reducer 不再通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据，而是直接从 HDFS 上获取数据，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444436-0efdf8b6-0082-47c1-9218-36b6e6bbee4f.webp#align=left&display=inline&height=541&margin=%5Bobject%20Object%5D&originHeight=541&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>快速实现这个方案后，我们做了几组简单的测试。结果表明：</p><ul><li>Mapper 与 Reducer 不多时，Shuffle 读写性能与原始方案相比无差异。<br></li><li>Mapper 与 Reducer 较多时，Shuffle 读变得非常慢。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444384-9bb761c9-fbe1-43c8-9a82-a51e97c97f8e.webp#align=left&display=inline&height=540&margin=%5Bobject%20Object%5D&originHeight=540&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>在上面的实验过程中，HDFS 发出了报警信息。如下图所示，HDFS Name Node Proxy 的 QPS 峰值达到 60 万。（注：字节跳动自研了 Node Name Proxy，并在 Proxy 层实现了缓存，因此读 QPS 可以支撑到这个量级）。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444590-4c232e53-8507-472c-844c-18249a03f85f.webp#align=left&display=inline&height=698&margin=%5Bobject%20Object%5D&originHeight=698&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>原因在于，总共 10000 Reducer，需要从 10000 个 Mapper 处读取数据文件和索引文件，总共需要读取 HDFS 10000 * 1000 * 2 = 2 亿次。</p><p>如果只是 Name Node 的单点性能问题，还可以通过一些简单的方法解决。例如在 Spark Driver 侧保存所有 Mapper 的 Block Location，然后 Driver 将该信息广播至所有 Executor，每个 Reducer 可以直接从 Executor 处获取 Block Location，然后无须连接 Name Node，而是直接从 Data Node 读取数据。但鉴于 Data Node 的线程模型，这种方案会对 Data Node 造成较大冲击。<br>最后我们选择了一种比较简单可行的方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444471-aaff74d3-ec02-4014-8d71-1b201eafdbf1.webp#align=left&display=inline&height=582&margin=%5Bobject%20Object%5D&originHeight=582&originWidth=1080&size=0&status=done&style=none&width=1080" alt><br><br><br>Mapper 的 Shuffle 输出数据仍然按原方案写本地磁盘，写完后上传到 HDFS。Reducer 仍然按原始方案通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据。如果失败了，则从 HDFS 读取。这种方案极大减少了对 HDFS 的访问频率。<br>该方案上线近一年：</p><ul><li>覆盖 57% 以上的 Spark Shuffle 数据。<br></li><li>使得 Spark 作业整体性能提升 14%。<br></li><li>天级大作业性能提升 18%。<br></li><li>小时级作业性能提升 12%。<br></li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444424-472f0478-d4b8-47de-a9ea-557b4c201c05.webp#align=left&display=inline&height=670&margin=%5Bobject%20Object%5D&originHeight=670&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><img src="https://cdn.nlark.com/yuque/0/2020/gif/1072113/1589621445065-2ed3ba00-5462-4f5f-9752-f2491fc7ea4d.gif#align=left&display=inline&height=1&margin=%5Bobject%20Object%5D&name=image.gif&originHeight=1&originWidth=1&size=70&status=done&style=none&width=1" alt="image.gif"><br>该方案旨在提升 Spark Shuffle 稳定性从而提升作业稳定性，但最终没有使用方差等指标来衡量稳定性的提升。原因在于每天集群负载不一样，整体方差较大。Shuffle 稳定性提升后，Stage Retry 大幅减少，整体作业执行时间减少，也即性能提升。最终通过对比使用该方案前后的总的作业执行时间来对比性能的提升，用于衡量该方案的效果。<br><strong>Shuffle性能优化实践与探索</strong><br>如上文所分析，Shuffle 性能问题的原因在于，Shuffle Write 由 Mapper 完成，然后 Reducer 需要从所有 Mapper 处读取数据。这种模型，我们称之为以 Mapper 为中心的 Shuffle。它的问题在于：</p><ul><li>Mapper 侧会有 M 次顺序写 IO。<br></li><li>Mapper 侧会有 M * N * 2 次随机读 IO（这是最大的性能瓶颈）。<br></li><li>Mapper 侧的 External Shuffle Service 必须与 Mapper 位于同一台机器，无法做到有效的存储计算分离，Shuffle 服务无法独立扩展。<br></li></ul><p>针对上述问题，我们提出了以 Reducer 为中心的，存储计算分离的 Shuffle 方案，如下图所示。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/webp/1072113/1589621444445-e51ea3d0-904b-48ff-af72-22657a86b3ba.webp#align=left&display=inline&height=730&margin=%5Bobject%20Object%5D&originHeight=730&originWidth=1200&size=0&status=done&style=none&width=1200" alt><br><br><br>该方案的原理是，Mapper 直接将属于不同 Reducer 的数据写到不同的 Shuffle Service。在上图中，总共 2 个 Mapper，5 个 Reducer，5 个 Shuffle Service。所有 Mapper 都将属于 Reducer 0 的数据远程流式发送给 Shuffle Service 0，并由它顺序写入磁盘。Reducer 0 只需要从 Shuffle Service 0 顺序读取所有数据即可，无需再从 M 个 Mapper 取数据。该方案的优势在于：</p><ul><li>将 M * N * 2 次随机 IO 变为 N 次顺序 IO。<br></li><li>Shuffle Service 可以独立于 Mapper 或者 Reducer 部署，从而做到独立扩展，做到存储计算分离。<br></li><li>Shuffle Service 可将数据直接存于 HDFS 等高可用存储，因此可同时解决 Shuffle 稳定性问题。<br></li></ul><p>我的分享就到这里，谢谢大家。<br><a name="23kL1"></a></p><h2 id="QA集锦"><a href="#QA集锦" class="headerlink" title="QA集锦"></a>QA集锦</h2><p><strong>- 提问：物化列新增一列，是否需要修改历史数据？</strong><br>回答：历史数据太多，不适合修改历史数据。</p><p><strong>- 提问：如果用户的请求同时包含新数据和历史数据，如何处理？</strong><br>回答：一般而言，用户修改数据都是以 Partition 为单位。所以我们在 Partition Parameter 上保存了物化列相关信息。如果用户的查询同时包含了新 Partition 与历史 Partition，我们会在新 Partition 上针对物化列进行 SQL Rewrite，历史 Partition 不 Rewrite，然后将新老 Partition 进行 Union，从而在保证数据正确性的前提下尽可能充分利用物化列的优势。</p><p><strong>- 提问：你好，你们针对用户的场景，做了很多挺有价值的优化。像物化列、物化视图，都需要根据用户的查询 Pattern 进行设置。目前你们是人工分析这些查询，还是有某种机制自动去分析并优化？</strong><br>回答：目前我们主要是通过一些审计信息辅助人工分析。同时我们也正在做物化列与物化视图的推荐服务，最终做到智能建设物化列与物化视图。</p><p><strong>- 提问：刚刚介绍的基于 HDFS 的 Spark Shuffle 稳定性提升方案，是否可以异步上传 Shuffle 数据至 HDFS？</strong><br>回答：这个想法挺好，我们之前也考虑过，但基于几点考虑，最终没有这样做。第一，单 Mapper 的 Shuffle 输出数据量一般很小，上传到 HDFS 耗时在 2 秒以内，这个时间开销可以忽略；第二，我们广泛使用 External Shuffle Service 和 Dynamic Allocation，Mapper 执行完成后可能 Executor 就回收了，如果要异步上传，就必须依赖其它组件，这会提升复杂度，ROI 较低。</p><p><a name="BtHKy"></a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>字节的这篇分享，我真的太喜欢了，所有的优化点，都是拿真实的业务场景进行举例，虽然上文有的技术点在我们的场景中还没必要去做到这种优化程度，但是如此实在的来源于线上的方案，非常容易理解。我们公司目前的数据量要比字节的量小很多，最近在新闻上看到抖音的日活已经达到了4亿，所以我们在数仓中的数据，还没有用到更细粒度的Bucket表，分区表就已经完全可以满足我们的需求。上文中的物化列方案，我觉得很新颖，但是从工程师的角度来讲，在物化列方案中，多维护一张表，添加了复杂度和运营成本，我们在数据的存储中，尽可量的回去避免复杂结构的数据类型，这样会降低存储端和计算端代码的复杂度。这篇文章是针对Spark SQL的优化方面，可以说基本上每个大数据公司都会用到Spark SQL，上述的优化方案肯定会帮助到更多的大数据团队 💪</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:12 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践&lt;br&gt;原文链接：&lt;a href
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了（二）</title>
    <link href="cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2019-09-09T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:14.168Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。<br><br><br>由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。<br>这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><br>映入眼帘的是满屏的扫描和ssh尝试登陆<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">lines <span class="number">1105</span><span class="number">-1127</span>/<span class="number">1127</span> (END)</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">49</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><p><br>看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护<br>对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@*** ~]<span class="comment"># useradd one             #创建用户</span></span><br><span class="line">[root@*** ~]<span class="comment"># passwd one              #设置密码</span></span><br></pre></td></tr></table></figure><p><br>输入新用户密码<br>首先确保文件 /etc/sudoers 中<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%wheel    ALL=(ALL)    ALL</span><br><span class="line">```  </span><br><span class="line">没有被注释</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```linux</span><br><span class="line">usermod -g wheel onerocket</span><br></pre></td></tr></table></figure><p><br>设置只有指定用户组才能使用su命令切换到root用户<br><br><br>在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"><span class="comment"># Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限</span></span><br><span class="line">sed -i <span class="string">'/pam_wheel.so use_uid/c\auth            required        pam_wheel.so use_uid '</span> /etc/pam.d/su</span><br><span class="line">n=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`</span><br><span class="line"><span class="keyword">if</span> [ $n -eq <span class="number">0</span> ];then</span><br><span class="line">echo SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defs</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><br>打开SSHD的配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。<br><br><br>修改sshd默认端口<br>虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。<br>打开sshd配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>找到#Port 22 删掉注释<br><br><br><em>服务器端口最大可以开到65536</em><br><br><br>同时再添加一个Port 61024 （随意设置）<br><br><br>Port 22<br>Port 61024<br><br><br>重启sshd服务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service sshd restart      <span class="comment">#centos6系列</span></span><br><span class="line">systemctl restart sshd  <span class="comment">#centos7系列</span></span><br><span class="line">firewall-cmd --add-port=<span class="number">61024</span>/tcp</span><br></pre></td></tr></table></figure><p><br>测试，使用新用户，新端口进行登录<br><br><br>如果登陆成功后，再将Port22注释掉，重启sshd服务。<br>到这里，关于远程登录的防护工作，就做好了。<br>最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了</title>
    <link href="cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/"/>
    <id>cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/</id>
    <published>2019-08-24T02:26:15.000Z</published>
    <updated>2020-04-04T12:00:09.871Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h1 id="服务器自述"><a href="#服务器自述" class="headerlink" title="服务器自述"></a>服务器自述</h1><p>我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。<br><br><br>而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢<br><a name="-3"></a></p><h1 id="为什么黑我的服务器"><a href="#为什么黑我的服务器" class="headerlink" title="为什么黑我的服务器"></a>为什么黑我的服务器</h1><p>这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。<br><br><br>但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。<br><br><br>那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…<br><br><br>那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404954445-f30a25a0-5939-4773-b5d9-8bb6a7c53b02.png#align=left&display=inline&height=892&name=1.png&originHeight=892&originWidth=1244&size=1778564&status=done&style=none&width=1244" alt="1.png"><br><br><br>曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器<br><a name="-4"></a></p><h1 id="如何发现被黑"><a href="#如何发现被黑" class="headerlink" title="如何发现被黑"></a>如何发现被黑</h1><p>回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？<br><br><br>最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作<br><br><br>1.关闭了Linux防火墙<br><br><br>2.没有设置redis访问密码<br><br><br>3.没有更改redis默认端口<br><br><br>4.开放了任意IP可以远程连接<br><br><br>以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。<br><br><br>当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。<br><br><br>但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i tcp:<span class="number">6379</span></span><br></pre></td></tr></table></figure><p><br>使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。<br><br><br>但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想<br>直到我运行：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404912911-97736d80-2ee3-455d-a948-6d134f4e2663.png#align=left&display=inline&height=534&name=2.png&originHeight=534&originWidth=3338&size=980508&status=done&style=none&width=3338" alt="2.png"><br>发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了<br><a name="-5"></a></p><h1 id="做了哪些急救工作"><a href="#做了哪些急救工作" class="headerlink" title="做了哪些急救工作"></a>做了哪些急救工作</h1><p>这时，感觉自己开始投入了一场对抗战<br><br><br>1.首先查找植入程序的位置。<br>在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。<br><br><br>2.删除清理可疑文件<br><br><br>杀死进程<br><br><br>删除了正在运行的程序文件还有安装包<br>3.查看所有用户的定时任务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd |cut -f <span class="number">1</span> -d:crontab -uXXX -l</span><br></pre></td></tr></table></figure><p><br>4.开启防火墙<br><br><br>仅开放会使用到的端口<br>5.修改redis默认端口<br><br><br>redis.conf中的port<br>6.添加redis授权密码<br><br><br>redis.conf中的requirepass<br>7.修改绑定远程绑定ip<br><br><br>redis.conf中的bind<br>最后重启redis服务！<br><a name="-6"></a></p><h1 id="从中学到了什么"><a href="#从中学到了什么" class="headerlink" title="从中学到了什么"></a>从中学到了什么</h1><p>明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～<br>最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;服务器自述&quot;&gt;&lt;a href=&quot;#服务器自述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Linux" scheme="cpeixin.cn/categories/Linux/"/>
    
    
      <category term="服务器安全" scheme="cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫 - 动态爬取</title>
    <link href="cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
    <id>cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</id>
    <published>2019-06-12T15:26:15.000Z</published>
    <updated>2020-04-04T17:11:17.062Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。<br><br><br>到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。<br><br><br>那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。<br><br><br>那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843101-98e57df8-c124-4923-aa9f-7c42ce2288b6.png#align=left&display=inline&height=1300&originHeight=1300&originWidth=3356&size=0&status=done&style=none&width=3356" alt><br><br><br>注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843056-e12f0f79-905f-4420-abfd-fb85625767ac.png#align=left&display=inline&height=1150&originHeight=1150&originWidth=2266&size=0&status=done&style=none&width=2266" alt><br><br><br>点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：<br>Headers：请求和响应的详细信息<br>Preview：响应体格式化之后的显示<br>Response：响应体原始内容<br>Cookies：Cookies<br>Timing：时间开销<br><br><br>通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401842263-072689cf-5dea-4f8b-b103-0758d9c5e52e.png#align=left&display=inline&height=262&originHeight=262&originWidth=1276&size=0&status=done&style=none&width=1276" alt><br><br><br>上面的截图展示了这次请求的请求方式、请求地址等信息。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843887-cf8bff5f-4570-4ad4-8681-f0f3eae929b3.png#align=left&display=inline&height=884&originHeight=884&originWidth=2652&size=0&status=done&style=none&width=2652" alt><br><br><br>上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843009-4797a28d-3070-421e-a45c-86781d7f580d.png#align=left&display=inline&height=188&originHeight=188&originWidth=848&size=0&status=done&style=none&width=848" alt><br><br><br>上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。<br><br><br>那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。<br><br><br>我们回过头重新观察一下返回的json，格式化之后的层级关系如下：<br><br><br>很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">json_obj = json.loads(<span class="string">"&#123;'key': 'value'&#125;"</span>)  <span class="comment"># 字符串到对象</span></span><br><span class="line">json_str = json.dumps(json_obj)            <span class="comment"># 对象到字符串</span></span><br></pre></td></tr></table></figure><p><br>json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会
      
    
    </summary>
    
    
      <category term="python" scheme="cpeixin.cn/categories/python/"/>
    
    
      <category term="爬虫" scheme="cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>SHC：使用 Spark SQL 高效地读写 HBase</title>
    <link href="cpeixin.cn/2019/05/16/SHC%EF%BC%9A%E4%BD%BF%E7%94%A8-Spark-SQL-%E9%AB%98%E6%95%88%E5%9C%B0%E8%AF%BB%E5%86%99-HBase/"/>
    <id>cpeixin.cn/2019/05/16/SHC%EF%BC%9A%E4%BD%BF%E7%94%A8-Spark-SQL-%E9%AB%98%E6%95%88%E5%9C%B0%E8%AF%BB%E5%86%99-HBase/</id>
    <published>2019-05-16T02:27:59.000Z</published>
    <updated>2020-05-16T02:31:46.066Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:12 GMT+0800 (GMT+08:00) --><p><br>Apache <a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a> 和 Apache <a href="https://www.iteblog.com/archives/tag/hbase/" target="_blank" rel="external nofollow noopener noreferrer">HBase</a> 是两个使用比较广泛的大数据组件。很多场景需要使用 <a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a> 分析/查询 <a href="https://www.iteblog.com/archives/tag/hbase/" target="_blank" rel="external nofollow noopener noreferrer">HBase</a> 中的数据，而目前 <a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a> 内置是支持很多数据源的，其中就包括了 HBase，但是内置的读取数据源还是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：</p><ul><li>一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；</li><li>TableInputFormat 中不支持 BulkGet；</li><li>不能享受到 Spark SQL 内置的 catalyst 引擎的优化。</li></ul><p><br>从另一个方面来讲，如果先不谈关于性能的方面，那么你在写HBase的过程中，是不是步骤感觉很麻烦呢，Foreach每条数据Put到HBase中，往往写入逻辑部分的代码就要写很长很长的一段。<br><br><br>基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。<br><br><br><a href="https://github.com/hortonworks-spark/shc" target="_blank" rel="external nofollow noopener noreferrer">项目地址</a><br>具体的引入方式，在之前的Spark SQL入门中有写到<br><a name="EjSTz"></a></p><h2 id="Catalog"><a href="#Catalog" class="headerlink" title="Catalog"></a>Catalog</h2><p>对于每个表，必须提供一个目录，其中包括行键和具有预定义列族的数据类型的列，并定义hbase列与表模式之间的映射。目录是用户定义的json格式。<br></p><p><a name="eXdJt"></a></p><h2 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h2><p>支持Java基本类型。将来，将支持其他数据类型，这些数据类型依赖于用户指定的Serdes。SHC支持三种内部Serdes：Avro，Phoenix和PrimitiveType。用户可以通过在目录中定义“ tableCoder”来指定要使用的serde。为此，请参考示例和单元测试。以Avro为例。用户定义的Serdes将负责将字节数组转换为Avro对象，而连接器将负责将Avro对象转换为催化剂支持的数据类型。用户定义新Serde时，需要使其“实现”特征’SHCDataType’。<br><br><br>请注意，如果用户希望DataFrame仅处理字节数组，则可以指定二进制类型。然后，用户可以获得每个列为字节数组的催化剂行。用户可以使用定制的解串器进一步对它进行反序列化，或者直接在DataFrame的RDD上进行操作。<br></p><p><a name="j3Svo"></a></p><h2 id="数据局部性"><a href="#数据局部性" class="headerlink" title="数据局部性"></a>数据局部性</h2><p>当Spark Worker节点与hbase区域服务器位于同一位置时，通过标识区域服务器位置并将执行程序与区域服务器一起定位来实现数据局部性。每个执行程序将仅对位于同一主机上的同一部分数据执行Scan / BulkGet。<br></p><p><a name="TccVy"></a></p><h2 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h2><p>该库使用HBase提供的现有标准HBase过滤器，并且不能在协处理器上运行。<br></p><p><a name="eEPLD"></a></p><h2 id="分区修剪"><a href="#分区修剪" class="headerlink" title="分区修剪"></a>分区修剪</h2><p>通过从谓词中提取行键，我们将scan / BulkGet划分为多个非重叠区域，只有具有请求数据的区域服务器才会执行scan / BulkGet。当前，分区修剪是在行键的第一维上执行的。请注意，需要仔细定义WHERE条件。否则，结果扫描可能会包含一个比用户预期大的区域。例如，以下条件将导致完全扫描（rowkey1是行键的第一维，而column是常规的hbase列）。其中rowkey1&gt;“ abc” OR列=“ xyz”<br></p><p><a name="lDJh4"></a></p><h2 id="扫描和批量获取"><a href="#扫描和批量获取" class="headerlink" title="扫描和批量获取"></a>扫描和批量获取</h2><p>都通过指定WHERE子句向用户公开，例如，其中column&gt; x和column &lt;y用于扫描，而column = x用于get。所有操作都在执行程序中执行，而驱动程序仅构造这些操作。在内部，我们将它们转换为扫描或获取或两者结合，从而将Iterator [Row]返回至催化剂引擎。<br></p><p><a name="P0Sof"></a></p><h2 id="可创建的数据源"><a href="#可创建的数据源" class="headerlink" title="可创建的数据源"></a>可创建的数据源</h2><p>该库支持从HBase读取/向HBase写入。<br><a name="qW63X"></a></p><h2 id="应用用途"><a href="#应用用途" class="headerlink" title="应用用途"></a>应用用途</h2><p>下面说明了如何使用连接器的基本步骤。有关更多详细信息和高级用例（例如Avro和复合键支持），请参考存储库中的示例。<br><a name="DjKzs"></a></p><h3 id="定义了HBase目录"><a href="#定义了HBase目录" class="headerlink" title="定义了HBase目录"></a>定义了HBase目录</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">        |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span>table1<span class="string">"&#125;,</span></span><br><span class="line"><span class="string">        |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">        |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">          |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">      |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>上面定义了一个名称为table1，行键为键，列数为（col1-col8）的HBase表的架构。请注意，行键还必须详细定义为具有特定cf（行键）的列（col0）。<br></p><p><a name="xic7k"></a></p><h3 id="写入HBase表以填充数据"><a href="#写入HBase表以填充数据" class="headerlink" title="写入HBase表以填充数据"></a>写入HBase表以填充数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(data).toDF.write.options(</span><br><span class="line">  <span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; catalog, <span class="type">HBaseTableCatalog</span>.newTable -&gt; <span class="string">"5"</span>))</span><br><span class="line">  .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure><p>给定具有指定架构的DataFrame，上面将创建一个具有5个区域的HBase表并将该DataFrame保存在其中。请注意，如果未指定HBaseTableCatalog.newTable，则必须预先创建表。<br></p><p><a name="wSV5W"></a></p><h3 id="在HBase表的顶部执行DataFrame操作"><a href="#在HBase表的顶部执行DataFrame操作" class="headerlink" title="在HBase表的顶部执行DataFrame操作"></a>在HBase表的顶部执行DataFrame操作</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withCatalog</span></span>(cat: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  sqlContext</span><br><span class="line">  .read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;cat))</span><br><span class="line">  .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">  .load()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a name="yQeDt"></a></p><h3 id="复杂查询"><a href="#复杂查询" class="headerlink" title="复杂查询"></a>复杂查询</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br><span class="line"><span class="keyword">val</span> s = df.filter((($<span class="string">"col0"</span> &lt;= <span class="string">"row050"</span> &amp;&amp; $<span class="string">"col0"</span> &gt; <span class="string">"row040"</span>) ||</span><br><span class="line">  $<span class="string">"col0"</span> === <span class="string">"row005"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> === <span class="string">"row020"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> ===  <span class="string">"r20"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> &lt;= <span class="string">"row005"</span>) &amp;&amp;</span><br><span class="line">  ($<span class="string">"col4"</span> === <span class="number">1</span> ||</span><br><span class="line">  $<span class="string">"col4"</span> === <span class="number">42</span>))</span><br><span class="line">  .select(<span class="string">"col0"</span>, <span class="string">"col1"</span>, <span class="string">"col4"</span>)</span><br><span class="line">s.show</span><br></pre></td></tr></table></figure><p><a name="mKwKp"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="szhEb"></a></p><h3 id="SQL支持"><a href="#SQL支持" class="headerlink" title="SQL支持"></a>SQL支持</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Load the dataframe</span></span><br><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br><span class="line"><span class="comment">//SQL example</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"table"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select count(col1) from table"</span>).show</span><br></pre></td></tr></table></figure><br><a name="raaVM"></a> ## 支持Avro模式 该连接器完全支持所有avro模式。用户可以在其目录中使用完整记录模式或部分字段模式作为数据类型（有关更多详细信息，请参阅[此处](https://github.com/hortonworks-spark/shc/wiki/2.-Native-Avro-Support)）。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schema_array = <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">type": "</span><span class="string">array", "</span><span class="string">items": ["</span><span class="string">string","</span><span class="string">null"]&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> schema_record =</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">namespace": "</span>example.<span class="string">avro",</span></span><br><span class="line"><span class="string">     |   "</span><span class="string">type": "</span><span class="string">record",      "</span><span class="string">name": "</span><span class="type">User</span><span class="string">",</span></span><br><span class="line"><span class="string">     |    "</span><span class="string">fields": [      &#123;"</span><span class="string">name": "</span><span class="string">name", "</span><span class="string">type": "</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">     |      &#123;"</span><span class="string">name": "</span>favorite_<span class="string">number",  "</span><span class="string">type": ["</span><span class="string">int", "</span><span class="string">null"]&#125;,</span></span><br><span class="line"><span class="string">     |        &#123;"</span><span class="string">name": "</span>favorite_<span class="string">color", "</span><span class="string">type": ["</span><span class="string">string", "</span><span class="string">null"]&#125;      ]    &#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> catalog = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">        |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">htable"&#125;,</span></span><br><span class="line"><span class="string">        |"</span><span class="string">rowkey":"</span>key1<span class="string">",</span></span><br><span class="line"><span class="string">        |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">          |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key1<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">avro":"</span>schema_<span class="string">array"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">avro":"</span>schema_<span class="string">record"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;</span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">      |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"> <span class="keyword">val</span> df = sqlContext.read.options(<span class="type">Map</span>(<span class="string">"schema_array"</span>-&gt;schema_array,<span class="string">"schema_record"</span>-&gt;schema_record, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).load()</span><br><span class="line">df.write.options(<span class="type">Map</span>(<span class="string">"schema_array"</span>-&gt;schema_array,<span class="string">"schema_record"</span>-&gt;schema_record, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).save()</span><br></pre></td></tr></table></figure><a name="LqW7H"></a> ####<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> complex = <span class="string">s""</span><span class="string">"MAP&lt;int, struct&lt;varchar:string&gt;&gt;"</span><span class="string">""</span></span><br><span class="line"><span class="keyword">val</span> schema =</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">namespace": "</span>example.<span class="string">avro",</span></span><br><span class="line"><span class="string">     |   "</span><span class="string">type": "</span><span class="string">record",      "</span><span class="string">name": "</span><span class="type">User</span><span class="string">",</span></span><br><span class="line"><span class="string">     |    "</span><span class="string">fields": [      &#123;"</span><span class="string">name": "</span><span class="string">name", "</span><span class="string">type": "</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">     |      &#123;"</span><span class="string">name": "</span>favorite_<span class="string">number",  "</span><span class="string">type": ["</span><span class="string">int", "</span><span class="string">null"]&#125;,</span></span><br><span class="line"><span class="string">     |        &#123;"</span><span class="string">name": "</span>favorite_<span class="string">color", "</span><span class="string">type": ["</span><span class="string">string", "</span><span class="string">null"]&#125;      ]    &#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> catalog = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">        |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">htable"&#125;,</span></span><br><span class="line"><span class="string">        |"</span><span class="string">rowkey":"</span>key1:key2<span class="string">",</span></span><br><span class="line"><span class="string">        |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">          |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key1<span class="string">", "</span><span class="string">type":"</span><span class="string">binary"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">avro":"</span>schema1<span class="string">"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">double",        "</span><span class="string">sedes":"</span>org.apache.spark.sql.execution.datasources.hbase.<span class="type">DoubleSedes</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">          |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span>$<span class="string">complex"&#125;</span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">      |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line">   </span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.options(<span class="type">Map</span>(<span class="string">"schema1"</span>-&gt;schema, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).load()</span><br><span class="line">df.write.options(<span class="type">Map</span>(<span class="string">"schema1"</span>-&gt;schema, <span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;catalog)).format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>).save()</span><br></pre></td></tr></table></figure><p>以上说明了我们的下一步，其中包括复合键支持，复杂数据类型，客户化Serde和Avro的支持。请注意，尽管所有主要部分都包含在当前代码库中，但现在可能无法运行。<br><br><br></p><p><a name="XtWKn"></a></p><h2 id="SHC查询优化"><a href="#SHC查询优化" class="headerlink" title="SHC查询优化"></a>SHC查询优化</h2><p><br>SHC 主要使用下面的几种优化，使得 Spark 获取 HBase 的数据扫描范围得到减少，提高了数据读取的效率。<br><strong><br></strong>将使用 Rowkey 的查询转换成 get 查询**<br>我们都知道，HBase 中使用 Get 查询的效率是非常高的，所以如果查询的过滤条件是针对 RowKey 进行的，那么我们可以将它转换成 Get 查询。为了说明这点，我们使用下面的例子进行说明。假设我们定义好的 HBase catalog 如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> catalog = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">  |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">iteblog", "</span>tableC<span class="string">oder":"</span><span class="type">PrimitiveType</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">  |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">  |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">    |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">id", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">    |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">  |&#125;</span></span><br><span class="line"><span class="string">|&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>那么如果有类似下面的查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"iteblog_table"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from iteblog_table where id = 1"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from iteblog_table where id = 1 or id = 2"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from iteblog_table where id in (1, 2)"</span>)</span><br></pre></td></tr></table></figure><p>因为查询条件直接是针对 RowKey 进行的，所以这种情况直接可以转换成 Get 或者 BulkGet 请求的。第一个 SQL 查询过程类似于下面过程<br><a href="https://www.iteblog.com/pic/hbase/shc_get-iteblog.png" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1589595330346-3b5f02d0-85b1-4e32-98d8-7d65433517d4.png#align=left&display=inline&height=409&margin=%5Bobject%20Object%5D&originHeight=409&originWidth=1032&size=0&status=done&style=none&width=1032" alt></a><br><br><br>但是，如果碰到非 RowKey 的过滤，那么这种查询是需要扫描 HBase 的全表的。上面的查询在 shc 里面就是将 HBase 里面的所有数据拿到，然后<strong>传输到 Spark ，再通过 Spark 里面进行过滤</strong>，可见 shc 在这种情况下效率是很低下的。<br><br><br>注意，上面的查询在 shc 返回的结果是错误的。具体原因是在将 id = 1 or col7 = ‘xxx’ 查询条件进行合并时，丢弃了所有的查找条件，相当于返回表的所有数据。定位到代码可以参见下面的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">or</span></span>[<span class="type">T</span>](left: <span class="type">HRF</span>[<span class="type">T</span>],</span><br><span class="line">            right: <span class="type">HRF</span>[<span class="type">T</span>])(<span class="keyword">implicit</span> ordering: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">HRF</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ranges = <span class="type">ScanRange</span>.or(left.ranges, right.ranges)</span><br><span class="line">    <span class="keyword">val</span> typeFilter = <span class="type">TypedFilter</span>.or(left.tf, right.tf)</span><br><span class="line">    <span class="type">HRF</span>(ranges, typeFilter, left.handled &amp;&amp; right.handled)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同理，类似于下面的查询在 shc 里面其实都是全表扫描，并且将所有的数据返回到 Spark 层面上再进行一次过滤。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sql(<span class="string">"select id, col6, col8 from iteblog_table where id = 1 or col7 &lt;= 'xxx'"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select id, col6, col8 from iteblog_table where id = 1 or col7 &gt;= 'xxx'"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select id, col6, col8 from iteblog_table where col7 = 'xxx'"</span>)</span><br></pre></td></tr></table></figure><p>很显然，这种方式查询效率并不高，一种可行的方案是将算子下推到 HBase 层面，在 HBase 层面通过 SingleColumnValueFilter 过滤一部分数据，然后再返回到 Spark，这样可以节省很多数据的传输。<br></p><p><a name="RHA0U"></a></p><h2 id="组合-RowKey-的查询优化"><a href="#组合-RowKey-的查询优化" class="headerlink" title="组合 RowKey 的查询优化"></a>组合 RowKey 的查询优化</h2><p><br>shc 还支持组合 RowKey 的方式来建表，具体如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cat</span> </span>=</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">     |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">iteblog", "</span>tableC<span class="string">oder":"</span><span class="type">PrimitiveType</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">     |"</span><span class="string">rowkey":"</span>key1:key2<span class="string">",</span></span><br><span class="line"><span class="string">     |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">     |"</span>col00<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key1<span class="string">", "</span><span class="string">type":"</span><span class="string">string", "</span><span class="string">length":"</span><span class="number">6</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col01<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span>key2<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">     |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">     |&#125;</span></span><br><span class="line"><span class="string">     |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>上面的 col00 和 col01 两列组合成一个 rowkey，并且 col00 排在前面，col01 排在后面。比如 col00 =’row002’，col01 = 2，那么组合的 rowkey 为 row002\x00\x00\x00\x02。那么在组合 Rowkey 的查询 shc 都有哪些优化呢？现在我们有如下查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0"</span>).show()</span><br></pre></td></tr></table></figure><p>根据上面的信息，RowKey 其实是由 col00 和 col01 组合而成的，那么上面的查询其实可以将 col00 和 col01 进行拼接，然后组合成一个 RowKey，然后上面的查询其实可以转换成一个 Get 查询。但是在 shc 里面，上面的查询是转换成一个 scan 和一个 get 查询的。scan 的 startRow 为 row000，endRow 为 row000\xff\xff\xff\xff；get 的 rowkey 为 row000\xff\xff\xff\xff，然后再将所有符合条件的数据返回，最后再在 Spark 层面上做一次过滤，得到最后查询的结果。因为 shc 里面组合键查询的代码还没完善，所以当前实现应该不是最终的。<br>在 shc 里面下面两条 SQL 查询下沉到 HBase 的逻辑一致</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 = 'row000'"</span>).show()df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0"</span>).show()</span><br></pre></td></tr></table></figure><p>唯一区别是在 Spark 层面上的过滤。<br><a name="aR7vS"></a></p><h2 id="scan-查询优化"><a href="#scan-查询优化" class="headerlink" title="scan 查询优化"></a>scan 查询优化</h2><p>如果我们的查询有 &lt; 或 &gt; 等查询过滤条件，比如下面的查询条件：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sqlContext.sql(<span class="string">"select col00, col01, col1 from iteblog where col00 &gt; 'row000' and col00 &lt; 'row005'"</span>).show()</span><br></pre></td></tr></table></figure><p>这个在 shc 里面转换成 HBase 的过滤为一条 get 和 一个 scan，具体为 get 的 Rowkey 为 row0005\xff\xff\xff\xff；scan 的 startRow 为 row000，endRow 为 row005\xff\xff\xff\xff，然后将查询的结果返回到 spark 层面上进行过滤。<br>总体来说，shc 能在一定程度上对查询进行优化，避免了全表扫描。但是经过评测，shc 其实还有很多地方不够完善，算子下沉并没有下沉到 HBase 层面上进行。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:12 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;Apache &lt;a href=&quot;https://www.iteblog.com/archives/tag/spark/&quot; target=
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>shadowsock-vps搭建VPN</title>
    <link href="cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/"/>
    <id>cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/</id>
    <published>2019-04-19T15:26:15.000Z</published>
    <updated>2020-04-04T17:11:07.011Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><a name="-1"></a></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>还有10天左右就要回国了，由于职业的需要，对Google的依赖的越来越大的，那么回国后怎么才能‘科学上网’呢？之前在国内的时候，有使用过Lantern，稳定性和速度都还是不错了，可惜后来被和谐了。所以今天准备尝试搭建VPN，自己独立使用，一边搭建一边将过程记录下来。<br><a name="-2"></a></p><h1 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1><p>VPS: VPS（Virtual Private Server 虚拟专用服务器）技术，将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器 [1] 技术，和虚拟化技术 [2] 。在容器或虚拟机中，每个VPS都可分配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。<br><br><br>VPS为使用者提供了管理配置的自由，可用于企业虚拟化，也可以用于IDC资源租用。<br><br><br>VPN: VPN的学名叫虚拟专用网，洋文叫“Virtual Private Network”。维基百科的介绍在“这里”。本来这玩意儿主要是用于商业公司，为了让那些不在公司里的员工（比如出差在外的）能够方便地访问公司的内部网络。为了防止黑客冒充公司的员工，从外部访问公司的内部网络，VPN 软件都会提供强大的加密功能。而这个加密功能，也就让它顺便成为翻墙的利器。<br><a name="-3"></a></p><h1 id="科学上网原理"><a href="#科学上网原理" class="headerlink" title="科学上网原理"></a>科学上网原理</h1><p>VPN浏览外网的原理<br><br><br>使用 VPN 通常需要先安装客户端软件。当你运行 VPN 客户端，它会尝试联到 VPN 服务器（这点跟加密代理类似）。一旦和 VPN 服务器建立连接，VPN 客户端就会在你的系统中建立了一个虚拟局域网。而且，你的系统中也会多出一个虚拟网卡（在 Windows 下，可以用 ipconfig /all 命令，看到这多出来的网卡）。这样一来，你的系统中就有不止一块网卡。这就引出一个问题：那些访问网络的程序，它的数据流应该通过哪个网卡进出？<br>为了解决此问题，VPN 客户端通常会修改你系统的路由表，让那些数据流，优先从虚拟的网卡进出。由于虚拟的网卡是通往 VPN 服务器的，当数据流到达 VPN 服务器之后，VPN 服务器再帮你把数据流转向到真正的目的地。<br><br><br>前面说了，VPN 为了保证安全，都采用强加密的方式传输数据。这样一来，GFW 就无法分析你的网络数据流，进行敏感词过滤。所以，使用墙外的VPN服务器，无形中就能达到翻墙的效果。<br><a name="-4"></a></p><h1 id="方案选择"><a href="#方案选择" class="headerlink" title="方案选择"></a>方案选择</h1><p>VPN是一个大类，其中有很多实现的方法，防火长城现在将 VPN 屏蔽的已经所剩无几，后来大家看到了SSH，使用SSH的sock5很稳定，但是特征也十分明显，防火长城可以对其直接进行定向干扰。<br><br><br>而除了VPN，对于翻墙大家仍然有很多方法，比如Shadowsocks 、Lantern、VPNGate 等等，而实际上无论哪种方式，他们本身都需要一台服务器作为中间人进行消息传递。而VPS虚拟专用服务器就十分适合担当这个角色，并且由于VPS平时就作为商品在各类云服务器平台上售卖，自行购买并搭建相当方便，唯一需要的就是人们对于服务器的操作技术。<br><br><br><strong>而这次选择的方案是：VPS+Shadowsocks</strong><br>**<br>Shadowsocks特点：<br><br><br>省电，在电量查看里几乎看不到它的身影；<br><br><br>支持开机自启动，且断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴；<br><br><br>可使用自己的服务器，安全和速度的保证；<br><br><br>支持区分国内外流量，传统VPN在翻出墙外后访问国内站点会变慢；<br><br><br>可对应用设置单独代理，5.0之后的系统无需root。<br><br><br>Shadowsocks 目前不容易被封杀主要是因为：<br>建立在socks5协议之上，socks5是运用很广泛的协议，所以没办法直接封杀socks5协议<br>使用socks5协议建立连接，而没有使用VPN中的服务端身份验证和密钥协商过程。而是在服务端和客户端直接写死密钥和加密算法。所以防火墙很难找到明显的特征，因为这就是个普通的socks5协议。<br><br><br>Shadowsock搭建也比较简单，所以很多人自己架设VPS搭建，个人使用流量也很小，没法通过流量监控方式封杀。<br>自定义加密方式和密钥。因为加密主要主要是防止被检测，所以要选择安全系数高的加密方式。之前RC4会很容易被破解，而导致被封杀。所以现在推荐使用AES加密。而在客户端和服务端自定义密钥，泄露的风险相对较小。<br>所以如果是自己搭建的Shadosocks被封的概率很小，但是如果是第三方的Shadeowsocks，密码是server定的，你的数据很可能遭受到中间人攻击。<br><a name="-5"></a></p><h1 id="开工"><a href="#开工" class="headerlink" title="开工"></a>开工</h1><p><a name="vps"></a></p><h2 id="购买vps"><a href="#购买vps" class="headerlink" title="购买vps"></a>购买vps</h2><p>首先我们需要购买一台境外的服务器，接着我们在这台云服务器里面安装代理服务，那么以后我们上网的时候就可以通过它来中转，轻松畅快的畅游全网了。<br>购买VPS,我选择了<a href="https://www.vultr.com/" target="_blank" rel="external nofollow noopener noreferrer">vultr</a>，大家用过都说好，购买的过程也很方便。<br><br><br>第一步：选择离中国较近国家的服务器。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400540784-be727007-a613-4b5b-a3d5-9b5fbf9734a7.png#align=left&display=inline&height=1468&name=step1.png&originHeight=1468&originWidth=3066&size=659813&status=done&style=none&width=3066" alt="step1.png"><br><br><br><br><br>第二步：选择服务器配置和系统<br><br><br>这里，系统选择的是CentOS 7,配置的话，如果只是自己浏览网页的话，选择最低配置就好。其他的选项可以略过。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400556831-e2446e46-8c7e-4292-97a8-b89e6d983c9c.png#align=left&display=inline&height=1594&name=step2.png&originHeight=1594&originWidth=2792&size=669959&status=done&style=none&width=2792" alt="step2.png"><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235703-e6afdea2-828f-42a0-a955-67960c8710bc.png#align=left&display=inline&height=1812&originHeight=1812&originWidth=2600&size=0&status=done&style=none&width=2600" alt><br><br><br>第三步：支付和部署<br><br><br>支付可以选择支付宝支付，非常方便。购买成功后，点击Server中的“+”号，来部署你刚刚选择的服务器。<br>第四步：登陆服务器<br><br><br>查看服务器详情 Server Details,根据提供的服务器信息，登陆服务器。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235150-95a87ca7-8392-404e-a5d9-c4a606b8ae7e.png#align=left&display=inline&height=1166&originHeight=1166&originWidth=2728&size=0&status=done&style=none&width=2728" alt><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235639-e36e0afa-200d-49ad-a324-350004d08d09.png#align=left&display=inline&height=1232&originHeight=1232&originWidth=2612&size=0&status=done&style=none&width=2612" alt><br><br><br>我是使用Mac本身终端ssh到服务器上的，因为Mac上多数的SSH客户端要么收费，要么不好用，要么安装过程非常繁琐。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p <span class="number">22</span> root@ip</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235316-408c45f7-df95-4c8c-9428-9f476c6c7190.png#align=left&display=inline&height=244&originHeight=244&originWidth=1304&size=0&status=done&style=none&width=1304" alt><br><a name="shadowsocks"></a></p><h2 id="搭建shadowsocks服务器"><a href="#搭建shadowsocks服务器" class="headerlink" title="搭建shadowsocks服务器"></a>搭建shadowsocks服务器</h2><p>连接到你的 vultr 服务器之后，接下来就可以使用几个命令让你快速搭建一个属于自己的 ss 服务器：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install wget</span><br></pre></td></tr></table></figure><p><br>接着执行安装shadowsocks：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span><br></pre></td></tr></table></figure><p><br>获取 <a href="http://shadowsocks.sh/" target="_blank" rel="external nofollow noopener noreferrer">shadowsocks.sh</a> 读取权限：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x shadowsocks.sh</span><br></pre></td></tr></table></figure><p><br>设置你的 ss 密码和端口号：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./shadowsocks.sh <span class="number">2</span>&gt;&amp;<span class="number">1</span> | tee shadowsocks.log</span><br></pre></td></tr></table></figure><p><br>接下来后就可以设置密码和端口号了<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400236071-267db22c-ea03-4f8b-969d-86ae5efc4d68.png#align=left&display=inline&height=306&originHeight=306&originWidth=1086&size=0&status=done&style=none&width=1086" alt><br><br><br>密码和端口号可以使用默认的，也可以直接重新输入新的。<br>选择加密方式<br><br><br>设置完密码和端口号之后，我们选择加密方式，这里选择 7 ，使用aes-256-cfb的加密模式<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235435-525e1564-eb61-47e0-b9b1-a28683979702.png#align=left&display=inline&height=684&originHeight=684&originWidth=914&size=0&status=done&style=none&width=914" alt><br><br><br>接着按任意键进行安装。<br>安装ss完成后<br><br><br>会给你显示你需要连接 vpn 的信息：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400239145-4d70810e-bc73-4cfa-bc8c-4da35e3e0dcf.png#align=left&display=inline&height=464&originHeight=464&originWidth=1186&size=0&status=done&style=none&width=1186" alt><br>搞定，将这些信息保存起来，那么这时候你就可以使用它们来科学上网啦。<br><a name="bbr"></a></p><h2 id="使用BBR加速上网"><a href="#使用BBR加速上网" class="headerlink" title="使用BBR加速上网"></a>使用BBR加速上网</h2><p>安装 BBR<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh</span><br></pre></td></tr></table></figure><p><br>获取读写权限<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x bbr.sh</span><br></pre></td></tr></table></figure><p><br>启动BBR安装<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bbr.sh</span><br></pre></td></tr></table></figure><p><br>接着按任意键，开始安装，坐等一会。安装完成一会之后它会提示我们是否重新启动vps，我们输入 y 确定重启服务器。<br>重新启动之后，输入：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep bbr</span><br></pre></td></tr></table></figure><p><br>如果看到 tcp_bbr 就说明 BBR 已经启动了。<br><a name="-6"></a></p><h2 id="客户端进行连接"><a href="#客户端进行连接" class="headerlink" title="客户端进行连接"></a>客户端进行连接</h2><p><a name="windowsshadowsocks"></a></p><h3 id="windows使用Shadowsocks"><a href="#windows使用Shadowsocks" class="headerlink" title="windows使用Shadowsocks"></a>windows使用Shadowsocks</h3><p>windows点击下载：<a href="https://pan.baidu.com/s/19m0AfTkPDSRj0bfYrGpbIg" target="_blank" rel="external nofollow noopener noreferrer">Shadowsocks windows客户端</a><br>打开 Shadowsocks 客户端，输入ip地址，密码，端口，和加密方式。接着点击确定，右下角会有个小飞机按钮，右键–&gt;启动代理。<br><a name="androidshadowsocks"></a></p><h3 id="Android使用Shadowsocks"><a href="#Android使用Shadowsocks" class="headerlink" title="Android使用Shadowsocks"></a>Android使用Shadowsocks</h3><p>Android点击下载：<a href="https://pan.baidu.com/s/1coAkZn-GuYHu5eIKaHECxA" target="_blank" rel="external nofollow noopener noreferrer">Shadowsocks Android客户端</a><br>打开apk安装，接着打开APP，输入ip地址，密码，端口，和加密方式。即可科学上网。<br><a name="iphoneshadowsocks"></a></p><h3 id="iPhone使用Shadowsocks"><a href="#iPhone使用Shadowsocks" class="headerlink" title="iPhone使用Shadowsocks"></a>iPhone使用Shadowsocks</h3><p>iPhone要下载的app需要在appstore下载，但是需要用美区账号才能下载，而且这个APP需要钱。在这里提供一种解决方案，就是可以再搭建一个<a href="https://wistbean.github.io/ipsec,l2tp_vpn.html#%E4%BD%BF%E7%94%A8-IPsec-L2TP-%E8%84%9A%E6%9C%AC%E6%90%AD%E5%BB%BA" target="_blank" rel="external nofollow noopener noreferrer">IPsec/L2TP</a> VPN,专门给你的iPhone使用。<br><a name="mac"></a></p><h3 id="Mac配置"><a href="#Mac配置" class="headerlink" title="Mac配置"></a>Mac配置</h3><p>用的是Mac电脑，所以点击相关链接。东西都挂在github上，下载对应的zip文件，下载完成后安装并运行起来。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235178-7ccd25cd-8b34-4103-8cc8-204e84672cfe.png#align=left&display=inline&height=748&originHeight=748&originWidth=1302&size=0&status=done&style=none&width=1302" alt><br><br><br>点击图标，进入 服务器设置<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400239120-45698c54-ac3b-48b9-9c4d-9f3537938f87.png#align=left&display=inline&height=926&originHeight=926&originWidth=1302&size=0&status=done&style=none&width=1302" alt><br><br><br>主要有四个地方要填，服务器的地址，端口号，加密方法，密码。服务器地址即为之前 Main controls选项中的IP地址。端口号、加密方法、密码必须与之前 Shadowsocks Server 中的信息一一匹配，否则会连接失败。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235629-2861fc8f-5bfc-4610-993f-49064774e693.png#align=left&display=inline&height=1156&originHeight=1156&originWidth=1292&size=0&status=done&style=none&width=1292" alt><br><br><br>设置完成后点击确定，然后服务器选择这个配置，默认选中PAC自动模式，确保Shadowsocks状态为On，这时候打开谷歌试试~<br>接着就可以上外网了 😂</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="工具" scheme="cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="shadowsock" scheme="cpeixin.cn/tags/shadowsock/"/>
    
  </entry>
  
  <entry>
    <title>Flink apply()&amp;process() 讲解</title>
    <link href="cpeixin.cn/2019/03/03/Flink-apply-process-%E8%AE%B2%E8%A7%A3/"/>
    <id>cpeixin.cn/2019/03/03/Flink-apply-process-%E8%AE%B2%E8%A7%A3/</id>
    <published>2019-03-02T17:01:15.000Z</published>
    <updated>2020-06-03T01:19:30.498Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p>在处理流数据计算时，我们在对流数据使用了keyby()和window()后，需要对分组后的数据做分组处理，那么除了对分组数据直接做reduce()，aggregate()等聚合操作之外，还有另一种场景就是对分组后的数据，每一个key对应的Iterable做稍微复杂一点的数据计算或者数据的整合或者变换。<br><br><br>那么这里我们就可以使用apply()或者process()来实现更底层的计算逻辑。那么两者之间有什么区别呢？我们来看一下两者的源码：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1591115550749-5e415429-c939-4be1-8ff1-b1d9a0b879b5.png#align=left&display=inline&height=1244&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-06-03%20%E4%B8%8A%E5%8D%8812.31.12.png&originHeight=1244&originWidth=1560&size=304238&status=done&style=none&width=1560" alt="屏幕快照 2020-06-03 上午12.31.12.png"><br>从上面的图片中，我们可以看到，两个方法的注释居然是一样的。内部的实现则不同，分别调用了ScalaProcessWindowFunctionWrapper和ScalaWindowFunctionWrapper<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1591116264932-148b3fe5-0efa-4fb9-a84b-e6c3e96c4eb6.png#align=left&display=inline&height=818&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-06-03%20%E4%B8%8A%E5%8D%8812.35.04.png&originHeight=818&originWidth=1524&size=208907&status=done&style=none&width=1524" alt="屏幕快照 2020-06-03 上午12.35.04.png"><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1591116172141-1502df20-ca88-4795-857d-bb7a192d2433.png#align=left&display=inline&height=986&margin=%5Bobject%20Object%5D&name=image.png&originHeight=986&originWidth=1430&size=240541&status=done&style=none&width=1430" alt="image.png"></p><p>我们来看ScalaProcessWindowFunctionWrapper，内部实现的process中，传入了context上下文，还有其他window相关有用的参数，从功能上来讲，process已经包含了apply能提供的功能，apply()是旧版的process()，并且没有提供上下文信息和其他window的高级功能，例如每个窗口的keyed state。apply()将在某个时间被弃用。process算子常见的有ProcessFunction和KeyedProcessFunction两种计算的方式，具体的实现可以看源码。</p><p>process和apply算子最大的区别在于process可以自己定时触发计算的定时器,在processElement方法定义定时器 context.timerService().registerEventTimeTimer(timestamp);<br><br><br>当定时器时间到达，会回调onTimer()方法的计算任务时器允许应用程序对processing time和event time的变化做出反应，每次对processElement()的调用都会得到一个Context对象，该对象允许访问元素事件时间的时间戳和TimeServer。TimeServer可以用来为尚未发生的event-time或者processing-time注册回调，当定时器的时间到达时，onTimer(…)方法会被调用。在这个调用期间，所有的状态都会限定到创建定时器的键，并允许定时器操纵键控状态(keyed states)。</p><p><strong>apply算子和process算子</strong><br>**<br>(1) window数据apply算子对应的fuction<br>WindowFunction(keyed window) 与 AllWindowFunction(no key window)<br><br><br>(2) window数据process算子对应的fuction<br>process算子可以获取到window窗口的Context信息 而 apply算子无法获取到<br>ProcessWindowFunction(keyed-window) 和 ProcessAllWindowFunction(no keyd window)<br>keyed-window 和 nokeyed-window<br><br><br>(3) 普通流数据，process算子,对应的fuction<br>KeyedProcessFunction： A keyed function that processes elements of a stream. （可实现定时任务）<br>ProcessFunction：A function that processes elements of a stream.（可实现定时任务）<br><br><br><br><br>下面我们用一个小实例来演示两种方法的使用<br><br><br>需求：求每个user，周一至周日，每天成绩的排序详情<br><br><br>数据格式：user001,1,95（用户ID，星期几，成绩）<br><br><br>程序如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> data_stream.function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.&#123;<span class="type">ProcessWindowFunction</span>, <span class="type">WindowFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>, <span class="type">WindowedStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.parallel.immutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">keyby_datastream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserGrade</span>(<span class="params">user_id: <span class="type">String</span>, weekday: <span class="type">String</span>, grade: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">UserGradeList</span>(<span class="params">user_id: <span class="type">String</span>, gradeList: <span class="type">List</span>[<span class="type">Int</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">8888</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> userGradeStream: <span class="type">DataStream</span>[<span class="type">UserGrade</span>] = socketStream.map((data: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> user_array: <span class="type">Array</span>[<span class="type">String</span>] = data.split(<span class="string">","</span>)</span><br><span class="line">      <span class="type">UserGrade</span>(user_array(<span class="number">0</span>), user_array(<span class="number">1</span>), user_array(<span class="number">2</span>).toInt)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyWondowStream: <span class="type">WindowedStream</span>[<span class="type">UserGrade</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] = userGradeStream.keyBy((_: <span class="type">UserGrade</span>).user_id)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    Base interface for functions that are evaluated over keyed (grouped) windows.</span></span><br><span class="line">    <span class="comment">//    trait WindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function with Serializable</span></span><br><span class="line"><span class="comment">//    val resultStream: DataStream[(String, List[Int])] = keyWondowStream.apply(new WindowFunction[UserGrade, (String, List[Int]), String, TimeWindow] &#123;</span></span><br><span class="line"><span class="comment">//      override def apply(key: String, window: TimeWindow, input: Iterable[UserGrade], out: Collector[(String, List[Int])]): Unit = &#123;</span></span><br><span class="line"><span class="comment">//        var gradeList: ListBuffer[Int] = ListBuffer[Int]()</span></span><br><span class="line"><span class="comment">//        for (data &lt;- input) &#123;</span></span><br><span class="line"><span class="comment">//          gradeList += data.grade</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//        out.collect((key,gradeList.toList.sorted))</span></span><br><span class="line"><span class="comment">//      &#125;</span></span><br><span class="line"><span class="comment">//    &#125;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        val resultStream: DataStream[String] = keyWondowStream.apply(new WindowFunction[UserGrade, String, String, TimeWindow] &#123;</span></span><br><span class="line"><span class="comment">//          override def apply(key: String, window: TimeWindow, input: Iterable[UserGrade], out: Collector[String]): Unit = &#123;</span></span><br><span class="line"><span class="comment">//            var gradeList: ListBuffer[Int] = ListBuffer[Int]()</span></span><br><span class="line"><span class="comment">//            for (data &lt;- input)&#123;</span></span><br><span class="line"><span class="comment">//              gradeList+=data.grade</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//            val result: String = key + "===" + gradeList.toList.sorted.toString</span></span><br><span class="line"><span class="comment">//            out.collect(result)</span></span><br><span class="line"><span class="comment">//          &#125;</span></span><br><span class="line"><span class="comment">//        &#125;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resultStream: <span class="type">DataStream</span>[<span class="type">UserGradeList</span>] = keyWondowStream.process(<span class="keyword">new</span> <span class="type">ProcessWindowFunction</span>[<span class="type">UserGrade</span>, <span class="type">UserGradeList</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>]&#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">UserGrade</span>], out: <span class="type">Collector</span>[<span class="type">UserGradeList</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">                <span class="keyword">var</span> gradeList: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="type">ListBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">                <span class="keyword">for</span> (data &lt;- elements) &#123;</span><br><span class="line">                  gradeList += data.grade</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                out.collect(<span class="type">UserGradeList</span>(key,gradeList.toList.sorted))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    resultStream.print(<span class="string">"==="</span>)</span><br><span class="line">    env.execute(<span class="string">"process apply function"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br>控制台输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">user001,1,95</span><br><span class="line">user001,2,200</span><br><span class="line">user001,3,97</span><br><span class="line">user001,4,189</span><br><span class="line">user001,5,99</span><br><span class="line">user001,6,100</span><br><span class="line">user001,7,40</span><br><span class="line">user002,1,1</span><br><span class="line">user002,2,5</span><br><span class="line">user002,3,2</span><br><span class="line">user002,4,88</span><br><span class="line">user002,5,4</span><br><span class="line">user002,6,33</span><br><span class="line">user002,7,22</span><br></pre></td></tr></table></figure><p><br>结果打印：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&#x3D;:3&gt; UserGradeList(user001,List(40, 95, 97, 99, 100, 189, 200))</span><br><span class="line">&#x3D;&#x3D;&#x3D;:2&gt; UserGradeList(user002,List(1, 2, 4, 5, 22, 33, 88))</span><br></pre></td></tr></table></figure><p><br>这里有一点需要注意，我在程序中使用了scala可变集合ListBuffer，程序执行后，则报错<strong>Caused by: java.lang.NumberFormatException: Not a version: 9</strong><br><strong><br></strong>后来Google得到，这是Flink版本和Scala版本之间引起的错误，这里要注意，原来的版本对应是Flink 1.10和Scala 2.11.8，后来Scala版本修改为2.11.12，这个问题就解决了。**<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在处理流数据计算时，我们在对流数据使用了keyby()和window()后，需要对分组后的数据做分组处理，那么除了对分组数据直接做reduce(
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="cpeixin.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Structured Streaming 重温</title>
    <link href="cpeixin.cn/2019/02/10/Structured-Streaming-%E9%87%8D%E6%B8%A9/"/>
    <id>cpeixin.cn/2019/02/10/Structured-Streaming-%E9%87%8D%E6%B8%A9/</id>
    <published>2019-02-10T08:03:44.000Z</published>
    <updated>2020-05-09T04:09:47.215Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --><p><a name="N4GjR"></a></p><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>_<br>Structured Streaming 则是在 Spark 2.0 加入的经过重新设计的全新流式引擎。它的模型十分简洁，易于理解。一个流的数据源从逻辑上来说就是一个不断增长的动态表格，随着时间的推移，新数据被持续不断地添加到表格的末尾。用户可以使用 Dataset/DataFrame 或者 SQL 来对这个动态数据源进行实时查询。每次查询在逻辑上就是对当前的表格内容执行一次 SQL 查询。如何执行查询则是由用户通过触发器（Trigger）来设定。用户既可以设定定期执行，也可以让查询尽可能快地执行，从而达到实时的效果。最后，系统通过 checkpointing 和 Write-Ahead Logs来确保端到端的一次容错保证。一个流的输出有多种模式，既可以是基于整个输入执行查询后的完整结果，也可以选择只输出与上次查询相比的差异，或者就是简单地追加最新的结果。这个模型对于熟悉 SQL 的用户来说很容易掌握，对流的查询跟查询一个表格几乎完全一样。<br><br><br>在内部，默认情况下，结构化流查询是使用_微批量处理_引擎_处理的_，该引擎将数据流作为一系列小批量作业进行处理，从而实现了低至100毫秒的端到端延迟以及 exactly-once的容错保证。但是，自Spark 2.3起，我们引入了一种称为“ <strong>连续处理”</strong>的新低延迟处理模式，该模式可以实现一次最少保证的低至1毫秒的端到端延迟。在不更改查询中的Dataset / DataFrame操作的情况下，您将能够根据应用程序需求选择模式<br><br><br></p><p><a name="YF5tx"></a></p><h2 id="Structured-Streaming-是对-Spark-Streaming-的改进么？"><a href="#Structured-Streaming-是对-Spark-Streaming-的改进么？" class="headerlink" title="Structured Streaming 是对 Spark Streaming 的改进么？"></a>Structured Streaming 是对 Spark Streaming 的改进么？</h2><p><br>Structured Streaming 并不是对 Spark Streaming 的简单改进，而是我们吸取了过去几年在开发 Spark SQL 和 Spark Streaming 过程中的经验教训，以及 Spark 社区和 Databricks 众多客户的反馈，重新开发的全新流式引擎，致力于为批处理和流处理提供统一的高性能 API。同时，在这个新的引擎中，我们也很容易实现之前在 Spark Streaming 中很难实现的一些功能，<strong>比如 Event Time 的支持</strong>，Stream-Stream Join，毫秒级延迟<br><br><br>类似于 Dataset/DataFrame 代替 Spark Core 的 RDD 成为为 Spark 用户编写批处理程序的首选，Dataset/DataFrame 也将替代 Spark Streaming 的 DStream，成为编写流处理程序的首选。<br></p><p><a name="HuvDe"></a></p><h2 id="Structured-Streaming-的-Spark-有什么优劣势吗？"><a href="#Structured-Streaming-的-Spark-有什么优劣势吗？" class="headerlink" title="Structured Streaming 的 Spark 有什么优劣势吗？"></a>Structured Streaming 的 Spark 有什么优劣势吗？</h2><ul><li>简洁的模型。Structured Streaming 的模型很简洁，易于理解。用户可以直接把一个流想象成是无限增长的表格。</li><li>一致的 API。由于和 Spark SQL 共用大部分 API，对 Spaprk SQL 熟悉的用户很容易上手，代码也十分简洁。同时批处理和流处理程序还可以共用代码，不需要开发两套不同的代码，显著提高了开发效率。</li><li>卓越的性能。Structured Streaming 在与 Spark SQL 共用 API 的同时，也直接使用了 Spark SQL 的 Catalyst 优化器和 Tungsten，数据处理性能十分出色。此外，Structured Streaming 还可以直接从未来 Spark SQL 的各种性能优化中受益。</li><li>多语言支持。Structured Streaming 直接支持目前 Spark SQL 支持的语言，包括 Scala，Java，Python，R 和 SQL。用户可以选择自己喜欢的语言进行开发。</li></ul><br><br>呃～～ 关于Structured Streaming的介绍就说到这里，如果想看更详细更准确的介绍呢，还是乖乖的去官网吧。在2017年10月份的时候，新立项的一个app用户行为实时项目，我有意使用Structured Streaming，所以就调研了一下，自己写了一个demo，我记忆里那时写Structured很别扭，就像一个模版一样，输入源和输出源都被规定好了函数和参数，并且在那时候测试后的时候，不怎么稳定，而且官方并没有给出成熟的版本，当时所测试的功能还都是 alpha 版本，所以当时就还是使用了Spark Streaming<p>不过现在来看，Structured Streaming 越来越成熟，Spark Streaming感觉似乎停止了更新。Structured streaming应该是Spark流处理的未来，但是很难替代Flink。Flink在流处理上的天然优势很难被Spark超越。<br><br><br>在读完了Structured Streaming官网后，还是亲手的写一些实例感受一下，以后做架构的时候，如果适合的话，还可以加进来。</p><p><a name="9BWc2"></a></p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><br><a name="TJ1U5"></a> ### complete, append, update<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">StreamingQuery</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.&#123;<span class="type">JSON</span>, <span class="type">JSONObject</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">structured_kafka</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger:<span class="type">Logger</span> = <span class="type">Logger</span>.getRootLogger</span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">kafka_format</span>(<span class="params">date_time: <span class="type">String</span>, keyword_list: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">spark</span></span>: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Structrued-Streaming"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafka_df: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(<span class="string">"kafka"</span>)</span><br><span class="line">      .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">      .option(<span class="string">"subscribe"</span>, <span class="string">"weibo_keyword"</span>)</span><br><span class="line">      .option(<span class="string">"startingOffsets"</span>, <span class="string">"earliest"</span>)</span><br><span class="line">      .option(<span class="string">"includeTimestamp"</span>, value = <span class="literal">true</span>)</span><br><span class="line"><span class="comment">//      .option("endingOffsets", "latest")</span></span><br><span class="line"><span class="comment">//      .option("startingOffsets", """&#123;"topic1":&#123;"0":23,"1":-2&#125;,"topic2":&#123;"0":-2&#125;&#125;""")</span></span><br><span class="line"><span class="comment">//      .option("endingOffsets", """&#123;"topic1":&#123;"0":50,"1":-1&#125;,"topic2":&#123;"0":-1&#125;&#125;""")</span></span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyvalue_df: <span class="type">DataFrame</span> = kafka_df</span><br><span class="line">      .selectExpr(<span class="string">"CAST(value AS STRING)"</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map((x: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> date_time: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"datetime"</span>)</span><br><span class="line">        <span class="keyword">val</span> keyword_list: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"keywordList"</span>)</span><br><span class="line">        (date_time, keyword_list)</span><br><span class="line">      &#125;)</span><br><span class="line">      .flatMap((x: (<span class="type">String</span>, <span class="type">String</span>)) =&gt;&#123;</span><br><span class="line">        x._2.split(<span class="string">","</span>).map((word: <span class="type">String</span>) =&gt;(x._1,word))</span><br><span class="line">      &#125;)</span><br><span class="line">      .toDF(<span class="string">"date_time"</span>, <span class="string">"keyword"</span>)</span><br><span class="line">      .groupBy(<span class="string">"keyword"</span>).count()</span><br><span class="line">      .orderBy($<span class="string">"count"</span>.desc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> query: <span class="type">StreamingQuery</span> = keyvalue_df.writeStream</span><br><span class="line">      .outputMode(<span class="string">"complete"</span>) <span class="comment">//append</span></span><br><span class="line">      .format(<span class="string">"console"</span>)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>其中需要注意：<br>_<br>_Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;_<br>_<br>Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;<br><br>未进行aggregate的stream不能sort<br>_<br>_ <a name="5prwS"></a> ### window窗口 _<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.&#123;<span class="type">StreamingQuery</span>, <span class="type">Trigger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">structured_kafka_window</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger:<span class="type">Logger</span> = <span class="type">Logger</span>.getRootLogger</span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">kafka_format</span>(<span class="params">date_time: <span class="type">String</span>, keyword_list: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">spark</span></span>: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Structrued-Streaming"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafka_df: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(<span class="string">"kafka"</span>)</span><br><span class="line">      .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">      .option(<span class="string">"subscribe"</span>, <span class="string">"weibo_keyword"</span>)</span><br><span class="line">      .option(<span class="string">"startingOffsets"</span>, <span class="string">"latest"</span>)</span><br><span class="line">      .option(<span class="string">"includeTimestamp"</span>, value = <span class="literal">true</span>)</span><br><span class="line"><span class="comment">//      .option("endingOffsets", "latest")</span></span><br><span class="line"><span class="comment">//      .option("startingOffsets", """&#123;"topic1":&#123;"0":23,"1":-2&#125;,"topic2":&#123;"0":-2&#125;&#125;""")</span></span><br><span class="line"><span class="comment">//      .option("endingOffsets", """&#123;"topic1":&#123;"0":50,"1":-1&#125;,"topic2":&#123;"0":-1&#125;&#125;""")</span></span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyvalue_df: <span class="type">DataFrame</span> = kafka_df</span><br><span class="line">      .selectExpr(<span class="string">"CAST(value AS STRING)"</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map((x: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> date_time: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"datetime"</span>)</span><br><span class="line">        <span class="keyword">val</span> keyword_list: <span class="type">String</span> = <span class="type">JSON</span>.parseObject(x).getString(<span class="string">"keywordList"</span>)</span><br><span class="line">        (date_time, keyword_list)</span><br><span class="line">      &#125;)</span><br><span class="line">      .flatMap((x: (<span class="type">String</span>, <span class="type">String</span>)) =&gt;&#123;</span><br><span class="line">        x._2.split(<span class="string">","</span>).map((word: <span class="type">String</span>) =&gt;(x._1,word))</span><br><span class="line">      &#125;)</span><br><span class="line">      .toDF(<span class="string">"date_time"</span>, <span class="string">"keyword"</span>)</span><br><span class="line">      .groupBy(window($<span class="string">"date_time"</span>, <span class="string">"5 minutes"</span>, <span class="string">"1 minutes"</span>),$<span class="string">"keyword"</span>)</span><br><span class="line">      .count()</span><br><span class="line">      .orderBy(<span class="string">"window"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> query: <span class="type">StreamingQuery</span> = keyvalue_df.writeStream</span><br><span class="line">      .outputMode(<span class="string">"complete"</span>) <span class="comment">//append</span></span><br><span class="line">      .format(<span class="string">"console"</span>)</span><br><span class="line">      .option(<span class="string">"truncate"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">"5 seconds"</span>))</span><br><span class="line">      .start()</span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>这里关于window窗口的划分，我建议大家好好的研读一下源码：<br>**位置：package **org.apache.spark.sql.catalyst.analysis<br>![屏幕快照 2020-05-08 下午11.29.50.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1588952005543-007c1291-12fd-4148-a625-587b1a6149f3.png#align=left&display=inline&height=1694&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-08%20%E4%B8%8B%E5%8D%8811.29.50.png&originHeight=1694&originWidth=1936&size=488433&status=done&style=none&width=1936)<br><p><a name="cuL0b"></a></p><h3 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基于event-time的window，words包含timestamp和word两列</span></span><br><span class="line">word</span><br><span class="line">  .withWatermark(<span class="string">"timestamp"</span>, <span class="string">"30 minutes"</span>)<span class="comment">//某窗口结果为x，但是部分数据在这个窗口的最后一个timestamp过后还没到达，Spark在这会等30min，过后就不再更新x了。</span></span><br><span class="line">  .dropDuplicates(<span class="string">"User"</span>, <span class="string">"timestamp"</span>)</span><br><span class="line">  .groupBy(window(col(<span class="string">"timestamp"</span>), <span class="string">"10 minutes"</span>),col(<span class="string">"User"</span>))<span class="comment">// 10min后再加一个参数变为Sliding windows，表示每隔多久计算一次。</span></span><br><span class="line">  .count() </span><br><span class="line">  .writeStream</span><br><span class="line">  .queryName(<span class="string">"events_per_window"</span>)</span><br><span class="line">  .format(<span class="string">"memory"</span>)</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM events_per_window"</span>)</span><br></pre></td></tr></table></figure><p>watermark = max event time seen by the engine - late threshold，相当于Flink的BoundedOutOfOrdernessTximestampExtractor。<br><br><br>在window计算被触发时，Spark会删除结束时间低于当前wm的window的中间结果，属于该window的迟到数据“可能”会被忽略，越迟越可能被忽略，删除完后才更新wm，所以即便下一批没有数据加入，Spark所依据的wm也是新的，下下一批wm不变。<br><br><br>上面是update mode，如果是append模式，那么结果要等到trigger后发现window的结束时间低于更新后的水位线时才会出来。另外，max event time seen by the engine - late threshold机制意味着如果下一批计算没有更晚的数据加入，那么wm就不会前进，那么数据的append就会被延后。<br><br><br>Conditions for watermarking to clean aggregation state(as of Spark 2.1.1, subject to change in the future)</p><ul><li>不支持complete模式。</li><li>groupBy必须包含timestamp列或者window(col(timestamp))，withWatermark中的列要和前面的timestamp列相同</li><li>顺序必须是先withWatermark再到groupBy</li></ul><p><br>参考：<br><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="external nofollow noopener noreferrer">结构化流编程指南</a><br><a href="https://www.cnblogs.com/code2one/p/9872355.html" target="_blank" rel="external nofollow noopener noreferrer">Spark之Structured Streaming</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 08 2020 00:58:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;N4GjR&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;总览&quot;&gt;&lt;a href=&quot;#总览&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
      <category term="大数据" scheme="cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="cpeixin.cn/tags/spark/"/>
    
  </entry>
  
</feed>
