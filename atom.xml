<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布兰特 | 不忘初心</title>
  
  <subtitle>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="cpeixin.cn/"/>
  <updated>2020-04-04T09:07:47.559Z</updated>
  <id>cpeixin.cn/</id>
  
  <author>
    <name>Brent</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Repositories</title>
    <link href="cpeixin.cn/2020/04/04/repository/"/>
    <id>cpeixin.cn/2020/04/04/repository/</id>
    <published>2020-04-04T08:33:44.365Z</published>
    <updated>2020-04-04T09:07:47.559Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="cpeixin.cn/2020/04/03/hello-world/"/>
    <id>cpeixin.cn/2020/04/03/hello-world/</id>
    <published>2020-04-03T14:53:03.460Z</published>
    <updated>2020-04-03T14:53:03.460Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external nofollow noopener noreferrer">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external nofollow noopener noreferrer">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external nofollow noopener noreferrer">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external nofollow noopener noreferrer">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external nofollow noopener noreferrer">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external nofollow noopener noreferrer">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="external nofollow noopener noreferrer">Deployment</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external nofo
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python Flask &amp; Ajax 数据传输</title>
    <link href="cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/"/>
    <id>cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/</id>
    <published>2020-03-11T14:43:01.000Z</published>
    <updated>2020-03-11T15:07:56.671Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂</p><p>忙活三个小时，终于把前端和后端打通了～～</p><p>前端demo：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"&#123;&#123; url_for('send_message') &#125;&#125;"</span> <span class="attr">method</span>=<span class="string">"post"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span> =<span class="string">"domain"</span> <span class="attr">rows</span>=<span class="string">"30"</span> <span class="attr">cols</span>=<span class="string">"100"</span> <span class="attr">placeholder</span>=<span class="string">"请输入需要查询的域名,如cq5999.com"</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;input id="submit" type="submit" value="发送"&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">id</span>=<span class="string">"btn-bq"</span> <span class="attr">data-toggle</span>=<span class="string">"modal"</span> <span class="attr">data-target</span>=<span class="string">"#myModal"</span>&gt;</span>查询<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"send_content"</span>&gt;</span>向后台发送消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"send_content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"send"</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"发送"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"recv_content"</span>&gt;</span>从后台接收消息：<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"recv_content"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"recv_content"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 发送 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> message = $(<span class="string">"#send_content"</span>).val()</span></span><br><span class="line">        alert(message)</span><br><span class="line"><span class="javascript">        $.ajax(&#123;</span></span><br><span class="line"><span class="actionscript">            url:<span class="string">"/send_message"</span>,</span></span><br><span class="line"><span class="actionscript">            type:<span class="string">"POST"</span>,</span></span><br><span class="line">            data:&#123;</span><br><span class="line">                message:message</span><br><span class="line">            &#125;,</span><br><span class="line"><span class="actionscript">            dataType: <span class="string">'json'</span>,</span></span><br><span class="line"><span class="actionscript">            success:<span class="function"><span class="keyword">function</span> <span class="params">(data)</span> </span>&#123;</span></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 接收 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="javascript">    $(<span class="string">"#send"</span>).click(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">        $.getJSON(<span class="string">"/change_to_json"</span>,<span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#recv_content"</span>).val(data.message) <span class="comment">//将后端数据显示在前端</span></span></span><br><span class="line"><span class="javascript">            <span class="built_in">console</span>.log(<span class="string">"传到前端的数据的类型："</span> + <span class="keyword">typeof</span> (data.message))</span></span><br><span class="line"><span class="javascript">            $(<span class="string">"#send_content"</span>).val(<span class="string">""</span>)<span class="comment">//发送的输入框清空</span></span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>后端demo:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template, request, jsonify</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"index_v6.html"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/send_message', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_message</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_get = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    message_get = request.form[<span class="string">"domain"</span>].split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># message_get = request.form['message'] #input提交</span></span><br><span class="line">    print(<span class="string">"收到前端发过来的信息：%s"</span> % message_get)</span><br><span class="line">    print(<span class="string">"收到数据的类型为："</span> + str(type(message_get)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"收到消息"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/change_to_json', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_to_json</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> message_get</span><br><span class="line">    message_json = &#123;</span><br><span class="line">        <span class="string">"message"</span>: message_get</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jsonify(message_json)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>,debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂&lt;/p&gt;&lt;p&gt;忙活三个小时，终于把前端和后端打通了～～&lt;/p&gt;
      
    
    </summary>
    
    
    
      <category term="python" scheme="cpeixin.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python Flask接口设计-示例</title>
    <link href="cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/"/>
    <id>cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/</id>
    <published>2020-03-10T15:08:35.000Z</published>
    <updated>2020-03-11T15:10:21.792Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p><a name="LHF1q"></a></p><h3 id="Get-请求"><a href="#Get-请求" class="headerlink" title="Get 请求"></a>Get 请求</h3><p><strong><strong>开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。</strong></strong><br><strong><br>**</strong>方法 1:****</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["GET"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><p>此种方式对应的request请求方式：</p><ol><li>拼接请求链接, 直接请求：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0/test_1.0?name=ccc&amp;age=18</a></li><li>request 请求中带有参数，如下图</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583826674613-bc99538a-988e-4386-b8e6-9eb9fce1862f.png#align=left&display=inline&height=610&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%883.47.43.png&originHeight=610&originWidth=1424&size=98593&status=done&style=none&width=1424" alt="屏幕快照 2020-03-10 下午3.47.43.png"></p><p>方法 2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">banWordSingleStart</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> getWordStatus(word)</span><br></pre></td></tr></table></figure><p>此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字</p><p>请求链接：<a href="http://0.0.0.0/test_1.0?name=ccc&age=18" target="_blank" rel="external nofollow noopener noreferrer">http://0.0.0.0</a>/api/banWordSingle/输入词</p><p><a name="vJdOc"></a></p><h3 id="Post-请求"><a href="#Post-请求" class="headerlink" title="Post 请求"></a>Post 请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test_1.0", methods=["POST"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 默认返回内容</span></span><br><span class="line">  return_dict = &#123;<span class="string">'return_code'</span>: <span class="string">'200'</span>, <span class="string">'return_info'</span>: <span class="string">'处理成功'</span>, <span class="string">'result'</span>: <span class="literal">False</span>&#125;</span><br><span class="line">  <span class="comment"># 判断入参是否为空</span></span><br><span class="line">  <span class="keyword">if</span> request.args <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    return_dict[<span class="string">'return_code'</span>] = <span class="string">'5004'</span></span><br><span class="line">    return_dict[<span class="string">'return_info'</span>] = <span class="string">'请求参数为空'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">  <span class="comment"># 获取传入的params参数</span></span><br><span class="line">  get_data = request.args.to_dict()</span><br><span class="line">  name = get_data.get(<span class="string">'name'</span>)</span><br><span class="line">  age = get_data.get(<span class="string">'age'</span>)</span><br><span class="line">  <span class="comment"># 对参数进行操作</span></span><br><span class="line">  return_dict[<span class="string">'result'</span>] = tt(name, age)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> json.dumps(return_dict, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tt</span><span class="params">(name, age)</span>:</span></span><br><span class="line">  result_str = <span class="string">"%s今年%s岁"</span> % (name, age)</span><br><span class="line">  <span class="keyword">return</span> result_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  app.run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">8080</span>)</span><br></pre></td></tr></table></figure><p>请求方式：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583831085097-3a858ae4-259d-408d-a162-6a4ed8c5e291.png#align=left&display=inline&height=692&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%885.00.28.png&originHeight=692&originWidth=1438&size=99272&status=done&style=none&width=1438" alt="屏幕快照 2020-03-10 下午5.00.28.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;LHF1q&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;Get-请求&quot;&gt;&lt;a href=&quot;#Get-请求&quot; class=&quot;headerl
      
    
    </summary>
    
    
    
      <category term="python" scheme="cpeixin.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>IDEA install TabNine</title>
    <link href="cpeixin.cn/2020/01/22/IDEA-install-TabNine/"/>
    <id>cpeixin.cn/2020/01/22/IDEA-install-TabNine/</id>
    <published>2020-01-22T02:26:15.000Z</published>
    <updated>2020-03-28T13:31:45.816Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>TabNine是我目前遇到过最好的智能补全工具</p><p>TabNine基于GPT-2的插件</p><p>安装<br>IDEA编译器，找到plugins</p><p>Windows pycharm：File&gt;settings&gt;plugins;<br>Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins</p><p>查找 TabNine, 点击 install, 随后 restart</p><p>重启后：Help&gt;Edit Custom Properties…&gt;Create;</p><p>在跳出来的idea.properties中输入（注：英文字符） TabNine::config</p><p>随即会自动弹出TabNine激活页面；</p><p>激活<br>点击Activation Key下面的here；</p><p>输入你的邮箱号；</p><p>复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）</p><p>等待自动安装，观察页面（最下面有log可以看当前进度）；</p><p>激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；</p><p>确认激活完成，重启pycharm即可；</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;TabNine是我目前遇到过最好的智能补全工具&lt;/p&gt;&lt;p&gt;TabNine基于GPT-2的插件&lt;/p&gt;&lt;p&gt;安装&lt;br&gt;IDEA编译器，找到pl
      
    
    </summary>
    
    
    
      <category term="TabNine" scheme="cpeixin.cn/tags/TabNine/"/>
    
  </entry>
  
  <entry>
    <title>架构思想</title>
    <link href="cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/"/>
    <id>cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/</id>
    <published>2019-12-20T02:26:15.000Z</published>
    <updated>2020-03-28T14:23:45.848Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h2><a href="#" class="headerlink"></a></h2><p>关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。<br><br><br>从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。<br><br><br>具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。<br><br><br>架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。<br><a name="-3"></a></p><h4 id="架构思想"><a href="#架构思想" class="headerlink" title="架构思想"></a>架构思想</h4><p>CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度</p><ul><li><p>满足业务发展需求是最高准则</p></li><li><p>业务建模，抽象和枚举是两种方式，需要平衡，不能走极端</p></li><li><p>模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计</p></li><li><p>基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。</p></li><li><p>分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程</p></li><li><p>分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中</p></li><li><p>不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术</p></li><li><p>好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难</p></li><li><p>业务架构、系统架构、数据模型<br><a name="-4"></a></p><h4 id="面对一块新业务，如何系统架构？"><a href="#面对一块新业务，如何系统架构？" class="headerlink" title="面对一块新业务，如何系统架构？"></a>面对一块新业务，如何系统架构？</h4></li><li><p>业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。</p></li><li><p>系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？</p></li><li><p>系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？</p></li><li><p>模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。<br><a name="-5"></a></p><h4 id="如何模块化"><a href="#如何模块化" class="headerlink" title="如何模块化"></a>如何模块化</h4></li><li><p>基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。</p></li><li><p>基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。</p></li><li><p>基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。</p></li><li><p>基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。<br><a name="-6"></a></p><h4 id="提升系统的稳定性"><a href="#提升系统的稳定性" class="headerlink" title="提升系统的稳定性"></a>提升系统的稳定性</h4></li><li><p>流控</p></li></ul><p>双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。<br>另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。</p><ul><li>容灾</li></ul><p>最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。<br>另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。</p><ul><li>稳定性</li></ul><p>第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。</p><ul><li>容量规划</li></ul><p>做好容量规划、系统间强弱依赖关系梳理。<br>如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。<br><a name="-7"></a></p><h4 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h4><ul><li><p>分布式系统</p></li><li><p>分布式缓存</p></li><li><p>分布式数据<br><a name="api"></a></p><h4 id="API-和乐高积木有什么相似之处？"><a href="#API-和乐高积木有什么相似之处？" class="headerlink" title="API 和乐高积木有什么相似之处？"></a>API 和乐高积木有什么相似之处？</h4><p>相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。<br>对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。<br>你可以发现很多 API 和乐高积木的相似之处：</p></li><li><p>标准化：通用、标准化的组件，作为基本的构建块（building blocks）；<br></p></li><li><p>可用性：强调可用性，附有文档或使用说明；<br></p></li><li><p>可定制：为不同功能使用不同的API；<br></p></li><li><p>创造性：能够组合不同的 API 来创造混搭的结果；</p></li></ul><p><br>乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。<br>虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。<br><br><br>让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;关于什么
      
    
    </summary>
    
    
    
      <category term="架构" scheme="cpeixin.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>kali中文设置</title>
    <link href="cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/"/>
    <id>cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/</id>
    <published>2019-12-01T02:26:15.000Z</published>
    <updated>2020-03-28T13:28:43.471Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>更新源</p><p><a href="https://blog.csdn.net/qq_38333291/article/details/89764967" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_38333291/article/details/89764967</a></p><p>设置编码和中文字体安装</p><p><a href="http://www.linuxdiyf.com/linux/20701.html" target="_blank" rel="external nofollow noopener noreferrer">http://www.linuxdiyf.com/linux/20701.html</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;更新源&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_38333291/article/details/897
      
    
    </summary>
    
    
    
      <category term="kali" scheme="cpeixin.cn/tags/kali/"/>
    
  </entry>
  
  <entry>
    <title>分布式下的数据hash分布</title>
    <link href="cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/"/>
    <id>cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/</id>
    <published>2019-11-19T15:05:08.000Z</published>
    <updated>2019-11-19T15:05:08.402Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了（二）</title>
    <link href="cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2019-09-09T02:26:15.000Z</published>
    <updated>2020-03-28T14:12:10.008Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。<br><br><br>由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。<br>这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><br>映入眼帘的是满屏的扫描和ssh尝试登陆<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">54</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">314</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">lines <span class="number">1105</span><span class="number">-1127</span>/<span class="number">1127</span> (END)</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">49</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Failed password <span class="keyword">for</span> invalid user admin <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Received disconnect <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">303</span>]: Disconnected <span class="keyword">from</span> <span class="number">117.132</span><span class="number">.175</span><span class="number">.25</span> port <span class="number">42972</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Failed password <span class="keyword">for</span> invalid user ansible <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Received disconnect <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span>:<span class="number">11</span>: Bye Bye [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">302</span>]: Disconnected <span class="keyword">from</span> <span class="number">149.56</span><span class="number">.96</span><span class="number">.78</span> port <span class="number">44980</span> [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">50</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span> port <span class="number">45157</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">51</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65522</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_unix(sshd:auth): authentication failure; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">218.92</span><span class="number">.0</span><span class="number">.163</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">52</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">310</span>]: pam_succeed_if(sshd:auth): requirement <span class="string">"uid &gt;= 1000"</span> <span class="keyword">not</span> met by user <span class="string">"root"</span></span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Failed password <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: error: maximum authentication attempts exceeded <span class="keyword">for</span> root <span class="keyword">from</span> <span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span> port <span class="number">24184</span> ssh2 [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: Disconnecting: Too many authentication failures [preauth]</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM <span class="number">5</span> more authentication failures; logname= uid=<span class="number">0</span> euid=<span class="number">0</span> tty=ssh ruser= rhost=<span class="number">49.88</span><span class="number">.112</span><span class="number">.54</span>  user=root</span><br><span class="line">Sep <span class="number">09</span> <span class="number">11</span>:<span class="number">02</span>:<span class="number">53</span> <span class="number">4</span>Z-J16-A47 sshd[<span class="number">65525</span>]: PAM service(sshd) ignoring max retries; <span class="number">6</span> &gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><p><br>看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护<br>对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@*** ~]<span class="comment"># useradd one             #创建用户</span></span><br><span class="line">[root@*** ~]<span class="comment"># passwd one              #设置密码</span></span><br></pre></td></tr></table></figure><p><br>输入新用户密码<br>首先确保文件 /etc/sudoers 中<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%wheel    ALL=(ALL)    ALL</span><br><span class="line">```  </span><br><span class="line">没有被注释</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```linux</span><br><span class="line">usermod -g wheel onerocket</span><br></pre></td></tr></table></figure><p><br>设置只有指定用户组才能使用su命令切换到root用户<br><br><br>在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"><span class="comment"># Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限</span></span><br><span class="line">sed -i <span class="string">'/pam_wheel.so use_uid/c\auth            required        pam_wheel.so use_uid '</span> /etc/pam.d/su</span><br><span class="line">n=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`</span><br><span class="line"><span class="keyword">if</span> [ $n -eq <span class="number">0</span> ];then</span><br><span class="line">echo SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defs</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><br>打开SSHD的配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。<br><br><br>修改sshd默认端口<br>虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。<br>打开sshd配置文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><br>找到#Port 22 删掉注释<br><br><br><em>服务器端口最大可以开到65536</em><br><br><br>同时再添加一个Port 61024 （随意设置）<br><br><br>Port 22<br>Port 61024<br><br><br>重启sshd服务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service sshd restart      <span class="comment">#centos6系列</span></span><br><span class="line">systemctl restart sshd  <span class="comment">#centos7系列</span></span><br><span class="line">firewall-cmd --add-port=<span class="number">61024</span>/tcp</span><br></pre></td></tr></table></figure><p><br>测试，使用新用户，新端口进行登录<br><br><br>如果登陆成功后，再将Port22注释掉，重启sshd服务。<br>到这里，关于远程登录的防护工作，就做好了。<br>最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="cpeixin.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>我的服务器被黑了</title>
    <link href="cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/"/>
    <id>cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/</id>
    <published>2019-08-24T02:26:15.000Z</published>
    <updated>2020-03-28T14:57:34.282Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p><a name="-2"></a></p><h1 id="服务器自述"><a href="#服务器自述" class="headerlink" title="服务器自述"></a>服务器自述</h1><p>我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。<br><br><br>而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢<br><a name="-3"></a></p><h1 id="为什么黑我的服务器"><a href="#为什么黑我的服务器" class="headerlink" title="为什么黑我的服务器"></a>为什么黑我的服务器</h1><p>这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。<br><br><br>但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。<br><br><br>那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…<br><br><br>那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404954445-f30a25a0-5939-4773-b5d9-8bb6a7c53b02.png#align=left&display=inline&height=892&name=1.png&originHeight=892&originWidth=1244&size=1778564&status=done&style=none&width=1244" alt="1.png"><br><br><br>曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器<br><a name="-4"></a></p><h1 id="如何发现被黑"><a href="#如何发现被黑" class="headerlink" title="如何发现被黑"></a>如何发现被黑</h1><p>回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？<br><br><br>最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作<br><br><br>1.关闭了Linux防火墙<br><br><br>2.没有设置redis访问密码<br><br><br>3.没有更改redis默认端口<br><br><br>4.开放了任意IP可以远程连接<br><br><br>以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。<br><br><br>当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。<br><br><br>但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i tcp:<span class="number">6379</span></span><br></pre></td></tr></table></figure><p><br>使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。<br><br><br>但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想<br>直到我运行：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585404912911-97736d80-2ee3-455d-a948-6d134f4e2663.png#align=left&display=inline&height=534&name=2.png&originHeight=534&originWidth=3338&size=980508&status=done&style=none&width=3338" alt="2.png"><br>发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了<br><a name="-5"></a></p><h1 id="做了哪些急救工作"><a href="#做了哪些急救工作" class="headerlink" title="做了哪些急救工作"></a>做了哪些急救工作</h1><p>这时，感觉自己开始投入了一场对抗战<br><br><br>1.首先查找植入程序的位置。<br>在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。<br><br><br>2.删除清理可疑文件<br><br><br>杀死进程<br><br><br>删除了正在运行的程序文件还有安装包<br>3.查看所有用户的定时任务<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd |cut -f <span class="number">1</span> -d:crontab -uXXX -l</span><br></pre></td></tr></table></figure><p><br>4.开启防火墙<br><br><br>仅开放会使用到的端口<br>5.修改redis默认端口<br><br><br>redis.conf中的port<br>6.添加redis授权密码<br><br><br>redis.conf中的requirepass<br>7.修改绑定远程绑定ip<br><br><br>redis.conf中的bind<br>最后重启redis服务！<br><a name="-6"></a></p><h1 id="从中学到了什么"><a href="#从中学到了什么" class="headerlink" title="从中学到了什么"></a>从中学到了什么</h1><p>明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～<br>最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;服务器自述&quot;&gt;&lt;a href=&quot;#服务器自述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="cpeixin.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫 - 动态爬取</title>
    <link href="cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
    <id>cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</id>
    <published>2019-06-12T15:26:15.000Z</published>
    <updated>2020-03-28T13:24:07.047Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。<br><br><br>到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。<br><br><br>那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。<br><br><br>那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843101-98e57df8-c124-4923-aa9f-7c42ce2288b6.png#align=left&display=inline&height=1300&originHeight=1300&originWidth=3356&size=0&status=done&style=none&width=3356" alt><br><br><br>注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843056-e12f0f79-905f-4420-abfd-fb85625767ac.png#align=left&display=inline&height=1150&originHeight=1150&originWidth=2266&size=0&status=done&style=none&width=2266" alt><br><br><br>点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：<br>Headers：请求和响应的详细信息<br>Preview：响应体格式化之后的显示<br>Response：响应体原始内容<br>Cookies：Cookies<br>Timing：时间开销<br><br><br>通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401842263-072689cf-5dea-4f8b-b103-0758d9c5e52e.png#align=left&display=inline&height=262&originHeight=262&originWidth=1276&size=0&status=done&style=none&width=1276" alt><br><br><br>上面的截图展示了这次请求的请求方式、请求地址等信息。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843887-cf8bff5f-4570-4ad4-8681-f0f3eae929b3.png#align=left&display=inline&height=884&originHeight=884&originWidth=2652&size=0&status=done&style=none&width=2652" alt><br><br><br>上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585401843009-4797a28d-3070-421e-a45c-86781d7f580d.png#align=left&display=inline&height=188&originHeight=188&originWidth=848&size=0&status=done&style=none&width=848" alt><br><br><br>上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。<br><br><br>那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。<br><br><br>我们回过头重新观察一下返回的json，格式化之后的层级关系如下：<br><br><br>很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">json_obj = json.loads(<span class="string">"&#123;'key': 'value'&#125;"</span>)  <span class="comment"># 字符串到对象</span></span><br><span class="line">json_str = json.dumps(json_obj)            <span class="comment"># 对象到字符串</span></span><br></pre></td></tr></table></figure><p><br>json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会
      
    
    </summary>
    
    
    
      <category term="python" scheme="cpeixin.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>shadowsock-vps搭建VPN</title>
    <link href="cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/"/>
    <id>cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/</id>
    <published>2019-04-19T15:26:15.000Z</published>
    <updated>2020-03-28T13:18:01.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p><a name="-1"></a></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>还有10天左右就要回国了，由于职业的需要，对Google的依赖的越来越大的，那么回国后怎么才能‘科学上网’呢？之前在国内的时候，有使用过Lantern，稳定性和速度都还是不错了，可惜后来被和谐了。所以今天准备尝试搭建VPN，自己独立使用，一边搭建一边将过程记录下来。<br><a name="-2"></a></p><h1 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1><p>VPS: VPS（Virtual Private Server 虚拟专用服务器）技术，将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器 [1] 技术，和虚拟化技术 [2] 。在容器或虚拟机中，每个VPS都可分配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。<br><br><br>VPS为使用者提供了管理配置的自由，可用于企业虚拟化，也可以用于IDC资源租用。<br><br><br>VPN: VPN的学名叫虚拟专用网，洋文叫“Virtual Private Network”。维基百科的介绍在“这里”。本来这玩意儿主要是用于商业公司，为了让那些不在公司里的员工（比如出差在外的）能够方便地访问公司的内部网络。为了防止黑客冒充公司的员工，从外部访问公司的内部网络，VPN 软件都会提供强大的加密功能。而这个加密功能，也就让它顺便成为翻墙的利器。<br><a name="-3"></a></p><h1 id="科学上网原理"><a href="#科学上网原理" class="headerlink" title="科学上网原理"></a>科学上网原理</h1><p>VPN浏览外网的原理<br><br><br>使用 VPN 通常需要先安装客户端软件。当你运行 VPN 客户端，它会尝试联到 VPN 服务器（这点跟加密代理类似）。一旦和 VPN 服务器建立连接，VPN 客户端就会在你的系统中建立了一个虚拟局域网。而且，你的系统中也会多出一个虚拟网卡（在 Windows 下，可以用 ipconfig /all 命令，看到这多出来的网卡）。这样一来，你的系统中就有不止一块网卡。这就引出一个问题：那些访问网络的程序，它的数据流应该通过哪个网卡进出？<br>为了解决此问题，VPN 客户端通常会修改你系统的路由表，让那些数据流，优先从虚拟的网卡进出。由于虚拟的网卡是通往 VPN 服务器的，当数据流到达 VPN 服务器之后，VPN 服务器再帮你把数据流转向到真正的目的地。<br><br><br>前面说了，VPN 为了保证安全，都采用强加密的方式传输数据。这样一来，GFW 就无法分析你的网络数据流，进行敏感词过滤。所以，使用墙外的VPN服务器，无形中就能达到翻墙的效果。<br><a name="-4"></a></p><h1 id="方案选择"><a href="#方案选择" class="headerlink" title="方案选择"></a>方案选择</h1><p>VPN是一个大类，其中有很多实现的方法，防火长城现在将 VPN 屏蔽的已经所剩无几，后来大家看到了SSH，使用SSH的sock5很稳定，但是特征也十分明显，防火长城可以对其直接进行定向干扰。<br><br><br>而除了VPN，对于翻墙大家仍然有很多方法，比如Shadowsocks 、Lantern、VPNGate 等等，而实际上无论哪种方式，他们本身都需要一台服务器作为中间人进行消息传递。而VPS虚拟专用服务器就十分适合担当这个角色，并且由于VPS平时就作为商品在各类云服务器平台上售卖，自行购买并搭建相当方便，唯一需要的就是人们对于服务器的操作技术。<br><br><br><strong>而这次选择的方案是：VPS+Shadowsocks</strong><br>**<br>Shadowsocks特点：<br><br><br>省电，在电量查看里几乎看不到它的身影；<br><br><br>支持开机自启动，且断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴；<br><br><br>可使用自己的服务器，安全和速度的保证；<br><br><br>支持区分国内外流量，传统VPN在翻出墙外后访问国内站点会变慢；<br><br><br>可对应用设置单独代理，5.0之后的系统无需root。<br><br><br>Shadowsocks 目前不容易被封杀主要是因为：<br>建立在socks5协议之上，socks5是运用很广泛的协议，所以没办法直接封杀socks5协议<br>使用socks5协议建立连接，而没有使用VPN中的服务端身份验证和密钥协商过程。而是在服务端和客户端直接写死密钥和加密算法。所以防火墙很难找到明显的特征，因为这就是个普通的socks5协议。<br><br><br>Shadowsock搭建也比较简单，所以很多人自己架设VPS搭建，个人使用流量也很小，没法通过流量监控方式封杀。<br>自定义加密方式和密钥。因为加密主要主要是防止被检测，所以要选择安全系数高的加密方式。之前RC4会很容易被破解，而导致被封杀。所以现在推荐使用AES加密。而在客户端和服务端自定义密钥，泄露的风险相对较小。<br>所以如果是自己搭建的Shadosocks被封的概率很小，但是如果是第三方的Shadeowsocks，密码是server定的，你的数据很可能遭受到中间人攻击。<br><a name="-5"></a></p><h1 id="开工"><a href="#开工" class="headerlink" title="开工"></a>开工</h1><p><a name="vps"></a></p><h2 id="购买vps"><a href="#购买vps" class="headerlink" title="购买vps"></a>购买vps</h2><p>首先我们需要购买一台境外的服务器，接着我们在这台云服务器里面安装代理服务，那么以后我们上网的时候就可以通过它来中转，轻松畅快的畅游全网了。<br>购买VPS,我选择了<a href="https://www.vultr.com/" target="_blank" rel="external nofollow noopener noreferrer">vultr</a>，大家用过都说好，购买的过程也很方便。<br><br><br>第一步：选择离中国较近国家的服务器。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400540784-be727007-a613-4b5b-a3d5-9b5fbf9734a7.png#align=left&display=inline&height=1468&name=step1.png&originHeight=1468&originWidth=3066&size=659813&status=done&style=none&width=3066" alt="step1.png"><br><br><br><br><br>第二步：选择服务器配置和系统<br><br><br>这里，系统选择的是CentOS 7,配置的话，如果只是自己浏览网页的话，选择最低配置就好。其他的选项可以略过。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400556831-e2446e46-8c7e-4292-97a8-b89e6d983c9c.png#align=left&display=inline&height=1594&name=step2.png&originHeight=1594&originWidth=2792&size=669959&status=done&style=none&width=2792" alt="step2.png"><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235703-e6afdea2-828f-42a0-a955-67960c8710bc.png#align=left&display=inline&height=1812&originHeight=1812&originWidth=2600&size=0&status=done&style=none&width=2600" alt><br><br><br>第三步：支付和部署<br><br><br>支付可以选择支付宝支付，非常方便。购买成功后，点击Server中的“+”号，来部署你刚刚选择的服务器。<br>第四步：登陆服务器<br><br><br>查看服务器详情 Server Details,根据提供的服务器信息，登陆服务器。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235150-95a87ca7-8392-404e-a5d9-c4a606b8ae7e.png#align=left&display=inline&height=1166&originHeight=1166&originWidth=2728&size=0&status=done&style=none&width=2728" alt><br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235639-e36e0afa-200d-49ad-a324-350004d08d09.png#align=left&display=inline&height=1232&originHeight=1232&originWidth=2612&size=0&status=done&style=none&width=2612" alt><br><br><br>我是使用Mac本身终端ssh到服务器上的，因为Mac上多数的SSH客户端要么收费，要么不好用，要么安装过程非常繁琐。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p <span class="number">22</span> root@ip</span><br></pre></td></tr></table></figure><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235316-408c45f7-df95-4c8c-9428-9f476c6c7190.png#align=left&display=inline&height=244&originHeight=244&originWidth=1304&size=0&status=done&style=none&width=1304" alt><br><a name="shadowsocks"></a></p><h2 id="搭建shadowsocks服务器"><a href="#搭建shadowsocks服务器" class="headerlink" title="搭建shadowsocks服务器"></a>搭建shadowsocks服务器</h2><p>连接到你的 vultr 服务器之后，接下来就可以使用几个命令让你快速搭建一个属于自己的 ss 服务器：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install wget</span><br></pre></td></tr></table></figure><p><br>接着执行安装shadowsocks：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span><br></pre></td></tr></table></figure><p><br>获取 <a href="http://shadowsocks.sh/" target="_blank" rel="external nofollow noopener noreferrer">shadowsocks.sh</a> 读取权限：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x shadowsocks.sh</span><br></pre></td></tr></table></figure><p><br>设置你的 ss 密码和端口号：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./shadowsocks.sh <span class="number">2</span>&gt;&amp;<span class="number">1</span> | tee shadowsocks.log</span><br></pre></td></tr></table></figure><p><br>接下来后就可以设置密码和端口号了<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400236071-267db22c-ea03-4f8b-969d-86ae5efc4d68.png#align=left&display=inline&height=306&originHeight=306&originWidth=1086&size=0&status=done&style=none&width=1086" alt><br><br><br>密码和端口号可以使用默认的，也可以直接重新输入新的。<br>选择加密方式<br><br><br>设置完密码和端口号之后，我们选择加密方式，这里选择 7 ，使用aes-256-cfb的加密模式<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235435-525e1564-eb61-47e0-b9b1-a28683979702.png#align=left&display=inline&height=684&originHeight=684&originWidth=914&size=0&status=done&style=none&width=914" alt><br><br><br>接着按任意键进行安装。<br>安装ss完成后<br><br><br>会给你显示你需要连接 vpn 的信息：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400239145-4d70810e-bc73-4cfa-bc8c-4da35e3e0dcf.png#align=left&display=inline&height=464&originHeight=464&originWidth=1186&size=0&status=done&style=none&width=1186" alt><br>搞定，将这些信息保存起来，那么这时候你就可以使用它们来科学上网啦。<br><a name="bbr"></a></p><h2 id="使用BBR加速上网"><a href="#使用BBR加速上网" class="headerlink" title="使用BBR加速上网"></a>使用BBR加速上网</h2><p>安装 BBR<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh</span><br></pre></td></tr></table></figure><p><br>获取读写权限<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x bbr.sh</span><br></pre></td></tr></table></figure><p><br>启动BBR安装<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bbr.sh</span><br></pre></td></tr></table></figure><p><br>接着按任意键，开始安装，坐等一会。安装完成一会之后它会提示我们是否重新启动vps，我们输入 y 确定重启服务器。<br>重新启动之后，输入：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep bbr</span><br></pre></td></tr></table></figure><p><br>如果看到 tcp_bbr 就说明 BBR 已经启动了。<br><a name="-6"></a></p><h2 id="客户端进行连接"><a href="#客户端进行连接" class="headerlink" title="客户端进行连接"></a>客户端进行连接</h2><p><a name="windowsshadowsocks"></a></p><h3 id="windows使用Shadowsocks"><a href="#windows使用Shadowsocks" class="headerlink" title="windows使用Shadowsocks"></a>windows使用Shadowsocks</h3><p>windows点击下载：<a href="https://pan.baidu.com/s/19m0AfTkPDSRj0bfYrGpbIg" target="_blank" rel="external nofollow noopener noreferrer">Shadowsocks windows客户端</a><br>打开 Shadowsocks 客户端，输入ip地址，密码，端口，和加密方式。接着点击确定，右下角会有个小飞机按钮，右键–&gt;启动代理。<br><a name="androidshadowsocks"></a></p><h3 id="Android使用Shadowsocks"><a href="#Android使用Shadowsocks" class="headerlink" title="Android使用Shadowsocks"></a>Android使用Shadowsocks</h3><p>Android点击下载：<a href="https://pan.baidu.com/s/1coAkZn-GuYHu5eIKaHECxA" target="_blank" rel="external nofollow noopener noreferrer">Shadowsocks Android客户端</a><br>打开apk安装，接着打开APP，输入ip地址，密码，端口，和加密方式。即可科学上网。<br><a name="iphoneshadowsocks"></a></p><h3 id="iPhone使用Shadowsocks"><a href="#iPhone使用Shadowsocks" class="headerlink" title="iPhone使用Shadowsocks"></a>iPhone使用Shadowsocks</h3><p>iPhone要下载的app需要在appstore下载，但是需要用美区账号才能下载，而且这个APP需要钱。在这里提供一种解决方案，就是可以再搭建一个<a href="https://wistbean.github.io/ipsec,l2tp_vpn.html#%E4%BD%BF%E7%94%A8-IPsec-L2TP-%E8%84%9A%E6%9C%AC%E6%90%AD%E5%BB%BA" target="_blank" rel="external nofollow noopener noreferrer">IPsec/L2TP</a> VPN,专门给你的iPhone使用。<br><a name="mac"></a></p><h3 id="Mac配置"><a href="#Mac配置" class="headerlink" title="Mac配置"></a>Mac配置</h3><p>用的是Mac电脑，所以点击相关链接。东西都挂在github上，下载对应的zip文件，下载完成后安装并运行起来。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235178-7ccd25cd-8b34-4103-8cc8-204e84672cfe.png#align=left&display=inline&height=748&originHeight=748&originWidth=1302&size=0&status=done&style=none&width=1302" alt><br><br><br>点击图标，进入 服务器设置<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400239120-45698c54-ac3b-48b9-9c4d-9f3537938f87.png#align=left&display=inline&height=926&originHeight=926&originWidth=1302&size=0&status=done&style=none&width=1302" alt><br><br><br>主要有四个地方要填，服务器的地址，端口号，加密方法，密码。服务器地址即为之前 Main controls选项中的IP地址。端口号、加密方法、密码必须与之前 Shadowsocks Server 中的信息一一匹配，否则会连接失败。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585400235629-2861fc8f-5bfc-4610-993f-49064774e693.png#align=left&display=inline&height=1156&originHeight=1156&originWidth=1292&size=0&status=done&style=none&width=1292" alt><br><br><br>设置完成后点击确定，然后服务器选择这个配置，默认选中PAC自动模式，确保Shadowsocks状态为On，这时候打开谷歌试试~<br>接着就可以上外网了 😂</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
    
      <category term="shadowsock" scheme="cpeixin.cn/tags/shadowsock/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-K-Means_1</title>
    <link href="cpeixin.cn/2019/02/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-K-Means-1/"/>
    <id>cpeixin.cn/2019/02/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-K-Means-1/</id>
    <published>2019-02-01T15:26:13.000Z</published>
    <updated>2020-03-27T15:30:41.268Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，你可以理解这个算法的本质是确定 K 类的中心点，当你找到了这些中心点，也就完成了聚类。<br><br><br>那么请你和我思考以下三个问题：</p><ul><li>如何确定 K 类的中心点？</li><li>如何将其他点划分到 K 类中？</li><li>如何区分 K-Means 与 KNN？</li></ul><p><br>如果理解了上面这 3 个问题，那么对 K-Means 的原理掌握得也就差不多了。先请你和我思考一个场景，假设我有 20 支亚洲足球队，想要将它们按照成绩划分成 3 个等级，可以怎样划分？<br></p><p><a name="tdJch"></a></p><h3 id="K-Means-的工作原理"><a href="#K-Means-的工作原理" class="headerlink" title="K-Means 的工作原理"></a>K-Means 的工作原理</h3><p><br>对亚洲足球队的水平，你可能也有自己的判断。比如一流的亚洲球队有谁？你可能会说伊朗或韩国。二流的亚洲球队呢？你可能说是中国。三流的亚洲球队呢？你可能会说越南。其实这些都是靠我们的经验来划分的，那么伊朗、中国、越南可以说是三个等级的典型代表，也就是我们每个类的中心点。<br><br><br>所以回过头来，如何确定 K 类的中心点？一开始我们是可以随机指派的，当你确认了中心点后，就可以按照距离将其他足球队划分到不同的类别中。这也就是 K-Means 的中心思想，就是这么简单直接。<br><br><br>你可能会问：如果一开始，选择一流球队是中国，二流球队是伊朗，三流球队是韩国，中心点选择错了怎么办？其实不用担心，K-Means 有自我纠正机制，在不断的迭代过程中，会纠正中心点。中心点在整个迭代过程中，并不是唯一的，只是你需要一个初始值，一般算法会随机设置初始的中心点。好了，那我来把 K-Means 的工作原理给你总结下：<br></p><ol><li>选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；</li><li>将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点；</li><li>重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。</li></ol><p><a name="WeqHb"></a></p><h3 id="如何给亚洲球队做聚类"><a href="#如何给亚洲球队做聚类" class="headerlink" title="如何给亚洲球队做聚类"></a>如何给亚洲球队做聚类</h3><p><br>对于机器来说需要数据才能判断类中心点，所以我整理了 2015-2019 年亚洲球队的排名，如下表所示。我来说明一下数据概况。其中 2019 年国际足联的世界排名，2015 年亚洲杯排名均为实际排名。2018 年世界杯中，很多球队没有进入到决赛圈，所以只有进入到决赛圈的球队才有实际的排名。如果是亚洲区预选赛 12 强的球队，排名会设置为 40。如果没有进入亚洲区预选赛 12 强，球队排名会设置为 50。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585319665552-bf820517-59c1-4c80-97ea-7e8c4add82cc.png#align=left&display=inline&height=794&name=d8ac2a98aa728d64f919bac088ed574a.png&originHeight=794&originWidth=784&size=75133&status=done&style=none&width=784" alt="d8ac2a98aa728d64f919bac088ed574a.png"><br><br><br>针对上面的排名，我们首先需要做的是数据规范化。你可以把这些值划分到[0,1]或者按照均值为 0，方差为 1 的正态分布进行规范化。我先把数值都规范化到[0,1]的空间中，得到了以下的数值表：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585319721708-569fa0e7-9ae2-4f5e-8d72-73c785cd5d79.png#align=left&display=inline&height=789&name=a722eeab035fb13751a6dc5c0530ed17.png&originHeight=789&originWidth=726&size=104223&status=done&style=none&width=726" alt="a722eeab035fb13751a6dc5c0530ed17.png"><br><br><br>如果我们随机选取中国、日本、韩国为三个类的中心点，我们就需要看下这些球队到中心点的距离。距离有多种计算的方式，有关距离的计算我在 KNN 算法中也讲到过：</p><ul><li>欧氏距离</li><li>曼哈顿距离</li><li>切比雪夫距离</li><li>余弦距离</li></ul><p><br>欧氏距离是最常用的距离计算方式，这里我选择欧氏距离作为距离的标准，计算每个队伍分别到中国、日本、韩国的距离，然后根据距离远近来划分。我们看到大部分的队，会和中国队聚类到一起。这里我整理了距离的计算过程，比如中国和中国的欧氏距离为 0，中国和日本的欧式距离为 0.732003。如果按照中国、日本、韩国为 3 个分类的中心点，欧氏距离的计算结果如下表所示：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585319859553-47a6149f-6259-4ce9-814c-31982c39c9fd.png#align=left&display=inline&height=828&name=b603ccdb93420c8455aea7278efaece9.png&originHeight=828&originWidth=616&size=115865&status=done&style=none&width=616" alt="b603ccdb93420c8455aea7278efaece9.png"><br><br><br>然后我们再重新计算这三个类的中心点，如何计算呢？最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类，再根据球队的分类更新中心点的位置。计算过程这里不展开，最后一直迭代（重复上述的计算过程：计算中心点和划分分类）到分类不再发生变化，可以得到以下的分类结果：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585320001148-d48868fc-bab4-48c4-bc62-9b8007a29b54.png#align=left&display=inline&height=919&name=12c6039884ee99742fbbebf198425998.png&originHeight=919&originWidth=865&size=185098&status=done&style=none&width=865" alt="12c6039884ee99742fbbebf198425998.png"><br><br><br>所以我们能看出来第一梯队有日本、韩国、伊朗、沙特、澳洲；第二梯队有中国、伊拉克、阿联酋、乌兹别克斯坦；第三梯队有卡塔尔、泰国、越南、阿曼、巴林、朝鲜、印尼、叙利亚、约旦、科威特和巴勒斯坦。<br></p><p><a name="vFDux"></a></p><h3 id="如何使用-sklearn-中的-K-Means-算法"><a href="#如何使用-sklearn-中的-K-Means-算法" class="headerlink" title="如何使用 sklearn 中的 K-Means 算法"></a>如何使用 sklearn 中的 K-Means 算法</h3><p><br>sklearn 是 Python 的机器学习工具库，如果从功能上来划分，sklearn 可以实现分类、聚类、回归、降维、模型选择和预处理等功能。这里我们使用的是 sklearn 的聚类函数库，因此需要引用工具包，具体代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br></pre></td></tr></table></figure><p><br>当然 K-Means 只是 sklearn.cluster 中的一个聚类库，实际上包括 K-Means 在内，sklearn.cluster 一共提供了 9 种聚类方法，比如 Mean-shift，DBSCAN，Spectral clustering（谱聚类）等。这些聚类方法的原理和 K-Means 不同，这里不做介绍。我们看下 K-Means 如何创建：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KMeans(n_clusters=<span class="number">8</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>, tol=<span class="number">0.0001</span>, precompute_distances=<span class="string">'auto'</span>, verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, copy_x=<span class="literal">True</span>, n_jobs=<span class="number">1</span>, algorithm=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure><p><br>我们能看到在 K-Means 类创建的过程中，有一些主要的参数：</p><ul><li>n_clusters: 即 K 值，一般需要多试一些 K 值来保证更好的聚类效果。你可以随机设置一些 K 值，然后选择聚类效果最好的作为最终的 K 值；</li><li>max_iter： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长；</li><li>n_init：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。如果 K 值比较大的时候，你可以适当增大 n_init 这个值；</li><li>init： 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式。自己设置中心点一般是对于个性化的数据进行设置，很少采用。random 的方式则是完全随机的方式，一般推荐采用优化过的 k-means++ 方式；</li><li>algorithm：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的”auto”。简单说下这三个取值的区别，如果你选择”full”采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。</li></ul><p><br>在创建好 K-Means 类之后，就可以使用它的方法，最常用的是 fit 和 predict 这个两个函数。你可以单独使用 fit 函数和 predict 函数，也可以合并使用 fit_predict 函数。其中 fit(data) 可以对 data 数据进行 k-Means 聚类。 predict(data) 可以针对 data 中的每个样本，计算最近的类。现在我们要完整地跑一遍 20 支亚洲球队的聚类问题。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'data.csv'</span>, encoding=<span class="string">'gbk'</span>)</span><br><span class="line">train_x = data[[<span class="string">"2019年国际排名"</span>,<span class="string">"2018世界杯"</span>,<span class="string">"2015亚洲杯"</span>]]</span><br><span class="line">df = pd.DataFrame(train_x)</span><br><span class="line"><span class="comment"># kmeans = KMeans(n_clusters=3)</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>, tol=<span class="number">0.0001</span>, precompute_distances=<span class="string">'auto'</span>, verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, copy_x=<span class="literal">True</span>, n_jobs=<span class="number">1</span>, algorithm=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment"># 规范化到[0,1]空间</span></span><br><span class="line">min_max_scaler=preprocessing.MinMaxScaler()</span><br><span class="line">train_x=min_max_scaler.fit_transform(train_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># kmeans算法</span></span><br><span class="line">kmeans.fit(train_x)</span><br><span class="line">predict_y = kmeans.predict(train_x)</span><br><span class="line"><span class="comment"># 合并聚类结果，插入到原数据中</span></span><br><span class="line">result = pd.concat((data,pd.DataFrame(predict_y)),axis=<span class="number">1</span>)</span><br><span class="line">result.rename(&#123;<span class="number">0</span>:<span class="string">u'聚类'</span>&#125;,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p><br>结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">        国家  <span class="number">2019</span>年国际排名  <span class="number">2018</span>世界杯  <span class="number">2015</span>亚洲杯  聚类</span><br><span class="line"><span class="number">0</span>       中国         <span class="number">73</span>       <span class="number">40</span>        <span class="number">7</span>   <span class="number">2</span></span><br><span class="line"><span class="number">1</span>       日本         <span class="number">60</span>       <span class="number">15</span>        <span class="number">5</span>   <span class="number">0</span></span><br><span class="line"><span class="number">2</span>       韩国         <span class="number">61</span>       <span class="number">19</span>        <span class="number">2</span>   <span class="number">0</span></span><br><span class="line"><span class="number">3</span>       伊朗         <span class="number">34</span>       <span class="number">18</span>        <span class="number">6</span>   <span class="number">0</span></span><br><span class="line"><span class="number">4</span>       沙特         <span class="number">67</span>       <span class="number">26</span>       <span class="number">10</span>   <span class="number">0</span></span><br><span class="line"><span class="number">5</span>      伊拉克         <span class="number">91</span>       <span class="number">40</span>       <span class="number">4</span>    <span class="number">2</span></span><br><span class="line"><span class="number">6</span>      卡塔尔        <span class="number">101</span>       <span class="number">40</span>       <span class="number">13</span>   <span class="number">1</span></span><br><span class="line"><span class="number">7</span>      阿联酋         <span class="number">81</span>       <span class="number">40</span>        <span class="number">6</span>   <span class="number">2</span></span><br><span class="line"><span class="number">8</span>   乌兹别克斯坦         <span class="number">88</span>       <span class="number">40</span>        <span class="number">8</span>     <span class="number">2</span></span><br><span class="line"><span class="number">9</span>       泰国        <span class="number">122</span>       <span class="number">40</span>       <span class="number">17</span>   <span class="number">1</span></span><br><span class="line"><span class="number">10</span>      越南        <span class="number">102</span>       <span class="number">50</span>       <span class="number">17</span>   <span class="number">1</span></span><br><span class="line"><span class="number">11</span>      阿曼         <span class="number">87</span>       <span class="number">50</span>       <span class="number">12</span>   <span class="number">1</span></span><br><span class="line"><span class="number">12</span>      巴林        <span class="number">116</span>       <span class="number">50</span>       <span class="number">11</span>   <span class="number">1</span></span><br><span class="line"><span class="number">13</span>      朝鲜        <span class="number">110</span>       <span class="number">50</span>       <span class="number">14</span>   <span class="number">1</span></span><br><span class="line"><span class="number">14</span>      印尼        <span class="number">164</span>       <span class="number">50</span>       <span class="number">17</span>   <span class="number">1</span></span><br><span class="line"><span class="number">15</span>      澳洲         <span class="number">40</span>       <span class="number">30</span>        <span class="number">1</span>   <span class="number">0</span></span><br><span class="line"><span class="number">16</span>     叙利亚         <span class="number">76</span>       <span class="number">40</span>       <span class="number">17</span>      <span class="number">1</span></span><br><span class="line"><span class="number">17</span>      约旦        <span class="number">118</span>       <span class="number">50</span>        <span class="number">9</span>   <span class="number">1</span></span><br><span class="line"><span class="number">18</span>     科威特        <span class="number">160</span>       <span class="number">50</span>       <span class="number">15</span>      <span class="number">1</span></span><br><span class="line"><span class="number">19</span>    巴勒斯坦         <span class="number">96</span>       <span class="number">50</span>       <span class="number">16</span>      <span class="number">1</span></span><br></pre></td></tr></table></figure><p><a name="PGFOw"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>如何确定 K 类的中心点？其中包括了初始的设置，以及中间迭代过程中中心点的计算。在初始设置中，会进行 n_init 次的选择，然后选择初始中心点效果最好的为初始值。在每次分类更新后，你都需要重新确认每一类的中心点，一般采用均值的方式进行确认。<br><br><br>如何将其他点划分到 K 类中？这里实际上是关于距离的定义，我们知道距离有多种定义的方式，在 K-Means 和 KNN 中，我们都可以采用欧氏距离、曼哈顿距离、切比雪夫距离、余弦距离等。对于点的划分，就看它离哪个类的中心点的距离最近，就属于哪一类。<br><br><br>如何区分 K-Means 和 KNN 这两种算法呢？刚学过 K-Means 和 KNN 算法的同学应该能知道两者的区别，但往往过了一段时间，就容易混淆。所以我们可以从三个维度来区分 K-Means 和 KNN 这两个算法：<br></p><ul><li>首先，这两个算法解决数据挖掘的两类问题。K-Means 是聚类算法，KNN 是分类算法。</li><li>这两个算法分别是两种不同的学习方式。K-Means 是非监督学习，也就是不需要事先给出分类标签，而 KNN 是有监督学习，需要我们给出训练数据的分类标识。</li><li>最后，K 值的含义不同。K-Means 中的 K 值代表 K 类。KNN 中的 K 值代表 K 个最接近的邻居。</li></ul><p><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585322317887-46d24bc4-6cbc-4173-8d10-ae8a610ed00a.png#align=left&display=inline&height=588&name=eb60546c6a3d9bc6a1538049c26723c5.png&originHeight=588&originWidth=864&size=163570&status=done&style=none&width=864" alt="eb60546c6a3d9bc6a1538049c26723c5.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，你可以理解这个算法的本质是确定 K 类的中心
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-KNN_1</title>
    <link href="cpeixin.cn/2019/01/27/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-1/"/>
    <id>cpeixin.cn/2019/01/27/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-1/</id>
    <published>2019-01-26T16:14:49.000Z</published>
    <updated>2020-03-18T16:18:04.528Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行分类，统计了电影中打斗次数、接吻次数，当然还有其他的指标也可以被统计到，如下表所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584547000621-d502499d-8b11-462b-b989-5df35d2efdbd.png#align=left&display=inline&height=440&name=6dac3a9961e69aa86d80de32bdc00987.png&originHeight=440&originWidth=1134&size=74222&status=done&style=none&width=1134" alt="6dac3a9961e69aa86d80de32bdc00987.png"></p><p>我们很容易理解《战狼》《红海行动》《碟中谍 6》是动作片，《前任 3》《春娇救志明》《泰坦尼克号》是爱情片，但是有没有一种方法让机器也可以掌握这个分类的规则，当有一部新电影的时候，也可以对它的类型自动分类呢？</p><p>我们可以把打斗次数看成 X 轴，接吻次数看成 Y 轴，然后在二维的坐标轴上，对这几部电影进行标记，如下图所示。对于未知的电影 A，坐标为 (x,y)，我们需要看下离电影 A 最近的都有哪些电影，这些电影中的大多数属于哪个分类，那么电影 A 就属于哪个分类。实际操作中，我们还需要确定一个 K 值，也就是我们要观察离电影 A 最近的电影有多少个。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546016437-8281d3c6-6f5f-403d-ac2b-0e283a6cdead.png#align=left&display=inline&height=388&name=fa0aa02dae219b21de5984371950c3cc.png&originHeight=388&originWidth=674&size=72040&status=done&style=none&width=674" alt="fa0aa02dae219b21de5984371950c3cc.png"></p><p><a name="3YI61"></a></p><h3 id="KNN-的工作原理"><a href="#KNN-的工作原理" class="headerlink" title="KNN 的工作原理"></a>KNN 的工作原理</h3><p>“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。</p><p>整个计算过程分为三步：</p><ul><li>计算待分类物体与其他物体之间的距离；</li><li>统计距离最近的 K 个邻居；</li><li>对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。</li></ul><p><a name="OlXgV"></a></p><h3 id="K-值如何选择"><a href="#K-值如何选择" class="headerlink" title="K 值如何选择"></a>K 值如何选择</h3><p>你能看出整个 KNN 的分类过程，K 值的选择还是很重要的。那么问题来了，K 值选择多少是适合的呢？</p><p>如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。</p><p>如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。</p><p>所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。</p><p><a name="Dubk8"></a></p><h3 id="距离如何计算在-KNN"><a href="#距离如何计算在-KNN" class="headerlink" title="距离如何计算在 KNN"></a>距离如何计算在 KNN</h3><p>算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。</p><p>关于距离的计算方式有下面五种方式：</p><ol><li>欧氏距离；</li><li>曼哈顿距离；</li><li>闵可夫斯基距离；</li><li>切比雪夫距离；</li><li>余弦距离。</li></ol><p>其中前三种距离是 KNN 中最常用的距离，我给你分别讲解下。</p><p>欧氏距离是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546310792-966b911e-96e1-42f1-b59c-62b4fc328c98.png#align=left&display=inline&height=162&name=f8d4fe58ec9580a4ffad5cee263b1b80.png&originHeight=162&originWidth=748&size=18688&status=done&style=none&width=748" alt="f8d4fe58ec9580a4ffad5cee263b1b80.png"></p><p>同理，我们也可以求得两点在 n 维空间中的距离：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546284862-5cf34ec5-6a80-4214-b5b3-e0debb86c9f4.png#align=left&display=inline&height=190&name=40efe7cb4a2571e55438b55f8d37366a.png&originHeight=190&originWidth=1262&size=33639&status=done&style=none&width=1262" alt="40efe7cb4a2571e55438b55f8d37366a.png"></p><p>曼哈顿距离在几何空间中用的比较多。以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1584546337196-c21e88b1-bef3-4846-b927-126eb5ad7fbd.jpeg#align=left&display=inline&height=1500&name=dd19ca4f0be3f60b526e9ea0b7d13543.jpg&originHeight=1500&originWidth=1467&size=239780&status=done&style=none&width=1467" alt="dd19ca4f0be3f60b526e9ea0b7d13543.jpg"></p><p>闵可夫斯基距离不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546368933-d651d60a-3ccc-425d-8962-6cbcd4c5a72e.png#align=left&display=inline&height=238&name=4d614c3d6722c02e4ea03cb1e6653dc5.png&originHeight=238&originWidth=516&size=16729&status=done&style=none&width=516" alt="4d614c3d6722c02e4ea03cb1e6653dc5.png"></p><p>其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。</p><p>那么切比雪夫距离怎么计算呢？二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。其公式为p为极限无穷的情况：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546700060-d96ebbde-05be-4668-a0a2-17f8b8f1620e.png#align=left&display=inline&height=168&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-18%20%E4%B8%8B%E5%8D%8811.49.01.png&originHeight=168&originWidth=786&size=27593&status=done&style=none&width=786" alt="屏幕快照 2020-03-18 下午11.49.01.png"></p><p>余弦距离实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。</p><p><a name="eKgJh"></a></p><h3 id="KD-树"><a href="#KD-树" class="headerlink" title="KD 树"></a>KD 树</h3><p>其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。</p><p><a name="K1rKD"></a></p><h3 id="用-KNN-做回归"><a href="#用-KNN-做回归" class="headerlink" title="用 KNN 做回归"></a>用 KNN 做回归</h3><p>KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。</p><p>那么 KNN 如何做回归呢？对于一个新电影 X，我们要预测它的某个属性值，比如打斗次数，具体特征属性和数值如下所示。此时，我们会先计算待测点（新电影 X）到已知点的距离，选择距离最近的 K 个点。假设 K=3，此时最近的 3 个点（电影）分别是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数就是这 3 个点的该属性值的平均值，即 (100+95+105)/3=100 次。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546868363-1e581966-91b8-4011-a72b-2ac38608fab1.png#align=left&display=inline&height=396&name=35dc8cc7d781c94b0fbaa0b53c01f716.png&originHeight=396&originWidth=890&size=66224&status=done&style=none&width=890" alt="35dc8cc7d781c94b0fbaa0b53c01f716.png"></p><p><a name="aZ4z7"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>今天我给你讲了 KNN 的原理，以及 KNN 中的几个关键因素。比如针对 K 值的选择，我们一般采用交叉验证的方式得出。</p><p>针对样本点之间的距离的定义，常用的有 5 种表达方式，你也可以自己来定义两个样本之间的距离公式。不同的定义，适用的场景不同。比如在搜索关键词推荐中，余弦距离是更为常用的。</p><p>另外你也可以用 KNN 进行回归，通过 K 个邻居对新的点的属性进行值的预测。KNN 的理论简单直接，针对 KNN 中的搜索也有相应的 KD 树这个数据结构。KNN 的理论成熟，可以应用到线性和非线性的分类问题中，也可以用于回归分析。不过 KNN 需要计算测试点与样本点之间的距离，当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间。另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。</p><p>当然在实际工作中，我们需要考虑到各种可能存在的情况，比如针对某类样本少的情况，可以增加该类别的权重。同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546977892-60b1a6bc-e309-4c4a-be3e-203c729d42f2.png#align=left&display=inline&height=1017&name=d67073bef9247e1ca7a58ae7869f390f.png&originHeight=1017&originWidth=1172&size=243740&status=done&style=none&width=1172" alt="d67073bef9247e1ca7a58ae7869f390f.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-KNN_1</title>
    <link href="cpeixin.cn/2019/01/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-2/"/>
    <id>cpeixin.cn/2019/01/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-2/</id>
    <published>2019-01-25T16:14:49.000Z</published>
    <updated>2020-03-28T01:58:58.497Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行分类，统计了电影中打斗次数、接吻次数，当然还有其他的指标也可以被统计到，如下表所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584547000621-d502499d-8b11-462b-b989-5df35d2efdbd.png#align=left&display=inline&height=440&name=6dac3a9961e69aa86d80de32bdc00987.png&originHeight=440&originWidth=1134&size=74222&status=done&style=none&width=1134" alt="6dac3a9961e69aa86d80de32bdc00987.png"></p><p>我们很容易理解《战狼》《红海行动》《碟中谍 6》是动作片，《前任 3》《春娇救志明》《泰坦尼克号》是爱情片，但是有没有一种方法让机器也可以掌握这个分类的规则，当有一部新电影的时候，也可以对它的类型自动分类呢？</p><p>我们可以把打斗次数看成 X 轴，接吻次数看成 Y 轴，然后在二维的坐标轴上，对这几部电影进行标记，如下图所示。对于未知的电影 A，坐标为 (x,y)，我们需要看下离电影 A 最近的都有哪些电影，这些电影中的大多数属于哪个分类，那么电影 A 就属于哪个分类。实际操作中，我们还需要确定一个 K 值，也就是我们要观察离电影 A 最近的电影有多少个。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546016437-8281d3c6-6f5f-403d-ac2b-0e283a6cdead.png#align=left&display=inline&height=388&name=fa0aa02dae219b21de5984371950c3cc.png&originHeight=388&originWidth=674&size=72040&status=done&style=none&width=674" alt="fa0aa02dae219b21de5984371950c3cc.png"></p><p><a name="3YI61"></a></p><h3 id="KNN-的工作原理"><a href="#KNN-的工作原理" class="headerlink" title="KNN 的工作原理"></a>KNN 的工作原理</h3><p>“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。</p><p>整个计算过程分为三步：</p><ul><li>计算待分类物体与其他物体之间的距离；</li><li>统计距离最近的 K 个邻居；</li><li>对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。</li></ul><p><a name="OlXgV"></a></p><h3 id="K-值如何选择"><a href="#K-值如何选择" class="headerlink" title="K 值如何选择"></a>K 值如何选择</h3><p>你能看出整个 KNN 的分类过程，K 值的选择还是很重要的。那么问题来了，K 值选择多少是适合的呢？</p><p>如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。</p><p>如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。</p><p>所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。</p><p><a name="Dubk8"></a></p><h3 id="距离如何计算在-KNN"><a href="#距离如何计算在-KNN" class="headerlink" title="距离如何计算在 KNN"></a>距离如何计算在 KNN</h3><p>算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。</p><p>关于距离的计算方式有下面五种方式：</p><ol><li>欧氏距离；</li><li>曼哈顿距离；</li><li>闵可夫斯基距离；</li><li>切比雪夫距离；</li><li>余弦距离。</li></ol><p>其中前三种距离是 KNN 中最常用的距离，我给你分别讲解下。</p><p>欧氏距离是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546310792-966b911e-96e1-42f1-b59c-62b4fc328c98.png#align=left&display=inline&height=162&name=f8d4fe58ec9580a4ffad5cee263b1b80.png&originHeight=162&originWidth=748&size=18688&status=done&style=none&width=748" alt="f8d4fe58ec9580a4ffad5cee263b1b80.png"></p><p>同理，我们也可以求得两点在 n 维空间中的距离：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546284862-5cf34ec5-6a80-4214-b5b3-e0debb86c9f4.png#align=left&display=inline&height=190&name=40efe7cb4a2571e55438b55f8d37366a.png&originHeight=190&originWidth=1262&size=33639&status=done&style=none&width=1262" alt="40efe7cb4a2571e55438b55f8d37366a.png"></p><p>曼哈顿距离在几何空间中用的比较多。以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1584546337196-c21e88b1-bef3-4846-b927-126eb5ad7fbd.jpeg#align=left&display=inline&height=1500&name=dd19ca4f0be3f60b526e9ea0b7d13543.jpg&originHeight=1500&originWidth=1467&size=239780&status=done&style=none&width=1467" alt="dd19ca4f0be3f60b526e9ea0b7d13543.jpg"></p><p>闵可夫斯基距离不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546368933-d651d60a-3ccc-425d-8962-6cbcd4c5a72e.png#align=left&display=inline&height=238&name=4d614c3d6722c02e4ea03cb1e6653dc5.png&originHeight=238&originWidth=516&size=16729&status=done&style=none&width=516" alt="4d614c3d6722c02e4ea03cb1e6653dc5.png"></p><p>其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。</p><p>那么切比雪夫距离怎么计算呢？二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。其公式为p为极限无穷的情况：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546700060-d96ebbde-05be-4668-a0a2-17f8b8f1620e.png#align=left&display=inline&height=168&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-18%20%E4%B8%8B%E5%8D%8811.49.01.png&originHeight=168&originWidth=786&size=27593&status=done&style=none&width=786" alt="屏幕快照 2020-03-18 下午11.49.01.png"></p><p>余弦距离实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。</p><p><a name="eKgJh"></a></p><h3 id="KD-树"><a href="#KD-树" class="headerlink" title="KD 树"></a>KD 树</h3><p>其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。</p><p><a name="K1rKD"></a></p><h3 id="用-KNN-做回归"><a href="#用-KNN-做回归" class="headerlink" title="用 KNN 做回归"></a>用 KNN 做回归</h3><p>KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。</p><p>那么 KNN 如何做回归呢？对于一个新电影 X，我们要预测它的某个属性值，比如打斗次数，具体特征属性和数值如下所示。此时，我们会先计算待测点（新电影 X）到已知点的距离，选择距离最近的 K 个点。假设 K=3，此时最近的 3 个点（电影）分别是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数就是这 3 个点的该属性值的平均值，即 (100+95+105)/3=100 次。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546868363-1e581966-91b8-4011-a72b-2ac38608fab1.png#align=left&display=inline&height=396&name=35dc8cc7d781c94b0fbaa0b53c01f716.png&originHeight=396&originWidth=890&size=66224&status=done&style=none&width=890" alt="35dc8cc7d781c94b0fbaa0b53c01f716.png"></p><p><a name="aZ4z7"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>今天我给你讲了 KNN 的原理，以及 KNN 中的几个关键因素。比如针对 K 值的选择，我们一般采用交叉验证的方式得出。</p><p>针对样本点之间的距离的定义，常用的有 5 种表达方式，你也可以自己来定义两个样本之间的距离公式。不同的定义，适用的场景不同。比如在搜索关键词推荐中，余弦距离是更为常用的。</p><p>另外你也可以用 KNN 进行回归，通过 K 个邻居对新的点的属性进行值的预测。KNN 的理论简单直接，针对 KNN 中的搜索也有相应的 KD 树这个数据结构。KNN 的理论成熟，可以应用到线性和非线性的分类问题中，也可以用于回归分析。不过 KNN 需要计算测试点与样本点之间的距离，当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间。另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。</p><p>当然在实际工作中，我们需要考虑到各种可能存在的情况，比如针对某类样本少的情况，可以增加该类别的权重。同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584546977892-60b1a6bc-e309-4c4a-be3e-203c729d42f2.png#align=left&display=inline&height=1017&name=d67073bef9247e1ca7a58ae7869f390f.png&originHeight=1017&originWidth=1172&size=243740&status=done&style=none&width=1172" alt="d67073bef9247e1ca7a58ae7869f390f.png"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-SVM_2</title>
    <link href="cpeixin.cn/2019/01/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-2/"/>
    <id>cpeixin.cn/2019/01/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-2/</id>
    <published>2019-01-25T12:27:13.000Z</published>
    <updated>2020-03-28T12:51:04.700Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>SVM 是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。</p><p><a name="c0qdV"></a></p><h3 id="sklearn-中使用-SVM"><a href="#sklearn-中使用-SVM" class="headerlink" title="sklearn 中使用 SVM"></a>sklearn 中使用 SVM</h3><p>在 Python 的 sklearn 工具包中有 SVM 算法，首先需要引用工具包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br></pre></td></tr></table></figure><p>SVM 既可以做回归，也可以做分类器。</p><p>当用 SVM 做回归的时候，我们可以使用 SVR 或 LinearSVR。SVR 的英文是 Support Vector Regression。</p><p>这篇文章只讲分类，这里只是简单地提一下。当做分类器的时候，我们使用的是 SVC 或者 LinearSVC。SVC 的英文是 Support Vector Classification。</p><p>我简单说一下这两者之前的差别。</p><p>从名字上你能看出 LinearSVC 是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到 SVM 是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。</p><p>如果是针对非线性的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。</p><p><a name="eWSro"></a></p><h3 id="创建一个-SVM-分类器"><a href="#创建一个-SVM-分类器" class="headerlink" title="创建一个 SVM 分类器"></a>创建一个 SVM 分类器</h3><p>我们首先使用 SVC 的构造函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = svm.SVC(kernel=‘rbf’, C=<span class="number">1.0</span>, gamma=‘auto’)</span><br></pre></td></tr></table></figure><p>这里有三个重要的参数 kernel、C 和 gamma。</p><p>kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。</p><ol><li>linear：线性核函数</li><li>poly：多项式核函数</li><li>rbf：高斯核函数（默认）</li><li>sigmoid：sigmoid 核函数</li></ol><p>这四种函数代表不同的映射方式，你可能会问，在实际工作中，如何选择这 4 种核函数呢？我来给你解释一下：</p><p>线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。</p><p>多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。</p><p>高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。</p><p>了解深度学习的同学应该知道 sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。上面介绍的 4 种核函数，除了第一种线性核函数外，其余 3 种都可以处理线性不可分的数据。</p><p>参数 C 代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。</p><p>参数 gamma 代表核函数的系数，默认为样本特征数的倒数，即 gamma = 1 / n_features。在创建 SVM 分类器之后，就可以输入训练集对它进行训练。我们使用 model.fit(train_X,train_y)，传入训练集中的特征值矩阵 train_X 和分类标识 train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用 prediction=model.predict(test_X) 来对结果进行预测，传入测试集中的样本特征矩阵 test_X，可以得到测试集的预测分类结果 prediction。</p><p>同样我们也可以创建线性 SVM 分类器，使用 model=svm.LinearSVC()。在 LinearSVC 中没有 kernel 这个参数，限制我们只能使用线性核函数。由于 LinearSVC 对线性分类做了优化，对于数据量大的线性可分问题，使用 LinearSVC 的效率要高于 SVC。</p><p>如果你不知道数据集是否为线性，可以直接使用 SVC 类创建 SVM 分类器。</p><p>在训练和预测中，LinearSVC 和 SVC 一样，都是使用 model.fit(train_X,train_y) 和 model.predict(test_X)。</p><p><a name="ZQodv"></a></p><h3 id="SVM-进行乳腺癌检测"><a href="#SVM-进行乳腺癌检测" class="headerlink" title="SVM 进行乳腺癌检测"></a>SVM 进行乳腺癌检测</h3><p>在了解了如何创建和使用 SVM 分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集，点击这里进行下载 <a href="https://github.com/cystanford/breast_cancer_data/blob/master/data.csv" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/cystanford/breast_cancer_data/blob/master/data.csv</a>。医疗人员采集了患者乳腺肿块经过细针穿刺 (FNA) 后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584281681475-3c6ed20a-f7a6-4633-970e-e8be3f9ca5dc.png#align=left&display=inline&height=968&name=image.png&originHeight=968&originWidth=1482&size=238425&status=done&style=none&width=1482" alt="image.png"></p><p>数据表一共包括了 32 个字段，代表的含义如下：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584281699309-c39300dc-5a10-4181-8c0f-eb37dc117515.png#align=left&display=inline&height=1031&name=1e6af6fa8bebdfba10457c111b5e9c13.png&originHeight=1031&originWidth=622&size=325518&status=done&style=none&width=622" alt="1e6af6fa8bebdfba10457c111b5e9c13.png"></p><p>上面的表格中，mean 代表平均值，se 代表标准差，worst 代表最大值（3 个最大值的平均值）。每张图像都计算了相应的特征，得出了这 30 个特征值（不包括 ID 字段和分类标识结果字段 diagnosis），实际上是 10 个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry 和 fractal_dimension_mean）的 3 个维度，平均、标准差和最大值。这些特征值都保留了 4 位数字。字段中没有缺失的值。在 569 个患者中，一共有 357 个是良性，212 个是恶性。</p><p>好了，我们的目标是生成一个乳腺癌诊断的 SVM 分类器，并计算这个分类器的准确率。首先设定项目的执行流程：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584282681711-9d8d82c5-ea3c-489d-a0c1-1ddc0ed43531.png#align=left&display=inline&height=558&name=image.png&originHeight=558&originWidth=1216&size=382196&status=done&style=none&width=1216" alt="image.png"></p><p>首先我们需要加载数据源；在准备阶段，需要对加载的数据源进行探索，查看样本特征和特征值，这个过程你也可以使用数据可视化，它可以方便我们对数据及数据之间的关系进一步加深了解。</p><p>然后按照“完全合一”的准则来评估数据的质量，如果数据质量不高就需要做数据清洗。数据清洗之后，你可以做特征选择，方便后续的模型训练；</p><p>在分类阶段，选择核函数进行训练，如果不知道数据是否为线性，可以考虑使用 SVC(kernel=‘rbf’) ，也就是高斯核函数的 SVM 分类器。然后对训练好的模型用测试集进行评估。按照上面的流程，我们来编写下代码，加载数据并对数据做部分的探索：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集，你需要把数据放到目录中</span></span><br><span class="line">data = pd.read_csv(<span class="string">"./data.csv"</span>)</span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line"><span class="comment"># 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_columns'</span>, <span class="literal">None</span>)</span><br><span class="line">print(data.columns)</span><br><span class="line">print(data.head(<span class="number">5</span>))</span><br><span class="line">print(data.describe())</span><br></pre></td></tr></table></figure><p>运行结果中，你能看到 32 个字段里，id 是没有实际含义的，可以去掉。diagnosis 字段的取值为 B 或者 M，我们可以用 0 和 1 来替代。另外其余的 30 个字段，其实可以分成三组字段，下划线后面的 mean、se 和 worst 代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 将特征字段分成3组</span></span><br><span class="line">features_mean= list(data.columns[<span class="number">2</span>:<span class="number">12</span>])</span><br><span class="line">features_se= list(data.columns[<span class="number">12</span>:<span class="number">22</span>])</span><br><span class="line">features_worst=list(data.columns[<span class="number">22</span>:<span class="number">32</span>])</span><br><span class="line"><span class="comment"># 数据清洗</span></span><br><span class="line"><span class="comment"># ID列没有用，删除该列</span></span><br><span class="line">data.drop(<span class="string">"id"</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将B良性替换为0，M恶性替换为1</span></span><br><span class="line">data[<span class="string">'diagnosis'</span>]=data[<span class="string">'diagnosis'</span>].map(&#123;<span class="string">'M'</span>:<span class="number">1</span>,<span class="string">'B'</span>:<span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure><p>然后我们要做特征字段的筛选，首先需要观察下 features_mean 各变量之间的关系，这里我们可以用 DataFrame 的 corr() 函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 将肿瘤诊断结果可视化</span></span><br><span class="line">sns.countplot(data[<span class="string">'diagnosis'</span>],label=<span class="string">"Count"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 用热力图呈现features_mean字段之间的相关性</span></span><br><span class="line">corr = data[features_mean].corr()</span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">14</span>))</span><br><span class="line"><span class="comment"># annot=True显示每个方格的数据</span></span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>图表展示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584283328012-45c4e411-27f8-4334-b18e-b5d1dfa2c4e5.png#align=left&display=inline&height=658&name=a65435de48cee8091bd5f83d286ddb4d.png&originHeight=658&originWidth=864&size=31224&status=done&style=none&width=864" alt="a65435de48cee8091bd5f83d286ddb4d.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584289159025-e03f3a3a-eddc-4359-9594-23febd6cc5f5.png#align=left&display=inline&height=1586&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-16%20%E4%B8%8A%E5%8D%8812.16.14.png&originHeight=1586&originWidth=1568&size=318165&status=done&style=none&width=1568" alt="屏幕快照 2020-03-16 上午12.16.14.png"></p><p>热力图中对角线上的为单变量自身的相关系数是 1。颜色越浅代表相关性越大。所以你能看出来 radius_mean、perimeter_mean 和 area_mean 相关性非常大，compactness_mean、concavity_mean、concave_points_mean 这三个字段也是相关的，因此我们可以取其中的一个作为代表。</p><p>那么如何进行特征选择呢？特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。</p><p>我们能看到 mean、se 和 worst 这三组特征是对同一组内容的不同度量方式，我们可以保留 mean 这组特征，在特征选择中忽略掉 se 和 worst。同时我们能看到 mean 这组特征中，radius_mean、perimeter_mean、area_mean 这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean 这三个属性相关性大。</p><p>我们分别从这 2 类中选择 1 个属性作为代表，比如 radius_mean 和 compactness_mean。这样我们就可以把原来的 10 个属性缩减为 6 个属性，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">features_remain = [<span class="string">'radius_mean'</span>,<span class="string">'texture_mean'</span>, <span class="string">'smoothness_mean'</span>,<span class="string">'compactness_mean'</span>,</span><br><span class="line">                   <span class="string">'symmetry_mean'</span>, <span class="string">'fractal_dimension_mean'</span>]</span><br></pre></td></tr></table></figure><p>对特征进行选择之后，我们就可以准备训练集和测试集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 抽取30%的数据作为测试集，其余作为训练集</span></span><br><span class="line">train, test = train_test_split(data, test_size = <span class="number">0.3</span>)<span class="comment"># in this our main data is splitted into train and test</span></span><br><span class="line"><span class="comment"># 抽取特征选择的数值作为训练和测试数据</span></span><br><span class="line">train_X = train[features_remain]</span><br><span class="line">train_y=train[<span class="string">'diagnosis'</span>]</span><br><span class="line">test_X= test[features_remain]</span><br><span class="line">test_y =test[<span class="string">'diagnosis'</span>]</span><br></pre></td></tr></table></figure><p>在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">train_X = ss.fit_transform(train_X)</span><br><span class="line">test_X = ss.transform(test_X)</span><br></pre></td></tr></table></figure><p>最后我们可以让 SVM 做训练和预测了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 创建SVM分类器</span></span><br><span class="line">model = svm.SVC()</span><br><span class="line"><span class="comment"># 用训练集做训练</span></span><br><span class="line">model.fit(train_X,train_y)</span><br><span class="line"><span class="comment"># 用测试集做预测</span></span><br><span class="line">prediction=model.predict(test_X)</span><br><span class="line">print(<span class="string">'准确率: '</span>, metrics.accuracy_score(prediction,test_y))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">准确率:  <span class="number">0.9181286549707602</span></span><br></pre></td></tr></table></figure><p>乳腺癌诊断分类的 SVM 实战，从这个过程中整个执行的流程，包括数据加载、数据探索、数据清洗、特征选择、SVM 训练和结果评估等环节。sklearn 已经为我们提供了很好的工具，对上节课中讲到的 SVM 的创建和训练都进行了封装，让我们无需关心中间的运算细节。但正因为这样，我们更需要对每个流程熟练掌握，通过实战项目训练数据化思维和对数据的敏感度。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1584283983373-21da2dfb-3082-4a72-b3dd-6722a2d4af30.png#align=left&display=inline&height=309&name=797fe646ae4668139600fca2c50c5282.png&originHeight=309&originWidth=864&size=86329&status=done&style=none&width=864" alt="797fe646ae4668139600fca2c50c5282.png"></p><p>全部代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, metrics</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">data = pandas.read_csv(<span class="string">"data.csv"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pandas.set_option(<span class="string">'display.max_columns'</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将特征字段分成3组</span></span><br><span class="line">features_mean= list(data.columns[<span class="number">2</span>:<span class="number">12</span>])</span><br><span class="line">features_se= list(data.columns[<span class="number">12</span>:<span class="number">22</span>])</span><br><span class="line">features_worst=list(data.columns[<span class="number">22</span>:<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据清洗</span></span><br><span class="line"><span class="comment"># ID列没有用，删除该列</span></span><br><span class="line">data.drop(<span class="string">"id"</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将B良性替换为0，M恶性替换为1</span></span><br><span class="line">data[<span class="string">'diagnosis'</span>]=data[<span class="string">'diagnosis'</span>].map(&#123;<span class="string">'M'</span>:<span class="number">1</span>,<span class="string">'B'</span>:<span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 将肿瘤诊断结果可视化</span></span><br><span class="line"><span class="comment"># sns.countplot(data['diagnosis'],label="Count")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># # 用热力图呈现features_mean字段之间的相关性</span></span><br><span class="line"><span class="comment"># corr = data[features_mean].corr()</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(14,14))</span></span><br><span class="line"><span class="comment"># # annot=True显示每个方格的数据</span></span><br><span class="line"><span class="comment"># sns.heatmap(corr, annot=True)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">features_remain = [<span class="string">'radius_mean'</span>,<span class="string">'texture_mean'</span>, <span class="string">'smoothness_mean'</span>,<span class="string">'compactness_mean'</span>,<span class="string">'symmetry_mean'</span>, <span class="string">'fractal_dimension_mean'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽取30%的数据作为测试集，其余作为训练集</span></span><br><span class="line">train, test = train_test_split(data, test_size = <span class="number">0.3</span>)<span class="comment"># in this our main data is splitted into train and test</span></span><br><span class="line"><span class="comment"># 抽取特征选择的数值作为训练和测试数据</span></span><br><span class="line">train_X = train[features_remain]</span><br><span class="line">train_y=train[<span class="string">'diagnosis'</span>]</span><br><span class="line">test_X= test[features_remain]</span><br><span class="line">test_y =test[<span class="string">'diagnosis'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">train_X = ss.fit_transform(train_X)</span><br><span class="line">test_X = ss.transform(test_X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SVM分类器</span></span><br><span class="line">model = svm.SVC()</span><br><span class="line"><span class="comment"># 用训练集做训练</span></span><br><span class="line">model.fit(train_X,train_y)</span><br><span class="line"><span class="comment"># 用测试集做预测</span></span><br><span class="line">prediction=model.predict(test_X)</span><br><span class="line">print(<span class="string">'准确率: '</span>, metrics.accuracy_score(prediction,test_y))</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;SVM 是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-SVM_1</title>
    <link href="cpeixin.cn/2019/01/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-1/"/>
    <id>cpeixin.cn/2019/01/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-1/</id>
    <published>2019-01-24T14:27:13.000Z</published>
    <updated>2020-03-18T16:15:38.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p><a name="XR4XW"></a></p><h3 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM 支持向量机"></a>SVM 支持向量机</h3><p>SVM 的英文叫 Support Vector Machine，中文名为支持向量机。</p><p>它是常见的一种分类方法，在机器学习中，SVM 是有监督的学习模型。什么是有监督的学习模型呢？它指的是我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。</p><p>桌子上我放了红色和蓝色两种球，请你用一根棍子将这两种颜色的球分开。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848057858-85ca0b55-bc91-49b7-bed8-7d3bc5d55195.png#align=left&display=inline&height=704&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.44.57.png&originHeight=704&originWidth=1144&size=551983&status=done&style=none&width=1144" alt="屏幕快照 2020-03-10 下午9.44.57.png"></p><p>你可以很快想到解决方案，在红色和蓝色球之间画条直线就好了，如下图所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848114572-d05fbf11-0704-4b29-8958-afdbe3c401bb.png#align=left&display=inline&height=798&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.46.07.png&originHeight=798&originWidth=1054&size=558953&status=done&style=none&width=1054" alt="屏幕快照 2020-03-10 下午9.46.07.png"></p><p>这次难度升级，桌子上依然放着红色、蓝色两种球，但是它们的摆放不规律，如下图所示。如何用一根棍子把这两种颜色分开呢？</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848163531-759be244-8599-42a2-8431-7e3d7bb84f13.png#align=left&display=inline&height=638&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.46.52.png&originHeight=638&originWidth=1054&size=436782&status=done&style=none&width=1054" alt="屏幕快照 2020-03-10 下午9.46.52.png"></p><p>你可能想了想，认为一根棍子是分不开的。除非把棍子弯曲，像下面这样：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848206430-7ff055b4-a870-4c97-9ea9-c2703adc1d07.png#align=left&display=inline&height=644&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.47.44.png&originHeight=644&originWidth=1056&size=518833&status=done&style=none&width=1056" alt="屏幕快照 2020-03-10 下午9.47.44.png"></p><p>所以这里直线变成了曲线。如果在同一个平面上来看，红蓝两种颜色的球是很难分开的。那么有没有一种方式，可以让它们自然地分开呢？</p><p>这里你可能会灵机一动，猛拍一下桌子，这些小球瞬间腾空而起，如下图所示。在腾起的那一刹那，出现了一个水平切面，恰好把红、蓝两种颜色的球分开。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848325363-17ccf722-32b6-4ca6-b21a-46c246fa50e7.png#align=left&display=inline&height=1494&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.49.02.png&originHeight=1494&originWidth=1140&size=1239386&status=done&style=none&width=1140" alt="屏幕快照 2020-03-10 下午9.49.02.png"></p><p>在这里，二维平面变成了三维空间。原来的曲线变成了一个平面。这个平面，我们就叫做超平面。</p><p><a name="cmRWw"></a></p><h3 id="SVM-的工作原理用"><a href="#SVM-的工作原理用" class="headerlink" title="SVM 的工作原理用"></a>SVM 的工作原理用</h3><p>SVM 计算的过程就是帮我们找到那个超平面的过程，这个超平面就是我们的 SVM 分类器。我们再过头来看最简单的练习 1，其实我们可以有多种直线的划分，比如下图所示的直线 A、直线 B 和直线 C，究竟哪种才是更好的划分呢？</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848413801-0ceb1fff-52ca-4e58-9eea-1ea2271d5f50.png#align=left&display=inline&height=1076&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.50.59.png&originHeight=1076&originWidth=1062&size=531974&status=done&style=none&width=1062" alt="屏幕快照 2020-03-10 下午9.50.59.png"></p><p>很明显图中的直线 B 更靠近蓝色球，但是在真实环境下，球再多一些的话，蓝色球可能就被划分到了直线 B 的右侧，被认为是红色球。同样直线 A 更靠近红色球，在真实环境下，如果红色球再多一些，也可能会被误认为是蓝色球。所以相比于直线 A 和直线 B，直线 C 的划分更优，因为它的鲁棒性更强。</p><p>那怎样才能寻找到直线 C 这个更优的答案呢？这里，我们引入一个 SVM 特有的概念：分类间隔。实际上，我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：如图中的决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，<strong>两个极限位置 A 和 B 之间的分界线 C 就是最优决策面</strong>。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。</p><p>如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，<strong>而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解</strong>。</p><p><strong>点到超平面的距离公式</strong><br><strong><br></strong><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848862628-4e93bb3a-c356-41f0-8088-46138fb57573.png#align=left&display=inline&height=1022&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.58.33.png&originHeight=1022&originWidth=1040&size=517784&status=done&style=none&width=1040" alt="屏幕快照 2020-03-10 下午9.58.33.png"><strong><br></strong><br>**在上面这个例子中，如果我们把红蓝两种颜色的球放到一个三维空间里，你发现决策面就变成了一个平面。这里我们可以用线性函数来表示，如果在一维空间里就表示一个点，在二维空间里表示一条直线，在三维空间中代表一个平面，当然空间维数还可以更多，这样我们给这个线性函数起个名称叫做“超平面”。超平面的数学表达可以写成：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583848926092-6ff7478d-c627-4f06-9c39-3600dd492194.png#align=left&display=inline&height=226&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%889.59.41.png&originHeight=226&originWidth=1238&size=66855&status=done&style=none&width=1238" alt="屏幕快照 2020-03-10 下午9.59.41.png"></p><p>在这个公式里，w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。</p><p>SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。在这个过程中，<strong>支持向量就是离分类超平面最近的样本点</strong>，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。</p><p>所以说， <strong>SVM 就是求解最大分类间隔的过程</strong>，我们还需要对分类间隔的大小进行定义。</p><p>首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用 di 代表点 xi 到超平面 wxi+b=0 的欧氏距离。因此我们要求 di 的最小值，用它来代表这个样本到超平面的最短距离。di 可以用公式计算得出：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583849438170-2fa70c02-1e0f-4528-9473-613228d12a72.png#align=left&display=inline&height=404&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%8810.08.11.png&originHeight=404&originWidth=1060&size=59272&status=done&style=none&width=1060" alt="屏幕快照 2020-03-10 下午10.08.11.png"></p><p>其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导</p><p><strong>最大间隔的优化模型</strong></p><p>我们的目标就是找出所有分类间隔中最大的那个值对应的超平面。</p><p>在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开）。通过凸优化问题，最后可以求出最优的 w 和 b，也就是我们想要找的最优超平面。中间求解的过程会用到拉格朗日乘子，和 KKT（Karush-Kuhn-Tucker）条件。数学公式比较多，这里不进行展开。</p><p><strong>硬间隔、软间隔和非线性 SVM</strong></p><p>假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。我们知道，实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点。所以线性可分是个理想情况。这时，我们需要使用到软间隔 SVM（近似线性可分），比如下面这种情况：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583849887196-a50333bc-3465-4267-be20-ca3c6231ae0d.png#align=left&display=inline&height=1026&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%8810.13.13.png&originHeight=1026&originWidth=1048&size=693506&status=done&style=none&width=1048" alt="屏幕快照 2020-03-10 下午10.13.13.png"></p><p>另外还存在一种情况，就是非线性支持向量机。</p><p>比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状。那么这种情况下，不论是多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念：核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1583850072724-04450db0-5917-4314-977e-a514fe95efee.png#align=left&display=inline&height=804&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-03-10%20%E4%B8%8B%E5%8D%8810.16.46.png&originHeight=804&originWidth=1058&size=559074&status=done&style=none&width=1058" alt="屏幕快照 2020-03-10 下午10.16.46.png"></p><p><a name="pxzoC"></a></p><h3 id="用-SVM-如何解决多分类问题"><a href="#用-SVM-如何解决多分类问题" class="headerlink" title="用 SVM 如何解决多分类问题"></a>用 SVM 如何解决多分类问题</h3><p>SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。</p><p>针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。</p><ol><li>一对多法假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：<br>（1）样本 A 作为正集，B，C，D 作为负集；<br>（2）样本 B 作为正集，A，C，D 作为负集；<br>（3）样本 C 作为正集，A，B，D 作为负集；<br>（4）样本 D 作为正集，A，B，C 作为负集。<br><br><br>这种方法，针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。<br><br><br>2. 一对一法一对一法的初衷是想在训练的时候更加灵活。我们可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：<br>（1）分类器 1：A、B；<br>（2）分类器 2：A、C；<br>（3）分类器 3：B、C。<br><br><br>当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。<br></li></ol><p><a name="lPomc"></a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>SVM 分类器，它在文本分类尤其是针对二分类任务性能卓越。同样，针对多分类的情况，我们可以采用一对多，或者一对一的方法，多个二值分类器组合成一个多分类器。</p><p>另外关于 SVM 分类器的概念，我希望你能掌握以下的三个程度：</p><p>完全线性可分情况下的线性分类器，也就是线性可分的情况，是最原始的 SVM，它最核心的思想就是找到最大的分类间隔；<br><br><br>大部分线性可分情况下的线性分类器，引入了软间隔的概念。软间隔，就是允许一定量的样本分类错误；<br><br><br>线性不可分情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到了一个高维的空间中，从而变得线性可分。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a name=&quot;XR4XW&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;SVM-支持向量机&quot;&gt;&lt;a href=&quot;#SVM-支持向量机&quot; class=&quot;h
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-朴素贝叶斯（下）</title>
    <link href="cpeixin.cn/2019/01/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>cpeixin.cn/2019/01/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2019-01-21T02:34:38.000Z</published>
    <updated>2020-03-10T14:27:31.518Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p>朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。今天我带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是 sklearn 这个机器学习神器。</p><h3 id="sklearn机器学习包"><a href="#sklearn机器学习包" class="headerlink" title="sklearn机器学习包"></a>sklearn机器学习包</h3><p>sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是<strong>高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）</strong>。</p><p>这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：</p><ul><li><p>高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。</p></li><li><p>多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。</p></li><li><p>伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。</p></li></ul><p>伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。</p><h3 id="TF-IDF-值"><a href="#TF-IDF-值" class="headerlink" title="TF-IDF 值"></a>TF-IDF 值</h3><p>我在多项式朴素贝叶斯中提到了“词的 TF-IDF 值”，如何理解这个概念呢？</p><p>TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。</p><p>TF-IDF 实际上是两个词组 <strong>Term Frequency 和 Inverse Document Frequency</strong> 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。</p><p><strong>词频 TF</strong> 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。</p><p><strong>逆向文档频率 IDF</strong>，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。</p><p>所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。</p><p><strong>TF-IDF 如何计算</strong>首先我们看下词频 TF 和逆向文档概率 IDF 的公式。</p><p><img src="p1.png" alt="p1"></p><p><strong>为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。</strong></p><p><strong>TF-IDF=TF*IDF</strong></p><p>你可以看到，TF-IDF 值就是 TF 与 IDF 的乘积, 这样可以更准确地对文档进行分类。比如“我”这样的高频单词，虽然 TF 词频高，但是 IDF 值很低，整体的 TF-IDF 也不高。</p><p>我在这里举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。针对“this”，计算 TF-IDF 值：</p><p><img src="p2.png" alt="p2"></p><p>所以 TF-IDF=0.02*(-0.0414)=-8.28e-4。</p><p>针对“bayes”，计算 TF-IDF 值：</p><p><img src="p3.png" alt="p3"></p><p>TF-IDF=0.005*0.5229=2.61e-3。</p><p>很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。</p><p><strong>如何求 TF-IDF</strong></p><p>在 sklearn 中我们直接使用 TfidfVectorizer 类，它可以帮我们计算单词 TF-IDF 向量的值。</p><p>在这个类中，取 sklearn 计算的对数 log 时，底数是 e，不是 10。下面我来讲下如何创建 TfidfVectorizer 类。</p><p>创建 TfidfVectorizer 的方法是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)</span><br></pre></td></tr></table></figure><p>我们在创建的时候，有两个构造参数，可以自定义停用词 stop_words 和规律规则 token_pattern。需要注意的是传递的数据结构，停用词 stop_words 是一个列表 List 类型，而过滤规则 token_pattern 是正则表达式。什么是停用词？停用词就是在分类中没有用的词，这些词一般词频 TF 高，但是 IDF 很低，起不到分类的作用。为了节省空间和计算时间，我们把这些词作为停用词 stop words，告诉机器这些词不需要帮我计算。</p><p><img src="p4.png" alt="p4"></p><p>当我们创建好 TF-IDF 向量类型时，可以用 fit_transform 帮我们计算，返回给我们文本矩阵，该矩阵表示了每个单词在每个文档中的 TF-IDF 值。</p><p><img src="p5.png" alt="p5"></p><p>在我们进行 fit_transform 拟合模型后，我们可以得到更多的 TF-IDF 向量属性，比如，我们可以得到词汇的对应关系（字典类型）和向量的 IDF 值，当然也可以获取设置的停用词 stop_words。</p><p><img src="p6.png" alt="p6"></p><h3 id="如何对文档进行分类"><a href="#如何对文档进行分类" class="headerlink" title="如何对文档进行分类"></a>如何对文档进行分类</h3><p>如果我们要对文档进行分类，有两个重要的阶段：</p><p><img src="p7.png" alt="p7"></p><ol><li><p>基于分词的数据准备，包括分词、单词权重计算、去掉停用词；</p></li><li><p>应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。</p></li></ol><p>整个流程将分成下面几个模块：</p><ul><li>模块 1：对文档进行分词在准备阶段里，最重要的就是分词。那么如果给文档进行分词呢？英文文档和中文文档所使用的分词工具不同。</li></ul><p>在英文文档中，最常用的是 NTLK 包。NTLK 包中包含了英文的停用词 stop words、分词和标注方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">word_list = nltk.word_tokenize(text) <span class="comment">#分词</span></span><br><span class="line">nltk.pos_tag(word_list) <span class="comment">#标注单词的词性</span></span><br></pre></td></tr></table></figure><p>在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word_list = jieba.cut (text) <span class="comment">#中文分词</span></span><br></pre></td></tr></table></figure><ul><li>模块 2：加载停用词表</li></ul><p>我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop_words = [line.strip().decode(<span class="string">'utf-8'</span>) <span class="keyword">for</span> line <span class="keyword">in</span> io.open(<span class="string">'stop_words.txt'</span>).readlines()]</span><br></pre></td></tr></table></figure><ul><li>模块 3：计算单词的权重</li></ul><p>这里我们用到 sklearn 里的 TfidfVectorizer 类，上面我们介绍过它使用的方法。直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>)</span><br><span class="line">features = tf.fit_transform(train_contents)</span><br></pre></td></tr></table></figure><p>这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。一般很少设置 min_df，因为 min_df 通常都会很小。</p><ul><li>模块 4：生成朴素贝叶斯分类器</li></ul><p>我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。</p><p>这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。当 alpha=1 时，使用的是 Laplace 平滑。</p><p>Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。</p><p>当 0&lt; alpha &lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 多项式贝叶斯分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB  </span><br><span class="line">clf = MultinomialNB(alpha=<span class="number">0.001</span>).fit(train_features, train_labels)</span><br></pre></td></tr></table></figure><ul><li>模块 5：使用生成的分类器</li></ul><p>做预测首先我们需要得到测试集的特征矩阵。方法是用训练集的分词创建一个 TfidfVectorizer 类，使用同样的 stop_words 和 max_df，然后用这个 TfidfVectorizer 类对测试集的内容进行 fit_transform 拟合，得到测试集的特征矩阵 test_features。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">test_tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>, vocabulary=train_vocabulary)</span><br><span class="line">test_features=test_tf.fit_transform(test_contents)</span><br></pre></td></tr></table></figure><p>然后我们用训练好的分类器对新数据做预测。</p><p>方法是使用 predict 函数，传入测试集的特征矩阵 test_features，得到分类结果 predicted_labels。predict 函数做的工作就是求解所有后验概率并找出最大的那个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">predicted_labels=clf.predict(test_features)</span><br></pre></td></tr></table></figure><ul><li>模块 6：计算准确率</li></ul><p>计算准确率实际上是对分类模型的评估。我们可以调用 sklearn 中的 metrics 包，在 metrics 中提供了 accuracy_score 函数，方便我们对实际结果和预测的结果做对比，给出模型的准确率。使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> metrics.accuracy_score(test_labels, predicted_labels)</span><br></pre></td></tr></table></figure><h3 id="数据挖掘神器-sklearn"><a href="#数据挖掘神器-sklearn" class="headerlink" title="数据挖掘神器 sklearn"></a>数据挖掘神器 sklearn</h3><p>从数据挖掘的流程来看，一般包括了获取数据、数据清洗、模型训练、模型评估和模型部署这几个过程。</p><p>sklearn 中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。在文档分类的项目中，我们针对文档的特点，给出了基于分词的准备流程。一般来说 NTLK 包适用于英文文档，而 jieba 适用于中文文档。我们可以根据文档选择不同的包，对文档提取分词。</p><p>这些分词就是贝叶斯分类中最重要的特征属性。基于这些分词，我们得到分词的权重，即特征矩阵。通过特征矩阵与分类结果，我们就可以创建出朴素贝叶斯分类器，然后用分类器进行预测，最后预测结果与实际结果做对比即可以得到分类器在测试集上的准确率。</p><h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>文档共有 4 种类型：女性、体育、文学、校园</p><p>根据下面给定的训练数据和测试数据进行 朴素贝叶斯文本分类实战。</p><p>数据地址：<a href="https://github.com/cystanford/text_classification/tree/master/text%20classification" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/cystanford/text_classification/tree/master/text%20classification</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="comment"># 中文文本分类</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_words</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对文本进行切词</span></span><br><span class="line"><span class="string">    :param file_path: txt文本路径</span></span><br><span class="line"><span class="string">    :return: 用空格分词的字符串</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    text_with_spaces = <span class="string">''</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">'gb18030'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text = f.read()</span><br><span class="line">        textcut = jieba.cut(text)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> textcut:</span><br><span class="line">            text_with_spaces += word + <span class="string">' '</span></span><br><span class="line">    <span class="keyword">return</span> text_with_spaces</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text_category = [<span class="string">'女性'</span>, <span class="string">'体育'</span>, <span class="string">'文学'</span>, <span class="string">'校园'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadfile</span><span class="params">(data_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将路径下的所有文件加载</span></span><br><span class="line"><span class="string">    :param file_dir: 保存txt文件目录</span></span><br><span class="line"><span class="string">    :param label: 文档标签</span></span><br><span class="line"><span class="string">    :return: 分词后的文档列表和标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words_list = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> text_category:</span><br><span class="line">        file_path = data_path  + category + <span class="string">'/'</span></span><br><span class="line">        file_list = os.listdir(file_path)</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> file_list:</span><br><span class="line">            words_list.append(cut_words(file_path+file))</span><br><span class="line">        labels.append(category)</span><br><span class="line">    <span class="keyword">return</span> words_list, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stop_words</span><span class="params">(stop_words_path)</span>:</span></span><br><span class="line">    stop_words = open(stop_words_path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>).read()</span><br><span class="line">    stop_words = stop_words.encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8-sig'</span>)  <span class="comment"># 列表头部\ufeff处理</span></span><br><span class="line">    stop_words = stop_words.split(<span class="string">'\n'</span>)  <span class="comment"># 根据分隔符分隔</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stop_words</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    train_data_path = <span class="string">'train'</span> + <span class="string">'/'</span></span><br><span class="line">    test_data_path = <span class="string">'test'</span> + <span class="string">'/'</span></span><br><span class="line">    stop_words_path = <span class="string">'stop/stopword.txt'</span></span><br><span class="line">    train_words_list, train_labels = loadfile(train_data_path)</span><br><span class="line"></span><br><span class="line">    test_words_list, test_labels = loadfile(test_data_path)</span><br><span class="line"></span><br><span class="line">    stop_words = get_stop_words(stop_words_path)</span><br><span class="line"></span><br><span class="line">    tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    train_features = tf.fit_transform(train_words_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上面fit过了，这里transform</span></span><br><span class="line">    test_features = tf.transform(test_words_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多项式贝叶斯分类器</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line">    clf = MultinomialNB(alpha=<span class="number">0.001</span>).fit(train_features, train_labels)</span><br><span class="line">    predicted_labels = clf.predict(test_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    print(<span class="string">'准确率为：'</span>, metrics.accuracy_score(test_labels, predicted_labels))</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-朴素贝叶斯（上）</title>
    <link href="cpeixin.cn/2019/01/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <id>cpeixin.cn/2019/01/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8A%EF%BC%89/</id>
    <published>2019-01-20T15:19:16.000Z</published>
    <updated>2020-03-28T12:46:47.071Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><h3 id="贝叶斯原理"><a href="#贝叶斯原理" class="headerlink" title="贝叶斯原理"></a>贝叶斯原理</h3><p>贝叶斯原理是怎么来的呢？贝叶斯为了解决一个叫“逆向概率”问题写了一篇文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。什么是“逆向概率”呢？所谓“逆向概率”是相对“正向概率”而言。</p><p>正向概率的问题很容易理解，比如我们已经知道袋子里面有 N 个球，不是黑球就是白球，其中 M 个是黑球，那么把手伸进去摸一个球，就能知道摸出黑球的概率是多少。但这种情况往往是上帝视角，即了解了事情的全貌再做判断。</p><p>在现实生活中，我们很难知道事情的全貌。贝叶斯则从实际场景出发，提了一个问题：如果我们事先不知道袋子里面黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里面黑白球的比例么？正是这样的一个问题，影响了接下来近 200 年的统计学理论。这是因为，贝叶斯原理与其他统计学推断方法截然不同，它是建立在主观判断的基础上：在我们不了解所有客观事实的情况下，同样可以先估计一个值，然后根据实际结果不断进行修正。</p><p>我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那是不是实际上患“贝叶死”的概率也是 99.9% 呢？实际上不是的。</p><p>你自己想想，在 10000 个人中，还存在 0.1% 的误查的情况，也就是 10 个人没有患病但是被诊断成阳性。当然 10000 个人中，也确实存在一个患有贝叶死的人，他有 99.9% 的概率被检查出来。所以你可以粗算下，患病的这个人实际上是这 11 个人里面的一员，即实际患病比例是 1/11≈9%。</p><p>上面这个例子中，实际上涉及到了贝叶斯原理中的几个概念：</p><p><strong>先验概率：</strong>通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。</p><p><strong>后验概率：</strong>后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。</p><p><strong>条件概率：</strong>事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。</p><p><strong>似然函数（likelihood function）：</strong>你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。</p><p>那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。</p><p>介绍完贝叶斯原理中的这几个概念，我们再来看下贝叶斯原理，实际上贝叶斯原理就是求解后验概率，我们假设：A 表示事件 “测出为阳性”, 用 B1 表示“患有贝叶死”, B2 表示“没有患贝叶死”。</p><p>根据上面那道题，我们可以得到下面的信息。患有贝叶死的情况下，测出为阳性的概率为 P(A|B1)=99.9%，没有患贝叶死，但测出为阳性的概率为 P(A|B2)=0.1%。另外患有贝叶死的概率为 P(B1)=0.01%，没有患贝叶死的概率 P(B2)=99.99%。</p><p>那么我们检测出来为阳性，而且是贝叶死的概率 P(B1，A）=P(B1)<em>P(A|B1)=0.01%</em>99.9%=0.00999%。</p><p>这里 P(B1,A) 代表的是联合概率，同样我们可以求得 P(B2,A)=P(B2)<em>P(A|B2)=99.99%</em>0.1%=0.09999%。</p><p>然后我们想求得是检查为阳性的情况下，患有贝叶死的概率，也即是 P(B1|A)。所以检查出阳性，且患有贝叶死的概率为：</p><p><img src="p1.png" alt="p1"></p><p>检查出是阳性，但没有患有贝叶死的概率为：</p><p><img src="p2.png" alt="p2"></p><p>这里我们能看出来 0.01%+0.1% 均出现在了 P(B1|A) 和 P(B2|A) 的计算中作为分母。</p><p>我们把它称之为论据因子，也相当于一个权值因子。其中 P(B1）、P(B2) 就是先验概率，我们现在知道了观测值，就是被检测出来是阳性，来求患贝叶死的概率，也就是求后验概率。</p><p>求后验概率就是贝叶斯原理要求的，基于刚才求得的 P(B1|A)，P(B2|A)，我们可以总结出贝叶斯公式为：</p><p><img src="p3.png" alt="p3"></p><p>由此，我们可以得出通用的贝叶斯公式：</p><p><img src="p4.png" alt="p4"></p><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>讲完贝叶斯原理之后，我们再来看下今天重点要讲的算法，朴素贝叶斯。它是一种简单但极为强大的预测建模算法。</p><p>之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。</p><p>朴素贝叶斯模型由两种类型的概率组成：</p><ul><li>每个类别的概率P(Cj)；</li><li>每个属性的条件概率P(Ai|Cj)。</li></ul><p>我来举个例子说明下什么是类别概率和条件概率。</p><p>假设我有 7 个棋子，其中 3 个是白色的，4 个是黑色的。那么棋子是白色的概率就是 3/7，黑色的概率就是 4/7，这个就是类别概率。</p><p>假设我把这 7 个棋子放到了两个盒子里，其中盒子 A 里面有 2 个白棋，2 个黑棋；盒子 B 里面有 1 个白棋，2 个黑棋。那么在盒子 A 中抓到白棋的概率就是 1/2，抓到黑棋的概率也是 1/2，这个就是条件概率，也就是在某个条件（比如在盒子 A 中）下的概率。</p><p>在朴素贝叶斯中，我们要统计的是属性的条件概率，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。</p><p>那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。</p><p><img src="p5.png" alt="p5"></p><p>另外我想告诉你的是，贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。</p><p>贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，<strong>在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。</strong>朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。</p><p><img src="p6.png" alt="p6"></p><h3 id="朴素贝叶斯分类工作原理"><a href="#朴素贝叶斯分类工作原理" class="headerlink" title="朴素贝叶斯分类工作原理"></a>朴素贝叶斯分类工作原理</h3><p>朴素贝叶斯分类是常用的贝叶斯分类方法。我们日常生活中看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。</p><p><strong>离散数据案例</strong></p><p>我们遇到的数据可以分为两种，一种是离散数据，另一种是连续数据。那什么是离散数据呢？离散就是不连续的意思，有明确的边界，比如整数 1，2，3 就是离散数据，而 1 到 3 之间的任何数，就是连续数据，它可以取在这个区间里的任何数值。</p><p>我以下面的数据为例，这些是根据你之前的经验所获得的数据。然后给你一个新的数据：身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？</p><p><img src="p7.png" alt="p7"></p><p>针对这个问题，我们先确定一共有 3 个属性，假设我们用 A 代表属性，用 A1, A2, A3 分别为身高 = 高、体重 = 中、鞋码 = 中。一共有两个类别，假设用 C 代表类别，那么 C1,C2 分别是：男、女，在未知的情况下我们用 Cj 表示。那么我们想求在 A1、A2、A3 属性下，Cj 的概率，用条件概率表示就是 P(Cj|A1A2A3)。根据上面讲的贝叶斯的公式，我们可以得出：</p><p><img src="p8.png" alt="p8"></p><p>因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。</p><p>在这个公式里，因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。</p><p>我们假定 Ai 之间是相互独立的，那么：P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出 P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。</p><p>我分别求下这些条件下的概率：P(A1|C1)=1/2, P(A2|C1)=1/2, P(A3|C1)=1/4，P(A1|C2)=0, P(A2|C2)=1/2, P(A3|C2)=1/2，所以 P(A1A2A3|C1)=1/16, P(A1A2A3|C2)=0。</p><p>因为 P(A1A2A3|C1)P(C1)&gt;P(A1A2A3|C2)P(C2)，所以应该是 C1 类别，即男性。</p><p><strong>连续数据案例</strong></p><p>我们做了一个离散的数据案例，实际生活中我们得到的是连续的数值，比如下面这组数据：</p><p><img src="p9.png" alt="p9"></p><p>那么如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？</p><p>公式还是上面的公式，这里的困难在于，由于身高、体重、鞋码都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办呢？这时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。</p><p>有了密度函数，就可以把值代入，算出某一点的密度函数的值。比如，男性的身高是均值 179.5、标准差为 3.697 的正态分布。所以男性的身高为 180 的概率为 0.1069。怎么计算得出的呢? 你可以使用 EXCEL 的 NORMDIST(x,mean,standard_dev,cumulative) 函数，一共有 4 个参数：</p><ol><li><p>x：正态分布中，需要计算的数值；</p></li><li><p>Mean：正态分布的平均值；</p></li><li><p>Standard_dev：正态分布的标准差；</p></li><li><p>Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。</p></li></ol><p>当为 TRUE 时，函数结果为累积分布；为 False 时，函数结果为概率密度。这里我们使用的是 NORMDIST(180,179.5,3.697,0)=0.1069。</p><p>同理我们可以计算得出男性体重为 120 的概率为 0.000382324，男性鞋码为 41 号的概率为 0.120304111。所以我们可以计算得出：P(A1A2A3|C1)=P(A1|C1)P(A2|C1)P(A3|C1)=0.10690.0003823240.120304111=4.9169e-6</p><p>同理我们也可以计算出来该人为女的可能性：</p><p>P(A1A2A3|C2)=P(A1|C2)P(A2|C2)P(A3|C2)=0.000001474890.0153541440.120306074=2.7244e-9</p><p>很明显这组数据分类为男的概率大于分类为女的概率。当然在 Python 中，有第三方库可以直接帮我们进行上面的操作，这个我们会在下节课中介绍。这里主要是给你讲解下具体的运算原理。</p><h3 id="朴素贝叶斯分类器工作流程"><a href="#朴素贝叶斯分类器工作流程" class="headerlink" title="朴素贝叶斯分类器工作流程"></a>朴素贝叶斯分类器工作流程</h3><p>朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。流程可以用下图表示：</p><p><img src="p10.png" alt="p10"></p><p>从图片你也可以看出来，朴素贝叶斯分类器需要三个流程，我来给你一一讲解下这几个流程。</p><p>第一阶段：准备阶段在这个阶段我们需要确定特征属性，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。</p><p>第二阶段：训练阶段这个阶段就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。输入是特征属性和训练样本，输出是分类器。</p><p>第三阶段：应用阶段这个阶段是使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。</p><p><img src="p11.png" alt="p11"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;贝叶斯原理&quot;&gt;&lt;a href=&quot;#贝叶斯原理&quot; class=&quot;headerlink&quot; title=&quot;贝叶斯原理&quot;&gt;&lt;/a&gt;贝叶斯原理&lt;
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析-决策树实战-泰坦尼克号乘客生存预测</title>
    <link href="cpeixin.cn/2019/01/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/"/>
    <id>cpeixin.cn/2019/01/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/</id>
    <published>2019-01-15T14:12:49.000Z</published>
    <updated>2020-03-28T12:45:07.035Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --><p><br>在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。<br><br><br>基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。今天我来带你用决策树进行项目的实战。决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。<br><br><br>在了解决策树的原理后，今天我们用 sklearn 工具解决一个实际的问题：泰坦尼克号乘客的生存预测。<br><a name="sklearn"></a></p><h3 id="sklearn-中的决策树模型"><a href="#sklearn-中的决策树模型" class="headerlink" title="sklearn 中的决策树模型"></a>sklearn 中的决策树模型</h3><p>首先，我们需要掌握 sklearn 中自带的决策树分类器 DecisionTreeClassifier，方法如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br></pre></td></tr></table></figure><p><br>到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 gini</p><ul><li>entropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；</li><li>gini：默认参数，基于基尼系数。</li></ul><p><br>CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。我们通过设置 criterion=’entropy’可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在 sklearn 中是个什么东西？<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(class_weight=<span class="literal">None</span>, criterion=<span class="string">'entropy'</span>, max_depth=<span class="literal">None</span>,</span><br><span class="line">            max_features=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>,</span><br><span class="line">            min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>,</span><br><span class="line">            min_samples_leaf=<span class="number">1</span>, min_samples_split=<span class="number">2</span>,</span><br><span class="line">            min_weight_fraction_leaf=<span class="number">0.0</span>, presort=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">            splitter=<span class="string">'best'</span>)</span><br></pre></td></tr></table></figure><br><br>这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。<br><br>我整理了这些参数代表的含义：<br>![](https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397401268-c9ea5ebf-53aa-44e0-8588-13582a240b5d.png#align=left&display=inline&height=930&originHeight=930&originWidth=620&size=0&status=done&style=none&width=620)<br><br>在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。<br><br>下面这个表格是 fit 方法、predict 方法和 score 方法的作用:<br>![](https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397402135-ed1fff6c-1bee-49f9-bf07-9fb87d83e0f3.png#align=left&display=inline&height=158&originHeight=158&originWidth=468&size=0&status=done&style=none&width=468) <a name="titanic"></a> ### Titanic 乘客生存预测 <a name="-2"></a> #### 问题描述 泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据，具体数据你可以从 GitHub 上下载：[点我](https://github.com/cystanford/Titanic_Data)<br><br>其中数据集格式为 csv，一共有两个文件：<br>train.csv 是训练数据集，包含特征信息和存活与否的标签；<br>test.csv: 测试数据集，只包含特征信息。<br>现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。在训练集中，包括了以下字段，它们具体为：<br>![](https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397401155-e052cf89-fe96-49b3-b3fe-37539d7a67ae.png#align=left&display=inline&height=370&originHeight=370&originWidth=466&size=0&status=done&style=none&width=466) <a name="-3"></a> #### 生存预测的关键流程 我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：<br>![](https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397402088-8ce15112-7300-4b68-b34c-0bc6a0c960c0.png#align=left&display=inline&height=1470&originHeight=1470&originWidth=3202&size=0&status=done&style=none&width=3202)<br>**准备阶段**：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；<br><br>**分类阶段**：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。<br>下面，我分别对这些模块进行介绍。<br><br>**模块 1**：数据探索<br>数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。<br>那么如何进行数据探索呢？这里有一些函数你需要了解：<br>使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；<br>使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；<br>使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；<br>使用 head 查看前几行数据（默认是前 5 行）；<br>使用 tail 查看后几行数据（默认是最后 5 行）。<br>我们可以使用 Pandas 便捷地处理这些问题：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line"></span><br><span class="line">print(train_data.info())</span><br><span class="line">print(<span class="string">"==========================================="</span>)</span><br><span class="line">print(train_data.describe())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"==========================================="</span>)</span><br><span class="line"></span><br><span class="line">print(train_data.describe(include=[<span class="string">'O'</span>]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"==========================================="</span>)</span><br><span class="line"></span><br><span class="line">print(train_data.head())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"==========================================="</span>)</span><br><span class="line"></span><br><span class="line">print(train_data.tail())</span><br></pre></td></tr></table></figure><p><br>运行结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">12</span> columns):</span><br><span class="line">PassengerId    <span class="number">891</span> non-null int64</span><br><span class="line">Survived       <span class="number">891</span> non-null int64</span><br><span class="line">Pclass         <span class="number">891</span> non-null int64</span><br><span class="line">Name           <span class="number">891</span> non-null object</span><br><span class="line">Sex            <span class="number">891</span> non-null object</span><br><span class="line">Age            <span class="number">714</span> non-null float64</span><br><span class="line">SibSp          <span class="number">891</span> non-null int64</span><br><span class="line">Parch          <span class="number">891</span> non-null int64</span><br><span class="line">Ticket         <span class="number">891</span> non-null object</span><br><span class="line">Fare           <span class="number">891</span> non-null float64</span><br><span class="line">Cabin          <span class="number">204</span> non-null object</span><br><span class="line">Embarked       <span class="number">889</span> non-null object</span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">5</span>), object(<span class="number">5</span>)</span><br><span class="line">memory usage: <span class="number">83.7</span>+ KB</span><br><span class="line"><span class="literal">None</span></span><br><span class="line">===========================================</span><br><span class="line">       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare</span><br><span class="line">count   <span class="number">891.000000</span>  <span class="number">891.000000</span>  <span class="number">891.000000</span>  ...  <span class="number">891.000000</span>  <span class="number">891.000000</span>  <span class="number">891.000000</span></span><br><span class="line">mean    <span class="number">446.000000</span>    <span class="number">0.383838</span>    <span class="number">2.308642</span>  ...    <span class="number">0.523008</span>    <span class="number">0.381594</span>   <span class="number">32.204208</span></span><br><span class="line">std     <span class="number">257.353842</span>    <span class="number">0.486592</span>    <span class="number">0.836071</span>  ...    <span class="number">1.102743</span>    <span class="number">0.806057</span>   <span class="number">49.693429</span></span><br><span class="line">min       <span class="number">1.000000</span>    <span class="number">0.000000</span>    <span class="number">1.000000</span>  ...    <span class="number">0.000000</span>    <span class="number">0.000000</span>    <span class="number">0.000000</span></span><br><span class="line"><span class="number">25</span>%     <span class="number">223.500000</span>    <span class="number">0.000000</span>    <span class="number">2.000000</span>  ...    <span class="number">0.000000</span>    <span class="number">0.000000</span>    <span class="number">7.910400</span></span><br><span class="line"><span class="number">50</span>%     <span class="number">446.000000</span>    <span class="number">0.000000</span>    <span class="number">3.000000</span>  ...    <span class="number">0.000000</span>    <span class="number">0.000000</span>   <span class="number">14.454200</span></span><br><span class="line"><span class="number">75</span>%     <span class="number">668.500000</span>    <span class="number">1.000000</span>    <span class="number">3.000000</span>  ...    <span class="number">1.000000</span>    <span class="number">0.000000</span>   <span class="number">31.000000</span></span><br><span class="line">max     <span class="number">891.000000</span>    <span class="number">1.000000</span>    <span class="number">3.000000</span>  ...    <span class="number">8.000000</span>    <span class="number">6.000000</span>  <span class="number">512.329200</span></span><br><span class="line"></span><br><span class="line">[<span class="number">8</span> rows x <span class="number">7</span> columns]</span><br><span class="line">===========================================</span><br><span class="line">                            Name   Sex  Ticket    Cabin Embarked</span><br><span class="line">count                        <span class="number">891</span>   <span class="number">891</span>     <span class="number">891</span>      <span class="number">204</span>      <span class="number">889</span></span><br><span class="line">unique                       <span class="number">891</span>     <span class="number">2</span>     <span class="number">681</span>      <span class="number">147</span>        <span class="number">3</span></span><br><span class="line">top     Wick, Miss. Mary Natalie  male  <span class="number">347082</span>  B96 B98        S</span><br><span class="line">freq                           <span class="number">1</span>   <span class="number">577</span>       <span class="number">7</span>        <span class="number">4</span>      <span class="number">644</span></span><br><span class="line">===========================================</span><br><span class="line">   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked</span><br><span class="line"><span class="number">0</span>            <span class="number">1</span>         <span class="number">0</span>       <span class="number">3</span>  ...   <span class="number">7.2500</span>   NaN         S</span><br><span class="line"><span class="number">1</span>            <span class="number">2</span>         <span class="number">1</span>       <span class="number">1</span>  ...  <span class="number">71.2833</span>   C85         C</span><br><span class="line"><span class="number">2</span>            <span class="number">3</span>         <span class="number">1</span>       <span class="number">3</span>  ...   <span class="number">7.9250</span>   NaN         S</span><br><span class="line"><span class="number">3</span>            <span class="number">4</span>         <span class="number">1</span>       <span class="number">1</span>  ...  <span class="number">53.1000</span>  C123         S</span><br><span class="line"><span class="number">4</span>            <span class="number">5</span>         <span class="number">0</span>       <span class="number">3</span>  ...   <span class="number">8.0500</span>   NaN         S</span><br><span class="line"></span><br><span class="line">[<span class="number">5</span> rows x <span class="number">12</span> columns]</span><br><span class="line">===========================================</span><br><span class="line">     PassengerId  Survived  Pclass  ...   Fare Cabin  Embarked</span><br><span class="line"><span class="number">886</span>          <span class="number">887</span>         <span class="number">0</span>       <span class="number">2</span>  ...  <span class="number">13.00</span>   NaN         S</span><br><span class="line"><span class="number">887</span>          <span class="number">888</span>         <span class="number">1</span>       <span class="number">1</span>  ...  <span class="number">30.00</span>   B42         S</span><br><span class="line"><span class="number">888</span>          <span class="number">889</span>         <span class="number">0</span>       <span class="number">3</span>  ...  <span class="number">23.45</span>   NaN         S</span><br><span class="line"><span class="number">889</span>          <span class="number">890</span>         <span class="number">1</span>       <span class="number">1</span>  ...  <span class="number">30.00</span>  C148         C</span><br><span class="line"><span class="number">890</span>          <span class="number">891</span>         <span class="number">0</span>       <span class="number">3</span>  ...   <span class="number">7.75</span>   NaN         Q</span><br><span class="line"></span><br><span class="line">[<span class="number">5</span> rows x <span class="number">12</span> columns]</span><br></pre></td></tr></table></figure><p><br><strong>模块 2：数据清洗</strong><br>通过数据探索，我们发现 Age 和 Cabin 这三个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐；具体实现的代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'Age'</span>].fillna(train_data[<span class="string">'Age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Age'</span>].fillna(test_data[<span class="string">'Age'</span>].mean(),inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><br>Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。首先观察下 Embarked 字段的取值，方法如下：<br>首先观察下 Embarked 字段的取值，方法如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_data[<span class="string">'Embarked'</span>].value_counts())</span><br></pre></td></tr></table></figure><p><br>结果如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">S    <span class="number">644</span></span><br><span class="line">C    <span class="number">168</span></span><br><span class="line">Q     <span class="number">77</span></span><br></pre></td></tr></table></figure><p><br>我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><br><strong>模块 3：特征选择</strong><br>特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？通过数据探索我们发现，PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；<strong>Cabin 字段缺失值太多，可以放弃；</strong> Ticket 字段为船票号码，杂乱无章且无规律，可以放弃。<br><br><br>其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">features = [<span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Embarked'</span>]</span><br><span class="line">train_features = train_data[features]</span><br><span class="line">train_labels = train_data[<span class="string">'Survived'</span>]</span><br><span class="line">test_features = test_data[features]</span><br></pre></td></tr></table></figure><p><br>特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。<br><br><br>那该如何操作呢?我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">dvec=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">train_features=dvec.fit_transform(train_features.to_dict(orient=<span class="string">'record'</span>))</span><br></pre></td></tr></table></figure><p><br>你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(dvec.feature_names_)</span><br><span class="line">[<span class="string">'Age'</span>, <span class="string">'Embarked=C'</span>, <span class="string">'Embarked=Q'</span>, <span class="string">'Embarked=S'</span>, <span class="string">'Fare'</span>, <span class="string">'Parch'</span>, <span class="string">'Pclass'</span>, <span class="string">'Sex=female'</span>, <span class="string">'Sex=male'</span>, <span class="string">'SibSp'</span>]</span><br></pre></td></tr></table></figure><p><br>你可以看到原本是一列的 Embarked，变成了“Embarked=C”“Embarked=Q”“Embarked=S”三列。Sex 列变成了“Sex=female”“Sex=male”两列。这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。<br><br><br><strong>模块 4：决策树模型</strong><br>刚才我们已经讲了如何使用 sklearn 中的决策树模型。现在我们使用 ID3 算法，即在创建 DecisionTreeClassifier 时，设置 criterion=‘entropy’，然后使用 fit 进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="comment"># 构造ID3决策树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line"><span class="comment"># 决策树训练</span></span><br><span class="line">clf.fit(train_features, train_labels)</span><br></pre></td></tr></table></figure><p><br><strong>模块 5：模型预测 &amp; 评估</strong><br>在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labels：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_features=dvec.transform(test_features.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"><span class="comment"># 决策树预测</span></span><br><span class="line">pred_labels = clf.predict(test_features)</span><br></pre></td></tr></table></figure><p><br>在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的 score 函数计算下得到的结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到决策树准确率</span></span><br><span class="line">acc_decision_tree = round(clf.score(train_features, train_labels), <span class="number">6</span>)</span><br><span class="line">print(<span class="string">u'score准确率为 %.4lf'</span> % acc_decision_tree)</span><br></pre></td></tr></table></figure><p><br>运行结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score准确率为 <span class="number">0.9820</span></span><br></pre></td></tr></table></figure><p><br>你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。这是为什么呢？<br><br><br>因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比(<strong>test.csv数据中没有Survived</strong>)。如果我们使用 score 函数对训练集的准确率进行统计，正确率会接近于 100%（如上结果为 98.2%），无法对分类器的在实际环境下做准确率的评估。<br><br><br>那么有什么办法，来统计决策树分类器的准确率呢？这里可以使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。<br>K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。<br><strong>K 折交叉验证的原理是这样的：</strong></p><ol><li>将数据集平均分割成 K 个等份；<br></li><li>使用 1 份数据作为测试数据，其余作为训练数据；<br></li><li>计算测试准确率；<br></li><li>使用不同的测试集，重复 2、3 步骤。<br></li></ol><p><br>在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score 和 cross_val_score 两种函数的正确率的评估结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 使用K折交叉验证 统计决策树准确率</span></span><br><span class="line">print(<span class="string">u'cross_val_score准确率为 %.4lf'</span> % np.mean(cross_val_score(clf, train_features, train_labels, cv=<span class="number">10</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score准确率为 <span class="number">0.7835</span></span><br></pre></td></tr></table></figure><p><br>你可以看到，score 函数的准确率为 0.9820，cross_val_score 准确率为 0.7835。这里很明显，<strong>对于不知道测试集实际结果的，要使用 K 折交叉验证才能知道模型的准确率。</strong><br><strong>模块 6：决策树可视化</strong><br>sklearn 的决策树模型对我们来说，还是比较抽象的。我们可以使用 Graphviz 可视化工具帮我们把决策树呈现出来。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397403010-430bb637-799a-49af-af7a-d5a1c9784b03.png#align=left&display=inline&height=598&originHeight=598&originWidth=1716&size=0&status=done&style=none&width=1716" alt><br>安装 Graphviz 库需要下面的几步：<br>安装 graphviz 工具，这里是它的下载地址；<a href="http://www.graphviz.org/download/" target="_blank" rel="external nofollow noopener noreferrer">http://www.graphviz.org/download/</a><br>将 Graphviz 添加到环境变量 PATH 中；<br>需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。<br>这样你就可以在程序里面使用 Graphviz 对决策树模型进行呈现，最后得到一个决策树可视化的 PDF 文件，可视化结果文件 Source.gv.pdf 你可以在 GitHub 上下载：<a href="https://github.com/cystanford/Titanic_Data" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/cystanford/Titanic_Data</a><br><strong><br></strong>决策树模型使用技巧总结<strong><br>今天我用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。<br></strong><br><strong>通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化</strong>；模型准确率需要考虑是否有测试集的实际结果可以做对比，<strong>当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score</strong>；<br>Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397784656-cb8448e4-2dd0-4d2b-a0dc-182c9759766e.png#align=left&display=inline&height=940&name=p6.png&originHeight=940&originWidth=2308&size=395883&status=done&style=none&width=2308" alt="p6.png"><br>我上面讲了泰坦尼克乘客生存预测的六个关键模块，请你用 sklearn 中的决策树模型独立完成这个项目，对测试集中的乘客是否生存进行预测，并给出模型准确率评估。数据从 GitHub 上下载即可。<br>最后给你留一个思考题吧，我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，那么这两个函数有什么区别?<br><strong>项目完整代码</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'Age'</span>].fillna(train_data[<span class="string">'Age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Age'</span>].fillna(test_data[<span class="string">'Age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Fare'</span>].fillna(test_data[<span class="string">'Fare'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">将空值用此列最多值来补齐</span></span><br><span class="line"><span class="string">index 获取索引值。 [num] 下标获取返回值 ，ascending=True 降序排列</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">train_data[<span class="string">'Embarked'</span>].fillna(train_data[<span class="string">'Embarked'</span>].value_counts().index[<span class="number">0</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Embarked'</span>].fillna(test_data[<span class="string">'Embarked'</span>].value_counts().index[<span class="number">0</span>], inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">features = [<span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Embarked'</span>]</span><br><span class="line"></span><br><span class="line">train_features = train_data[features]</span><br><span class="line">train_labels = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line">test_features = test_data[features]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dvec=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">train_features=dvec.fit_transform(train_features.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造ID3决策树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line"><span class="comment"># 决策树训练</span></span><br><span class="line">clf.fit(train_features, train_labels)</span><br><span class="line"></span><br><span class="line">test_features=dvec.transform(test_features.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树预测</span></span><br><span class="line"><span class="comment"># pred_labels = clf.predict(test_features)</span></span><br><span class="line"><span class="comment"># # 得到决策树准确率</span></span><br><span class="line"><span class="comment"># acc_decision_tree = round(clf.score(train_features, train_labels), 6)</span></span><br><span class="line"><span class="comment"># print(u'score准确率为 %.4lf' % acc_decision_tree)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 使用K折交叉验证 统计决策树准确率</span></span><br><span class="line">print(<span class="string">u'cross_val_score准确率为 %.4lf'</span> % np.mean(cross_val_score(clf, train_features, train_labels, cv=<span class="number">10</span>)))</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Apr 04 2020 17:32:44 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;br&gt;在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直
      
    
    </summary>
    
    
    
      <category term="数据分析" scheme="cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
