<!-- build time:Thu Oct 08 2020 22:15:13 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta name="referrer" content="no-referrer"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Spark SQL 处理结构化数据：DataFrame和Dataset | 布兰特 | 不忘初心</title><meta name="description" content="RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集合完成数据处理的逻辑更像是程序员们的思维方式。对于数据处理逻辑，分析师们更习惯用 SQL 而非算子来表达。所以，Spark 借鉴了 Python 数据分析库 pandas 中 Da"><meta property="og:type" content="article"><meta property="og:title" content="Spark SQL 处理结构化数据：DataFrame和Dataset"><meta property="og:url" content="cpeixin.cn/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/index.html"><meta property="og:site_name" content="布兰特 | 不忘初心"><meta property="og:description" content="RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集合完成数据处理的逻辑更像是程序员们的思维方式。对于数据处理逻辑，分析师们更习惯用 SQL 而非算子来表达。所以，Spark 借鉴了 Python 数据分析库 pandas 中 Da"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779491-df2d23ff-d06c-4811-a270-c4f2fa58b698.png#align=left&display=inline&height=651&margin=%5Bobject%20Object%5D&originHeight=651&originWidth=1758&size=0&status=done&style=none&width=1758"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779457-41224808-31bf-44de-a1c2-a2446e2b32cf.png#align=left&display=inline&height=107&margin=%5Bobject%20Object%5D&originHeight=107&originWidth=100&size=0&status=done&style=none&width=100"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779524-643859bf-a822-4c2e-af47-2a1abec78e93.png#align=left&display=inline&height=120&margin=%5Bobject%20Object%5D&originHeight=120&originWidth=135&size=0&status=done&style=none&width=135"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779444-4ec1819a-9f1c-41b0-98cf-929ce2670dc4.png#align=left&display=inline&height=406&margin=%5Bobject%20Object%5D&originHeight=406&originWidth=570&size=0&status=done&style=none&width=570"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779509-a2a3e12a-a53e-48f2-931e-12b06cfb97b3.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&originHeight=94&originWidth=252&size=0&status=done&style=none&width=252"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779551-1af25ed4-6183-4ffb-8eac-67655265601e.png#align=left&display=inline&height=201&margin=%5Bobject%20Object%5D&originHeight=201&originWidth=194&size=0&status=done&style=none&width=194"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779441-814f7b75-4739-4bc9-9b39-c1be42fa2c85.png#align=left&display=inline&height=244&margin=%5Bobject%20Object%5D&originHeight=244&originWidth=211&size=0&status=done&style=none&width=211"><meta property="article:published_time" content="2018-09-03T14:56:04.000Z"><meta property="article:modified_time" content="2020-09-03T14:57:01.277Z"><meta property="article:author" content="Brent"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779491-df2d23ff-d06c-4811-a270-c4f2fa58b698.png#align=left&display=inline&height=651&margin=%5Bobject%20Object%5D&originHeight=651&originWidth=1758&size=0&status=done&style=none&width=1758"><link rel="canonical" href="cpeixin.cn/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/index.html"><link rel="alternate" href="/atom.xml" title="布兰特 | 不忘初心" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.0"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Brent</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">大数据工程师 &amp; 机器学习</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Malaysia</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBases/">DataBases</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">105</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">31</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%B6%E6%9E%84/">架构</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Apriori/" rel="tag">Apriori</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/" rel="tag">EM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ETL/" rel="tag">ETL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/" rel="tag">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HashMap/" rel="tag">HashMap</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/" rel="tag">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-Means/" rel="tag">K-Means</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/" rel="tag">KNN</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" rel="tag">LRU淘汰算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OLAP/" rel="tag">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/" rel="tag">PageRank</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parquet/" rel="tag">Parquet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Random-Forest/" rel="tag">Random Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flask/" rel="tag">flask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kali/" rel="tag">kali</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/" rel="tag">mapreduce</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paxos/" rel="tag">paxos</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shadowsock/" rel="tag">shadowsock</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skipList/" rel="tag">skipList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/" rel="tag">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" rel="tag">单例模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A0%86/" rel="tag">堆</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/" rel="tag">布隆过滤器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%A3%E5%88%97%E8%A1%A8/" rel="tag">散列表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" rel="tag">数据仓库</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" rel="tag">数据清洗</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" rel="tag">数据采集</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" rel="tag">时间序列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" rel="tag">服务器安全</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%88/" rel="tag">栈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" rel="tag">用户画像</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" rel="tag">红黑树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" rel="tag">线性表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag">词向量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8/" rel="tag">链表</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%9F%E5%88%97/" rel="tag">队列</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/Apriori/" style="font-size:13.11px">Apriori</a> <a href="/tags/Decision-Tree/" style="font-size:13.44px">Decision Tree</a> <a href="/tags/EM/" style="font-size:13.11px">EM</a> <a href="/tags/ETL/" style="font-size:13px">ETL</a> <a href="/tags/Flink/" style="font-size:14px">Flink</a> <a href="/tags/GPT-2/" style="font-size:13px">GPT-2</a> <a href="/tags/HBase/" style="font-size:13.56px">HBase</a> <a href="/tags/HashMap/" style="font-size:13px">HashMap</a> <a href="/tags/IDEA/" style="font-size:13px">IDEA</a> <a href="/tags/K-Means/" style="font-size:13px">K-Means</a> <a href="/tags/KNN/" style="font-size:13.11px">KNN</a> <a href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" style="font-size:13px">LRU淘汰算法</a> <a href="/tags/Naive-Bayes/" style="font-size:13.11px">Naive Bayes</a> <a href="/tags/OLAP/" style="font-size:13px">OLAP</a> <a href="/tags/PageRank/" style="font-size:13.11px">PageRank</a> <a href="/tags/Parquet/" style="font-size:13px">Parquet</a> <a href="/tags/Random-Forest/" style="font-size:13px">Random Forest</a> <a href="/tags/SVM/" style="font-size:13.11px">SVM</a> <a href="/tags/docker/" style="font-size:13.11px">docker</a> <a href="/tags/flask/" style="font-size:13.11px">flask</a> <a href="/tags/flink/" style="font-size:13px">flink</a> <a href="/tags/hdfs/" style="font-size:13.33px">hdfs</a> <a href="/tags/hive/" style="font-size:13.33px">hive</a> <a href="/tags/java/" style="font-size:13px">java</a> <a href="/tags/kafka/" style="font-size:13.67px">kafka</a> <a href="/tags/kali/" style="font-size:13px">kali</a> <a href="/tags/mapreduce/" style="font-size:13px">mapreduce</a> <a href="/tags/mysql/" style="font-size:13.22px">mysql</a> <a href="/tags/paxos/" style="font-size:13.11px">paxos</a> <a href="/tags/python/" style="font-size:13.56px">python</a> <a href="/tags/redis/" style="font-size:13.33px">redis</a> <a href="/tags/scala/" style="font-size:13.33px">scala</a> <a href="/tags/shadowsock/" style="font-size:13px">shadowsock</a> <a href="/tags/skipList/" style="font-size:13px">skipList</a> <a href="/tags/sklearn/" style="font-size:13px">sklearn</a> <a href="/tags/spark/" style="font-size:13.89px">spark</a> <a href="/tags/yarn/" style="font-size:13px">yarn</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size:13.11px">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size:13.22px">二叉树</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size:13px">动态规划</a> <a href="/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" style="font-size:13px">单例模式</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size:13px">回溯</a> <a href="/tags/%E5%A0%86/" style="font-size:13px">堆</a> <a href="/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/" style="font-size:13px">布隆过滤器</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size:13.78px">排序</a> <a href="/tags/%E6%95%A3%E5%88%97%E8%A1%A8/" style="font-size:13px">散列表</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" style="font-size:13.78px">数据仓库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" style="font-size:13px">数据清洗</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" style="font-size:13px">数据采集</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size:13px">数组</a> <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" style="font-size:13px">时间序列</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" style="font-size:13.11px">服务器安全</a> <a href="/tags/%E6%A0%88/" style="font-size:13px">栈</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:13px">深度学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:13px">爬虫</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size:13.33px">特征工程</a> <a href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" style="font-size:13.11px">用户画像</a> <a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" style="font-size:13px">红黑树</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" style="font-size:13px">线性表</a> <a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" style="font-size:13px">词向量</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size:13px">递归</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size:13px">逻辑回归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size:13.11px">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size:13px">队列</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">二月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">九月 2015</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/10/03/HBase%E4%B8%ADbloomfilter%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/" class="title">HBase中bloomfilter源码实现</a></p><p class="item-date"><time datetime="2020-10-03T08:10:56.000Z" itemprop="datePublished">2020-10-03</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/09/30/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%9C%A8HBase%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/" class="title">布隆过滤器在HBase中的应用</a></p><p class="item-date"><time datetime="2020-09-30T07:18:58.000Z" itemprop="datePublished">2020-09-30</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/09/27/%E6%8E%A2%E7%B4%A2B-%E6%A0%91%E5%92%8CLSM-Tree/" class="title">探索B+树和LSM-Tree</a></p><p class="item-date"><time datetime="2020-09-27T03:07:25.000Z" itemprop="datePublished">2020-09-27</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/09/13/Kafka-%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95/" class="title">Kafka 压缩算法</a></p><p class="item-date"><time datetime="2020-09-13T13:58:54.000Z" itemprop="datePublished">2020-09-13</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></p><p class="item-title"><a href="/2020/09/03/%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8Spark%E4%B8%AD%E8%8A%82%E7%BA%A6%E7%A9%BA%E9%97%B4%E4%BA%86%E4%B9%88%EF%BC%9F/" class="title">序列化在Spark中节约空间了么？</a></p><p class="item-date"><time datetime="2020-09-03T15:58:14.000Z" itemprop="datePublished">2020-09-03</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-Spark-SQL-处理结构化数据：DataFrame和Dataset" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Spark SQL 处理结构化数据：DataFrame和Dataset</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/" class="article-date"><time datetime="2018-09-03T14:56:04.000Z" itemprop="datePublished">2018-09-03</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/" class="leancloud_visitors" data-flag-title="Spark SQL 处理结构化数据：DataFrame和Dataset">0</span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 5.9k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 26(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p>RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集合完成数据处理的逻辑更像是程序员们的思维方式。对于数据处理逻辑，分析师们更习惯用 SQL 而非算子来表达。所以，Spark 借鉴了 Python 数据分析库 pandas 中 DataFrame 的概念，推出了 DataFrame、Dataset 与 Spark SQL。<br><br><br>在数据科学领域中，DataFrame 抽象了矩阵，如 R、pandas 中的 DataFrame；在数据工程领域，如 Spark SQL 中，DataFrame 更多地代表了关系型数据库中的表，这样就可以利用简单易学的 SQL 来进行数据分析；在 Spark 中，我们既可以用 Spark SQL + DataFrame 的组合实现海量数据分析，也可用 DataFrame + MLlib（Spark 机器学习库）的组合实现海量数据挖掘。<br><br><br>在计算机领域中，高级往往意味着简单、封装程度高，而与之对应的通常是复杂、底层。对于 Spark 编程来说，RDD + 算子的组合无疑是比较底层的，而 <strong>DataFrame + Spark SQL 的组合无论从学习成本，还是从性能开销上来说，都显著优于前者组合，所以无论是分析师还是程序员，这种方式才是使用 Spark 的首选。</strong> 此外，对于分析师来说，DataFrame 对他们来说并不陌生，熟悉的概念也能让他们快速上手。<br></p><p><a name="3CLh3"></a></p><h3 id="DataFrame、Dataset-的起源与演变"><a href="#DataFrame、Dataset-的起源与演变" class="headerlink" title="DataFrame、Dataset 的起源与演变"></a>DataFrame、Dataset 的起源与演变</h3><p>DataFrame 在 Spark 1.3 被引入，它的出现取代了 SchemaRDD，Dataset 最开始在 Spark 1.6 被引入，当时还属于实验性质，在 2.0 版本时正式成为 Spark 的一部分，并且在 Spark 2.0 中，DataFrame API 与 Dataset API 在形式上得到了统一。<br><br><br>Dataset API 提供了类型安全的面向对象编程接口。Dataset 可以通过将表达式和数据字段暴露给查询计划程序和 Tungsten 的快速内存编码，从而利用 Catalyst 优化器。但是，现在 DataFrame 和 Dataset 都作为 Apache Spark 2.0 的一部分，<strong>其实 DataFrame 现在是 Dataset Untyped API 的特殊情况</strong>。更具体地说：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame &#x3D; Dataset[Row]</span><br></pre></td></tr></table></figure><p>下面这张图比较清楚地表示了 DataFrame 与 Dataset 的变迁与关系。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779491-df2d23ff-d06c-4811-a270-c4f2fa58b698.png#align=left&display=inline&height=651&margin=%5Bobject%20Object%5D&originHeight=651&originWidth=1758&size=0&status=done&style=none&width=1758" alt><br><strong>由于 Python 不是类型安全的语言，所以 Spark Python API 没有 Dataset API，而只提供了 DataFrame API。当然，Java 和 Scala 就没有这种问题。</strong><br><a name="git0D"></a></p><h3 id="DataFrame-API"><a href="#DataFrame-API" class="headerlink" title="DataFrame API"></a>DataFrame API</h3><p>DataFrame 与 Dataset API 提供了简单的、统一的并且更富表达力的 API ，简言之，与 RDD 与算子的组合相比，DataFrame 与 Dataset API 更加高级，所以这也是为什么我将这个模块命名为 Spark 高级编程。<br>DataFrame 不仅可以使用 SQL 进行查询，其自身也具有灵活的 API 可以对数据进行查询，与 RDD API 相比，DataFrame API 包含了更多的应用语义，<strong>所谓应用语义，就是能让计算框架知道你的目标的信息</strong>，这样计算框架就能更有针对性地对作业进行优化，本课时主要介绍如何创建DataFrame 以及如何利用 DataFrame 进行查询。<br><a name="ijTdF"></a></p><h4 id="1、创建DataFrame"><a href="#1、创建DataFrame" class="headerlink" title="1、创建DataFrame"></a>1、创建DataFrame</h4><p>DataFrame 目前支持多种数据源、文件格式，如 Json、CSV 等，也支持由外部数据库直接读取数据生成，此外还支持由 RDD 通过类型反射生成，甚至还可以通过流式数据源生成，这在下个模块会详细介绍。DataFrame API 非常标准，创建 DataFrame 都通过 read 读取器进行读取。下面列举了如何读取几种常见格式的文件。</p><ul><li>读取 Json 文件。Json 文件如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br><span class="line">......</span><br><span class="line">val df &#x3D; spark.read.json(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.json&quot;)</span><br></pre></td></tr></table></figure><p>我们可以利用初始化好的 SparkSession（spark）读取 Json 格式文件。</p><ul><li><p>读取 CSV 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read.csv(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)</span><br></pre></td></tr></table></figure></li><li><p>从 Parquet 格式文件中生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read.parquet(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)</span><br></pre></td></tr></table></figure></li><li><p>从 ORC 格式文件中生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read.orc(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)</span><br><span class="line">关于 ORC 与 Parquet 文件格式会在后面详细介绍。</span><br></pre></td></tr></table></figure></li><li><p>从文本中生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read.text(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)</span><br></pre></td></tr></table></figure></li><li><p>通过 JDBC 连接外部数据库读取数据生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">.load()</span><br></pre></td></tr></table></figure><p>上面的代码表示通过 JDBC 相关配置，读取数据。</p></li><li><p>通过 RDD 反射生成。此种方法是字符串反射为 DataFrame 的 Schema，再和已经存在的 RDD 一起生成 DataFrame，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val schemaString &#x3D; &quot;id f1 f2 f3 f4&quot;</span><br><span class="line">&#x2F;&#x2F; 通过字符串转换和类型反射生成schema</span><br><span class="line">val fields &#x3D; schemaString.split(&quot; &quot;).map(fieldName &#x3D;&gt; StructField(fieldName, StringType, nullable &#x3D; true))</span><br><span class="line">val schema &#x3D; StructType(fields)</span><br><span class="line">&#x2F;&#x2F; 需要将RDD转化为RDD[Row]类型</span><br><span class="line">val rowRDD &#x3D; spark.sparkContext.textFile(textFilePath).map(_.split(&quot;,&quot;)).map(attributes &#x3D;&gt; </span><br><span class="line">Row(attributes(0), </span><br><span class="line">attributes(1),</span><br><span class="line">attributes(2),</span><br><span class="line">attributes(3),</span><br><span class="line">attributes(4).trim)</span><br><span class="line">)</span><br><span class="line">&#x2F;&#x2F; 生成DataFrame</span><br><span class="line">val df &#x3D; spark.createDataFrame(rowRDD, schema)</span><br></pre></td></tr></table></figure><p>注意这种方式需要隐式转换，需在转换前写上第一行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br></pre></td></tr></table></figure><p>DataFrame 初始化完成后，可以通过 show 方法来查看数据，Json、Parquet、ORC 等数据源是自带 Schema 的，而那些无 Schema 的数据源，DataFrame 会自己生成 Schema。<br>Json 文件生成的 DataFrame 如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779457-41224808-31bf-44de-a1c2-a2446e2b32cf.png#align=left&display=inline&height=107&margin=%5Bobject%20Object%5D&originHeight=107&originWidth=100&size=0&status=done&style=none&width=100" alt><br>CSV 文件生成的 DataFrame 如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779524-643859bf-a822-4c2e-af47-2a1abec78e93.png#align=left&display=inline&height=120&margin=%5Bobject%20Object%5D&originHeight=120&originWidth=135&size=0&status=done&style=none&width=135" alt><br><a name="PT06B"></a></p><h4 id="2、查询"><a href="#2、查询" class="headerlink" title="2、查询"></a>2、查询</h4><p>完成初始化的工作之后就可以使用 DataFrame API 进行查询了，DataFrame API 主要分为两种风格，一种依然是 RDD 算子风格，如 reduce、groupByKey、map、flatMap 等，另外一种则是 SQL 风格，如 select、where 等。<br><strong>2.1 算子风格</strong><br>我们简单选取几个有代表性的 RDD 算子风格的 API，具体如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def groupByKey[K: Encoder](func: T &#x3D;&gt; K): KeyValueGroupedDataset[K, T]</span><br></pre></td></tr></table></figure><p>与 RDD 算子版作用相同，返回类型为 Dataset。<br></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def map[U : Encoder](func: T &#x3D;&gt; U): Dataset[U]</span><br></pre></td></tr></table></figure><p>与 RDD 算子版作用相同，返回类型为 Dataset。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def flatMap[U : Encoder](func: T &#x3D;&gt; TraversableOnce[U]): Dataset[U]</span><br></pre></td></tr></table></figure><p>与 RDD 算子版作用相同，返回类型为 Dataset。<br>这些算子用法大同小异，但都需要传入 Encoder 参数，这可以通过隐式转换解决，在调用算子前，需加上：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br></pre></td></tr></table></figure><p><strong>2.2 SQL风格</strong><br>这类 API 的共同之处就是支持将部分 SQL 语法的字符串作为参数直接传入。</p><ul><li>select 和 where</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def select(cols: Column*): DataFrame</span><br><span class="line">def where(conditionExpr: String): Dataset[T]</span><br></pre></td></tr></table></figure><p>条件查询，例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;).where(&quot;name is not null and age &gt; 10&quot;).foreach(println(_))</span><br></pre></td></tr></table></figure><ul><li>groupBy</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def groupBy(col1: String, cols: String*): RelationalGroupedDataset</span><br></pre></td></tr></table></figure><p>分组统计。例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;name&quot;,&quot;age&quot;).groupBy(&quot;age&quot;).count().foreach(println(_))</span><br></pre></td></tr></table></figure><p>此外，某些 RDD 算子风格的 API 也可以传入部分 SQL 语法的字符串，如 filter。例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).foreach(println(_))</span><br></pre></td></tr></table></figure><ul><li>join</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame</span><br></pre></td></tr></table></figure><p>DataFrame API 还支持最普遍的连接操作，代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val leftDF &#x3D; ...</span><br><span class="line">val rightDF &#x3D; ...</span><br><span class="line">leftDF.join(rightDF, leftDF(&quot;pid&quot;) &#x3D;&#x3D;&#x3D; rightDF(&quot;fid&quot;), &quot;left_outer&quot;).foreach(println(_))</span><br></pre></td></tr></table></figure><p>其中 joinType 参数支持常用的连接类型，选项有 inner、cross、outer、full、full_outer、left、left_outer、right、right_outer、left_semi 和 left_anti，其中 cross 表示笛卡儿积，这在实际使用中比较少见；left_semi 是左半连接，是 Spark 对标准 SQL 中的 in 关键字的变通实现；left_anti 是 Spark 对标准 SQL 中的 not in 关键字的变通实现。<br><br><br>除了 groupBy 这种分组方式，DataFrame 还支持一些特别的分组方式如 pivot、rollup、cube 等，以及常用的分析函数，先来看一个数据集：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:92, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:75, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:62, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:96, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:98, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:78, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:90, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:76, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:74, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:68, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:95, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:81, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:95, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:91, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:85, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:77, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:63, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:99, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:79, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:85, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;</span><br></pre></td></tr></table></figure><p>以上是某班学生 3 年的成绩单，一共有 3 个维度，即 name、subject 和 year，度量为 grade，也就是成绩，因此，这个 DataFrame 可以看成三维数据立方体，如下图所示。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779444-4ec1819a-9f1c-41b0-98cf-929ce2670dc4.png#align=left&display=inline&height=406&margin=%5Bobject%20Object%5D&originHeight=406&originWidth=570&size=0&status=done&style=none&width=570" alt><br>现在需要统计每个学生各科目 3 年的平均成绩，该操作可以通过下面的方式实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.groupBy(&quot;name&quot;,&quot;subject&quot;).avg(&quot;subject&quot;)</span><br></pre></td></tr></table></figure><p><br>但是，这种形式使结果数据集只有两列——name 和 subject，不利于进一步分析，而利用 DataFrame 的数据透视 pivot 功能无疑更加方便。 pivot 功能在 pandas、Excel 等分析工具已得到了广泛应用，<strong>用户想使用透视功能，需要指定分组规则、需要透视的列以及聚合的维度列。</strong><br><br><br>所谓“透视”比较形象，即在分组结果上，对每一组进行“透视”，透视的结果会导致每一组基于透视列展开，最后再根据聚合操作进行聚合，统计每个学生每科 3 年平均成绩实现如下：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.groupBy(&quot;name&quot;).pivot(&quot;subject&quot;).avg(&quot;grade&quot;).show()</span><br></pre></td></tr></table></figure><p>结果如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779509-a2a3e12a-a53e-48f2-931e-12b06cfb97b3.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&originHeight=94&originWidth=252&size=0&status=done&style=none&width=252" alt><br><br><br>除了 groupBy 之外，DataFrame 还提供 rollup 和 cube 的方式进行分组聚合，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def rollup(col1: String, cols: String*): RelationalGroupedDataset</span><br></pre></td></tr></table></figure><p><br>rollup 也是用来进行分组统计，只不过分组逻辑有所不同，假设 rollup(A,B,C)，其中 A、B、C 分别为 3 列，那么会先对 A、B、C 进行分组，然后依次对 A、B 进行分组、对 A 进行分组、对全表进行分组，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.rollup(&quot;name&quot;, &quot;subject&quot;).avg(&quot;grade&quot;).show()</span><br></pre></td></tr></table></figure><p>结果如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779551-1af25ed4-6183-4ffb-8eac-67655265601e.png#align=left&display=inline&height=201&margin=%5Bobject%20Object%5D&originHeight=201&originWidth=194&size=0&status=done&style=none&width=194" alt><br>可以看到，除了按照 name + subject 的组合键进行分组，还分别对每个人进行了分组，如 Michael,null，此外还将全表分为了一组，如 null,null。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def cube(col1: String, cols: String*): RelationalGroupedDataset</span><br></pre></td></tr></table></figure><p><br>cube 与 rollup 类似，分组依据有所不同，仍以 cube(A,B,C) 为例，分组依据分别是 (A,B,C)、(A,B)、(A,C)、(B,C)、(A)、(B)、(C)、全表，执行：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.cube(&quot;name&quot;, &quot;subject&quot;).avg(&quot;grade&quot;).show()</span><br></pre></td></tr></table></figure><p>结果如下：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1599143779441-814f7b75-4739-4bc9-9b39-c1be42fa2c85.png#align=left&display=inline&height=244&margin=%5Bobject%20Object%5D&originHeight=244&originWidth=211&size=0&status=done&style=none&width=211" alt><br>可以看到与 rollup 不同，这里还分别对每个科目进行分组，如 null、math。<br>在实际使用中，你应该尽量选用并习惯于用 SQL 风格的算子完成开发任务，SQL 风格的查询 API 不光表现力强，另外也非常易读。<br><a name="efZJT"></a></p><h4><a href="#" class="headerlink"></a></h4><p><a name="ovgX1"></a></p><h4 id="3、写出"><a href="#3、写出" class="headerlink" title="3、写出"></a>3、写出</h4><p>与创建 DataFrame 的 read 读取器相对应，写出为 write 输出器 API。下面列举了如何输出几种常见格式的文件。</p><ul><li><p>写出为 Json 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.json(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)</span><br></pre></td></tr></table></figure></li><li><p>写出为 Parquet 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.parquet(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)</span><br></pre></td></tr></table></figure></li><li><p>写出为 ORC 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.orc(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)</span><br></pre></td></tr></table></figure></li><li><p>写出为文本文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.text(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)</span><br></pre></td></tr></table></figure></li><li><p>写出为 CSV 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val saveOptions &#x3D; Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;path&quot; -&gt; &quot;csvout&quot;)</span><br><span class="line">df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;)</span><br><span class="line">.write</span><br><span class="line">.format(&quot;com.databricks.spark.csv&quot;)</span><br><span class="line">.mode(SaveMode.Overwrite)</span><br><span class="line">.options(saveOptions)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure></li></ul><p><br>我们还可以在保存时对格式已经输出的方式进行设定，例如本例中是保留表头，并且输出方式是 Overwrite，输出方式有 Append、ErrorIfExist、Ignore、Overwrite，分别代表追加到已有输出路径中、如果输出路径存在则报错、存在则停止、存在则覆盖。</p><ul><li>写出到关系型数据库：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val prop &#x3D; new java.util.Properties</span><br><span class="line">prop.setProperty(&quot;user&quot;,&quot;spark&quot;)</span><br><span class="line">prop.setProperty(&quot;password&quot;,&quot;123&quot;)</span><br><span class="line">df.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;test&quot;,&quot;tablename&quot;,prop)</span><br></pre></td></tr></table></figure>写出到关系型数据库同样基于 JDBC ，用此种方式写入关系型数据库，表名可以不存在。<br><a name="wqMv4"></a><h3 id="Dataset-API"><a href="#Dataset-API" class="headerlink" title="Dataset API"></a>Dataset API</h3>从本质上来说，DataFrame 只是 Dataset 的一种特殊情况，在 Spark 2.x 中已经得到了统一：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame &#x3D; Dataset[Row]</span><br></pre></td></tr></table></figure>因此，在使用 DataFrame API 的过程中，很容易就会自动转换为 Dataset[String]、Dataset[Int] 等类型。除此之外，用户还可以自定义类型。下面来看看 DataFrame 转成 Dataset 的例子，下面是一个 Json 文件，记录了学生的单科成绩：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:92, &quot;subject&quot;:&quot;Chinese&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;Chinese&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:75, &quot;subject&quot;:&quot;Chinese&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:62, &quot;subject&quot;:&quot;Chinese&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:96, &quot;subject&quot;:&quot;math&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:98, &quot;subject&quot;:&quot;math&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:78, &quot;subject&quot;:&quot;math&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;math&quot;&#125;</span><br></pre></td></tr></table></figure>代码如下：<br></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 首先定义StudentGrade类</span><br><span class="line">case class StudentGrade(name: String, subject: String, grade: Long)</span><br><span class="line">&#x2F;&#x2F; 生成DataFrame</span><br><span class="line">val dfSG &#x3D; spark.read.json(&quot;data&#x2F;examples&#x2F;target&#x2F;scala-2.11&#x2F;classes&#x2F;student_grade.json&quot;)</span><br><span class="line">&#x2F;&#x2F; 方法1:通过map函数手动转换为Dataset[StudentGrade]类型</span><br><span class="line">val dsSG: Dataset[StudentGrade] &#x3D; dfSG.map(a &#x3D;&gt; StudentGrade(a.getAs[String](0),a.getAs[String](1),a.getAs[Long](2)))</span><br><span class="line">&#x2F;&#x2F; 方法2:使用DataFrame的as函数进行转换</span><br><span class="line">val dsSG2: Dataset[StudentGrade] &#x3D; dfSG.as[StudentGrade]</span><br><span class="line">&#x2F;&#x2F; 方法3：通过RDD转换而成（基于同样内容的CSV文件）</span><br><span class="line">val dsSG3 &#x3D; spark.sparkContext.</span><br><span class="line">textFile(&quot;data&#x2F;examples&#x2F;target&#x2F;scala-2.11&#x2F;classes&#x2F;student_grade.csv&quot;).</span><br><span class="line">map[StudentGrade](row &#x3D;&gt; &#123;</span><br><span class="line">     val fields &#x3D; row.split(&quot;,&quot;)</span><br><span class="line">	 StudentGrade(</span><br><span class="line">	      fields(0).toString(),</span><br><span class="line">	      fields(1).toString(),</span><br><span class="line">	      fields(2).toLong</span><br><span class="line">	    )</span><br><span class="line">&#125;).toDS</span><br><span class="line">	 </span><br><span class="line">&#x2F;&#x2F; 求每科的平均分</span><br><span class="line">dsSG3.groupBy(&quot;subject&quot;).mean(&quot;grade&quot;).foreach(println(_))</span><br><span class="line">以上 3 种方法都可以将 DataFrame 转换为 Dataset 。转换完成后，就可以使用其 API 对数据进行分析，使用方式与 DataFrame 并无不同。</span><br></pre></td></tr></table></figure><p><a name="rKOjW"></a></p><h3 id="-1"><a href="#-1" class="headerlink"></a></h3><p><a name="UpJTr"></a></p><h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>在实际工作中，使用频率最高的当属 Spark SQL，通常一个大数据处理项目中，70% 的数据处理任务都是由 Spark SQL 完成，它贯穿于数据预处理、数据转换和最后的数据分析。由于 SQL 的学习成本低、用户基数大、函数丰富，Spark SQL 也通常是使用 Spark 最方便的方式。此外，由于 SQL 包含了丰富的应用语义，所以 Catalyst 优化器带来的性能巨大提升也使 Spark SQL 成为编写 Spark 作业的最佳方式。接下来我将为你介绍 Spark SQL 的使用。<br><br><br>从使用层面上来讲，要想用好 Spark SQL，只需要编写 SQL 就行了，本课时的最后简单介绍了下 SQL 的常用语法，方便没有接触过 SQL 的同学快速入门。<br><a name="n1Wfj"></a></p><h4 id="1、创建临时视图"><a href="#1、创建临时视图" class="headerlink" title="1、创建临时视图"></a>1、创建临时视图</h4><p>想使用 Spark SQL，可以先创建临时视图，相当于数据库中的表，这可以通过已经存在的 DataFrame、Dataset 直接生成；也可以直接从 Hive 元数据库中获取元数据信息直接进行查询。先来看看创建临时视图：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">case class StudentGrade(name: String, subject: String, grade: Long)</span><br><span class="line">&#x2F;&#x2F; 生成DataFrame</span><br><span class="line">val dfSG &#x3D; spark.read.json(&quot;data&#x2F;examples&#x2F;target&#x2F;scala-2.11&#x2F;classes&#x2F;student_grade.json&quot;)</span><br><span class="line">&#x2F;&#x2F; 生成Dataset</span><br><span class="line">val dsSG &#x3D; dfSG.map(</span><br><span class="line">	  a &#x3D;&gt; StudentGrade( </span><br><span class="line">	      a.getAs[String](&quot;name&quot;),</span><br><span class="line">	      a.getAs[String](&quot;subject&quot;),</span><br><span class="line">	      a.getAs[Long](&quot;grade&quot;)</span><br><span class="line">	  )</span><br><span class="line">)</span><br><span class="line">	 </span><br><span class="line">&#x2F;&#x2F; 创建临时视图</span><br><span class="line">dfSG.createOrReplaceTempView(&quot;student_grade_df&quot;)</span><br><span class="line">dsSG.createOrReplaceTempView(&quot;student_grade_ds&quot;)</span><br><span class="line">&#x2F;&#x2F; 计算每科的平均分</span><br><span class="line">spark.sql(&quot;SELECT subject, AVG(grade) FROM student_grade_df GROUP BY subject&quot;).show()</span><br><span class="line">spark.sql(&quot;SELECT subject, AVG(grade) FROM student_grade_ds GROUP BY subject&quot;).show()</span><br></pre></td></tr></table></figure><p>对于 Dataset 来说，对象类型的数据结构会作为临时视图的元数据，在 SQL 中可以直接使用。<br><a name="EqLWQ"></a></p><h4 id="2、使用Hive元数据"><a href="#2、使用Hive元数据" class="headerlink" title="2、使用Hive元数据"></a>2、使用Hive元数据</h4><p>随着 Spark 越来越流行，有很多情况，需要将 Hive 作业改写成 Spark SQL 作业，Spark SQL 可以通过 hive-site.xml 文件的配置，直接读取 Hive 元数据。这样，改写的工作量就小了很多，代码如下：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val spark &#x3D; SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.master(&quot;local[*]&quot;)</span><br><span class="line">.appName(&quot;Hive on Spark&quot;)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line">&#x2F;&#x2F; 直接查询</span><br><span class="line">spark.sql(…………)</span><br></pre></td></tr></table></figure><p>代码中通过 enableHiveSupport 方法开启对 Hive 的支持，但需要将 Hive 配置文件 hive-site.xml 复制到 Spark 的配置文件夹下。<br><a name="Ruekt"></a></p><h4 id="3、查询语句"><a href="#3、查询语句" class="headerlink" title="3、查询语句"></a>3、查询语句</h4><p>Spark 的 SQL 语法源于 Presto （一种支持 SQL 的大规模并行处理技术，适合 OLAP），在源码中我们可以看见，Spark 的 SQL 解析引擎直接采用了 Presto 的 SQL 语法文件。查询是 Spark SQL 的核心功能，Spark SQL 的查询语句模式如下：<br>复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[ WITH with_query [, ...] ]</span><br><span class="line">SELECT [ ALL | DISTINCT ] select_expr [, ...]</span><br><span class="line">[ FROM from_item [, ...] ]</span><br><span class="line">[ WHERE condition ]</span><br><span class="line">[ GROUP BY expression [, ...] ]</span><br><span class="line">[ HAVING condition]</span><br><span class="line">[ UNION [ ALL | DISTINCT ] select ]</span><br><span class="line">[ ORDER BY expression [ ASC | DESC ] [, ...] ]</span><br><span class="line">[ LIMIT count ]</span><br></pre></td></tr></table></figure><p>其中 from_item 为以下之一：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ]</span><br><span class="line">from_item join_type from_item [ ON join_condition | USING ( join_column [, ...] ) ]</span><br></pre></td></tr></table></figure><p>该模式基本涵盖了 Spark SQL 中查询语句的各种写法。<br><strong><br></strong>3.1 SELECT 与 FROM 子句**<br>SELECT 与 FROM 是构成查询语句的最小单元，SELECT 后面跟列名表示要查询的列，或者用 * 表示所有列，FROM 后面跟表名，示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT name, grade FROM student_grade t;</span><br></pre></td></tr></table></figure><p><br>在使用过程中，对列名和表名都可以赋予别名，这里对 student_grade 赋予别名 t，此外我们还可以对某一列用关键字 DISTINCT 进行去重，默认为 ALL，表示不去重：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT( DISTINCT name) FROM student_grade;</span><br></pre></td></tr></table></figure><p>上面这条 SQL 代表统计有多少学生参加了考试。<br><strong><br></strong>3.2 WHERE 子句**<br>WHERE 子句经常和 SELECT 配合使用，用来过滤参与查询的数据集，WHERE 后面一般会由运算符组合成谓词表达式（返回值为 True 或者 False ），例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM student_grade WHERE grade &gt; 90;</span><br><span class="line">SELECT * FROM student_grade WHERE name IS NOT NULL;</span><br><span class="line">SELECT * FROM student_grade WHERE name LIKE &quot;*ndy&quot;;</span><br></pre></td></tr></table></figure><p>常见的运算符还有 !=、&lt;&gt; 等，此外还可以用逻辑运算符：AND、OR 组合谓词表达式进行查询，例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM student_grade WHERE grade &gt; 90 AND name IS NOT NULL</span><br></pre></td></tr></table></figure><p><strong><br></strong>3.3 GROUP BY 子句**<br>GROUP BY 子句用于对 SELECT 语句的输出进行分组，分组中是匹配值的数据行。GROUP BY 子句支持指定列名或列序号（从 1 开始）表达式。以下查询是等价的，都会对 subject 列进行分组，第一个查询使用列序号，第二个查询使用列名：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT avg(grade), subject FROM student_grade GROUP BY 2;</span><br><span class="line">SELECT avg(grade), subject FROM student_grade GROUP BY subject;</span><br></pre></td></tr></table></figure><p><strong>使用 GROUP BY 子句时需注意，出现在 SELECT 后面的列，要么同时出现在 GROUP BY 后面，要么就在聚合函数中。</strong><br><strong><br></strong>3.4 HAVING 子句**<br>HAVING 子句与聚合函数以及 GROUP BY 子句配合使用，用来过滤分组统计的结果。HAVING 子句去掉不满足条件的分组。在分组和聚合计算完成后，HAVING 对分组进行过滤。例如以下查询会过滤掉平均分大于 90 分的科目：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT subject,AVG(grade) </span><br><span class="line">FROM student_grade </span><br><span class="line">GROUP BY subject </span><br><span class="line">HAVING AVG(grade) &lt; 90;</span><br></pre></td></tr></table></figure><p><strong><br></strong>3.5 UNION 子句**<br>UNION 子句用于将多个查询语句的结果合并为一个结果集：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query UNION [ALL | DISTINCT] query</span><br></pre></td></tr></table></figure><p>参数 ALL 或 DISTINCT 可以控制最终结果集包含哪些行。如果指定参数 ALL，则包含全部行，即使行完全相同；如果指定参数 DISTINCT ，则合并结果集，结果集只有唯一不重复的行；如果不指定参数，执行时默认使用 DISTINCT。下面这句 SQL 是将两个班级的成绩进行合并：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM student_grade_class1</span><br><span class="line">UNION ALL </span><br><span class="line">SELECT * FROM student_grade_class2;</span><br></pre></td></tr></table></figure><p>多个 UNION 子句会从左向右执行，除非用括号明确指定顺序。<br><strong><br></strong>3.6 ORDER BY 子句**<br>ORDER BY 子句按照一个或多个输出表达式对结果集排序：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ORDER BY expression [ ASC | DESC ] [ NULLS &#123; FIRST | LAST &#125; ] [, ...]</span><br></pre></td></tr></table></figure><p>每个表达式由列名或列序号（从 1 开始）组成。ORDER BY 子句作为查询的最后一步，在 GROUP BY 和 HAVING 子句之后。ASC 为默认升序，DESC 为降序。下面这句 SQL 会对结果进行过滤，并按照平均分进行排序，<strong>注意这里使用了列别名</strong>：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT subject,AVG(grade) avg </span><br><span class="line">FROM student_grade </span><br><span class="line">GROUP BY subject </span><br><span class="line">HAVING AVG(grade) &lt; 90 </span><br><span class="line">ORDER BY avg DESC;</span><br></pre></td></tr></table></figure><p><strong><br></strong>3.7 LIMIT 子句**<br>LIMIT 子句限制结果集的行数，这在查询大表时很有用。以下示例为对单科成绩进行排序并只返回前 3 名的记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM student_grade </span><br><span class="line">WHERE subject &#x3D; &#39;math&#39; </span><br><span class="line">ORDER BY grade DESC </span><br><span class="line">LIMIT 3;</span><br></pre></td></tr></table></figure><p><strong><br></strong>3.8 JOIN 子句**<br>JOIN 操作可以将多个有关联的表进行关联查询，下面这句 SQL 是查询数学成绩在 90 分以上的学生的院系，其中院系信息可以从学生基础信息表内，通过姓名连接得到：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT g.*, a.department </span><br><span class="line">FROM student_grade g </span><br><span class="line">JOIN student_basic b </span><br><span class="line">ON g.name &#x3D; b.name </span><br><span class="line">WHERE g.subject &#x3D; &#39;math&#39; and grade &gt; 90</span><br></pre></td></tr></table></figure><p>在这句 SQL 中，表 g 被称为驱动表或是左表，表 b 被称为右表。如前所述，Spark 支持多种连接类型。<br><a name="uE3K0"></a></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>由于 Spark 对于 SQL 支持得非常好，而 pandas 在这方面没那么强大，所以，在某些场景，你可以选择 Spark SQL 来代替 pandas，这有时对于分析师来说非常好用。</strong></p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="cpeixin.cn/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/" title="Spark SQL 处理结构化数据：DataFrame和Dataset" target="_blank" rel="external">cpeixin.cn/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external nofollow noopener noreferrer">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/cpeixin" target="_blank" class="img-burn thumb-sm visible-lg" rel="external nofollow noopener noreferrer"><img src="/images/avatar.jpg" class="img-rounded w-full" alt></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><span class="text-dark">Brent</span><small class="ml-1x">大数据工程师 &amp; 机器学习</small></a></h3><div>一心九用的工程师</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2018/09/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%8E%9F%E7%90%86/" title="PageRank 原理"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2018/09/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%8E%9F%E7%90%86/" title="数据分析-关联规则原理"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank" rel="external nofollow noopener noreferrer">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank" rel="external nofollow noopener noreferrer">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"SsxmBzBQ3R2S2zWTv0FrONel-gzGzoHsz",appKey:"w0K528Ye7NhOr07RHrzVzHbW",placeholder:"说点什么呢？",avatar:"mm",meta:meta,pageSize:"10",visitor:!0})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html><script type="text/javascript" src="//cdn.jsdelivr.net/gh/ygbhf/clicklove/clicklove.js"></script><!-- rebuild by neat -->