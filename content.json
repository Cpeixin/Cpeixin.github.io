{"meta":{"title":"布兰特 | 不忘初心","subtitle":"人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华","description":"","author":"Brent","url":"cpeixin.cn","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-04-04T08:29:08.762Z","updated":"2020-04-03T14:56:33.596Z","comments":false,"path":"/404.html","permalink":"cpeixin.cn/404.html","excerpt":"","text":""},{"title":"书单","date":"2020-04-04T08:29:08.739Z","updated":"2020-04-03T14:56:33.596Z","comments":false,"path":"books/index.html","permalink":"cpeixin.cn/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2020-04-04T14:37:07.111Z","updated":"2020-04-04T14:37:07.111Z","comments":false,"path":"about/index.html","permalink":"cpeixin.cn/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-04-04T08:29:08.728Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"categories/index.html","permalink":"cpeixin.cn/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-04-04T08:29:08.717Z","updated":"2020-04-03T14:56:33.597Z","comments":true,"path":"links/index.html","permalink":"cpeixin.cn/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-04-04T08:29:08.705Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"repository/index.html","permalink":"cpeixin.cn/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-04-04T08:29:08.693Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"tags/index.html","permalink":"cpeixin.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"def neverGrowUp()","slug":"def-neverGrowUp","date":"2020-04-05T16:00:00.000Z","updated":"2020-04-05T15:10:24.541Z","comments":true,"path":"2020/04/06/def-neverGrowUp/","link":"","permalink":"cpeixin.cn/2020/04/06/def-neverGrowUp/","excerpt":"","text":"123456789def neverGrowUp() while true: 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心","categories":[],"tags":[]},{"title":"抗疫英雄","slug":"抗疫英雄","date":"2020-04-04T14:45:15.000Z","updated":"2020-04-05T14:46:33.308Z","comments":true,"path":"2020/04/04/抗疫英雄/","link":"","permalink":"cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/","excerpt":"","text":"致敬缅怀每一位抗疫英雄","categories":[],"tags":[]},{"title":"python Flask & Ajax 数据传输","slug":"python-Flask-Ajax-数据传输","date":"2020-03-11T14:43:01.000Z","updated":"2020-04-04T17:13:00.080Z","comments":true,"path":"2020/03/11/python-Flask-Ajax-数据传输/","link":"","permalink":"cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/","excerpt":"","text":"帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂忙活三个小时，终于把前端和后端打通了～～前端demo：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;script src=\"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\"&gt;&lt;/script&gt;&lt;body&gt;&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;&lt;form action=\"&#123;&#123; url_for('send_message') &#125;&#125;\" method=\"post\"&gt; &lt;textarea name =\"domain\" rows=\"30\" cols=\"100\" placeholder=\"请输入需要查询的域名,如cq5999.com\"&gt;&lt;/textarea&gt; &lt;!--&lt;input id=\"submit\" type=\"submit\" value=\"发送\"&gt;--&gt; &lt;button type=\"submit\" id=\"btn-bq\" data-toggle=\"modal\" data-target=\"#myModal\"&gt;查询&lt;/button&gt;&lt;/form&gt;&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;&lt;div&gt; &lt;label for=\"send_content\"&gt;向后台发送消息：&lt;/label&gt; &lt;input id=\"send_content\" type=\"text\" name=\"send_content\"&gt; &lt;input id=\"send\" type=\"button\" value=\"发送\"&gt;&lt;/div&gt;&lt;div&gt; &lt;label for=\"recv_content\"&gt;从后台接收消息：&lt;/label&gt; &lt;input id=\"recv_content\" type=\"text\" name=\"recv_content\"&gt;&lt;/div&gt;&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;&lt;!-- 发送 --&gt;&lt;script type=\"text/javascript\"&gt; $(\"#send\").click(function () &#123; var message = $(\"#send_content\").val() alert(message) $.ajax(&#123; url:\"/send_message\", type:\"POST\", data:&#123; message:message &#125;, dataType: 'json', success:function (data) &#123; &#125; &#125;) &#125;)&lt;/script&gt;&lt;!-- 接收 --&gt;&lt;script type=\"text/javascript\"&gt; $(\"#send\").click(function () &#123; $.getJSON(\"/change_to_json\",function (data) &#123; $(\"#recv_content\").val(data.message) //将后端数据显示在前端 console.log(\"传到前端的数据的类型：\" + typeof (data.message)) $(\"#send_content\").val(\"\")//发送的输入框清空 &#125;) &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;后端demo:1234567891011121314151617181920212223242526272829303132333435from flask import Flask, render_template, request, jsonifyapp = Flask(__name__)@app.route('/')def index(): return render_template(\"index_v6.html\")@app.route('/send_message', methods=['POST'])def send_message(): global message_get message_get = \"\" message_get = request.form[\"domain\"].split('\\n') # message_get = request.form['message'] #input提交 print(\"收到前端发过来的信息：%s\" % message_get) print(\"收到数据的类型为：\" + str(type(message_get))) return \"收到消息\"@app.route('/change_to_json', methods=['GET'])def change_to_json(): global message_get message_json = &#123; \"message\": message_get &#125; return jsonify(message_json)if __name__ == '__main__': app.run(host='0.0.0.0', port=80,debug=True)","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"}]},{"title":"Python Flask接口设计-示例","slug":"Python-Flask接口设计-示例","date":"2020-03-10T15:08:35.000Z","updated":"2020-04-04T17:12:52.356Z","comments":true,"path":"2020/03/10/Python-Flask接口设计-示例/","link":"","permalink":"cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"Get 请求开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。**方法 1:****123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask import requestfrom flask import redirectfrom flask import jsonifyimport jsonapp = Flask(__name__)@app.route(\"/test_1.0\", methods=[\"GET\"])def check(): # 默认返回内容 return_dict = &#123;'return_code': '200', 'return_info': '处理成功', 'result': False&#125; # 判断入参是否为空 if request.args is None: return_dict['return_code'] = '5004' return_dict['return_info'] = '请求参数为空' return json.dumps(return_dict, ensure_ascii=False) # 获取传入的params参数 get_data = request.args.to_dict() name = get_data.get('name') age = get_data.get('age') # 对参数进行操作 return_dict['result'] = tt(name, age) return json.dumps(return_dict, ensure_ascii=False)# 功能函数def tt(name, age): result_str = \"%s今年%s岁\" % (name, age) return result_strif __name__ == '__main__': app.run(host='0.0.0.0', port=80)此种方式对应的request请求方式：拼接请求链接, 直接请求：http://0.0.0.0/test_1.0?name=ccc&amp;age=18request 请求中带有参数，如下图方法 2:123@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])def banWordSingleStart(word): return getWordStatus(word)此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字请求链接：http://0.0.0.0/api/banWordSingle/输入词Post 请求123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask import requestfrom flask import redirectfrom flask import jsonifyimport jsonapp = Flask(__name__)@app.route(\"/test_1.0\", methods=[\"POST\"])def check(): # 默认返回内容 return_dict = &#123;'return_code': '200', 'return_info': '处理成功', 'result': False&#125; # 判断入参是否为空 if request.args is None: return_dict['return_code'] = '5004' return_dict['return_info'] = '请求参数为空' return json.dumps(return_dict, ensure_ascii=False) # 获取传入的params参数 get_data = request.args.to_dict() name = get_data.get('name') age = get_data.get('age') # 对参数进行操作 return_dict['result'] = tt(name, age) return json.dumps(return_dict, ensure_ascii=False)# 功能函数def tt(name, age): result_str = \"%s今年%s岁\" % (name, age) return result_strif __name__ == '__main__': app.run(host='0.0.0.0', port=8080)请求方式：","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"}]},{"title":"IDEA install TabNine","slug":"IDEA-install-TabNine","date":"2020-01-22T02:26:15.000Z","updated":"2020-04-04T11:06:48.223Z","comments":true,"path":"2020/01/22/IDEA-install-TabNine/","link":"","permalink":"cpeixin.cn/2020/01/22/IDEA-install-TabNine/","excerpt":"","text":"TabNine是我目前遇到过最好的智能补全工具TabNine基于GPT-2的插件安装IDEA编译器，找到pluginsWindows pycharm：File&gt;settings&gt;plugins;Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins查找 TabNine, 点击 install, 随后 restart重启后：Help&gt;Edit Custom Properties…&gt;Create;在跳出来的idea.properties中输入（注：英文字符） TabNine::config随即会自动弹出TabNine激活页面；激活点击Activation Key下面的here；输入你的邮箱号；复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）等待自动安装，观察页面（最下面有log可以看当前进度）；激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；确认激活完成，重启pycharm即可；","categories":[{"name":"开发工具","slug":"开发工具","permalink":"cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"cpeixin.cn/tags/IDEA/"}]},{"title":"GPT-2 Chinese 自动生成文章 - 环境准备","slug":"GPT-2-Chinese-自动生成文章-环境准备","date":"2020-01-01T14:28:43.000Z","updated":"2020-04-13T09:28:23.224Z","comments":true,"path":"2020/01/01/GPT-2-Chinese-自动生成文章-环境准备/","link":"","permalink":"cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"Google ColabColaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。优点 免费！ 免费！免费！**谷歌云盘当登录账号进入谷歌云盘时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：引入Colab设置GPU环境打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”挂载和切换工作目录1234567from google.colab import drivedrive.mount('/content/drive')import os# os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录os.chdir('/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese') ## 共享云盘的目录其中： My Drive 代表你的google网盘根目录code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录在Colab中运行任务下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。.ipynb文件内容","categories":[{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"}],"tags":[{"name":"GPT-2","slug":"GPT-2","permalink":"cpeixin.cn/tags/GPT-2/"}]},{"title":"架构思想","slug":"架构思想","date":"2019-12-20T02:26:15.000Z","updated":"2020-04-04T11:23:45.206Z","comments":true,"path":"2019/12/20/架构思想/","link":"","permalink":"cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/","excerpt":"","text":"关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。架构思想CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度满足业务发展需求是最高准则业务建模，抽象和枚举是两种方式，需要平衡，不能走极端模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难业务架构、系统架构、数据模型面对一块新业务，如何系统架构？业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。如何模块化基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。提升系统的稳定性流控双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。容灾最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。稳定性第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。容量规划做好容量规划、系统间强弱依赖关系梳理。如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。分布式架构分布式系统分布式缓存分布式数据API 和乐高积木有什么相似之处？相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。你可以发现很多 API 和乐高积木的相似之处：标准化：通用、标准化的组件，作为基本的构建块（building blocks）；可用性：强调可用性，附有文档或使用说明；可定制：为不同功能使用不同的API；创造性：能够组合不同的 API 来创造混搭的结果；乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。","categories":[{"name":"架构","slug":"架构","permalink":"cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[]},{"title":"kali中文设置","slug":"kali中文设置","date":"2019-12-01T02:26:15.000Z","updated":"2020-04-04T11:06:21.313Z","comments":true,"path":"2019/12/01/kali中文设置/","link":"","permalink":"cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"更新源https://blog.csdn.net/qq_38333291/article/details/89764967设置编码和中文字体安装http://www.linuxdiyf.com/linux/20701.html","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"kali","slug":"kali","permalink":"cpeixin.cn/tags/kali/"}]},{"title":"分布式下的数据hash分布","slug":"分布式下的数据hash分布","date":"2019-11-19T15:05:08.000Z","updated":"2020-04-04T11:24:04.737Z","comments":true,"path":"2019/11/19/分布式下的数据hash分布/","link":"","permalink":"cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"【转载】字节跳动在Spark SQL上的核心优化实践","slug":"【转载】字节跳动在Spark-SQL上的核心优化实践","date":"2019-11-12T10:57:27.000Z","updated":"2020-05-16T10:59:03.835Z","comments":true,"path":"2019/11/12/【转载】字节跳动在Spark-SQL上的核心优化实践/","link":"","permalink":"cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践原文链接：https://juejin.im/post/5dc3ed336fb9a04a7847f25cSpark SQL 架构简介我们先简单聊一下Spark SQL 的架构。下面这张图描述了一条 SQL 提交之后需要经历的几个阶段，结合这些阶段就可以看到在哪些环节可以做优化。很多时候，做数据仓库建模的同学更倾向于直接写 SQL 而非使用 Spark 的 DSL。一条 SQL 提交之后会被 Parser 解析并转化为 Unresolved Logical Plan。它的重点是 Logical Plan 也即逻辑计划，它描述了希望做什么样的查询。Unresolved 是指该查询相关的一些信息未知，比如不知道查询的目标表的 Schema 以及数据位置。上述信息存于 Catalog 内。在生产环境中，一般由 Hive Metastore 提供 Catalog 服务。Analyzer 会结合 Catalog 将 Unresolved Logical Plan 转换为 Resolved Logical Plan。到这里还不够。不同的人写出来的 SQL 不一样，生成的 Resolved Logical Plan 也就不一样，执行效率也不一样。为了保证无论用户如何写 SQL 都可以高效的执行，Spark SQL 需要对 Resolved Logical Plan 进行优化，这个优化由 Optimizer 完成。Optimizer 包含了一系列规则，对 Resolved Logical Plan 进行等价转换，最终生成 Optimized Logical Plan。该 Optimized Logical Plan 不能保证是全局最优的，但至少是接近最优的。上述过程只与 SQL 有关，与查询有关，但是与 Spark 无关，因此无法直接提交给 Spark 执行。Query Planner 负责将 Optimized Logical Plan 转换为 Physical Plan，进而可以直接由 Spark 执行。由于同一种逻辑算子可以有多种物理实现。如 Join 有多种实现，ShuffledHashJoin、BroadcastHashJoin、BroadcastNestedLoopJoin、SortMergeJoin 等。因此 Optimized Logical Plan 可被 Query Planner 转换为多个 Physical Plan。如何选择最优的 Physical Plan 成为一件非常影响最终执行性能的事情。一种比较好的方式是，构建一个 Cost Model，并对所有候选的 Physical Plan 应用该 Model 并挑选 Cost 最小的 Physical Plan 作为最终的 Selected Physical Plan。Physical Plan 可直接转换成 RDD 由 Spark 执行。我们经常说“计划赶不上变化”，在执行过程中，可能发现原计划不是最优的，后续执行计划如果能根据运行时的统计信息进行调整可能提升整体执行效率。这部分动态调整由 Adaptive Execution 完成。后面介绍字节跳动在 Spark SQL 上做的一些优化，主要围绕这一节介绍的逻辑计划优化与物理计划优化展开。Spark SQL引擎优化Bucket Join改进在 Spark 里，实际并没有 Bucket Join 算子。这里说的 Bucket Join 泛指不需要 Shuffle 的 Sort Merge Join。下图展示了 Sort Merge Join 的基本原理。用虚线框代表的 Table 1 和 Table 2 是两张需要按某字段进行 Join 的表。虚线框内的 partition 0 到 partition m 是该表转换成 RDD 后的 Partition，而非表的分区。假设 Table 1 与 Table 2 转换为 RDD 后分别包含 m 和 k 个 Partition。为了进行 Join，需要通过 Shuffle 保证相同 Join Key 的数据在同一个 Partition 内且 Partition 内按 Key 排序，同时保证 Table 1 与 Table 2 经过 Shuffle 后的 RDD 的 Partition 数相同。如下图所示，经过 Shuffle 后只需要启动 n 个 Task，每个 Task 处理 Table 1 与 Table 2 中对应 Partition 的数据进行 Join 即可。如 Task 0 只需要顺序扫描 Shuffle 后的左右两边的 partition 0 即可完成 Join。该方法的优势是适用场景广，几乎可用于任意大小的数据集。劣势是每次 Join 都需要对全量数据进行 Shuffle，而 Shuffle 是最影响 Spark SQL 性能的环节。如果能避免 Shuffle 往往能大幅提升 Spark SQL 性能。对于大数据的场景来讲，数据一般是一次写入多次查询。如果经常对两张表按相同或类似的方式进行 Join，每次都需要付出 Shuffle 的代价。与其这样，不如让数据在写的时候，就让数据按照利于 Join 的方式分布，从而使得 Join 时无需进行 Shuffle。如下图所示，Table 1 与 Table 2 内的数据按照相同的 Key 进行分桶且桶数都为 n，同时桶内按该 Key 排序。对这两张表进行 Join 时，可以避免 Shuffle，直接启动 n 个 Task 进行 Join。字节跳动对 Spark SQL 的 BucketJoin 做了四项比较大的改进。改进一：支持与 Hive 兼容**在过去一段时间，字节跳动把大量的 Hive 作业迁移到了 SparkSQL。而 Hive 与 Spark SQL 的 Bucket 表不兼容。对于使用 Bucket 表的场景，如果直接更新计算引擎，会造成 Spark SQL 写入 Hive Bucket 表的数据无法被下游的 Hive 作业当成 Bucket 表进行 Bucket Join，从而造成作业执行时间变长，可能影响 SLA。为了解决这个问题，我们让 Spark SQL 支持 Hive 兼容模式，从而保证 Spark SQL 写入的 Bucket 表与 Hive 写入的 Bucket 表效果一致，并且这种表可以被 Hive 和 Spark SQL 当成 Bucket 表进行 Bucket Join 而不需要 Shuffle。通过这种方式保证 Hive 向 Spark SQL 的透明迁移。第一个需要解决的问题是，Hive 的一个 Bucket 一般只包含一个文件，而 Spark SQL 的一个 Bucket 可能包含多个文件。解决办法是动态增加一次以 Bucket Key 为 Key 并且并行度与 Bucket 个数相同的 Shuffle。第二个需要解决的问题是，Hive 1.x 的哈希方式与 Spark SQL 2.x 的哈希方式（Murmur3Hash）不同，使得相同的数据在 Hive 中的 Bucket ID 与 Spark SQL 中的 Bucket ID 不同而无法直接 Join。在 Hive 兼容模式下，我们让上述动态增加的 Shuffle 使用 Hive 相同的哈希方式，从而解决该问题。改进二：支持倍数关系Bucket Join**Spark SQL 要求只有 Bucket 相同的表才能（必要非充分条件）进行 Bucket Join。对于两张大小相差很大的表，比如几百 GB 的维度表与几十 TB （单分区）的事实表，它们的 Bucket 个数往往不同，并且个数相差很多，默认无法进行 Bucket Join。因此我们通过两种方式支持了倍数关系的 Bucket Join，即当两张 Bucket 表的 Bucket 数是倍数关系时支持 Bucket Join。第一种方式，Task 个数与小表 Bucket 个数相同。如下图所示，Table A 包含 3 个 Bucket，Table B 包含 6 个 Bucket。此时 Table B 的 bucket 0 与 bucket 3 的数据合集应该与 Table A 的 bucket 0 进行 Join。这种情况下，可以启动 3 个 Task。其中 Task 0 对 Table A 的 bucket 0 与 Table B 的 bucket 0 + bucket 3 进行 Join。在这里，需要对 Table B 的 bucket 0 与 bucket 3 的数据再做一次 merge sort 从而保证合集有序。如果 Table A 与 Table B 的 Bucket 个数相差不大，可以使用上述方式。如果 Table B 的 Bucket 个数是 Bucket A Bucket 个数的 10 倍，那上述方式虽然避免了 Shuffle，但可能因为并行度不够反而比包含 Shuffle 的 SortMergeJoin 速度慢。此时可以使用另外一种方式，即 Task 个数与大表 Bucket 个数相等，如下图所示。在该方案下，可将 Table A 的 3 个 Bucket 读多次。在上图中，直接将 Table A 与 Table A 进行 Bucket Union （新的算子，与 Union 类似，但保留了 Bucket 特性），结果相当于 6 个 Bucket，与 Table B 的 Bucket 个数相同，从而可以进行 Bucket Join。改进三：支持BucketJoin 降级**公司内部过去使用 Bucket 的表较少，在我们对 Bucket 做了一系列改进后，大量用户希望将表转换为 Bucket 表。转换后，表的元信息显示该表为 Bucket 表，而历史分区内的数据并未按 Bucket 表要求分布，在查询历史数据时会出现无法识别 Bucket 的问题。同时，由于数据量上涨快，平均 Bucket 大小也快速增长。这会造成单 Task 需要处理的数据量过大进而引起使用 Bucket 后的效果可能不如直接使用基于 Shuffle 的 Join。为了解决上述问题，我们实现了支持降级的 Bucket 表。基本原理是，每次修改 Bucket 信息（包含上述两种情况——将非 Bucket 表转为 Bucket 表，以及修改 Bucket 个数）时，记录修改日期。并且在决定使用哪种 Join 方式时，对于 Bucket 表先检查所查询的数据是否只包含该日期之后的分区。如果是，则当成 Bucket 表处理，支持 Bucket Join；否则当成普通无 Bucket 的表。改进四：支持超集对于一张常用表，可能会与另外一张表按 User 字段做 Join，也可能会与另外一张表按 User 和 App 字段做 Join，与其它表按 User 与 Item 字段进行 Join。而 Spark SQL 原生的 Bucket Join 要求 Join Key Set 与表的 Bucket Key Set 完全相同才能进行 Bucket Join。在该场景中，不同 Join 的 Key Set 不同，因此无法同时使用 Bucket Join。这极大的限制了 Bucket Join 的适用场景。针对此问题，我们支持了超集场景下的 Bucket Join。只要 Join Key Set 包含了 Bucket Key Set，即可进行 Bucket Join。如下图所示，Table X 与 Table Y，都按字段 A 分 Bucket。而查询需要对 Table X 与 Table Y 进行 Join，且 Join Key Set 为 A 与 B。此时，由于 A 相等的数据，在两表中的 Bucket ID 相同，那 A 与 B 各自相等的数据在两表中的 Bucket ID 肯定也相同，所以数据分布是满足 Join 要求的，不需要 Shuffle。同时，Bucket Join 还需要保证两表按 Join Key Set 即 A 和 B 排序，此时只需要对 Table X 与 Table Y 进行分区内排序即可。由于两边已经按字段 A 排序了，此时再按 A 与 B 排序，代价相对较低。物化列**Spark SQL 处理嵌套类型数据时，存在以下问题：读取大量不必要的数据：对于 Parquet / ORC 等列式存储格式，可只读取需要的字段，而直接跳过其它字段，从而极大节省 IO。而对于嵌套数据类型的字段，如下图中的 Map 类型的 people 字段，往往只需要读取其中的子字段，如 people.age。却需要将整个 Map 类型的 people 字段全部读取出来然后抽取出 people.age 字段。这会引入大量的无意义的 IO 开销。在我们的场景中，存在不少 Map 类型的字段，而且很多包含几十至几百个 Key，这也就意味着 IO 被放大了几十至几百倍。无法进行向量化读取：而向量化读能极大的提升性能。但截止到目前（2019年10月26日），Spark 不支持包含嵌套数据类型的向量化读取。这极大的影响了包含嵌套数据类型的查询性能不支持 Filter 下推：目前（2019年10月26日）的 Spark 不支持嵌套类型字段上的 Filter 的下推重复计算：JSON 字段，在 Spark SQL 中以 String 类型存在，严格来说不算嵌套数据类型。不过实践中也常用于保存不固定的多个字段，在查询时通过 JSON Path 抽取目标子字段，而大型 JSON 字符串的字段抽取非常消耗 CPU。对于热点表，频繁重复抽取相同子字段非常浪费资源。对于这个问题，做数仓的同学也想了一些解决方案。如下图所示，在名为 base_table 的表之外创建了一张名为 sub_table 的表，并且将高频使用的子字段 people.age 设置为一个额外的 Integer 类型的字段。下游不再通过 base_table 查询 people.age，而是使用 sub_table 上的 age 字段代替。通过这种方式，将嵌套类型字段上的查询转为了 Primitive 类型字段的查询，同时解决了上述问题。)这种方案存在明显缺陷：额外维护了一张表，引入了大量的额外存储/计算开销。无法在新表上查询新增字段的历史数据（如要支持对历史数据的查询，需要重跑历史作业，开销过大，无法接受）。表的维护方需要在修改表结构后修改插入数据的作业。需要下游查询方修改查询语句，推广成本较大。运营成本高：如果高频子字段变化，需要删除不再需要的独立子字段，并添加新子字段为独立字段。删除前，需要确保下游无业务使用该字段。而新增字段需要通知并推进下游业务方使用新字段。为解决上述所有问题，我们设计并实现了物化列。它的原理是：新增一个 Primitive 类型字段，比如 Integer 类型的 age 字段，并且指定它是 people.age 的物化字段。插入数据时，为物化字段自动生成数据，并在 Partition Parameter 内保存物化关系。因此对插入数据的作业完全透明，表的维护方不需要修改已有作业。查询时，检查所需查询的所有 Partition，如果都包含物化信息（people.age 到 age 的映射），直接将 select people.age 自动重写为 select age，从而实现对下游查询方的完全透明优化。同时兼容历史数据。下图展示了在某张核心表上使用物化列的收益：物化视图在 OLAP 领域，经常会对相同表的某些固定字段进行 Group By 和 Aggregate / Join 等耗时操作，造成大量重复性计算，浪费资源，且影响查询性能，不利于提升用户体验。我们实现了基于物化视图的优化功能：如上图所示，查询历史显示大量查询根据 user 进行 group by，然后对 num 进行 sum 或 count 计算。此时可创建一张物化视图，且对 user 进行 gorup by，对 num 进行 avg（avg 会自动转换为 count 和 sum）。用户对原始表进行 select user, sum(num) 查询时，Spark SQL 自动将查询重写为对物化视图的 select user, sum_num 查询。Spark SQL 引擎上的其它优化下图展示了我们在 Spark SQL 上进行的其它部分优化工作：Spark Shuffle稳定性提升与性能优化Spark Shuffle 存在的问题Shuffle的原理，很多同学应该已经很熟悉了。鉴于时间关系，这里不介绍过多细节，只简单介绍下基本模型。)如上图所示，我们将 Shuffle 上游 Stage 称为 Mapper Stage，其中的 Task 称为 Mapper。Shuffle 下游 Stage 称为 Reducer Stage，其中的 Task 称为 Reducer。每个 Mapper 会将自己的数据分为最多 N 个部分，N 为 Reducer 个数。每个 Reducer 需要去最多 M （Mapper 个数）个 Mapper 获取属于自己的那部分数据。这个架构存在两个问题：稳定性问题：Mapper 的 Shuffle Write 数据存于 Mapper 本地磁盘，只有一个副本。当该机器出现磁盘故障，或者 IO 满载，CPU 满载时，Reducer 无法读取该数据，从而引起 FetchFailedException，进而导致 Stage Retry。Stage Retry 会造成作业执行时间增长，直接影响 SLA。同时，执行时间越长，出现 Shuffle 数据无法读取的可能性越大，反过来又会造成更多 Stage Retry。如此循环，可能导致大型作业无法成功执行。性能问题：每个 Mapper 的数据会被大量 Reducer 读取，并且是随机读取不同部分。假设 Mapper 的 Shuffle 输出为 512MB，Reducer 有 10 万个，那平均每个 Reducer 读取数据 512MB / 100000 = 5.24KB。并且，不同 Reducer 并行读取数据。对于 Mapper 输出文件而言，存在大量的随机读取。而 HDD 的随机 IO 性能远低于顺序 IO。最终的现象是，Reducer 读取 Shuffle 数据非常慢，反映到 Metrics 上就是 Reducer Shuffle Read Blocked Time 较长，甚至占整个 Reducer 执行时间的一大半，如下图所示。基于HDFS的Shuffle稳定性提升经观察，引起 Shuffle 失败的最大因素不是磁盘故障等硬件问题，而是 CPU 满载和磁盘 IO 满载。)如上图所示，机器的 CPU 使用率接近 100%，使得 Mapper 侧的 Node Manager 内的 Spark External Shuffle Service 无法及时提供 Shuffle 服务。下图中 Data Node 占用了整台机器 IO 资源的 84%，部分磁盘 IO 完全打满，这使得读取 Shuffle 数据非常慢，进而使得 Reducer 侧无法在超时时间内读取数据，造成 FetchFailedException。无论是何种原因，问题的症结都是 Mapper 侧的 Shuffle Write 数据只保存在本地，一旦该节点出现问题，会造成该节点上所有 Shuffle Write 数据无法被 Reducer 读取。解决这个问题的一个通用方法是，通过多副本保证可用性。最初始的一个简单方案是，Mapper 侧最终数据文件与索引文件不写在本地磁盘，而是直接写到 HDFS。Reducer 不再通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据，而是直接从 HDFS 上获取数据，如下图所示。快速实现这个方案后，我们做了几组简单的测试。结果表明：Mapper 与 Reducer 不多时，Shuffle 读写性能与原始方案相比无差异。Mapper 与 Reducer 较多时，Shuffle 读变得非常慢。在上面的实验过程中，HDFS 发出了报警信息。如下图所示，HDFS Name Node Proxy 的 QPS 峰值达到 60 万。（注：字节跳动自研了 Node Name Proxy，并在 Proxy 层实现了缓存，因此读 QPS 可以支撑到这个量级）。原因在于，总共 10000 Reducer，需要从 10000 个 Mapper 处读取数据文件和索引文件，总共需要读取 HDFS 10000 * 1000 * 2 = 2 亿次。如果只是 Name Node 的单点性能问题，还可以通过一些简单的方法解决。例如在 Spark Driver 侧保存所有 Mapper 的 Block Location，然后 Driver 将该信息广播至所有 Executor，每个 Reducer 可以直接从 Executor 处获取 Block Location，然后无须连接 Name Node，而是直接从 Data Node 读取数据。但鉴于 Data Node 的线程模型，这种方案会对 Data Node 造成较大冲击。最后我们选择了一种比较简单可行的方案，如下图所示。Mapper 的 Shuffle 输出数据仍然按原方案写本地磁盘，写完后上传到 HDFS。Reducer 仍然按原始方案通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据。如果失败了，则从 HDFS 读取。这种方案极大减少了对 HDFS 的访问频率。该方案上线近一年：覆盖 57% 以上的 Spark Shuffle 数据。使得 Spark 作业整体性能提升 14%。天级大作业性能提升 18%。小时级作业性能提升 12%。该方案旨在提升 Spark Shuffle 稳定性从而提升作业稳定性，但最终没有使用方差等指标来衡量稳定性的提升。原因在于每天集群负载不一样，整体方差较大。Shuffle 稳定性提升后，Stage Retry 大幅减少，整体作业执行时间减少，也即性能提升。最终通过对比使用该方案前后的总的作业执行时间来对比性能的提升，用于衡量该方案的效果。Shuffle性能优化实践与探索如上文所分析，Shuffle 性能问题的原因在于，Shuffle Write 由 Mapper 完成，然后 Reducer 需要从所有 Mapper 处读取数据。这种模型，我们称之为以 Mapper 为中心的 Shuffle。它的问题在于：Mapper 侧会有 M 次顺序写 IO。Mapper 侧会有 M * N * 2 次随机读 IO（这是最大的性能瓶颈）。Mapper 侧的 External Shuffle Service 必须与 Mapper 位于同一台机器，无法做到有效的存储计算分离，Shuffle 服务无法独立扩展。针对上述问题，我们提出了以 Reducer 为中心的，存储计算分离的 Shuffle 方案，如下图所示。该方案的原理是，Mapper 直接将属于不同 Reducer 的数据写到不同的 Shuffle Service。在上图中，总共 2 个 Mapper，5 个 Reducer，5 个 Shuffle Service。所有 Mapper 都将属于 Reducer 0 的数据远程流式发送给 Shuffle Service 0，并由它顺序写入磁盘。Reducer 0 只需要从 Shuffle Service 0 顺序读取所有数据即可，无需再从 M 个 Mapper 取数据。该方案的优势在于：将 M * N * 2 次随机 IO 变为 N 次顺序 IO。Shuffle Service 可以独立于 Mapper 或者 Reducer 部署，从而做到独立扩展，做到存储计算分离。Shuffle Service 可将数据直接存于 HDFS 等高可用存储，因此可同时解决 Shuffle 稳定性问题。我的分享就到这里，谢谢大家。QA集锦- 提问：物化列新增一列，是否需要修改历史数据？回答：历史数据太多，不适合修改历史数据。- 提问：如果用户的请求同时包含新数据和历史数据，如何处理？回答：一般而言，用户修改数据都是以 Partition 为单位。所以我们在 Partition Parameter 上保存了物化列相关信息。如果用户的查询同时包含了新 Partition 与历史 Partition，我们会在新 Partition 上针对物化列进行 SQL Rewrite，历史 Partition 不 Rewrite，然后将新老 Partition 进行 Union，从而在保证数据正确性的前提下尽可能充分利用物化列的优势。- 提问：你好，你们针对用户的场景，做了很多挺有价值的优化。像物化列、物化视图，都需要根据用户的查询 Pattern 进行设置。目前你们是人工分析这些查询，还是有某种机制自动去分析并优化？回答：目前我们主要是通过一些审计信息辅助人工分析。同时我们也正在做物化列与物化视图的推荐服务，最终做到智能建设物化列与物化视图。- 提问：刚刚介绍的基于 HDFS 的 Spark Shuffle 稳定性提升方案，是否可以异步上传 Shuffle 数据至 HDFS？回答：这个想法挺好，我们之前也考虑过，但基于几点考虑，最终没有这样做。第一，单 Mapper 的 Shuffle 输出数据量一般很小，上传到 HDFS 耗时在 2 秒以内，这个时间开销可以忽略；第二，我们广泛使用 External Shuffle Service 和 Dynamic Allocation，Mapper 执行完成后可能 Executor 就回收了，如果要异步上传，就必须依赖其它组件，这会提升复杂度，ROI 较低。最后字节的这篇分享，我真的太喜欢了，所有的优化点，都是拿真实的业务场景进行举例，虽然上文有的技术点在我们的场景中还没必要去做到这种优化程度，但是如此实在的来源于线上的方案，非常容易理解。我们公司目前的数据量要比字节的量小很多，最近在新闻上看到抖音的日活已经达到了4亿，所以我们在数仓中的数据，还没有用到更细粒度的Bucket表，分区表就已经完全可以满足我们的需求。上文中的物化列方案，我觉得很新颖，但是从工程师的角度来讲，在物化列方案中，多维护一张表，添加了复杂度和运营成本，我们在数据的存储中，尽可量的回去避免复杂结构的数据类型，这样会降低存储端和计算端代码的复杂度。这篇文章是针对Spark SQL的优化方面，可以说基本上每个大数据公司都会用到Spark SQL，上述的优化方案肯定会帮助到更多的大数据团队 💪","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"我的服务器被黑了（二）","slug":"我的服务器被黑了（二）","date":"2019-09-09T02:26:15.000Z","updated":"2020-04-04T12:00:14.168Z","comments":true,"path":"2019/09/09/我的服务器被黑了（二）/","link":"","permalink":"cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令1journalctl -xe映入眼帘的是满屏的扫描和ssh尝试登陆1234567891011121314151617181920212223242526272829303132333435363738394041424344Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Failed password for invalid user admin from 117.132.175.25 port 42972 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Received disconnect from 117.132.175.25 port 42972:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Disconnected from 117.132.175.25 port 42972 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Failed password for invalid user ansible from 149.56.96.78 port 44980 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Received disconnect from 149.56.96.78 port 44980:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Disconnected from 149.56.96.78 port 44980 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Failed password for root from 218.92.0.163 port 45157 ssh2Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: error: maximum authentication attempts exceeded for root from 218.92.0.163 port 45157 ssh2 [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: error: maximum authentication attempts exceeded for root from 49.88.112.54 port 24184 ssh2 [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:54 4Z-J16-A47 sshd[314]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:54 4Z-J16-A47 sshd[314]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"lines 1105-1127/1127 (END)Sep 09 11:02:49 4Z-J16-A47 sshd[65522]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Failed password for invalid user admin from 117.132.175.25 port 42972 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Received disconnect from 117.132.175.25 port 42972:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Disconnected from 117.132.175.25 port 42972 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Failed password for invalid user ansible from 149.56.96.78 port 44980 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Received disconnect from 149.56.96.78 port 44980:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Disconnected from 149.56.96.78 port 44980 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Failed password for root from 218.92.0.163 port 45157 ssh2Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: error: maximum authentication attempts exceeded for root from 218.92.0.163 port 45157 ssh2 [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: error: maximum authentication attempts exceeded for root from 49.88.112.54 port 24184 ssh2 [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM service(sshd) ignoring max retries; 6 &gt; 3看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。12[root@*** ~]# useradd one #创建用户[root@*** ~]# passwd one #设置密码输入新用户密码首先确保文件 /etc/sudoers 中1234567%wheel ALL=(ALL) ALL``` 没有被注释```linuxusermod -g wheel onerocket设置只有指定用户组才能使用su命令切换到root用户在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。1234567#! /bin/bash# Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限sed -i '/pam_wheel.so use_uid/c\\auth required pam_wheel.so use_uid ' /etc/pam.d/sun=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`if [ $n -eq 0 ];thenecho SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defsfi打开SSHD的配置文件1vim /etc/ssh/sshd_config查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。修改sshd默认端口虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。打开sshd配置文件1vi /etc/ssh/sshd_config找到#Port 22 删掉注释服务器端口最大可以开到65536同时再添加一个Port 61024 （随意设置）Port 22Port 61024重启sshd服务123service sshd restart #centos6系列systemctl restart sshd #centos7系列firewall-cmd --add-port=61024/tcp测试，使用新用户，新端口进行登录如果登陆成功后，再将Port22注释掉，重启sshd服务。到这里，关于远程登录的防护工作，就做好了。最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"}]},{"title":"我的服务器被黑了","slug":"我的服务器被黑了","date":"2019-08-24T02:26:15.000Z","updated":"2020-04-04T12:00:09.871Z","comments":true,"path":"2019/08/24/我的服务器被黑了/","link":"","permalink":"cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/","excerpt":"","text":"服务器自述我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢为什么黑我的服务器这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器如何发现被黑回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作1.关闭了Linux防火墙2.没有设置redis访问密码3.没有更改redis默认端口4.开放了任意IP可以远程连接以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。1lsof -i tcp:6379使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想直到我运行：1top发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了做了哪些急救工作这时，感觉自己开始投入了一场对抗战1.首先查找植入程序的位置。在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。2.删除清理可疑文件杀死进程删除了正在运行的程序文件还有安装包3.查看所有用户的定时任务1cat /etc/passwd |cut -f 1 -d:crontab -uXXX -l4.开启防火墙仅开放会使用到的端口5.修改redis默认端口redis.conf中的port6.添加redis授权密码redis.conf中的requirepass7.修改绑定远程绑定ipredis.conf中的bind最后重启redis服务！从中学到了什么明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"}]},{"title":"python爬虫 - 动态爬取","slug":"python爬虫之动态爬取","date":"2019-06-12T15:26:15.000Z","updated":"2020-04-04T17:11:17.062Z","comments":true,"path":"2019/06/12/python爬虫之动态爬取/","link":"","permalink":"cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/","excerpt":"","text":"我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：Headers：请求和响应的详细信息Preview：响应体格式化之后的显示Response：响应体原始内容Cookies：CookiesTiming：时间开销通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：上面的截图展示了这次请求的请求方式、请求地址等信息。上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。我们回过头重新观察一下返回的json，格式化之后的层级关系如下：很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：1234import jsonjson_obj = json.loads(\"&#123;'key': 'value'&#125;\") # 字符串到对象json_str = json.dumps(json_obj) # 对象到字符串json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"SHC：使用 Spark SQL 高效地读写 HBase","slug":"SHC：使用-Spark-SQL-高效地读写-HBase","date":"2019-05-16T02:27:59.000Z","updated":"2020-05-16T02:31:46.066Z","comments":true,"path":"2019/05/16/SHC：使用-Spark-SQL-高效地读写-HBase/","link":"","permalink":"cpeixin.cn/2019/05/16/SHC%EF%BC%9A%E4%BD%BF%E7%94%A8-Spark-SQL-%E9%AB%98%E6%95%88%E5%9C%B0%E8%AF%BB%E5%86%99-HBase/","excerpt":"","text":"Apache Spark 和 Apache HBase 是两个使用比较广泛的大数据组件。很多场景需要使用 Spark 分析/查询 HBase 中的数据，而目前 Spark 内置是支持很多数据源的，其中就包括了 HBase，但是内置的读取数据源还是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；TableInputFormat 中不支持 BulkGet；不能享受到 Spark SQL 内置的 catalyst 引擎的优化。从另一个方面来讲，如果先不谈关于性能的方面，那么你在写HBase的过程中，是不是步骤感觉很麻烦呢，Foreach每条数据Put到HBase中，往往写入逻辑部分的代码就要写很长很长的一段。基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。项目地址具体的引入方式，在之前的Spark SQL入门中有写到Catalog对于每个表，必须提供一个目录，其中包括行键和具有预定义列族的数据类型的列，并定义hbase列与表模式之间的映射。目录是用户定义的json格式。数据类型转换支持Java基本类型。将来，将支持其他数据类型，这些数据类型依赖于用户指定的Serdes。SHC支持三种内部Serdes：Avro，Phoenix和PrimitiveType。用户可以通过在目录中定义“ tableCoder”来指定要使用的serde。为此，请参考示例和单元测试。以Avro为例。用户定义的Serdes将负责将字节数组转换为Avro对象，而连接器将负责将Avro对象转换为催化剂支持的数据类型。用户定义新Serde时，需要使其“实现”特征’SHCDataType’。请注意，如果用户希望DataFrame仅处理字节数组，则可以指定二进制类型。然后，用户可以获得每个列为字节数组的催化剂行。用户可以使用定制的解串器进一步对它进行反序列化，或者直接在DataFrame的RDD上进行操作。数据局部性当Spark Worker节点与hbase区域服务器位于同一位置时，通过标识区域服务器位置并将执行程序与区域服务器一起定位来实现数据局部性。每个执行程序将仅对位于同一主机上的同一部分数据执行Scan / BulkGet。谓词下推该库使用HBase提供的现有标准HBase过滤器，并且不能在协处理器上运行。分区修剪通过从谓词中提取行键，我们将scan / BulkGet划分为多个非重叠区域，只有具有请求数据的区域服务器才会执行scan / BulkGet。当前，分区修剪是在行键的第一维上执行的。请注意，需要仔细定义WHERE条件。否则，结果扫描可能会包含一个比用户预期大的区域。例如，以下条件将导致完全扫描（rowkey1是行键的第一维，而column是常规的hbase列）。其中rowkey1&gt;“ abc” OR列=“ xyz”扫描和批量获取都通过指定WHERE子句向用户公开，例如，其中column&gt; x和column &lt;y用于扫描，而column = x用于get。所有操作都在执行程序中执行，而驱动程序仅构造这些操作。在内部，我们将它们转换为扫描或获取或两者结合，从而将Iterator [Row]返回至催化剂引擎。可创建的数据源该库支持从HBase读取/向HBase写入。应用用途下面说明了如何使用连接器的基本步骤。有关更多详细信息和高级用例（例如Avro和复合键支持），请参考存储库中的示例。定义了HBase目录123456789101112131415def catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"table1\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"cf4\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"cf5\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"cf6\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"cf7\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"cf8\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125; |&#125;\"\"\".stripMargin上面定义了一个名称为table1，行键为键，列数为（col1-col8）的HBase表的架构。请注意，行键还必须详细定义为具有特定cf（行键）的列（col0）。写入HBase表以填充数据1234sc.parallelize(data).toDF.write.options( Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; \"5\")) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .save()给定具有指定架构的DataFrame，上面将创建一个具有5个区域的HBase表并将该DataFrame保存在其中。请注意，如果未指定HBaseTableCatalog.newTable，则必须预先创建表。在HBase表的顶部执行DataFrame操作1234567def withCatalog(cat: String): DataFrame = &#123; sqlContext .read .options(Map(HBaseTableCatalog.tableCatalog-&gt;cat)) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .load()&#125;复杂查询12345678910val df = withCatalog(catalog)val s = df.filter((($\"col0\" &lt;= \"row050\" &amp;&amp; $\"col0\" &gt; \"row040\") || $\"col0\" === \"row005\" || $\"col0\" === \"row020\" || $\"col0\" === \"r20\" || $\"col0\" &lt;= \"row005\") &amp;&amp; ($\"col4\" === 1 || $\"col4\" === 42)) .select(\"col0\", \"col1\", \"col4\")s.showSQL支持12345// Load the dataframeval df = withCatalog(catalog)//SQL exampledf.createOrReplaceTempView(\"table\")sqlContext.sql(\"select count(col1) from table\").show ## 支持Avro模式 该连接器完全支持所有avro模式。用户可以在其目录中使用完整记录模式或部分字段模式作为数据类型（有关更多详细信息，请参阅[此处](https://github.com/hortonworks-spark/shc/wiki/2.-Native-Avro-Support)）。1234567891011121314151617181920val schema_array = s\"\"\"&#123;\"type\": \"array\", \"items\": [\"string\",\"null\"]&#125;\"\"\".stripMarginval schema_record = s\"\"\"&#123;\"namespace\": \"example.avro\", | \"type\": \"record\", \"name\": \"User\", | \"fields\": [ &#123;\"name\": \"name\", \"type\": \"string\"&#125;, | &#123;\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]&#125;, | &#123;\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]&#125; ] &#125;\"\"\".stripMarginval catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"htable\"&#125;, |\"rowkey\":\"key1\", |\"columns\":&#123; |\"col1\":&#123;\"cf\":\"rowkey\", \"col\":\"key1\", \"type\":\"double\"&#125;, |\"col2\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"avro\":\"schema_array\"&#125;, |\"col3\":&#123;\"cf\":\"cf1\", \"col\":\"col2\", \"avro\":\"schema_record\"&#125;, |\"col4\":&#123;\"cf\":\"cf1\", \"col\":\"col3\", \"type\":\"double\"&#125;, |\"col5\":&#123;\"cf\":\"cf1\", \"col\":\"col4\", \"type\":\"string\"&#125; |&#125; |&#125;\"\"\".stripMargin val df = sqlContext.read.options(Map(\"schema_array\"-&gt;schema_array,\"schema_record\"-&gt;schema_record, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").load()df.write.options(Map(\"schema_array\"-&gt;schema_array,\"schema_record\"-&gt;schema_record, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").save() ####12345678910111213141516171819202122val complex = s\"\"\"MAP&lt;int, struct&lt;varchar:string&gt;&gt;\"\"\"val schema = s\"\"\"&#123;\"namespace\": \"example.avro\", | \"type\": \"record\", \"name\": \"User\", | \"fields\": [ &#123;\"name\": \"name\", \"type\": \"string\"&#125;, | &#123;\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]&#125;, | &#123;\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]&#125; ] &#125;\"\"\".stripMarginval catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"htable\"&#125;, |\"rowkey\":\"key1:key2\", |\"columns\":&#123; |\"col1\":&#123;\"cf\":\"rowkey\", \"col\":\"key1\", \"type\":\"binary\"&#125;, |\"col2\":&#123;\"cf\":\"rowkey\", \"col\":\"key2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"avro\":\"schema1\"&#125;, |\"col4\":&#123;\"cf\":\"cf1\", \"col\":\"col2\", \"type\":\"string\"&#125;, |\"col5\":&#123;\"cf\":\"cf1\", \"col\":\"col3\", \"type\":\"double\", \"sedes\":\"org.apache.spark.sql.execution.datasources.hbase.DoubleSedes\"&#125;, |\"col6\":&#123;\"cf\":\"cf1\", \"col\":\"col4\", \"type\":\"$complex\"&#125; |&#125; |&#125;\"\"\".stripMargin val df = sqlContext.read.options(Map(\"schema1\"-&gt;schema, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").load()df.write.options(Map(\"schema1\"-&gt;schema, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").save()以上说明了我们的下一步，其中包括复合键支持，复杂数据类型，客户化Serde和Avro的支持。请注意，尽管所有主要部分都包含在当前代码库中，但现在可能无法运行。SHC查询优化SHC 主要使用下面的几种优化，使得 Spark 获取 HBase 的数据扫描范围得到减少，提高了数据读取的效率。将使用 Rowkey 的查询转换成 get 查询**我们都知道，HBase 中使用 Get 查询的效率是非常高的，所以如果查询的过滤条件是针对 RowKey 进行的，那么我们可以将它转换成 Get 查询。为了说明这点，我们使用下面的例子进行说明。假设我们定义好的 HBase catalog 如下：123456789101112131415val catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"iteblog\", \"tableCoder\":\"PrimitiveType\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"id\", \"type\":\"int\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"cf4\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"cf5\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"cf6\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"cf7\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"cf8\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125;|&#125;\"\"\".stripMargin那么如果有类似下面的查询12345val df = withCatalog(catalog)df.createOrReplaceTempView(\"iteblog_table\")sqlContext.sql(\"select * from iteblog_table where id = 1\")sqlContext.sql(\"select * from iteblog_table where id = 1 or id = 2\")sqlContext.sql(\"select * from iteblog_table where id in (1, 2)\")因为查询条件直接是针对 RowKey 进行的，所以这种情况直接可以转换成 Get 或者 BulkGet 请求的。第一个 SQL 查询过程类似于下面过程但是，如果碰到非 RowKey 的过滤，那么这种查询是需要扫描 HBase 的全表的。上面的查询在 shc 里面就是将 HBase 里面的所有数据拿到，然后传输到 Spark ，再通过 Spark 里面进行过滤，可见 shc 在这种情况下效率是很低下的。注意，上面的查询在 shc 返回的结果是错误的。具体原因是在将 id = 1 or col7 = ‘xxx’ 查询条件进行合并时，丢弃了所有的查找条件，相当于返回表的所有数据。定位到代码可以参见下面的123456def or[T](left: HRF[T], right: HRF[T])(implicit ordering: Ordering[T]): HRF[T] = &#123; val ranges = ScanRange.or(left.ranges, right.ranges) val typeFilter = TypedFilter.or(left.tf, right.tf) HRF(ranges, typeFilter, left.handled &amp;&amp; right.handled)&#125;同理，类似于下面的查询在 shc 里面其实都是全表扫描，并且将所有的数据返回到 Spark 层面上再进行一次过滤。123sqlContext.sql(\"select id, col6, col8 from iteblog_table where id = 1 or col7 &lt;= 'xxx'\")sqlContext.sql(\"select id, col6, col8 from iteblog_table where id = 1 or col7 &gt;= 'xxx'\")sqlContext.sql(\"select id, col6, col8 from iteblog_table where col7 = 'xxx'\")很显然，这种方式查询效率并不高，一种可行的方案是将算子下推到 HBase 层面，在 HBase 层面通过 SingleColumnValueFilter 过滤一部分数据，然后再返回到 Spark，这样可以节省很多数据的传输。组合 RowKey 的查询优化shc 还支持组合 RowKey 的方式来建表，具体如下：1234567891011121314151617def cat = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"iteblog\", \"tableCoder\":\"PrimitiveType\"&#125;, |\"rowkey\":\"key1:key2\", |\"columns\":&#123; |\"col00\":&#123;\"cf\":\"rowkey\", \"col\":\"key1\", \"type\":\"string\", \"length\":\"6\"&#125;, |\"col01\":&#123;\"cf\":\"rowkey\", \"col\":\"key2\", \"type\":\"int\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"cf4\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"cf5\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"cf6\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"cf7\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"cf8\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125; |&#125;\"\"\".stripMargin上面的 col00 和 col01 两列组合成一个 rowkey，并且 col00 排在前面，col01 排在后面。比如 col00 =’row002’，col01 = 2，那么组合的 rowkey 为 row002\\x00\\x00\\x00\\x02。那么在组合 Rowkey 的查询 shc 都有哪些优化呢？现在我们有如下查询1df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0\").show()根据上面的信息，RowKey 其实是由 col00 和 col01 组合而成的，那么上面的查询其实可以将 col00 和 col01 进行拼接，然后组合成一个 RowKey，然后上面的查询其实可以转换成一个 Get 查询。但是在 shc 里面，上面的查询是转换成一个 scan 和一个 get 查询的。scan 的 startRow 为 row000，endRow 为 row000\\xff\\xff\\xff\\xff；get 的 rowkey 为 row000\\xff\\xff\\xff\\xff，然后再将所有符合条件的数据返回，最后再在 Spark 层面上做一次过滤，得到最后查询的结果。因为 shc 里面组合键查询的代码还没完善，所以当前实现应该不是最终的。在 shc 里面下面两条 SQL 查询下沉到 HBase 的逻辑一致1df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 = 'row000'\").show()df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0\").show()唯一区别是在 Spark 层面上的过滤。scan 查询优化如果我们的查询有 &lt; 或 &gt; 等查询过滤条件，比如下面的查询条件：1df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 &gt; 'row000' and col00 &lt; 'row005'\").show()这个在 shc 里面转换成 HBase 的过滤为一条 get 和 一个 scan，具体为 get 的 Rowkey 为 row0005\\xff\\xff\\xff\\xff；scan 的 startRow 为 row000，endRow 为 row005\\xff\\xff\\xff\\xff，然后将查询的结果返回到 spark 层面上进行过滤。总体来说，shc 能在一定程度上对查询进行优化，避免了全表扫描。但是经过评测，shc 其实还有很多地方不够完善，算子下沉并没有下沉到 HBase 层面上进行。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"shadowsock-vps搭建VPN","slug":"shadowsock-vps搭建VPN","date":"2019-04-19T15:26:15.000Z","updated":"2020-04-04T17:11:07.011Z","comments":true,"path":"2019/04/19/shadowsock-vps搭建VPN/","link":"","permalink":"cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/","excerpt":"","text":"前言还有10天左右就要回国了，由于职业的需要，对Google的依赖的越来越大的，那么回国后怎么才能‘科学上网’呢？之前在国内的时候，有使用过Lantern，稳定性和速度都还是不错了，可惜后来被和谐了。所以今天准备尝试搭建VPN，自己独立使用，一边搭建一边将过程记录下来。名词解释VPS: VPS（Virtual Private Server 虚拟专用服务器）技术，将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器 [1] 技术，和虚拟化技术 [2] 。在容器或虚拟机中，每个VPS都可分配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。VPS为使用者提供了管理配置的自由，可用于企业虚拟化，也可以用于IDC资源租用。VPN: VPN的学名叫虚拟专用网，洋文叫“Virtual Private Network”。维基百科的介绍在“这里”。本来这玩意儿主要是用于商业公司，为了让那些不在公司里的员工（比如出差在外的）能够方便地访问公司的内部网络。为了防止黑客冒充公司的员工，从外部访问公司的内部网络，VPN 软件都会提供强大的加密功能。而这个加密功能，也就让它顺便成为翻墙的利器。科学上网原理VPN浏览外网的原理使用 VPN 通常需要先安装客户端软件。当你运行 VPN 客户端，它会尝试联到 VPN 服务器（这点跟加密代理类似）。一旦和 VPN 服务器建立连接，VPN 客户端就会在你的系统中建立了一个虚拟局域网。而且，你的系统中也会多出一个虚拟网卡（在 Windows 下，可以用 ipconfig /all 命令，看到这多出来的网卡）。这样一来，你的系统中就有不止一块网卡。这就引出一个问题：那些访问网络的程序，它的数据流应该通过哪个网卡进出？为了解决此问题，VPN 客户端通常会修改你系统的路由表，让那些数据流，优先从虚拟的网卡进出。由于虚拟的网卡是通往 VPN 服务器的，当数据流到达 VPN 服务器之后，VPN 服务器再帮你把数据流转向到真正的目的地。前面说了，VPN 为了保证安全，都采用强加密的方式传输数据。这样一来，GFW 就无法分析你的网络数据流，进行敏感词过滤。所以，使用墙外的VPN服务器，无形中就能达到翻墙的效果。方案选择VPN是一个大类，其中有很多实现的方法，防火长城现在将 VPN 屏蔽的已经所剩无几，后来大家看到了SSH，使用SSH的sock5很稳定，但是特征也十分明显，防火长城可以对其直接进行定向干扰。而除了VPN，对于翻墙大家仍然有很多方法，比如Shadowsocks 、Lantern、VPNGate 等等，而实际上无论哪种方式，他们本身都需要一台服务器作为中间人进行消息传递。而VPS虚拟专用服务器就十分适合担当这个角色，并且由于VPS平时就作为商品在各类云服务器平台上售卖，自行购买并搭建相当方便，唯一需要的就是人们对于服务器的操作技术。而这次选择的方案是：VPS+Shadowsocks**Shadowsocks特点：省电，在电量查看里几乎看不到它的身影；支持开机自启动，且断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴；可使用自己的服务器，安全和速度的保证；支持区分国内外流量，传统VPN在翻出墙外后访问国内站点会变慢；可对应用设置单独代理，5.0之后的系统无需root。Shadowsocks 目前不容易被封杀主要是因为：建立在socks5协议之上，socks5是运用很广泛的协议，所以没办法直接封杀socks5协议使用socks5协议建立连接，而没有使用VPN中的服务端身份验证和密钥协商过程。而是在服务端和客户端直接写死密钥和加密算法。所以防火墙很难找到明显的特征，因为这就是个普通的socks5协议。Shadowsock搭建也比较简单，所以很多人自己架设VPS搭建，个人使用流量也很小，没法通过流量监控方式封杀。自定义加密方式和密钥。因为加密主要主要是防止被检测，所以要选择安全系数高的加密方式。之前RC4会很容易被破解，而导致被封杀。所以现在推荐使用AES加密。而在客户端和服务端自定义密钥，泄露的风险相对较小。所以如果是自己搭建的Shadosocks被封的概率很小，但是如果是第三方的Shadeowsocks，密码是server定的，你的数据很可能遭受到中间人攻击。开工购买vps首先我们需要购买一台境外的服务器，接着我们在这台云服务器里面安装代理服务，那么以后我们上网的时候就可以通过它来中转，轻松畅快的畅游全网了。购买VPS,我选择了vultr，大家用过都说好，购买的过程也很方便。第一步：选择离中国较近国家的服务器。第二步：选择服务器配置和系统这里，系统选择的是CentOS 7,配置的话，如果只是自己浏览网页的话，选择最低配置就好。其他的选项可以略过。第三步：支付和部署支付可以选择支付宝支付，非常方便。购买成功后，点击Server中的“+”号，来部署你刚刚选择的服务器。第四步：登陆服务器查看服务器详情 Server Details,根据提供的服务器信息，登陆服务器。我是使用Mac本身终端ssh到服务器上的，因为Mac上多数的SSH客户端要么收费，要么不好用，要么安装过程非常繁琐。1ssh -p 22 root@ip搭建shadowsocks服务器连接到你的 vultr 服务器之后，接下来就可以使用几个命令让你快速搭建一个属于自己的 ss 服务器：1yum install wget接着执行安装shadowsocks：1wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh获取 shadowsocks.sh 读取权限：1chmod +x shadowsocks.sh设置你的 ss 密码和端口号：1./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log接下来后就可以设置密码和端口号了密码和端口号可以使用默认的，也可以直接重新输入新的。选择加密方式设置完密码和端口号之后，我们选择加密方式，这里选择 7 ，使用aes-256-cfb的加密模式接着按任意键进行安装。安装ss完成后会给你显示你需要连接 vpn 的信息：搞定，将这些信息保存起来，那么这时候你就可以使用它们来科学上网啦。使用BBR加速上网安装 BBR1wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh获取读写权限1chmod +x bbr.sh启动BBR安装1./bbr.sh接着按任意键，开始安装，坐等一会。安装完成一会之后它会提示我们是否重新启动vps，我们输入 y 确定重启服务器。重新启动之后，输入：1lsmod | grep bbr如果看到 tcp_bbr 就说明 BBR 已经启动了。客户端进行连接windows使用Shadowsockswindows点击下载：Shadowsocks windows客户端打开 Shadowsocks 客户端，输入ip地址，密码，端口，和加密方式。接着点击确定，右下角会有个小飞机按钮，右键–&gt;启动代理。Android使用ShadowsocksAndroid点击下载：Shadowsocks Android客户端打开apk安装，接着打开APP，输入ip地址，密码，端口，和加密方式。即可科学上网。iPhone使用ShadowsocksiPhone要下载的app需要在appstore下载，但是需要用美区账号才能下载，而且这个APP需要钱。在这里提供一种解决方案，就是可以再搭建一个IPsec/L2TP VPN,专门给你的iPhone使用。Mac配置用的是Mac电脑，所以点击相关链接。东西都挂在github上，下载对应的zip文件，下载完成后安装并运行起来。点击图标，进入 服务器设置主要有四个地方要填，服务器的地址，端口号，加密方法，密码。服务器地址即为之前 Main controls选项中的IP地址。端口号、加密方法、密码必须与之前 Shadowsocks Server 中的信息一一匹配，否则会连接失败。设置完成后点击确定，然后服务器选择这个配置，默认选中PAC自动模式，确保Shadowsocks状态为On，这时候打开谷歌试试~接着就可以上外网了 😂","categories":[{"name":"工具","slug":"工具","permalink":"cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"shadowsock","slug":"shadowsock","permalink":"cpeixin.cn/tags/shadowsock/"}]},{"title":"Structured Streaming 重温","slug":"Structured-Streaming-重温","date":"2019-02-10T08:03:44.000Z","updated":"2020-05-09T04:09:47.215Z","comments":true,"path":"2019/02/10/Structured-Streaming-重温/","link":"","permalink":"cpeixin.cn/2019/02/10/Structured-Streaming-%E9%87%8D%E6%B8%A9/","excerpt":"","text":"总览_Structured Streaming 则是在 Spark 2.0 加入的经过重新设计的全新流式引擎。它的模型十分简洁，易于理解。一个流的数据源从逻辑上来说就是一个不断增长的动态表格，随着时间的推移，新数据被持续不断地添加到表格的末尾。用户可以使用 Dataset/DataFrame 或者 SQL 来对这个动态数据源进行实时查询。每次查询在逻辑上就是对当前的表格内容执行一次 SQL 查询。如何执行查询则是由用户通过触发器（Trigger）来设定。用户既可以设定定期执行，也可以让查询尽可能快地执行，从而达到实时的效果。最后，系统通过 checkpointing 和 Write-Ahead Logs来确保端到端的一次容错保证。一个流的输出有多种模式，既可以是基于整个输入执行查询后的完整结果，也可以选择只输出与上次查询相比的差异，或者就是简单地追加最新的结果。这个模型对于熟悉 SQL 的用户来说很容易掌握，对流的查询跟查询一个表格几乎完全一样。在内部，默认情况下，结构化流查询是使用_微批量处理_引擎_处理的_，该引擎将数据流作为一系列小批量作业进行处理，从而实现了低至100毫秒的端到端延迟以及 exactly-once的容错保证。但是，自Spark 2.3起，我们引入了一种称为“ 连续处理”的新低延迟处理模式，该模式可以实现一次最少保证的低至1毫秒的端到端延迟。在不更改查询中的Dataset / DataFrame操作的情况下，您将能够根据应用程序需求选择模式Structured Streaming 是对 Spark Streaming 的改进么？Structured Streaming 并不是对 Spark Streaming 的简单改进，而是我们吸取了过去几年在开发 Spark SQL 和 Spark Streaming 过程中的经验教训，以及 Spark 社区和 Databricks 众多客户的反馈，重新开发的全新流式引擎，致力于为批处理和流处理提供统一的高性能 API。同时，在这个新的引擎中，我们也很容易实现之前在 Spark Streaming 中很难实现的一些功能，比如 Event Time 的支持，Stream-Stream Join，毫秒级延迟类似于 Dataset/DataFrame 代替 Spark Core 的 RDD 成为为 Spark 用户编写批处理程序的首选，Dataset/DataFrame 也将替代 Spark Streaming 的 DStream，成为编写流处理程序的首选。Structured Streaming 的 Spark 有什么优劣势吗？简洁的模型。Structured Streaming 的模型很简洁，易于理解。用户可以直接把一个流想象成是无限增长的表格。一致的 API。由于和 Spark SQL 共用大部分 API，对 Spaprk SQL 熟悉的用户很容易上手，代码也十分简洁。同时批处理和流处理程序还可以共用代码，不需要开发两套不同的代码，显著提高了开发效率。卓越的性能。Structured Streaming 在与 Spark SQL 共用 API 的同时，也直接使用了 Spark SQL 的 Catalyst 优化器和 Tungsten，数据处理性能十分出色。此外，Structured Streaming 还可以直接从未来 Spark SQL 的各种性能优化中受益。多语言支持。Structured Streaming 直接支持目前 Spark SQL 支持的语言，包括 Scala，Java，Python，R 和 SQL。用户可以选择自己喜欢的语言进行开发。呃～～ 关于Structured Streaming的介绍就说到这里，如果想看更详细更准确的介绍呢，还是乖乖的去官网吧。在2017年10月份的时候，新立项的一个app用户行为实时项目，我有意使用Structured Streaming，所以就调研了一下，自己写了一个demo，我记忆里那时写Structured很别扭，就像一个模版一样，输入源和输出源都被规定好了函数和参数，并且在那时候测试后的时候，不怎么稳定，而且官方并没有给出成熟的版本，当时所测试的功能还都是 alpha 版本，所以当时就还是使用了Spark Streaming不过现在来看，Structured Streaming 越来越成熟，Spark Streaming感觉似乎停止了更新。Structured streaming应该是Spark流处理的未来，但是很难替代Flink。Flink在流处理上的天然优势很难被Spark超越。在读完了Structured Streaming官网后，还是亲手的写一些实例感受一下，以后做架构的时候，如果适合的话，还可以加进来。实例 ### complete, append, update123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.apache.spark.sql.streaming.StreamingQueryimport org.apache.spark.sql.&#123;DataFrame, Dataset, SparkSession&#125;import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.log4j.&#123;Level, Logger&#125;object structured_kafka &#123; val logger:Logger = Logger.getRootLogger Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; case class kafka_format(date_time: String, keyword_list: String) val spark: SparkSession = SparkSession .builder() .appName(\"Structrued-Streaming\") .master(\"local[2]\") .getOrCreate() import spark.implicits._ val kafka_df: DataFrame = spark .readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"localhost:9092\") .option(\"subscribe\", \"weibo_keyword\") .option(\"startingOffsets\", \"earliest\") .option(\"includeTimestamp\", value = true)// .option(\"endingOffsets\", \"latest\")// .option(\"startingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":23,\"1\":-2&#125;,\"topic2\":&#123;\"0\":-2&#125;&#125;\"\"\")// .option(\"endingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":50,\"1\":-1&#125;,\"topic2\":&#123;\"0\":-1&#125;&#125;\"\"\") .load() val keyvalue_df: DataFrame = kafka_df .selectExpr(\"CAST(value AS STRING)\") .as[String] .map((x: String) =&gt; &#123; val date_time: String = JSON.parseObject(x).getString(\"datetime\") val keyword_list: String = JSON.parseObject(x).getString(\"keywordList\") (date_time, keyword_list) &#125;) .flatMap((x: (String, String)) =&gt;&#123; x._2.split(\",\").map((word: String) =&gt;(x._1,word)) &#125;) .toDF(\"date_time\", \"keyword\") .groupBy(\"keyword\").count() .orderBy($\"count\".desc) val query: StreamingQuery = keyvalue_df.writeStream .outputMode(\"complete\") //append .format(\"console\") .start() query.awaitTermination() &#125;&#125;其中需要注意：__Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;__Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;未进行aggregate的stream不能sort__ ### window窗口 _1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import com.alibaba.fastjson.JSONimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.sql.streaming.&#123;StreamingQuery, Trigger&#125;import org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.functions._object structured_kafka_window &#123; val logger:Logger = Logger.getRootLogger Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; case class kafka_format(date_time: String, keyword_list: String) val spark: SparkSession = SparkSession .builder() .appName(\"Structrued-Streaming\") .master(\"local[2]\") .getOrCreate() import spark.implicits._ val kafka_df: DataFrame = spark .readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"localhost:9092\") .option(\"subscribe\", \"weibo_keyword\") .option(\"startingOffsets\", \"latest\") .option(\"includeTimestamp\", value = true)// .option(\"endingOffsets\", \"latest\")// .option(\"startingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":23,\"1\":-2&#125;,\"topic2\":&#123;\"0\":-2&#125;&#125;\"\"\")// .option(\"endingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":50,\"1\":-1&#125;,\"topic2\":&#123;\"0\":-1&#125;&#125;\"\"\") .load() val keyvalue_df: DataFrame = kafka_df .selectExpr(\"CAST(value AS STRING)\") .as[String] .map((x: String) =&gt; &#123; val date_time: String = JSON.parseObject(x).getString(\"datetime\") val keyword_list: String = JSON.parseObject(x).getString(\"keywordList\") (date_time, keyword_list) &#125;) .flatMap((x: (String, String)) =&gt;&#123; x._2.split(\",\").map((word: String) =&gt;(x._1,word)) &#125;) .toDF(\"date_time\", \"keyword\") .groupBy(window($\"date_time\", \"5 minutes\", \"1 minutes\"),$\"keyword\") .count() .orderBy(\"window\") val query: StreamingQuery = keyvalue_df.writeStream .outputMode(\"complete\") //append .format(\"console\") .option(\"truncate\", \"false\") .trigger(Trigger.ProcessingTime(\"5 seconds\")) .start() query.awaitTermination() &#125;&#125;这里关于window窗口的划分，我建议大家好好的研读一下源码：**位置：package **org.apache.spark.sql.catalyst.analysis![屏幕快照 2020-05-08 下午11.29.50.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1588952005543-007c1291-12fd-4148-a625-587b1a6149f3.png#align=left&display=inline&height=1694&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-08%20%E4%B8%8B%E5%8D%8811.29.50.png&originHeight=1694&originWidth=1936&size=488433&status=done&style=none&width=1936)Watermark12345678910111213// 基于event-time的window，words包含timestamp和word两列word .withWatermark(\"timestamp\", \"30 minutes\")//某窗口结果为x，但是部分数据在这个窗口的最后一个timestamp过后还没到达，Spark在这会等30min，过后就不再更新x了。 .dropDuplicates(\"User\", \"timestamp\") .groupBy(window(col(\"timestamp\"), \"10 minutes\"),col(\"User\"))// 10min后再加一个参数变为Sliding windows，表示每隔多久计算一次。 .count() .writeStream .queryName(\"events_per_window\") .format(\"memory\") .outputMode(\"complete\") .start()spark.sql(\"SELECT * FROM events_per_window\")watermark = max event time seen by the engine - late threshold，相当于Flink的BoundedOutOfOrdernessTximestampExtractor。在window计算被触发时，Spark会删除结束时间低于当前wm的window的中间结果，属于该window的迟到数据“可能”会被忽略，越迟越可能被忽略，删除完后才更新wm，所以即便下一批没有数据加入，Spark所依据的wm也是新的，下下一批wm不变。上面是update mode，如果是append模式，那么结果要等到trigger后发现window的结束时间低于更新后的水位线时才会出来。另外，max event time seen by the engine - late threshold机制意味着如果下一批计算没有更晚的数据加入，那么wm就不会前进，那么数据的append就会被延后。Conditions for watermarking to clean aggregation state(as of Spark 2.1.1, subject to change in the future)不支持complete模式。groupBy必须包含timestamp列或者window(col(timestamp))，withWatermark中的列要和前面的timestamp列相同顺序必须是先withWatermark再到groupBy参考：结构化流编程指南Spark之Structured Streaming","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"深度学习 Let's go","slug":"深度学习-Let-s-go","date":"2019-01-01T15:03:48.000Z","updated":"2020-05-06T16:54:06.970Z","comments":true,"path":"2019/01/01/深度学习-Let-s-go/","link":"","permalink":"cpeixin.cn/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Let-s-go/","excerpt":"","text":"前段时间讲了数据挖掘十大经典算法，在实战中也了解了随机森林、逻辑回归的概念及工具使用。这些算法都属于传统的机器学习算法。你肯定听说过这两年很火的深度学习，那么机器学习算法和深度学习有什么关联呢？在这篇文章中，我们会通过以下几个方面了解深度学习：数据挖掘、机器学习和深度学习的区别是什么？这些概念都代表什么？我们通过深度学习让机器具备人的能力，甚至某些技能的水平超过人类，比如图像识别、下棋对弈等。那么深度学习的大脑是如何工作的？深度学习是基于神经网络构建的，都有哪些常用的网络模型？深度学习有三个重要的应用领域，这三个应用领域分别是什么？数据挖掘，机器学习，深度学习的区别是什么？实际上数据挖掘和机器学习在很大程度上是重叠的。一些常用算法，比如 K-Means、KNN、SVM、决策树和朴素贝叶斯等，既可以说是数据挖掘算法，又可以说是机器学习算法。那么数据挖掘和机器学习之间有什么区别呢？数据挖掘通常是从现有的数据中提取规律模式（pattern）以及使用算法模型（model）。核心目的是找到这些数据变量之间的关系，因此我们也会通过数据可视化对变量之间的关系进行呈现，用算法模型挖掘变量之间的关联关系。通常情况下，我们只能判断出来变量 A 和变量 B 是有关系的，但并不一定清楚这两者之间有什么具体关系。在我们谈论数据挖掘的时候，更强调的是从数据中挖掘价值。机器学习是人工智能的一部分，它指的是通过训练数据和算法模型让机器具有一定的智能。一般是通过已有的数据来学习知识，并通过各种算法模型形成一定的处理能力，比如分类、聚类、预测、推荐能力等。这样当有新的数据进来时，就可以通过训练好的模型对这些数据进行预测，也就是通过机器的智能帮我们完成某些特定的任务。深度学习属于机器学习的一种，它的目标同样是让机器具有智能，只是与传统的机器学习算法不同，它是通过神经网络来实现的。神经网络就好比是机器的大脑，刚开始就像一个婴儿一样，是一张白纸。但通过多次训练之后，“大脑”就可以逐渐具备某种能力。这个训练过程中，我们只需要告诉这个大脑输入数据是什么，以及对应的输出结果是什么即可。通过多次训练，“大脑”中的多层神经网络的参数就会自动优化，从而得到一个适应于训练数据的模型。所以你能看到在传统的机器学习模型中，我们都会讲解模型的算法原理，比如 K-Means 的算法原理，KNN 的原理等。而到了神经网络，我们更关注的是网络结构，以及网络结构中每层神经元的传输机制。我们不需要告诉机器具体的特征规律是什么，只需把我们想要训练的数据和对应的结果告诉机器大脑即可。深度学习会自己找到数据的特征规律！而传统机器学习往往需要专家（我们）来告诉机器采用什么样的模型算法，这就是深度学习与传统机器学习最大的区别。另外深度学习的神经网络结构通常比较深，一般都是 5 层以上，甚至也有 101 层或更多的层数。这些深度的神经网络可以让机器更好地自动捕获数据的特征。神经网络是如何工作的神经网络可以说是机器的大脑，经典的神经网络结构可以用下面的图来表示。这里有一些概念你需要了解。节点：神经网络是由神经元组成的，也称之为节点，它们分布在神经网络的各个层中，这些层包括输入层，输出层和隐藏层。输入层：负责接收信号，并分发到隐藏层。一般我们将数据传给输入层。输出层：负责输出计算结果，一般来说输出层节点数等于我们要分类的个数。隐藏层：除了输入层和输出层外的神经网络都属于隐藏层，隐藏层可以是一层也可以是多层，每个隐藏层都会把前一层节点传输出来的数据进行计算（你可以理解是某种抽象表示），这相当于把数据抽象到另一个维度的空间中，可以更好地提取和计算数据的特征。工作原理：神经网络就好比一个黑盒子，我们只需要告诉这个黑盒子输入数据和输出数据，神经网络就可以自我训练。一旦训练好之后，就可以像黑盒子一样使用，当你传入一个新的数据时，它就会告诉你对应的输出结果。在训练过程中，神经网络主要是通过前向传播和反向传播机制运作的。什么是前向传播和反向传播呢？前向传播：数据从输入层传递到输出层的过程叫做前向传播。这个过程的计算结果通常是通过上一层的神经元的输出经过矩阵运算和激活函数得到的。这样就完成了每层之间的神经元数据的传输。反向传播：当前向传播作用到输出层得到分类结果之后，我们需要与实际值进行比对，从而得到误差。反向传播也叫作误差反向传播，核心原理是通过代价函数对网络中的参数进行修正，这样更容易让网络参数得到收敛。所以，整个神经网络训练的过程就是不断地通过前向 - 反向传播迭代完成的，当达到指定的迭代次数或者达到收敛标准的时候即可以停止训练。然后我们就可以拿训练好的网络模型对新的数据进行预测。当然，深度神经网络是基于神经网络发展起来的，它的原理与神经网络的原理一样，只不过强调了模型结构的深度，通常有 5 层以上，这样模型的学习能力会更强大。常用的神经网络都有哪些按照中间层功能的不同，神经网络可以分为三种网络结构，分别为 FNN、CNN 和 RNN。FNN（Fully-connected Neural Network）指的是全连接神经网络，全连接的意思是每一层的神经元与上一层的所有神经元都是连接的。不过在实际使用中，全连接的参数会过多，导致计算量过大。因此在实际使用中全连接神经网络的层数一般比较少。CNN 叫作卷积神经网络，在图像处理中有广泛的应用，了解图像识别的同学对这个词一定不陌生。CNN 网络中，包括了卷积层、池化层和全连接层。这三个层都有什么作用呢？卷积层相当于一个滤镜的作用，它可以把图像进行分块，对每一块的图像进行变换操作。池化层相当于对神经元的数据进行降维处理，这样输出的维数就会减少很多，从而降低整体的计算量。全连接层通常是输出层的上一层，它将上一层神经元输出的数据转变成一维的向量。RNN 称为循环神经网络，它的特点是神经元的输出可以在下一个时刻作用到自身，这样 RNN 就可以看做是在时间上传递的神经网络。它可以应用在语音识别、自然语言处理等与上下文相关的场景。深度学习网络往往包括了这三种网络的变种形成，常用的深度神经网络包括 AlexNet、VGG19、GoogleNet、ResNet 等，我总结了这些网络的特点，你可以看下：你能看出随着时间的推进，提出的深度学习网络层数越来越深，Top-5 错误率越来越低。你可能会问什么是 Top-5 错误率，实际上这些网络结构的提出和一个比赛相关，这个比赛叫做 ILSVRC，英文全称叫做 Large Scale Visual Recognition Challenge。它是一个关于大规模图像可视化识别的比赛，所基于的数据集就是著名的 ImageNet 数据集，一共包括了 1400 万张图片，涵盖 2 万多个类别。表格中的 AlexNet 就是 2012 年的 ILSVRC 冠军，当时的 Top-5 正确率是 84.7%，VGG 和 GoogleNet 是 2014 年 ILSVRC 比赛的模型，其中 GoogleNet 是当时比赛的冠军，而 VGG 是当时比赛的亚军，它的效率低于 GoogleNet。VGG 有两个版本，VGG16 和 VGG19，分别是 16 层和 19 层的 VGG 网络，这两者没有本质的区别，只是网络深度不同。到了 2015 年，比赛冠军是 ResNet，Top-5 正确率达到了 96.43%。ResNet 也有不同的版本，比如 ResNet50、ResNet101 和 ResNet152 等，名称后面的数字代表的是不同的网络深度。之后 ResNet 在其他图像比赛中也多次拿到冠军。深度学习的应用领域从 ImageNet 跑出来的这些优秀模型都是基于 CNN 卷积神经网络的。实际上深度学习有三大应用领域，图像识别就是其中之一，其他领域分别是语音识别和自然语言处理。这三个应用领域有一个共同的特性，就是都来自于信号处理。我们人类平时会处理图像信息，语音信息以及语言文字信息。机器可以帮助我们完成这三个应用里的某些工作。比如图像识别领域中图像分类和物体检测就是两个核心的任务。我们可以让机器判断图像中都有哪些物体，类别是什么，以及这些物体所处的位置。图像识别被广泛应用在安防检测中。此外人脸识别也是图像识别重要的应用场景。Siri 大家一定不陌生，此外还有我们使用的智能电视等，都采用了语音识别技术。语音识别技术可以识别人类的语音指令并进行交互。在语音导航中，还采用了语音合成技术，这样就可以让机器模拟人的声音为我们服务，Siri 语音助手也采用了语音识别和合成的技术。自然语言处理的英文缩写是 NLP，它被广泛应用到自动问答、智能客服、过滤垃圾邮件和短信等领域中。在电商领域，我们可以通过 NLP 自动给商品评论打标签，在用户决策的时候提供数据支持。在自动问答中，我们可以输入自己想问的问题，让机器来回答，比如在百度中输入“姚明的老婆”，就会自动显示出”叶莉“。此外这些技术还可以相互组合为我们提供服务，比如在无人驾驶中就采用了图像识别、语音识别等技术。在超市购物中也采用了集成图像识别、意图识别等技术等。总结今天我们大概了解了一下深度学习。深度学习也是机器学习的一种。我们之前讲解了数据挖掘十大经典算法，还有逻辑回归、随机森林算法等，这些都是传统的机器学习算法。在日常工作中，可以满足大部分的机器学习任务。但是对于数据量更大，更开放性的问题，我们就可以采用深度学习的算法，让机器自己来找规律，而不是通过我们指定的算法来找分类规律。所以深度学习的普适性会更强一些，但也并不代表深度学习就优于机器学习。一方面深度学习需要大量的数据，另一方面深度学习的学习时间，和需要的计算资源都要大于传统的机器学习。你能看到各种深度学习的训练集一般都还是比较大的，比如 ImageNet 就包括了 1400 万张图片。如果我们没有提供大量的训练数据，训练出来的深度模型识别结果未必好于传统的机器学习。实际上神经网络最早是在 1986 年提出来的，之后不温不火，直到 ImageNet 于 2009 年提出，在 2010 年开始举办每年的 ImageNet 大规模视觉识别挑战赛（ILSVRC），深度学习才得到迅猛发展。2016 年 Google 研发的 AlphaGo 击败了人类冠军李世石，更是让人们看到了深度学习的力量。一个好问题的提出，可以激发无穷的能量，这是科技进步的源泉，也是为什么在科学上，我们会有各种公开的数据集。一个好的数据集就代表了一个好的问题和使用场景。正是这些需求的出现，才能让我们的算法有更好的用武之地，同时也有了各种算法相互比拼的平台。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"比特币走势预测","slug":"比特币走势预测","date":"2018-10-04T16:43:58.000Z","updated":"2020-05-01T14:21:22.500Z","comments":true,"path":"2018/10/05/比特币走势预测/","link":"","permalink":"cpeixin.cn/2018/10/05/%E6%AF%94%E7%89%B9%E5%B8%81%E8%B5%B0%E5%8A%BF%E9%A2%84%E6%B5%8B/","excerpt":"","text":"我们之前介绍了数据挖掘算法中的分类、聚类、回归和关联分析算法，那么对于比特币走势的预测，采用哪种方法比较好呢？可能有些人会认为采用回归分析会好一些，因为预测的结果是连续的数值类型。实际上，数据挖掘算法还有一种叫时间序列分析的算法，时间序列分析模型建立了观察结果与时间变化的关系，能帮我们预测未来一段时间内的结果变化情况。那么时间序列分析和回归分析有哪些区别呢？首先，在选择模型前，我们需要确定结果与变量之间的关系。回归分析训练得到的是目标变量 y 与自变量 x（一个或多个）的相关性，然后通过新的自变量 x 来预测目标变量 y。而时间序列分析得到的是目标变量 y 与时间的相关性。另外，回归分析擅长的是多变量与目标结果之间的分析，即便是单一变量，也往往与时间无关。而时间序列分析建立在时间变化的基础上，它会分析目标变量的趋势、周期、时期和不稳定因素等。这些趋势和周期都是在时间维度的基础上，我们要观察的重要特征。那么针对今天要进行的预测比特币走势的项目，我们都需要掌握哪些目标呢？了解时间序列预测的概念，以及常用的模型算法，包括 AR、MA、ARMA、ARIMA 模型等；掌握并使用 ARMA 模型工具，对一个时间序列数据进行建模和预测；对比特币的历史数据进行时间序列建模，并预测未来 6 个月的走势。时间序列预测关于时间序列，你可以把它理解为按照时间顺序组成的数字序列。实际上在中国古代的农业社会中，人们就将一年中不同时间节点和天气的规律总结了下来，形成了二十四节气，也就是从时间序列中观察天气和太阳的规律（只是当时没有时间序列模型和相应工具），从而使得农业得到迅速发展。在现代社会，时间序列在金融、经济、商业领域拥有广泛的应用。在时间序列预测模型中，有一些经典的模型，包括 AR、MA、ARMA、ARIMA。我来给你简单介绍一下。AR 的英文全称叫做 Auto Regressive，中文叫自回归模型。这个算法的思想比较简单，它认为过去若干时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点。在我们日常生活环境中就存在白噪声，在数据挖掘的过程中，你可以把它理解为一个期望为 0，方差为常数的纯随机过程。AR 模型还存在一个阶数，称为 AR（p）模型，也叫作 p 阶自回归模型。它指的是通过这个时刻点的前 p 个点，通过线性组合再加上白噪声来预测当前时刻点的值。MA 的英文全称叫做 Moving Average，中文叫做滑动平均模型。它与 AR 模型大同小异，AR 模型是历史时序值的线性组合，MA 是通过历史白噪声进行线性组合来影响当前时刻点。AR 模型中的历史白噪声是通过影响历史时序值，从而间接影响到当前时刻点的预测值。同样 MA 模型也存在一个阶数，称为 MA(q) 模型，也叫作 q 阶移动平均模型。我们能看到 AR 和 MA 模型都存在阶数，在 AR 模型中，我们用 p 表示，在 MA 模型中我们用 q 表示，这两个模型大同小异，与 AR 模型不同的是 MA 模型是历史白噪声的线性组合。ARMA 的英文全称是 Auto Regressive Moving Average，中文叫做自回归滑动平均模型，也就是 AR 模型和 MA 模型的混合。相比 AR 模型和 MA 模型，它有更准确的估计。同样 ARMA 模型存在 p 和 q 两个阶数，称为 ARMA(p,q) 模型。ARIMA 的英文全称是 Auto Regressive Integrated Moving Average 模型，中文叫差分自回归滑动平均模型，也叫求合自回归滑动平均模型。相比于 ARMA，ARIMA 多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARIMA 的原理和ARMA 模型一样。相比于 ARMA(p,q) 的两个阶数，ARIMA 是一个三元组的阶数 (p,d,q)，称为 ARIMA(p,d,q) 模型。其中 d 是差分阶数。ARMA 模型工具上面介绍的 AR，MA，ARMA，ARIMA 四种模型，你只需要了解基础概念即可，中间涉及到的一些数学公式这里不进行展开。在实际工作中，我们更多的是使用工具，我在这里主要讲解下如何使用 ARMA 模型工具。在使用 ARMA 工具前，你需要先引用相关工具包：1from statsmodels.tsa.arima_model import ARMA然后通过 ARMA(endog,order,exog=None) 创建 ARMA 类，这里有一些主要的参数简单说明下：endog：英文是 endogenous variable，代表内生变量，又叫非政策性变量，它是由模型决定的，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目中需要用到的变量。order：代表是 p 和 q 的值，也就是 ARMA 中的阶数。exog：英文是 exogenous variables，代表外生变量。外生变量和内生变量一样是经济模型中的两个重要变量。相对于内生变量而言，外生变量又称作为政策性变量，在经济机制内受外部因素的影响，不是我们模型要研究的变量。举个例子，如果我们想要创建 ARMA(7,0) 模型，可以写成：ARMA(data,(7,0))，其中 data 是我们想要观察的变量，(7,0) 代表 (p,q) 的阶数。创建好之后，我们可以通过 fit 函数进行拟合，通过 predict(start, end) 函数进行预测，其中 start 为预测的起始时间，end 为预测的终止时间。下面我们使用 ARMA 模型对一组时间序列做建模，代码如下：1234567891011121314151617181920212223242526# coding:utf-8# 用ARMA进行时间序列预测import pandas as pdimport matplotlib.pyplot as pltimport statsmodels.api as smfrom statsmodels.tsa.arima_model import ARMAfrom statsmodels.graphics.api import qqplot# 创建数据data = [5922, 5308, 5546, 5975, 2704, 1767, 4111, 5542, 4726, 5866, 6183, 3199, 1471, 1325, 6618, 6644, 5337, 7064, 2912, 1456, 4705, 4579, 4990, 4331, 4481, 1813, 1258, 4383, 5451, 5169, 5362, 6259, 3743, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 7644, 6472, 8680, 6366, 5252, 8223, 8181, 10548, 11823, 14640, 9873, 6613, 14415, 13204, 14982, 9690, 10693, 8276, 4519, 7865, 8137, 10022, 7646, 8749, 5246, 4736, 9705, 7501, 9587, 10078, 9732, 6986, 4385, 8451, 9815, 10894, 10287, 9666, 6072, 5418]data=pd.Series(data)data_index = sm.tsa.datetools.dates_from_range('1901','1990')# 绘制数据图data.index = pd.Index(data_index)data.plot(figsize=(12,8))plt.show()# 创建ARMA模型# 创建ARMA模型arma = ARMA(data,(7,0)).fit()print('AIC: %0.4lf' %arma.aic)# 模型预测predict_y = arma.predict('1990', '2000')# 预测结果绘制fig, ax = plt.subplots(figsize=(12, 8))ax = data.ix['1901':].plot(ax=ax)predict_y.plot(ax=ax)plt.show())我创建了 1901 年 -1990 年之间的时间序列数据 data，然后创建 ARMA(7,0) 模型，并传入时间序列数据 data，使用 fit 函数拟合，然后对 1990 年 -2000 年之间的数据进行预测，最后绘制预测结果。你能看到 ARMA 工具的使用还是很方便的，只是我们需要 p 和 q 的取值。实际项目中，我们可以给 p 和 q 指定一个范围，让 ARMA 都运行一下，然后选择最适合的模型。你可能会问，怎么判断一个模型是否适合？我们需要引入 AIC 准则，也叫作赤池消息准则，它是衡量统计模型拟合好坏的一个标准，数值越小代表模型拟合得越好。在这个例子中，你能看到 ARMA(7,0) 这个模型拟合出来的 AIC 是 1619.6323（并不一定是最优）。对比特币走势进行预测我们都知道比特币的走势除了和历史数据以外，还和很多外界因素相关，比如用户的关注度，各国的政策，币圈之间是否打架等等。当然这些外界的因素不是我们这节课需要考虑的对象。假设我们只考虑比特币以往的历史数据，用 ARMA 这个时间序列模型预测比特币的走势。比特币历史数据（从 2012-01-01 到 2018-10-31）可以从 GitHub 上下载：https://github.com/cystanford/bitcoin。你能看到数据一共包括了 8 个字段，代表的含义如下：我们的目标是构造 ARMA 时间序列模型，预测比特币（平均）价格走势。p 和 q 参数具体选择多少呢？我们可以设置一个区间范围，然后选择 AIC 最低的 ARMA 模型。首先我们需要加载数据。在准备阶段，我们需要先探索数据，采用数据可视化方式查看比特币的历史走势。按照不同的时间尺度（天，月，季度，年）可以将数据压缩，得到不同尺度的数据，然后做可视化呈现。这 4 个时间尺度上，我们选择月作为预测模型的时间尺度，相应的，我们选择 Weighted_Price 这个字段的数值作为观察结果，在原始数据中，Weighted_Price 对应的是比特币每天的平均价格，当我们以“月”为单位进行压缩的时候，对应的 Weighted_Price 得到的就是当月的比特币平均价格。压缩代码如下：1df_month = df.resample('M').mean()最后在预测阶段创建 ARMA 时间序列模型。我们并不知道 p 和 q 取什么值时，模型最优，因此我们可以给它们设置一个区间范围，比如都是 range(0,3)，然后计算不同模型的 AIC 数值，选择最小的 AIC 数值对应的那个 ARMA 模型。最后用这个最优的 ARMA 模型预测未来 8 个月的比特币平均价格走势，并将结果做可视化呈现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# -*- coding: utf-8 -*-# 比特币走势预测，使用时间序列ARMAimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom statsmodels.tsa.arima_model import ARMAimport warningsfrom itertools import productfrom datetime import datetimewarnings.filterwarnings('ignore')# 数据加载df = pd.read_csv('./bitcoin_2012-01-01_to_2018-10-31.csv')# 将时间作为df的索引df.Timestamp = pd.to_datetime(df.Timestamp)df.index = df.Timestamp# 数据探索print(df.head())# 按照月，季度，年来统计df_month = df.resample('M').mean()df_Q = df.resample('Q-DEC').mean()df_year = df.resample('A-DEC').mean()# 按照天，月，季度，年来显示比特币的走势fig = plt.figure(figsize=[15, 7])plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.suptitle('比特币金额（美金）', fontsize=20)plt.subplot(221)plt.plot(df.Weighted_Price, '-', label='按天')plt.legend()plt.subplot(222)plt.plot(df_month.Weighted_Price, '-', label='按月')plt.legend()plt.subplot(223)plt.plot(df_Q.Weighted_Price, '-', label='按季度')plt.legend()plt.subplot(224)plt.plot(df_year.Weighted_Price, '-', label='按年')plt.legend()plt.show()# 设置参数范围ps = range(0, 3)qs = range(0, 3)parameters = product(ps, qs)parameters_list = list(parameters)# 寻找最优ARMA模型参数，即best_aic最小results = []best_aic = float(\"inf\") # 正无穷for param in parameters_list: try: model = ARMA(df_month.Weighted_Price,order=(param[0], param[1])).fit() except ValueError: print('参数错误:', param) continue aic = model.aic if aic &lt; best_aic: best_model = model best_aic = aic best_param = param results.append([param, model.aic])# 输出最优模型result_table = pd.DataFrame(results)result_table.columns = ['parameters', 'aic']print('最优模型: ', best_model.summary())# 比特币预测df_month2 = df_month[['Weighted_Price']]date_list = [datetime(2018, 11, 30), datetime(2018, 12, 31), datetime(2019, 1, 31), datetime(2019, 2, 28), datetime(2019, 3, 31), datetime(2019, 4, 30), datetime(2019, 5, 31), datetime(2019, 6, 30)]future = pd.DataFrame(index=date_list, columns= df_month.columns)df_month2 = pd.concat([df_month2, future])df_month2['forecast'] = best_model.predict(start=0, end=91)# 比特币预测结果显示plt.figure(figsize=(20,7))df_month2.Weighted_Price.plot(label='实际金额')df_month2.forecast.plot(color='r', ls='--', label='预测金额')plt.legend()plt.title('比特币金额（月）')plt.xlabel('时间')plt.ylabel('美金')plt.show()结果：))我们通过 product 函数创建了 (p,q) 在 range(0,3) 范围内的所有可能组合，并对每个 ARMA(p,q) 模型进行了 AIC 数值计算，保存了 AIC 数值最小的模型参数。然后用这个模型对比特币的未来 8 个月进行了预测。从结果中你能看到，在 2018 年 10 月之后 8 个月的时间里，比特币会触底到 4000 美金左右，实际上比特币在这个阶段确实降低到了 4000 元美金甚至更低。在时间尺度的选择上，我们选择了月，这样就对数据进行了降维，也节约了 ARMA 的模型训练时间。你能看到比特币金额（美金）这张图中，按月划分的比特币走势和按天划分的比特币走势差别不大，在减少了局部的波动的同时也能体现出比特币的趋势，这样就节约了 ARMA 的模型训练时间。总结今天我给你讲了一个比特币趋势预测的实战项目。通过这个项目你应该能体会到，当我们对一个数值进行预测的时候，如果考虑的是多个变量和结果之间的关系，可以采用回归分析，如果考虑单个时间维度与结果的关系，可以使用时间序列分析。根据比特币的历史数据，我们使用 ARMA 模型对比特币未来 8 个月的走势进行了预测，并对结果进行了可视化显示。你能看到 ARMA 工具还是很好用的，虽然比特币的走势受很多外在因素影响，比如政策环境。不过当我们掌握了这些历史数据，也不妨用时间序列模型来分析预测一下。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"时间序列","slug":"时间序列","permalink":"cpeixin.cn/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"}]},{"title":"信用卡诈骗分析","slug":"信用卡诈骗分析","date":"2018-10-02T16:43:58.000Z","updated":"2020-05-01T14:20:36.160Z","comments":true,"path":"2018/10/03/信用卡诈骗分析/","link":"","permalink":"cpeixin.cn/2018/10/03/%E4%BF%A1%E7%94%A8%E5%8D%A1%E8%AF%88%E9%AA%97%E5%88%86%E6%9E%90/","excerpt":"","text":"上一篇文章中，我们用随机森林以及之前讲过的 SVM、决策树和 KNN 分类器对信用卡违约数据进行了分析，这节课我们来研究下信用卡欺诈。相比于信用卡违约的比例，信用卡欺诈的比例更小，但是危害极大。如何通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷风险是我们这次项目的目标。通过今天的学习，你需要掌握以下几个方面：了解逻辑回归分类，以及如何在 sklearn 中使用它；信用卡欺诈属于二分类问题，欺诈交易在所有交易中的比例很小，对于这种数据不平衡的情况，到底采用什么样的模型评估标准会更准确；完成信用卡欺诈分析的实战项目，并通过数据可视化对数据探索和模型结果评估进一步加强了解。构建逻辑回归分类器逻辑回归虽然不在我们讲解的十大经典数据挖掘算法里面，但也是常用的数据挖掘算法。逻辑回归，也叫作 logistic 回归。虽然名字中带有“回归”，但它实际上是分类方法，主要解决的是二分类问题，当然它也可以解决多分类问题，只是二分类更常见一些。在逻辑回归中使用了 Logistic 函数，也称为 Sigmoid 函数。Sigmoid 函数是在深度学习中经常用到的函数之一，函数公式为：函数的图形如下所示，类似 S 状：你能看出 g(z) 的结果在 0-1 之间，当 z 越大的时候，g(z) 越大，当 z 趋近于无穷大的时候，g(z) 趋近于 1。同样当 z 趋近于无穷小的时候，g(z) 趋近于 0。同时，函数值以 0.5 为中心。为什么逻辑回归算法是基于 Sigmoid 函数实现的呢？你可以这样理解：我们要实现一个二分类任务，0 即为不发生，1 即为发生。我们给定一些历史数据 X 和 y。其中 X 代表样本的 n 个特征，y 代表正例和负例，也就是 0 或 1 的取值。通过历史样本的学习，我们可以得到一个模型，当给定新的 X 的时候，可以预测出 y。这里我们得到的 y 是一个预测的概率，通常不是 0% 和 100%，而是中间的取值，那么我们就可以认为概率大于 50% 的时候，即为发生（正例），概率小于 50% 的时候，即为不发生（负例）。这样就完成了二分类的预测。逻辑回归模型的求解这里不做介绍，我们来看下如何使用 sklearn 中的逻辑回归工具。在 sklearn 中，我们使用 LogisticRegression() 函数构建逻辑回归分类器，函数里有一些常用的构造参数：penalty：惩罚项，取值为 l1 或 l2，默认为 l2。当模型参数满足高斯分布的时候，使用 l2，当模型参数满足拉普拉斯分布的时候，使用 l1；solver：代表的是逻辑回归损失函数的优化方法。有 5 个参数可选，分别为 liblinear、lbfgs、newton-cg、sag 和 saga。默认为 liblinear，适用于数据量小的数据集，当数据量大的时候可以选用 sag 或 saga 方法。max_iter：算法收敛的最大迭代次数，默认为 10。n_jobs：拟合和预测的时候 CPU 的核数，默认是 1，也可以是整数，如果是 -1 则代表 CPU 的核数。当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。模型评估指标我们之前对模型做评估时，通常采用的是准确率 (accuracy)，它指的是分类器正确分类的样本数与总体样本数之间的比例。这个指标对大部分的分类情况是有效的，不过当分类结果严重不平衡的时候，准确率很难反应模型的好坏。举个例子，对于机场安检中恐怖分子的判断，就不能采用准确率对模型进行评估。我们知道恐怖分子的比例是极低的，因此当我们用准确率做判断时，如果准确率高达 99.999%，就说明这个模型一定好么？其实正因为现实生活中恐怖分子的比例极低，就算我们不能识别出一个恐怖分子，也会得到非常高的准确率。因为准确率的评判标准是正确分类的样本个数与总样本数之间的比例。因此非恐怖分子的比例会很高，就算我们识别不出来恐怖分子，正确分类的个数占总样本的比例也会很高，也就是准确率高。实际上我们应该更关注恐怖分子的识别，这里先介绍下数据预测的四种情况：TP、FP、TN、FN。我们用第二个字母 P 或 N 代表预测为正例还是负例，P 为正，N 为负。第一个字母 T 或 F 代表的是预测结果是否正确，T 为正确，F 为错误。所以四种情况分别为：TP：预测为正，判断正确；FP：预测为正，判断错误；TN：预测为负，判断正确；FN：预测为负，判断错误。我们知道样本总数 =TP+FP+TN+FN，预测正确的样本数为 TP+TN，因此准确率 Accuracy = (TP+TN)/(TP+TN+FN+FP)。实际上，对于分类不平衡的情况，有两个指标非常重要，它们分别是精确度和召回率。精确率 P = TP/ (TP+FP)，对应上面恐怖分子这个例子，在所有判断为恐怖分子的人数中，真正是恐怖分子的比例。召回率 R = TP/ (TP+FN)，也称为查全率。代表的是恐怖分子被正确识别出来的个数与恐怖分子总数的比例。为什么要统计召回率和精确率这两个指标呢？假设我们只统计召回率，当召回率等于 100% 的时候，模型是否真的好呢？举个例子，假设我们把机场所有的人都认为是恐怖分子，恐怖分子都会被正确识别，这个数字与恐怖分子的总数比例等于 100%，但是这个结果是没有意义的。如果我们认为机场里所有人都是恐怖分子的话，那么非恐怖分子（极高比例）都会认为是恐怖分子，误判率太高了，所以我们还需要统计精确率作为召回率的补充。实际上有一个指标综合了精确率和召回率，可以更好地评估模型的好坏。这个指标叫做 F1，用公式表示为：F1 作为精确率 P 和召回率 R 的调和平均，数值越大代表模型的结果越好。信用卡欺诈分析我们来看一个信用卡欺诈分析的项目，这个数据集你可以从百度网盘（因为数据集大于 100M，所以采用了网盘）中下载：链接：https://pan.baidu.com/s/14F8WuX0ZJntdB_r1EC08HA 提取码：58gp数据集包括了 2013 年 9 月份两天时间内的信用卡交易数据，284807 笔交易中，一共有 492 笔是欺诈行为。输入数据一共包括了 28 个特征 V1，V2，……V28 对应的取值，以及交易时间 Time 和交易金额 Amount。为了保护数据隐私，我们不知道 V1 到 V28 这些特征代表的具体含义，只知道这 28 个特征值是通过 PCA 变换得到的结果。另外字段 Class 代表该笔交易的分类，Class=0 为正常（非欺诈），Class=1 代表欺诈。我们的目标是针对这个数据集构建一个信用卡欺诈分析的分类器，采用的是逻辑回归。从数据中你能看到欺诈行为只占到了 492/284807=0.172%，数据分类结果的分布是非常不平衡的，因此我们不能使用准确率评估模型的好坏，而是需要统计 F1 值（综合精确率和召回率）。我们先梳理下整个项目的流程：加载数据；准备阶段：我们需要探索数据，用数据可视化的方式查看分类结果的情况，以及随着时间的变化，欺诈交易和正常交易的分布情况。上面已经提到过，V1-V28 的特征值都经过 PCA 的变换，但是其余的两个字段，Time 和 Amount 还需要进行规范化。Time 字段和交易本身是否为欺诈交易无关，因此我们不作为特征选择，只需要对 Amount 做数据规范化就行了。同时数据集没有专门的测试集，使用 train_test_split 对数据集进行划分；分类阶段：我们需要创建逻辑回归分类器，然后传入训练集数据进行训练，并传入测试集预测结果，将预测结果与测试集的结果进行比对。这里的模型评估指标用到了精确率、召回率和 F1 值。同时我们将精确率 - 召回率进行了可视化呈现。基于上面的流程，具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# -*- coding:utf-8 -*-# 使用逻辑回归对信用卡欺诈进行分类import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport itertoolsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matrix, precision_recall_curvefrom sklearn.preprocessing import StandardScalerimport warningswarnings.filterwarnings('ignore') # 混淆矩阵可视化def plot_confusion_matrix(cm, classes, normalize = False, title = 'Confusion matrix\"', cmap = plt.cm.Blues) : plt.figure() plt.imshow(cm, interpolation = 'nearest', cmap = cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation = 0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) : plt.text(j, i, cm[i, j], horizontalalignment = 'center', color = 'white' if cm[i, j] &gt; thresh else 'black') plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') plt.show() # 显示模型评估结果def show_metrics(): tp = cm[1,1] fn = cm[1,0] fp = cm[0,1] tn = cm[0,0] print('精确率: &#123;:.3f&#125;'.format(tp/(tp+fp))) print('召回率: &#123;:.3f&#125;'.format(tp/(tp+fn))) print('F1值: &#123;:.3f&#125;'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))))# 绘制精确率-召回率曲线def plot_precision_recall(): plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post') plt.fill_between(recall, precision, step ='post', alpha = 0.2, color = 'b') plt.plot(recall, precision, linewidth=2) plt.xlim([0.0,1]) plt.ylim([0.0,1.05]) plt.xlabel('召回率') plt.ylabel('精确率') plt.title('精确率-召回率 曲线') plt.show(); # 数据加载data = pd.read_csv('./creditcard.csv')# 数据探索print(data.describe())# 设置plt正确显示中文plt.rcParams['font.sans-serif'] = ['SimHei']# 绘制类别分布plt.figure()ax = sns.countplot(x = 'Class', data = data)plt.title('类别分布')plt.show()# 显示交易笔数，欺诈交易笔数num = len(data)num_fraud = len(data[data['Class']==1]) print('总交易笔数: ', num)print('诈骗交易笔数：', num_fraud)print('诈骗交易比例：&#123;:.6f&#125;'.format(num_fraud/num))# 欺诈和正常交易可视化f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))bins = 50ax1.hist(data.Time[data.Class == 1], bins = bins, color = 'deeppink')ax1.set_title('诈骗交易')ax2.hist(data.Time[data.Class == 0], bins = bins, color = 'deepskyblue')ax2.set_title('正常交易')plt.xlabel('时间')plt.ylabel('交易次数')plt.show()# 对Amount进行数据规范化data['Amount_Norm'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))# 特征选择y = np.array(data.Class.tolist())data = data.drop(['Time','Amount','Class'],axis=1)X = np.array(data.as_matrix())# 准备训练集和测试集train_x, test_x, train_y, test_y = train_test_split (X, y, test_size = 0.1, random_state = 33) # 逻辑回归分类clf = LogisticRegression()clf.fit(train_x, train_y)predict_y = clf.predict(test_x)# 预测样本的置信分数score_y = clf.decision_function(test_x) # 计算混淆矩阵，并显示cm = confusion_matrix(test_y, predict_y)class_names = [0,1]# 显示混淆矩阵plot_confusion_matrix(cm, classes = class_names, title = '逻辑回归 混淆矩阵')# 显示模型评估分数show_metrics()# 计算精确率，召回率，阈值用于可视化precision, recall, thresholds = precision_recall_curve(test_y, score_y)plot_precision_recall()结果：123456789101112 Time V1 ... Amount Classcount 284807.000000 2.848070e+05 ... 284807.000000 284807.000000mean 94813.859575 1.165980e-15 ... 88.349619 0.001727std 47488.145955 1.958696e+00 ... 250.120109 0.041527min 0.000000 -5.640751e+01 ... 0.000000 0.00000025% 54201.500000 -9.203734e-01 ... 5.600000 0.00000050% 84692.000000 1.810880e-02 ... 22.000000 0.00000075% 139320.500000 1.315642e+00 ... 77.165000 0.000000max 172792.000000 2.454930e+00 ... 25691.160000 1.000000[8 rows x 31 columns]1234总交易笔数: 284807诈骗交易笔数： 492诈骗交易比例：0.001727)1234精确率: 0.841召回率: 0.617F1值: 0.712你能看出来欺诈交易的笔数为 492 笔，占所有交易的比例是很低的，即 0.001727，我们可以通过数据可视化的方式对欺诈交易和正常交易的分布进行呈现。另外通过可视化，我们也能看出精确率和召回率之间的关系，当精确率高的时候，召回率往往很低，召回率高的时候，精确率会比较低。代码有一些模块需要说明下。我定义了 plot_confusion_matrix 函数对混淆矩阵进行可视化。什么是混淆矩阵呢？混淆矩阵也叫误差矩阵，实际上它就是 TP、FP、TN、FN 这四个数值的矩阵表示，帮助我们判断预测值和实际值相比，对了多少。从这个例子中，你能看出 TP=37，FP=7，FN=23。所以精确率 P=TP/(TP+FP)=37/(37+7)=0.841，召回率 R=TP/(TP+FN)=37/(37+23)=0.617。然后使用了 sklearn 中的 precision_recall_curve 函数，通过预测值和真实值来计算精确率 - 召回率曲线。precision_recall_curve 函数会计算在不同概率阈值情况下的精确率和召回率。最后定义 plot_precision_recall 函数，绘制曲线。总结今天我给你讲了逻辑回归的概念和相关工具的使用，另外学习了在数据样本不平衡的情况下，如何评估模型。这里你需要了解精确率，召回率和 F1 的概念和计算方式。最后在信用卡欺诈分析的项目中，我们使用了逻辑回归工具，并对混淆矩阵进行了计算，同时在模型结果评估中，使用了精确率、召回率和 F1 值，最后得到精确率 - 召回率曲线的可视化结果。从这个项目中你能看出来，不是所有的分类都是样本平衡的情况，针对正例比例极低的情况，比如信用卡欺诈、某些疾病的识别，或者是恐怖分子的判断等，都需要采用精确率 - 召回率来进行统计。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"逻辑回归","slug":"逻辑回归","permalink":"cpeixin.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}]},{"title":"信用卡风险评估","slug":"信用卡风险评估","date":"2018-09-30T17:21:02.000Z","updated":"2020-05-01T14:20:47.150Z","comments":true,"path":"2018/10/01/信用卡风险评估/","link":"","permalink":"cpeixin.cn/2018/10/01/%E4%BF%A1%E7%94%A8%E5%8D%A1%E9%A3%8E%E9%99%A9%E8%AF%84%E4%BC%B0/","excerpt":"","text":"今天我来带你做一个数据挖掘的项目。在数据挖掘的过程中，我们经常会遇到一些问题，比如：如何选择各种分类器，到底选择哪个分类算法，是 SVM，决策树，还是 KNN？如何优化分类器的参数，以便得到更好的分类准确率？这两个问题，是数据挖掘核心的问题。当然对于一个新的项目，我们还有其他的问题需要了解，比如掌握数据探索和数据可视化的方式，还需要对数据的完整性和质量做评估。这些内容我在之前的课程中都有讲到过。今天的学习主要围绕下面的三个目标，并通过它们完成信用卡违约率项目的实战这三个目标分别是：创建各种分类器，包括已经掌握的 SVM、决策树、KNN 分类器，以及随机森林分类器；掌握 GridSearchCV 工具，优化算法模型的参数；使用 Pipeline 管道机制进行流水线作业。因为在做分类之前，我们还需要一些准备过程，比如数据规范化，或者数据降维等。构建随机森林分类器在算法篇中，我主要讲了数据挖掘十大经典算法。实际工作中，你也可能会用到随机森林。随机森林的英文是 Random Forest，英文简写是 RF。它实际上是一个包含多个决策树的分类器，每一个子分类器都是一棵 CART 分类回归树。所以随机森林既可以做分类，又可以做回归。当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。当它做回归的时候，输出结果是每棵 CART 树的回归结果的平均值。在 sklearn 中，我们使用 RandomForestClassifier() 构造随机森林模型，函数里有一些常用的构造参数：当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。使用 GridSearchCV 工具对模型参数进行调优在做分类算法的时候，我们需要经常调节网络参数（对应上面的构造参数），目的是得到更好的分类结果。实际上一个分类算法有很多参数，取值范围也比较广，那么该如何调优呢？Python 给我们提供了一个很好用的工具 GridSearchCV，它是 Python 的参数自动搜索模块。我们只要告诉它想要调优的参数有哪些以及参数的取值范围，它就会把所有的情况都跑一遍，然后告诉我们哪个参数是最优的，结果如何。使用 GridSearchCV 模块需要先引用工具包，方法如下：1from sklearn.model_selection import GridSearchCV然后我们使用 GridSearchCV(estimator, param_grid, cv=None, scoring=None) 构造参数的自动搜索模块，这里有一些主要的参数需要说明下：构造完 GridSearchCV 之后，我们就可以使用 fit 函数拟合训练，使用 predict 函数预测，这时预测采用的是最优参数情况下的分类器。这里举一个简单的例子，我们用 sklearn 自带的 IRIS 数据集，采用随机森林对 IRIS 数据分类。假设我们想知道 n_estimators 在 1-10 的范围内取哪个值的分类结果最好，可以编写代码：12345678910111213141516# -*- coding: utf-8 -*-# 使用RandomForest对IRIS数据集进行分类# 利用GridSearchCV寻找最优参数from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisrf = RandomForestClassifier()parameters = &#123;\"n_estimators\": range(1,11)&#125;iris = load_iris()# 使用GridSearchCV进行参数调优clf = GridSearchCV(estimator=rf, param_grid=parameters)# 对iris数据集进行分类clf.fit(iris.data, iris.target)print(\"最优分数： %.4lf\" %clf.best_score_)print(\"最优参数：\", clf.best_params_)123运行结果如下：最优分数： 0.9667最优参数： &#123;'n_estimators': 6&#125;你能看到当我们采用随机森林作为分类器的时候，最优准确率是 0.9667，当 n_estimators=6 的时候，是最优参数，也就是随机森林一共有 6 个子决策树。使用 Pipeline 管道机制进行流水线作业做分类的时候往往都是有步骤的，比如先对数据进行规范化处理，你也可以用 PCA 方法（一种常用的降维方法）对数据降维，最后使用分类器分类。Python 有一种 Pipeline 管道机制。管道机制就是让我们把每一步都按顺序列下来，从而创建 Pipeline 流水线作业。每一步都采用 (‘名称’, 步骤) 的方式来表示。我们需要先采用 StandardScaler 方法对数据规范化，即采用数据规范化为均值为 0，方差为 1 的正态分布，然后采用 PCA 方法对数据进行降维，最后采用随机森林进行分类。具体代码如下：1234567from sklearn.model_selection import GridSearchCVpipeline = Pipeline([ ('scaler', StandardScaler()), ('pca', PCA()), ('randomforestclassifier', RandomForestClassifier())])那么我们现在采用 Pipeline 管道机制，用随机森林对 IRIS 数据集做一下分类。先用 StandardScaler 方法对数据规范化，然后再用随机森林分类，编写代码如下：123456789101112131415161718192021# -*- coding: utf-8 -*-# 使用RandomForest对IRIS数据集进行分类# 利用GridSearchCV寻找最优参数,使用Pipeline进行流水作业from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinerf = RandomForestClassifier()parameters = &#123;\"randomforestclassifier__n_estimators\": range(1,11)&#125;iris = load_iris()pipeline = Pipeline([ ('scaler', StandardScaler()), ('randomforestclassifier', rf)])# 使用GridSearchCV进行参数调优clf = GridSearchCV(estimator=pipeline, param_grid=parameters)# 对iris数据集进行分类clf.fit(iris.data, iris.target)print(\"最优分数： %.4lf\" %clf.best_score_)print(\"最优参数：\", clf.best_params_)123运行结果：最优分数： 0.9667最优参数： &#123;'randomforestclassifier__n_estimators': 9&#125;你能看到是否采用数据规范化对结果还是有一些影响的，有了 GridSearchCV 和 Pipeline 这两个工具之后，我们在使用分类器的时候就会方便很多。对信用卡违约率进行分析我们现在来做一个信用卡违约率的项目，这个数据集你可以从 GitHub 上下载：https://github.com/cystanford/credit_default。这个数据集是台湾某银行 2005 年 4 月到 9 月的信用卡数据，数据集一共包括 25 个字段，具体含义如下：现在我们的目标是要针对这个数据集构建一个分析信用卡违约率的分类器。具体选择哪个分类器，以及分类器的参数如何优化，我们可以用 GridSearchCV 这个工具跑一遍。先梳理下整个项目的流程：加载数据；准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更直观的了解，比如我们想要了解信用卡违约率和不违约率的人数。因为数据集没有专门的测试集，我们还需要使用 train_test_split 划分数据集。分类阶段：之所以把数据规范化放到这个阶段，是因为我们可以使用 Pipeline 管道机制，将数据规范化设置为第一步，分类为第二步。因为我们不知道采用哪个分类器效果好，所以我们需要多用几个分类器，比如 SVM、决策树、随机森林和 KNN。然后通过 GridSearchCV 工具，找到每个分类器的最优参数和最优分数，最终找到最适合这个项目的分类器和该分类器的参数。基于上面的流程，具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import matplotlibimport pandas as pdfrom matplotlib import pyplot as pltimport seaborn as snsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import train_test_split, GridSearchCV# 构造各种分类器from sklearn.neighbors import KNeighborsClassifierfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierclassifiers = [ SVC(random_state = 1, kernel = 'rbf'), DecisionTreeClassifier(random_state = 1, criterion = 'gini'), RandomForestClassifier(random_state = 1, criterion = 'gini'), KNeighborsClassifier(metric = 'minkowski'),]# 分类器名称classifier_names = [ 'svc', 'decisiontreeclassifier', 'randomforestclassifier', 'kneighborsclassifier',]# 分类器参数classifier_param_grid = [ &#123;'svc__C':[1], 'svc__gamma':[0.01]&#125;, &#123;'decisiontreeclassifier__max_depth':[6,9,11]&#125;, &#123;'randomforestclassifier__n_estimators':[3,5,6]&#125; , &#123;'kneighborsclassifier__n_neighbors':[4,6,8]&#125;,]# 对具体的分类器进行GridSearchCV参数调优def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = 'accuracy'): response = &#123;&#125; gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score) # 寻找最优的参数 和最优的准确率分数 search = gridsearch.fit(train_x, train_y) print(\"GridSearch最优参数：\", search.best_params_) print(\"GridSearch最优分数： %0.4lf\" %search.best_score_) predict_y = gridsearch.predict(test_x) print(\"准确率 %0.4lf\" %accuracy_score(test_y, predict_y)) response['predict_y'] = predict_y response['accuracy_score'] = accuracy_score(test_y,predict_y) return responsedef get_data(): data = pd.read_csv('UCI_Credit_Card.csv') # 数据条数和字段数量 print(data.shape) # 数据探索 print(data.describe) return datadef show_data(data): # 查看下一个月的违约率 next_month = data['default.payment.next.month'].value_counts() print(next_month) # 统计违约率结果 df = pd.DataFrame(&#123;'default.payment.next.month': next_month.index, 'values': next_month.values&#125;) print(df) # 正常显示中文标签 # plt.rcParams['font.sans-serif'] = ['FangSong'] # 用来正常显示中文标签 plt.rcParams[\"font.family\"] = 'Arial Unicode MS' plt.figure(figsize=(6, 6)) plt.title('信用卡违约率客户 (违约：1，守约：0)') sns.set_color_codes('pastel') sns.barplot(x='default.payment.next.month', y=\"values\", data=df) locs, labels = plt.xticks() plt.show()def get_feature(data): # 特征选择，去掉ID字段、最后一个结果字段即可 data.drop(['ID'], inplace=True, axis=1) # ID这个字段没有用 target = data['default.payment.next.month'].values columns = data.columns.tolist() columns.remove('default.payment.next.month') features = data[columns].values return features, targetdef main(): data = get_data() show_data(data) features, target = get_feature(data) # 30%作为测试集，其余作为训练集 train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1) for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid): pipeline = Pipeline([ ('scaler', StandardScaler()), (model_name, model) ]) result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid, score='accuracy')if __name__ == '__main__': main()结果：1234567891011121314151617181920212223242526272829303132(30000, 25)&lt;bound method NDFrame.describe of ID LIMIT_BAL SEX ... PAY_AMT5 PAY_AMT6 default.payment.next.month0 1 20000.0 2 ... 0.0 0.0 11 2 120000.0 2 ... 0.0 2000.0 12 3 90000.0 2 ... 1000.0 5000.0 03 4 50000.0 2 ... 1069.0 1000.0 04 5 50000.0 1 ... 689.0 679.0 0 ... ... ... ... ... ... ...29995 29996 220000.0 1 ... 5000.0 1000.0 029996 29997 150000.0 1 ... 0.0 0.0 029997 29998 30000.0 1 ... 2000.0 3100.0 129998 29999 80000.0 1 ... 52964.0 1804.0 129999 30000 50000.0 1 ... 1000.0 1000.0 1[30000 rows x 25 columns]&gt;0 233641 6636Name: default.payment.next.month, dtype: int64 default.payment.next.month values0 0 233641 1 6636GridSearch最优参数： &#123;'svc__C': 1, 'svc__gamma': 0.01&#125;GridSearch最优分数： 0.8186准确率 0.8172GridSearch最优参数： &#123;'decisiontreeclassifier__max_depth': 6&#125;GridSearch最优分数： 0.8208准确率 0.8113GridSearch最优参数： &#123;'randomforestclassifier__n_estimators': 6&#125;GridSearch最优分数： 0.8004准确率 0.7994GridSearch最优参数： &#123;'kneighborsclassifier__n_neighbors': 8&#125;GridSearch最优分数： 0.8040准确率 0.8036从结果中，我们能看到 SVM 分类器的准确率最高，测试准确率为 0.8172。在决策树分类中，我设置了 3 种最大深度，当最大深度 =6 时结果最优，测试准确率为 0.8113；在随机森林分类中，我设置了 3 个决策树个数的取值，取值为 6 时结果最优，测试准确率为 0.7994；在 KNN 分类中，我设置了 3 个 n 的取值，取值为 8 时结果最优，测试准确率为 0.8036。总结今天我给你讲了随机森林的概念及工具的使用，另外针对数据挖掘算法中经常采用的参数调优，也介绍了 GridSearchCV 工具这个利器。并将这两者结合起来，在信用卡违约分析这个项目中进行了使用。很多时候，我们不知道该采用哪种分类算法更适合。即便是对于一种分类算法，也有很多参数可以调优，每个参数都有一定的取值范围。我们可以把想要采用的分类器，以及这些参数的取值范围都设置到数组里，然后使用 GridSearchCV 工具进行调优。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Random Forest","slug":"Random-Forest","permalink":"cpeixin.cn/tags/Random-Forest/"}]},{"title":"数据分析 - PageRank 实战","slug":"数据分析 - PageRank-实战","date":"2018-09-12T16:17:42.000Z","updated":"2020-05-01T14:20:06.568Z","comments":true,"path":"2018/09/13/数据分析 - PageRank-实战/","link":"","permalink":"cpeixin.cn/2018/09/13/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%AE%9E%E6%88%98/","excerpt":"","text":"上文讲了 PageRank 算法经常被用到网络关系的分析中，比如在社交网络中计算个人的影响力，计算论文的影响力或者网站的影响力等。今天我们就来做一个关于 PageRank 算法的实战，在这之前，你需要思考三个问题：如何使用工具完成 PageRank 算法，包括使用工具创建网络图，设置节点、边、权重等，并通过创建好的网络图计算节点的 PR 值；对于一个实际的项目，比如希拉里的 9306 封邮件（工具包中邮件的数量），如何使用 PageRank 算法挖掘出有影响力的节点，并且绘制网络图；如何对创建好的网络图进行可视化，如果网络中的节点数较多，如何筛选重要的节点进行可视化，从而得到精简的网络关系图。如何使用工具实现 PageRank 算法PageRank 算法工具在 sklearn 中并不存在，我们需要找到新的工具包。实际上有一个关于图论和网络建模的工具叫 NetworkX，它是用 Python 语言开发的工具，内置了常用的图与网络分析算法，可以方便我们进行网络数据分析。上节课，我举了一个网页权重的例子，假设一共有 4 个网页 A、B、C、D，它们之间的链接信息如图所示：针对这个例子，我们看下用 NetworkX 如何计算 A、B、C、D 四个网页的 PR 值，具体代码如下：123456789import networkx as nx# 创建有向图G = nx.DiGraph() # 有向图之间边的关系edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"), (\"B\", \"A\"), (\"B\", \"D\"), (\"C\", \"A\"), (\"D\", \"B\"), (\"D\", \"C\")]for edge in edges: G.add_edge(edge[0], edge[1])pagerank_list = nx.pagerank(G, alpha=1)print(\"pagerank值是：\", pagerank_list)结果：1pagerank值是： &#123;&#39;A&#39;: 0.33333396911621094, &#39;B&#39;: 0.22222201029459634, &#39;C&#39;: 0.22222201029459634, &#39;D&#39;: 0.22222201029459634&#125;我们通过 NetworkX 创建了一个有向图之后，设置了节点之间的边，然后使用 PageRank 函数就可以求得节点的 PR 值，结果和上节课中我们人工模拟的结果一致。好了，运行完这个例子之后，我们来看下 NetworkX 工具都有哪些常用的操作。1. 关于图的创建图可以分为无向图和有向图，在 NetworkX 中分别采用不同的函数进行创建。无向图指的是不用节点之间的边的方向，使用 nx.Graph() 进行创建；有向图指的是节点之间的边是有方向的，使用 nx.DiGraph() 来创建。在上面这个例子中，存在 A→D 的边，但不存在 D→A 的边。2. 关于节点的增加、删除和查询如果想在网络中增加节点，可以使用 G.add_node(‘A’) 添加一个节点，也可以使用 G.add_nodes_from([‘B’,‘C’,‘D’,‘E’]) 添加节点集合。如果想要删除节点，可以使用 G.remove_node(node) 删除一个指定的节点，也可以使用 G.remove_nodes_from([‘B’,‘C’,‘D’,‘E’]) 删除集合中的节点。那么该如何查询节点呢？如果你想要得到图中所有的节点，就可以使用 G.nodes()，也可以用 G.number_of_nodes() 得到图中节点的个数。3. 关于边的增加、删除、查询增加边与添加节点的方式相同，使用 G.add_edge(“A”, “B”) 添加指定的“从 A 到 B”的边，也可以使用 add_edges_from 函数从边集合中添加。我们也可以做一个加权图，也就是说边是带有权重的，使用 add_weighted_edges_from 函数从带有权重的边的集合中添加。在这个函数的参数中接收的是 1 个或多个三元组[u,v,w]作为参数，u、v、w 分别代表起点、终点和权重。另外，我们可以使用 remove_edge 函数和 remove_edges_from 函数删除指定边和从边集合中删除。另外可以使用 edges() 函数访问图中所有的边，使用 number_of_edges() 函数得到图中边的个数。以上是关于图的基本操作，如果我们创建了一个图，并且对节点和边进行了设置，就可以找到其中有影响力的节点，原理就是通过 PageRank 算法，使用 nx.pagerank(G) 这个函数，函数中的参数 G 代表创建好的图。如何用 PageRank 揭秘希拉里邮件中的人物关系了解了 NetworkX 工具的基础使用之后，我们来看一个实际的案例：希拉里邮件人物关系分析。希拉里邮件事件相信你也有耳闻，对这个数据的背景我们就不做介绍了。你可以从 GitHub 上下载这个数据集：https://github.com/cystanford/PageRank。整个数据集由三个文件组成：Aliases.csv，Emails.csv 和 Persons.csv，其中 Emails 文件记录了所有公开邮件的内容，发送者和接收者的信息。Persons 这个文件统计了邮件中所有人物的姓名及对应的 ID。因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用 Aliases 文件来查询别名和人物的对应关系。整个数据集包括了 9306 封邮件和 513 个人名，数据集还是比较大的。不过这一次我们不需要对邮件的内容进行分析，只需要通过邮件中的发送者和接收者（对应 Emails.csv 文件中的 MetadataFrom 和 MetadataTo 字段）来绘制整个关系网络。因为涉及到的人物很多，因此我们需要通过 PageRank 算法计算每个人物在邮件关系网络中的权重，最后筛选出来最有价值的人物来进行关系网络图的绘制。了解了数据集和项目背景之后，我们来设计到执行的流程步骤：首先我们需要加载数据源；在准备阶段：我们需要对数据进行探索，在数据清洗过程中，因为邮件中存在别名的情况，因此我们需要统一人物名称。另外邮件的正文并不在我们考虑的范围内，只统计邮件中的发送者和接收者，因此我们筛选 MetadataFrom 和 MetadataTo 这两个字段作为特征。同时，发送者和接收者可能存在多次邮件往来，需要设置权重来统计两人邮件往来的次数。次数越多代表这个边（从发送者到接收者的边）的权重越高；在挖掘阶段：我们主要是对已经设置好的网络图进行 PR 值的计算，但邮件中的人物有 500 多人，有些人的权重可能不高，我们需要筛选 PR 值高的人物，绘制出他们之间的往来关系。在可视化的过程中，我们可以通过节点的 PR 值来绘制节点的大小，PR 值越大，节点的绘制尺寸越大。设置好流程之后，实现的代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# -*- coding: utf-8 -*-# 用 PageRank 挖掘希拉里邮件中的重要任务关系import pandas as pdimport networkx as nximport numpy as npfrom collections import defaultdictimport matplotlib.pyplot as plt# 数据加载emails = pd.read_csv(\"./input/Emails.csv\")# 读取别名文件file = pd.read_csv(\"./input/Aliases.csv\")aliases = &#123;&#125;for index, row in file.iterrows(): aliases[row['Alias']] = row['PersonId']# 读取人名文件file = pd.read_csv(\"./input/Persons.csv\")persons = &#123;&#125;for index, row in file.iterrows(): persons[row['Id']] = row['Name']# 针对别名进行转换 def unify_name(name): # 姓名统一小写 name = str(name).lower() # 去掉, 和 @后面的内容 name = name.replace(\",\",\"\").split(\"@\")[0] # 别名转换 if name in aliases.keys(): return persons[aliases[name]] return name# 画网络图def show_graph(graph, layout='spring_layout'): # 使用 Spring Layout 布局，类似中心放射状 if layout == 'circular_layout': positions=nx.circular_layout(graph) else: positions=nx.spring_layout(graph) # 设置网络图中的节点大小，大小与 pagerank 值相关，因为 pagerank 值很小所以需要 *20000 nodesize = [x['pagerank']*20000 for v,x in graph.nodes(data=True)] # 设置网络图中的边长度 edgesize = [np.sqrt(e[2]['weight']) for e in graph.edges(data=True)] # 绘制节点 nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=0.4) # 绘制边 nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=0.2) # 绘制节点的 label nx.draw_networkx_labels(graph, positions, font_size=10) # 输出希拉里邮件中的所有人物关系图 plt.show()# 将寄件人和收件人的姓名进行规范化emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)emails.MetadataTo = emails.MetadataTo.apply(unify_name)# 设置遍的权重等于发邮件的次数edges_weights_temp = defaultdict(list)for row in zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText): temp = (row[0], row[1]) if temp not in edges_weights_temp: edges_weights_temp[temp] = 1 else: edges_weights_temp[temp] = edges_weights_temp[temp] + 1# 转化格式 (from, to), weight =&gt; from, to, weightedges_weights = [(key[0], key[1], val) for key, val in edges_weights_temp.items()]# 创建一个有向图graph = nx.DiGraph()# 设置有向图中的路径及权重 (from, to, weight)graph.add_weighted_edges_from(edges_weights)# 计算每个节点（人）的 PR 值，并作为节点的 pagerank 属性pagerank = nx.pagerank(graph)# 将 pagerank 数值作为节点的属性nx.set_node_attributes(graph, name = 'pagerank', values=pagerank)# 画网络图show_graph(graph)# 将完整的图谱进行精简# 设置 PR 值的阈值，筛选大于阈值的重要核心节点pagerank_threshold = 0.005# 复制一份计算好的网络图small_graph = graph.copy()# 剪掉 PR 值小于 pagerank_threshold 的节点for n, p_rank in graph.nodes(data=True): if p_rank['pagerank'] &lt; pagerank_threshold: small_graph.remove_node(n)# 画网络图,采用circular_layout布局让筛选出来的点组成一个圆show_graph(small_graph, 'circular_layout')结果如下：针对代码中的几个模块我做个简单的说明：1. 函数定义人物的名称需要统一，因此我设置了 unify_name 函数，同时设置了 show_graph 函数将网络图可视化。NetworkX 提供了多种可视化布局，这里我使用 spring_layout 布局，也就是呈中心放射状。除了 spring_layout 外，NetworkX 还有另外三种可视化布局，circular_layout（在一个圆环上均匀分布节点），random_layout（随机分布节点 ），shell_layout（节点都在同心圆上）。2. 计算边权重邮件的发送者和接收者的邮件往来可能不止一次，我们需要用两者之间邮件往来的次数计算这两者之间边的权重，所以我用 edges_weights_temp 数组存储权重。而上面介绍过在 NetworkX 中添加权重边（即使用 add_weighted_edges_from 函数）的时候，接受的是 u、v、w 的三元数组，因此我们还需要对格式进行转换，具体转换方式见代码。3.PR 值计算及筛选我使用 nx.pagerank(graph) 计算了节点的 PR 值。由于节点数量很多，我们设置了 PR 值阈值，即 pagerank_threshold=0.005，然后遍历节点，删除小于 PR 值阈值的节点，形成新的图 small_graph，最后对 small_graph 进行可视化（对应运行结果的第二张图）。总结我们通过矩阵乘法求得网页的权重，这我们使用 NetworkX 可以得到相同的结果。另外我带你用 PageRank 算法做了一次实战，我们将一个复杂的网络图，通过 PR 值的计算、筛选，最终得到了一张精简的网络图。在这个过程中我们学习了 NetworkX 工具的使用，包括创建图、节点、边及 PR 值的计算。实际上掌握了 PageRank 的理论之后，在实战中往往就是一行代码的事。但项目与理论不同，项目中涉及到的数据量比较大，你会花 80% 的时间（或 80% 的代码量）在预处理过程中，比如今天的项目中，我们对别名进行了统一，对边的权重进行计算，同时还需要把计算好的结果以可视化的方式呈现。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"}]},{"title":"PageRank 原理","slug":"数据分析 - PageRank-原理","date":"2018-09-10T14:37:25.000Z","updated":"2020-05-01T14:19:56.808Z","comments":true,"path":"2018/09/10/数据分析 - PageRank-原理/","link":"","permalink":"cpeixin.cn/2018/09/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%8E%9F%E7%90%86/","excerpt":"","text":"互联网发展到现在，搜索引擎已经非常好用，基本上输入关键词，都能找到匹配的内容，质量还不错。但在 1998 年之前，搜索引擎的体验并不好。早期的搜索引擎，会遇到下面的两类问题：返回结果质量不高：搜索结果不考虑网页的质量，而是通过时间顺序进行检索；容易被人钻空子：搜索引擎是基于检索词进行检索的，页面中检索词出现的频次越高，匹配度越高，这样就会出现网页作弊的情况。有些网页为了增加搜索引擎的排名，故意增加某个检索词的频率。基于这些缺陷，当时 Google 的创始人拉里·佩奇提出了 PageRank 算法，目的就是要找到优质的网页，这样 Google 的排序结果不仅能找到用户想要的内容，而且还会从众多网页中筛选出权重高的呈现给用户。Google 的两位创始人都是斯坦福大学的博士生，他们提出的 PageRank 算法受到了论文影响力因子的评价启发。当一篇论文被引用的次数越多，证明这篇论文的影响力越大。正是这个想法解决了当时网页检索质量不高的问题。PageRank 的简化模型我们先来看下 PageRank 是如何计算的。我假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：这里有两个概念你需要了解一下。出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：u 为待评估的页面，Bu 为页面 u 的入链集合。针对入链集合中的任意页面 v，它能给 u 带来的影响力是其自身的影响力 PR(v) 除以 v 页面的出链数量，即页面 v 把影响力 PR(v) 平均分配给了它的出链，这样统计所有能给 u 带来链接的页面 v，得到的总和就是网页 u 的影响力，即为 PR(u)。所以你能看到，出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。在这个例子中，你能看到 A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：转移矩阵解释：第一列是A的出链的概率A-&gt;A: 0 A-&gt;B: 1/3 A-&gt;C: 1/3 A-&gt;D: 1/3第二列是B的的出链的概率B-&gt;A: 1/2 B-&gt;B: 0 B-&gt;C:0 B-&gt;D: 1/2第三列是C的出链概率C-&gt;A:1 C-&gt;B:0 C-&gt;C:0 C-&gt;D: 0第四列是D的出链概率D-&gt;A: 0 D-&gt;B:1/2 D-&gt;C:1/2 D-&gt;D: 0我们假设 A、B、C、D 四个页面的初始影响力都是相同的，即：当进行第一次转移之后，各页面的影响力 w1 变为：然后我们再用转移矩阵乘以 w1 得到 w2 结果，直到第 n 次迭代后 wn 影响力不再发生变化，可以收敛到 (0.3333，0.2222，0.2222，0.2222），也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。你能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。至此，我们模拟了一个简化的 PageRank 的计算过程，实际情况会比这个复杂，可能会面临两个问题：1. 等级泄露（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。2. 等级沉没（Rank Sink）：如果一个网页只有出链，没有入链（如下图所示），计算的过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。针对等级泄露和等级沉没的情况，我们需要灵活处理。比如针对等级泄露的情况，我们可以把没有出链的节点，先从图中去掉，等计算完所有节点的 PR 值之后，再加上该节点进行计算。不过这种方法会导致新的等级泄露的节点的产生，所以工作量还是很大的。有没有一种方法，可以同时解决等级泄露和等级沉没这两个问题呢？PageRank 的随机浏览模型为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。其中 N 为网页总数，这样我们又可以重新迭代网页的权重计算了，因为加入了阻尼因子 d，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理（这里不进行讲解）也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。PageRank 在社交影响力评估中的应用网页之间会形成一个网络，是我们的互联网，论文之间也存在着相互引用的关系，可以说我们所处的环境就是各种网络的集合。只要是有网络的地方，就存在出链和入链，就会有 PR 权重的计算，也就可以运用我们今天讲的 PageRank 算法。我们可以把 PageRank 算法延展到社交网络领域中。比如在微博上，如果我们想要计算某个人的影响力，该怎么做呢？一个人的微博粉丝数并不一定等于他的实际影响力。如果按照 PageRank 算法，还需要看这些粉丝的质量如何。如果有很多明星或者大 V 关注，那么这个人的影响力一定很高。如果粉丝是通过购买僵尸粉得来的，那么即使粉丝数再多，影响力也不高。同样，在工作场景中，比如说脉脉这个社交软件，它计算的就是个人在职场的影响力。如果你的工作关系是李开复、江南春这样的名人，那么你的职场影响力一定会很高。反之，如果你是个学生，在职场上被链入的关系比较少的话，职场影响力就会比较低。同样，如果你想要看一个公司的经营能力，也可以看这家公司都和哪些公司有合作。如果它合作的都是世界 500 强企业，那么这个公司在行业内一定是领导者，如果这个公司的客户都是小客户，即使数量比较多，业内影响力也不一定大。除非像淘宝一样，有海量的中小客户，最后大客户也会找上门来寻求合作。所以权重高的节点，往往会有一些权重同样很高的节点在进行合作。PageRank 给我们带来的启发PageRank 可以说是 Google 搜索引擎重要的技术之一，在 1998 年帮助 Google 获得了搜索引擎的领先优势，现在 PageRank 已经比原来复杂很多，但它的思想依然能带给我们很多启发。比如，如果你想要自己的媒体影响力有所提高，就尽量要混在大 V 圈中；如果想找到高职位的工作，就尽量结识公司高层，或者认识更多的猎头，因为猎头和很多高职位的人员都有链接关系。同样，PageRank 也可以帮我们识别链接农场。链接农场指的是网页为了链接而链接，填充了一些没有用的内容。这些页面相互链接或者指向了某一个网页，从而想要得到更高的权重。总结今天我给你讲了 PageRank 的算法原理，对简化的 PageRank 模型进行了模拟。针对简化模型中存在的等级泄露和等级沉没这两个问题，PageRank 的随机浏览模型引入了阻尼因子 d 来解决。同样，PageRank 有很广的应用领域，在许多网络结构中都有应用，比如计算一个人的微博影响力等。它也告诉我们，在社交网络中，链接的质量非常重要。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"}]},{"title":"数据分析-关联规则原理","slug":"数据分析-关联规则原理","date":"2018-09-02T14:19:09.000Z","updated":"2020-05-01T14:19:31.444Z","comments":true,"path":"2018/09/02/数据分析-关联规则原理/","link":"","permalink":"cpeixin.cn/2018/09/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%8E%9F%E7%90%86/","excerpt":"","text":"关联规则这个概念，最早是由 Agrawal 等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于关联规则的 Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。关联规则挖掘可以让我们从数据集中发现项与项（item 与 item）之间的关系，它在我们的生活中有很多应用场景，“购物篮分析”就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。在今天的内容中，希望你能带着问题，和我一起来搞懂以下几个知识点：搞懂关联规则中的几个重要概念：支持度、置信度、提升度；Apriori 算法的工作原理；在实际工作中，我们该如何进行关联规则挖掘。关联规则概念搞懂关联规则中的几个概念我举一个超市购物的例子，下面是几名客户购买的商品列表：什么是支持度呢？支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。在这个例子中，我们能看到“牛奶”出现了 4 次，那么这 5 笔订单中“牛奶”的支持度就是 4/5=0.8。同样“牛奶 + 面包”出现了 3 次，那么这 5 笔订单中“牛奶 + 面包”的支持度就是 3/5=0.6。什么是置信度呢？它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度 (牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2 次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。什么是提升度呢？我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升的”程度。还是看上面的例子，如果我们单纯看置信度 (可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品 A 对商品 B 的提升度：提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。所以提升度有三种可能：提升度 (A→B)&gt;1：代表有提升；提升度 (A→B)=1：代表有没有提升，也没有下降；提升度 (A→B)&lt;1：代表有下降。提升度 (牛奶→啤酒)Apriori 的工作原理明白了关联规则中支持度、置信度和提升度这几个重要概念，我们来看下 Apriori 算法是如何工作的。首先我们把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：Apriori 算法其实就是查找频繁项集 (frequent itemset) **的过程，所以首先我们需要定义什么是频繁项集。频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集**，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。我们再来看下这个例子，假设我随机指定最小支持度是 50%，也就是 0.5。我们来看下 Apriori 算法是如何运算的。首先，我们先计算单个商品的支持度，也就是得到 K=1 项的支持度：因为最小支持度是 0.5，所以你能看到商品 4、6 是不符合最小支持度的，不属于频繁项集，于是经过筛选商品的频繁项集就变成：在这个基础上，我们将商品两两组合， 根据订单编号图，得到 k=2 项的支持度：我们再筛掉小于最小值支持度的商品组合，可以得到：我们再将商品进行 K=3 项的商品组合，可以得到：商品项集支持度1，2，33/51，2，51/51，3，52/52，3，52/5再筛掉小于最小值支持度的商品组合，可以得到：通过上面这个过程，我们可以得到 K=3 项的频繁项集{1,2,3}，也就是{牛奶、面包、尿布}的组合。到这里，你已经和我模拟了一遍整个 Apriori 算法的流程，下面我来给你总结下 Apriori 算法的递归流程：K=1，计算 K 项集的支持度；筛选掉小于最小支持度的项集；如果项集为空，则对应 K-1 项集的结果为最终结果。否则 K=K+1，重复 1-3 步。Apriori 的改进算法：FP-Growth 算法我们刚完成了 Apriori 算法的模拟，你能看到 Apriori 在计算的过程中有以下几个缺点：可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；每次计算都需要重新扫描数据集，来计算每个项集的支持度。所以 Apriori 算法会浪费很多计算空间和计算时间，为此人们提出了 FP-Growth 算法，它的特点是：创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；整个生成过程只遍历数据集 2 次，大大减少了计算量。所以在实际工作中，我们常用 FP-Growth 来做频繁项集的挖掘，下面我给你简述下 FP-Growth 的原理。1. 创建项头表（item header table）创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。2. 构造 FP 树FP 树的根节点记为 NULL 节点。整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。3. 通过 FP 树挖掘频繁项集到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。具体的操作会用到一个概念，叫“条件模式基”，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。我以“啤酒”的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：你能看出来，相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以“啤酒”为节点的 FP 子树，也就是说，在频繁项集中一定要含有“啤酒”这个项。你可以再看下原始的数据，其中订单 1{牛奶、面包、尿布}和订单 5{牛奶、面包、尿布、可乐}并不存在“啤酒”这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以“啤酒”为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。条件模式基不包括“啤酒”节点，而且祖先节点如果小于最小支持度就会被剪枝，所以“啤酒”的条件模式基为空。同理，我们可以求得“面包”的条件模式基为：所以可以求得面包的频繁项集为{尿布，面包}，{尿布，牛奶，面包}。同样，我们还可以求得牛奶，尿布的频繁项集，这里就不再计算展示。总结今天我给你讲了 Apriori 算法，它是在“购物篮分析”中常用的关联规则挖掘算法，在 Apriori 算法中你最主要是需要明白支持度、置信度、提升度这几个概念，以及 Apriori 迭代计算频繁项集的工作流程。Apriori 算法在实际工作中需要对数据集扫描多次，会消耗大量的计算时间，所以在 2000 年 FP-Growth 算法被提出来，它只需要扫描两次数据集即可以完成关联规则的挖掘。FP-Growth 算法最主要的贡献就是提出了 FP 树和项头表，通过 FP 树减少了频繁项集的存储以及计算时间。当然 Apriori 的改进算法除了 FP-Growth 算法以外，还有 CBA 算法、GSP 算法，这里就不进行介绍。你能发现一种新理论的提出，往往是先从最原始的概念出发，提出一种新的方法。原始概念最接近人们模拟的过程，但往往会存在空间和时间复杂度过高的情况。所以后面其他人会对这个方法做改进型的创新，重点是在空间和时间复杂度上进行降维，比如采用新型的数据结构。你能看出树在存储和检索中是一个非常好用的数据结构。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"}]},{"title":"数据分析-关联规则 实战","slug":"数据分析-关联规则-实战","date":"2018-09-01T14:19:27.000Z","updated":"2020-05-01T14:19:45.546Z","comments":true,"path":"2018/09/01/数据分析-关联规则-实战/","link":"","permalink":"cpeixin.cn/2018/09/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-%E5%AE%9E%E6%88%98/","excerpt":"","text":"昨天讲解了关联规则挖掘的原理。关联规则挖掘在生活中有很多使用场景，不仅是商品的捆绑销售，甚至在挑选演员决策上，你也能通过关联规则挖掘看出来某个导演选择演员的倾向。今天我来带你用 Apriori 算法做一个项目实战。你需要掌握的是以下几点：熟悉上节课讲到的几个重要概念：支持度、置信度和提升度；熟悉与掌握 Apriori 工具包的使用；在实际问题中，灵活运用。包括数据集的准备等。如何使用 AprioriApriori 虽然是十大算法之一，不过在 sklearn 工具包中并没有它，也没有 FP-Growth 算法。这里教你个方法，来选择 Python 中可以使用的工具包，你可以通过https://pypi.org/ 搜索工具包。这个网站提供的工具包都是 Python 语言的，你能找到 8 个 Python 语言的 Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即 efficient-apriori。后面我会讲到为什么推荐这个工具包。首先你需要通过 pip install efficient-apriori 安装这个工具包。然后看下如何使用它，核心的代码就是这一行：1itemsets, rules = apriori(data, min_support, min_confidence)其中 data 是我们要提供的数据集，它是一个 list 数组类型。min_support 参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。min_confidence 是最小置信度，数值也代表百分比，比如 1 代表 100%。一般来说最小支持度常见的取值有0.5，0.1, 0.05。最小置信度常见的取值有1.0, 0.9, 0.8。可以通过尝试一些取值，然后观察关联结果的方式来调整最小值尺度和最小置信度的取值。关于支持度、置信度和提升度，我们再来简单回忆下。支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的概率越大。置信度是一个条件概念，就是在 A 发生的情况下，B 发生的概率是多少。提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升了多少”。接下来我们用这个工具包，跑一下上节课中讲到的超市购物的例子。下面是客户购买的商品列表：具体实现的代码如下：123456789101112from efficient_apriori import apriori# 设置数据集data = [('牛奶','面包','尿布'), ('可乐','面包', '尿布', '啤酒'), ('牛奶','尿布', '啤酒', '鸡蛋'), ('面包', '牛奶', '尿布', '啤酒'), ('面包', '牛奶', '尿布', '可乐')]# 挖掘频繁项集和频繁规则itemsets, rules = apriori(data, min_support=0.5, min_confidence=1)print(itemsets)print(rules)结果：1234&#123;1: &#123;('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 4&#125;, 2: &#123;('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3&#125;, 3: &#123;('尿布', '牛奶', '面包'): 3&#125;&#125;[&#123;啤酒&#125; -&gt; &#123;尿布&#125;, &#123;牛奶&#125; -&gt; &#123;尿布&#125;, &#123;面包&#125; -&gt; &#123;尿布&#125;, &#123;牛奶, 面包&#125; -&gt; &#123;尿布&#125;]你能从代码中看出来，data 是个 List 数组类型，其中每个值都可以是一个集合。实际上你也可以把 data 数组中的每个值设置为 List 数组类型，比如：123456data = [['牛奶','面包','尿布'], ['可乐','面包', '尿布', '啤酒'], ['牛奶','尿布', '啤酒', '鸡蛋'], ['面包', '牛奶', '尿布', '啤酒'], ['面包', '牛奶', '尿布', '可乐']]两者的运行结果是一样的，efficient-apriori 工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。而其他的 Apriori 算法可能会因为考虑了先后顺序，出现计算频繁项集结果不对的情况。所以这里采用的是 efficient-apriori 这个工具包。**挖掘-导演是如何选择演员在实际工作中，数据集是需要自己来准备的，比如今天我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的 Python 爬虫进行数据采集。不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。先来梳理下采集的工作流程。首先我们先在https://movie.douban.com搜索框中输入导演姓名，比如“宁浩”。页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：页面默认是 15 条数据反馈，第一页会返回 16 条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用“/”分割。有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在 csv 文件中。完整的抓取代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# -*- coding: utf-8 -*-# 下载某个导演的电影数据集from efficient_apriori import apriorifrom lxml import etreeimport timefrom selenium import webdriverimport csvdriver = webdriver.Chrome()# 设置想要下载的导演 数据集director = u'宁浩'# 写CSV文件file_name = './' + director + '.csv'base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&amp;cat=1002&amp;start='out = open(file_name,'w', newline='', encoding='utf-8-sig')csv_write = csv.writer(out, dialect='excel')flags=[]# 下载指定页面的数据def download(request_url): driver.get(request_url) time.sleep(1) html = driver.find_element_by_xpath(\"//*\").get_attribute(\"outerHTML\") html = etree.HTML(html) # 设置电影名称，导演演员 的XPATH movie_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']\") name_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']\") # 获取返回的数据个数 num = len(movie_lists) if num &gt; 15: #第一页会有16条数据 # 默认第一个不是，所以需要去掉 movie_lists = movie_lists[1:] name_lists = name_lists[1:] for (movie, name_list) in zip(movie_lists, name_lists): # 会存在数据为空的情况 if name_list.text is None: continue # 显示下演员名称 print(name_list.text) names = name_list.text.split('/') # 判断导演是否为指定的director if names[0].strip() == director and movie.text not in flags: # 将第一个字段设置为电影名称 names[0] = movie.text flags.append(movie.text) csv_write.writerow(names) print('OK') # 代表这页数据下载成功 print(num) if num &gt;= 14: #有可能一页会有14个电影 # 继续下一页 return True else: # 没有下一页 return False# 开始的ID为0，每页增加15start = 0while start&lt;10000: #最多抽取1万部电影 request_url = base_url + str(start) # 下载数据，并返回是否有下一页 flag = download(request_url) if flag: start = start + 15 else: breakout.close()print('finished')爬取的代码在这里就不赘述了，其中有一点就是这里用到了selenium模拟打开窗口爬取。下面是爬取下来的数据：我们用获取到的少量宁浩数据，来做一次关联规则分析：12345678910111213141516171819# -*- coding: utf-8 -*-from efficient_apriori import aprioriimport csvdirector = u'宁浩'file_name = './'+director+'.csv'lists = csv.reader(open(file_name, 'r', encoding='utf-8-sig'))# 数据加载data = []for names in lists: name_new = [] for name in names: # 去掉演员数据中的空格 name_new.append(name.strip()) data.append(name_new[1:])# 挖掘频繁项集和关联规则itemsets, rules = apriori(data, min_support=0.5, min_confidence=1)print(itemsets)print(rules)代码中使用的 apriori 方法和开头中用 Apriori 获取购物篮规律的方法类似，比如代码中都设定了最小支持度和最小置信系数，这样我们可以找到支持度大于 50%，置信系数为 1 的频繁项集和关联规则。这是最后的运行结果：123&#123;1: &#123;('徐峥',): 5, ('黄渤',): 6&#125;, 2: &#123;('徐峥', '黄渤'): 5&#125;&#125;[&#123;徐峥&#125; -&gt; &#123;黄渤&#125;]你能看出来，宁浩导演喜欢用徐峥和黄渤，并且有徐峥的情况下，一般都会用黄渤。你也可以用上面的代码来挖掘下其他导演选择演员的规律。总结Apriori 算法的核心就是理解频繁项集和关联规则。在算法运算的过程中，还要重点掌握对支持度、置信度和提升度的理解。在工具使用上，你可以使用 efficient-apriori 这个工具包，它会把每一条数据中的项（item）放到一个集合（篮子）里来处理，不考虑项（item）之间的先后顺序。在实际运用中你还需要灵活处理，比如导演如何选择演员这个案例，虽然工具的使用会很方便，但重要的还是数据挖掘前的准备过程，也就是获取某个导演的电影数据集。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"}]},{"title":"数据分析 - EM聚类 实战","slug":"数据分析-EM聚类-实战","date":"2018-08-18T14:18:53.000Z","updated":"2020-05-01T14:16:07.111Z","comments":true,"path":"2018/08/18/数据分析-EM聚类-实战/","link":"","permalink":"cpeixin.cn/2018/08/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-EM%E8%81%9A%E7%B1%BB-%E5%AE%9E%E6%88%98/","excerpt":"","text":"今天进行 EM 的实战。上篇讲了 EM 算法的原理，EM 算法相当于一个聚类框架，里面有不同的聚类模型，比如 GMM 高斯混合模型，或者 HMM 隐马尔科夫模型。其中你需要理解的是 EM 的两个步骤，E 步和 M 步：E 步相当于通过初始化的参数来估计隐含变量，M 步是通过隐含变量来反推优化参数。最后通过 EM 步骤的迭代得到最终的模型参数。今天我们进行 EM 算法的实战，你需要思考的是：如何使用 EM 算法工具完成聚类？什么情况下使用聚类算法？我们用聚类算法的任务目标是什么？面对王者荣耀的英雄数据，EM 算法能帮助我们分析出什么？如何使用 EM 工具包在 Python 中有第三方的 EM 算法工具包。由于 EM 算法是一个聚类框架，所以你需要明确你要用的具体算法，比如是采用 GMM 高斯混合模型，还是 HMM 隐马尔科夫模型。这节课我们主要讲解 GMM 的使用，在使用前你需要引入工具包：1from sklearn.mixture import GaussianMixture我们看下如何在 sklearn 中创建 GMM 聚类。首先我们使用 gmm = GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100) 来创建 GMM 聚类，其中有几个比较主要的参数（GMM 类的构造参数比较多，我筛选了一些主要的进行讲解），我分别来讲解下：1.n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果你不指定 n_components，最终的聚类结果都会为同一个值。2.covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。协方差类型有 4 种取值：covariance_type=full，代表完全协方差，也就是元素都不为 0；covariance_type=tied，代表相同的完全协方差；covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。3.max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。创建完 GMM 聚类器之后，我们就可以传入数据让它进行迭代拟合。我们使用 fit 函数，传入样本特征矩阵，模型会自动生成聚类器，然后使用 prediction=gmm.predict(data) 来对数据进行聚类，传入你想进行聚类的数据，可以得到聚类结果 prediction。你能看出来拟合训练和预测可以传入相同的特征矩阵，这是因为聚类是无监督学习，你不需要事先指定聚类的结果，也无法基于先验的结果经验来进行学习。只要在训练过程中传入特征值矩阵，机器就会按照特征值矩阵生成聚类器，然后就可以使用这个聚类器进行聚类了。如何用 EM 算法对王者荣耀数据进行聚类了解了 GMM 聚类工具之后，我们看下如何对王者荣耀的英雄数据进行聚类。首先我们知道聚类的原理是“人以群分，物以类聚”。通过聚类算法把特征值相近的数据归为一类，不同类之间的差异较大，这样就可以对原始数据进行降维。通过分成几个组（簇），来研究每个组之间的特性。或者我们也可以把组（簇）的数量适当提升，这样就可以找到可以互相替换的英雄，比如你的对手选择了你擅长的英雄之后，你可以选择另一个英雄作为备选。我们先看下数据长什么样子：这里我们收集了 69 名英雄的 20 个特征属性，这些属性分别是最大生命、生命成长、初始生命、最大法力、法力成长、初始法力、最高物攻、物攻成长、初始物攻、最大物防、物防成长、初始物防、最大每 5 秒回血、每 5 秒回血成长、初始每 5 秒回血、最大每 5 秒回蓝、每 5 秒回蓝成长、初始每 5 秒回蓝、最大攻速和攻击范围等。现在我们需要对王者荣耀的英雄数据进行聚类，我们先设定项目的执行流程：首先我们需要加载数据源；在准备阶段，我们需要对数据进行探索，包括采用数据可视化技术，让我们对英雄属性以及这些属性之间的关系理解更加深刻，然后对数据质量进行评估，是否进行数据清洗，最后进行特征选择方便后续的聚类算法；聚类阶段：选择适合的聚类模型，这里我们采用 GMM 高斯混合模型进行聚类，并输出聚类结果，对结果进行分析。按照上面的步骤，我们来编写下代码。完整的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-import pandas as pdimport csvimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.mixture import GaussianMixturefrom sklearn.preprocessing import StandardScaler # 数据加载，避免中文乱码问题data_ori = pd.read_csv('./heros7.csv', encoding = 'gb18030')features = [u'最大生命',u'生命成长',u'初始生命',u'最大法力', u'法力成长',u'初始法力',u'最高物攻',u'物攻成长',u'初始物攻',u'最大物防',u'物防成长',u'初始物防', u'最大每5秒回血', u'每5秒回血成长', u'初始每5秒回血', u'最大每5秒回蓝', u'每5秒回蓝成长', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']data = data_ori[features] # 对英雄属性之间的关系进行可视化分析# 设置plt正确显示中文plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号# 用热力图呈现features_mean字段之间的相关性corr = data[features].corr()plt.figure(figsize=(14,14))# annot=True显示每个方格的数据sns.heatmap(corr, annot=True)plt.show() # 相关性大的属性保留一个，因此可以对属性进行降维features_remain = [u'最大生命', u'初始生命', u'最大法力', u'最高物攻', u'初始物攻', u'最大物防', u'初始物防', u'最大每5秒回血', u'最大每5秒回蓝', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']data = data_ori[features_remain]data[u'最大攻速'] = data[u'最大攻速'].apply(lambda x: float(x.strip('%'))/100)data[u'攻击范围']=data[u'攻击范围'].map(&#123;'远程':1,'近战':0&#125;)# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()data = ss.fit_transform(data)# 构造GMM聚类gmm = GaussianMixture(n_components=30, covariance_type='full')gmm.fit(data)# 训练数据prediction = gmm.predict(data)print(prediction)# 将分组结果输出到CSV文件中data_ori.insert(0, '分组', prediction)data_ori.to_csv('./hero_out.csv', index=False, sep=',')1234[28 14 8 9 5 5 15 8 3 14 18 14 9 7 16 18 13 3 5 4 19 12 4 12 12 12 4 17 24 2 7 2 2 24 2 2 24 6 20 22 22 24 24 2 2 22 14 20 14 24 26 29 27 25 25 28 11 1 23 5 11 0 10 28 21 29 29 29 17]同时你也能看到输出的聚类结果文件 hero_out.csv（它保存在你本地运行的文件夹里，程序会自动输出这个文件，你可以自己看下）。我来简单讲解下程序的几个模块。关于引用包首先我们会用 DataFrame 数据结构来保存读取的数据，最后的聚类结果会写入到 CSV 文件中，因此会用到 pandas 和 CSV 工具包。另外我们需要对数据进行可视化，采用热力图展现属性之间的相关性，这里会用到 matplotlib.pyplot 和 seaborn 工具包。在数据规范化中我们使用到了 Z-Score 规范化，用到了 StandardScaler 类，最后我们还会用到 sklearn 中的 GaussianMixture 类进行聚类。数据可视化的探索你能看到我们将 20 个英雄属性之间的关系用热力图呈现了出来，中间的数字代表两个属性之间的关系系数，最大值为 1，代表完全正相关，关系系数越大代表相关性越大。从图中你能看出来“最大生命”“生命成长”和“初始生命”这三个属性的相关性大，我们只需要保留一个属性即可。同理我们也可以对其他相关性大的属性进行筛选，保留一个。你在代码中可以看到，我用 features_remain 数组保留了特征选择的属性，这样就将原本的 20 个属性降维到了 13 个属性。关于数据规范化我们能看到“最大攻速”这个属性值是百分数，不适合做矩阵运算，因此我们需要将百分数转化为小数。我们也看到“攻击范围”这个字段的取值为远程或者近战，也不适合矩阵运算，我们将取值做个映射，用 1 代表远程，0 代表近战。然后采用 Z-Score 规范化，对特征矩阵进行规范化。在聚类阶段我们采用了 GMM 高斯混合模型，并将结果输出到 CSV 文件中。这里我将输出的结果截取了一段（设置聚类个数为 30）：第一列代表的是分组（簇），我们能看到张飞、程咬金分到了一组，牛魔、白起是一组，老夫子自己是一组，达摩、典韦是一组。聚类的特点是相同类别之间的属性值相近，不同类别的属性值差异大。因此如果你擅长用典韦这个英雄，不防试试达摩这个英雄。同样你也可以在张飞和程咬金中进行切换。这样就算你的英雄被别人选中了，你依然可以有备选的英雄可以使用。总结今天我带你一起做了 EM 聚类的实战，具体使用的是 GMM 高斯混合模型。从整个流程中可以看出，我们需要经过数据加载、数据探索、数据可视化、特征选择、GMM 聚类和结果分析等环节。聚类和分类不一样，聚类是无监督的学习方式，也就是我们没有实际的结果可以进行比对，所以聚类的结果评估不像分类准确率一样直观，那么有没有聚类结果的评估方式呢？这里我们可以采用 Calinski-Harabaz 指标，代码如下：123from sklearn.metrics import calinski_harabaz_scoreprint(calinski_harabaz_score(data, prediction))指标分数越高，代表聚类效果越好，也就是相同类中的差异性小，不同类之间的差异性大。当然具体聚类的结果含义，我们需要人工来分析，也就是当这些数据被分成不同的类别之后，具体每个类表代表的含义。另外聚类算法也可以作为其他数据挖掘算法的预处理阶段，这样我们就可以将数据进行降维了。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"}]},{"title":"数据分析-K-Means_1","slug":"数据分析-K-Means-1","date":"2018-08-01T15:26:13.000Z","updated":"2020-05-01T14:15:41.551Z","comments":true,"path":"2018/08/01/数据分析-K-Means-1/","link":"","permalink":"cpeixin.cn/2018/08/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-K-Means-1/","excerpt":"","text":"数据分析 - K-Means 原理K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，你可以理解这个算法的本质是确定 K 类的中心点，当你找到了这些中心点，也就完成了聚类。那么请你和我思考以下三个问题：如何确定 K 类的中心点？如何将其他点划分到 K 类中？如何区分 K-Means 与 KNN？如果理解了上面这 3 个问题，那么对 K-Means 的原理掌握得也就差不多了。先请你和我思考一个场景，假设我有 20 支亚洲足球队，想要将它们按照成绩划分成 3 个等级，可以怎样划分？K-Means 的工作原理对亚洲足球队的水平，你可能也有自己的判断。比如一流的亚洲球队有谁？你可能会说伊朗或韩国。二流的亚洲球队呢？你可能说是中国。三流的亚洲球队呢？你可能会说越南。其实这些都是靠我们的经验来划分的，那么伊朗、中国、越南可以说是三个等级的典型代表，也就是我们每个类的中心点。所以回过头来，如何确定 K 类的中心点？一开始我们是可以随机指派的，当你确认了中心点后，就可以按照距离将其他足球队划分到不同的类别中。这也就是 K-Means 的中心思想，就是这么简单直接。你可能会问：如果一开始，选择一流球队是中国，二流球队是伊朗，三流球队是韩国，中心点选择错了怎么办？其实不用担心，K-Means 有自我纠正机制，在不断的迭代过程中，会纠正中心点。中心点在整个迭代过程中，并不是唯一的，只是你需要一个初始值，一般算法会随机设置初始的中心点。好了，那我来把 K-Means 的工作原理给你总结下：选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点；重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。如何给亚洲球队做聚类对于机器来说需要数据才能判断类中心点，所以我整理了 2015-2019 年亚洲球队的排名，如下表所示。我来说明一下数据概况。其中 2019 年国际足联的世界排名，2015 年亚洲杯排名均为实际排名。2018 年世界杯中，很多球队没有进入到决赛圈，所以只有进入到决赛圈的球队才有实际的排名。如果是亚洲区预选赛 12 强的球队，排名会设置为 40。如果没有进入亚洲区预选赛 12 强，球队排名会设置为 50。针对上面的排名，我们首先需要做的是数据规范化。你可以把这些值划分到[0,1]或者按照均值为 0，方差为 1 的正态分布进行规范化。我先把数值都规范化到[0,1]的空间中，得到了以下的数值表：如果我们随机选取中国、日本、韩国为三个类的中心点，我们就需要看下这些球队到中心点的距离。距离有多种计算的方式，有关距离的计算我在 KNN 算法中也讲到过：欧氏距离曼哈顿距离切比雪夫距离余弦距离欧氏距离是最常用的距离计算方式，这里我选择欧氏距离作为距离的标准，计算每个队伍分别到中国、日本、韩国的距离，然后根据距离远近来划分。我们看到大部分的队，会和中国队聚类到一起。这里我整理了距离的计算过程，比如中国和中国的欧氏距离为 0，中国和日本的欧式距离为 0.732003。如果按照中国、日本、韩国为 3 个分类的中心点，欧氏距离的计算结果如下表所示：然后我们再重新计算这三个类的中心点，如何计算呢？最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类，再根据球队的分类更新中心点的位置。计算过程这里不展开，最后一直迭代（重复上述的计算过程：计算中心点和划分分类）到分类不再发生变化，可以得到以下的分类结果：所以我们能看出来第一梯队有日本、韩国、伊朗、沙特、澳洲；第二梯队有中国、伊拉克、阿联酋、乌兹别克斯坦；第三梯队有卡塔尔、泰国、越南、阿曼、巴林、朝鲜、印尼、叙利亚、约旦、科威特和巴勒斯坦。如何使用 sklearn 中的 K-Means 算法sklearn 是 Python 的机器学习工具库，如果从功能上来划分，sklearn 可以实现分类、聚类、回归、降维、模型选择和预处理等功能。这里我们使用的是 sklearn 的聚类函数库，因此需要引用工具包，具体代码如下：1from sklearn.cluster import KMeans当然 K-Means 只是 sklearn.cluster 中的一个聚类库，实际上包括 K-Means 在内，sklearn.cluster 一共提供了 9 种聚类方法，比如 Mean-shift，DBSCAN，Spectral clustering（谱聚类）等。这些聚类方法的原理和 K-Means 不同，这里不做介绍。我们看下 K-Means 如何创建：1KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')我们能看到在 K-Means 类创建的过程中，有一些主要的参数：n_clusters: 即 K 值，一般需要多试一些 K 值来保证更好的聚类效果。你可以随机设置一些 K 值，然后选择聚类效果最好的作为最终的 K 值；max_iter： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长；n_init：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。如果 K 值比较大的时候，你可以适当增大 n_init 这个值；init： 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式。自己设置中心点一般是对于个性化的数据进行设置，很少采用。random 的方式则是完全随机的方式，一般推荐采用优化过的 k-means++ 方式；algorithm：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的”auto”。简单说下这三个取值的区别，如果你选择”full”采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。在创建好 K-Means 类之后，就可以使用它的方法，最常用的是 fit 和 predict 这个两个函数。你可以单独使用 fit 函数和 predict 函数，也可以合并使用 fit_predict 函数。其中 fit(data) 可以对 data 数据进行 k-Means 聚类。 predict(data) 可以针对 data 中的每个样本，计算最近的类。现在我们要完整地跑一遍 20 支亚洲球队的聚类问题。1234567891011121314151617181920212223# coding: utf-8from sklearn.cluster import KMeansfrom sklearn import preprocessingimport pandas as pdimport numpy as np# 输入数据data = pd.read_csv('data.csv', encoding='gbk')train_x = data[[\"2019年国际排名\",\"2018世界杯\",\"2015亚洲杯\"]]df = pd.DataFrame(train_x)# kmeans = KMeans(n_clusters=3)kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')# 规范化到[0,1]空间min_max_scaler=preprocessing.MinMaxScaler()train_x=min_max_scaler.fit_transform(train_x)# kmeans算法kmeans.fit(train_x)predict_y = kmeans.predict(train_x)# 合并聚类结果，插入到原数据中result = pd.concat((data,pd.DataFrame(predict_y)),axis=1)result.rename(&#123;0:u'聚类'&#125;,axis=1,inplace=True)print(result)结果：123456789101112131415161718192021 国家 2019年国际排名 2018世界杯 2015亚洲杯 聚类0 中国 73 40 7 21 日本 60 15 5 02 韩国 61 19 2 03 伊朗 34 18 6 04 沙特 67 26 10 05 伊拉克 91 40 4 26 卡塔尔 101 40 13 17 阿联酋 81 40 6 28 乌兹别克斯坦 88 40 8 29 泰国 122 40 17 110 越南 102 50 17 111 阿曼 87 50 12 112 巴林 116 50 11 113 朝鲜 110 50 14 114 印尼 164 50 17 115 澳洲 40 30 1 016 叙利亚 76 40 17 117 约旦 118 50 9 118 科威特 160 50 15 119 巴勒斯坦 96 50 16 1总结如何确定 K 类的中心点？其中包括了初始的设置，以及中间迭代过程中中心点的计算。在初始设置中，会进行 n_init 次的选择，然后选择初始中心点效果最好的为初始值。在每次分类更新后，你都需要重新确认每一类的中心点，一般采用均值的方式进行确认。如何将其他点划分到 K 类中？这里实际上是关于距离的定义，我们知道距离有多种定义的方式，在 K-Means 和 KNN 中，我们都可以采用欧氏距离、曼哈顿距离、切比雪夫距离、余弦距离等。对于点的划分，就看它离哪个类的中心点的距离最近，就属于哪一类。如何区分 K-Means 和 KNN 这两种算法呢？刚学过 K-Means 和 KNN 算法的同学应该能知道两者的区别，但往往过了一段时间，就容易混淆。所以我们可以从三个维度来区分 K-Means 和 KNN 这两个算法：首先，这两个算法解决数据挖掘的两类问题。K-Means 是聚类算法，KNN 是分类算法。这两个算法分别是两种不同的学习方式。K-Means 是非监督学习，也就是不需要事先给出分类标签，而 KNN 是有监督学习，需要我们给出训练数据的分类标识。最后，K 值的含义不同。K-Means 中的 K 值代表 K 类。KNN 中的 K 值代表 K 个最接近的邻居。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"K-Means","slug":"K-Means","permalink":"cpeixin.cn/tags/K-Means/"}]},{"title":"数据分析-KNN_2","slug":"数据分析-KNN-2","date":"2018-07-26T16:14:49.000Z","updated":"2020-05-01T14:14:37.744Z","comments":true,"path":"2018/07/27/数据分析-KNN-2/","link":"","permalink":"cpeixin.cn/2018/07/27/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-2/","excerpt":"","text":"通过 sklearn 中自带的手写数字数据集来进行实战。如何在 sklearn 中使用 KNN在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。如果是做分类，你需要引用：1from sklearn.neighbors import KNeighborsClassifier如果是做回归，你需要引用：1from sklearn.neighbors import KNeighborsRegressor从名字上你也能看出来 Classifier 对应的是分类，Regressor 对应的是回归。一般来说如果一个算法有 Classifier 类，都能找到相应的 Regressor 类。比如在决策树分类中，你可以使用 DecisionTreeClassifier，也可以使用决策树来做回归 DecisionTreeRegressor。好了，我们看下如何在 sklearn 中创建 KNN 分类器。这里，我们使用构造函数1KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)这里有几个比较主要的参数，我分别来讲解下：1.n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。2.weights：是用来确定邻居的权重，有三种方式：weights=uniform，代表所有邻居的权重相同；weights=distance，代表权重是距离的倒数，即与距离成反比；自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。3.algorithm：用来规定计算邻居的方法，它有四种方式：algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。4.leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。如何用 KNN 对手写数字进行识别分类手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。今天我们用 sklearn 自带的手写数字数据集做 KNN 分类，你可以把这个数据集理解成一个简版的 MNIST 数据集，它只包括了 1797 幅数字图像，每幅图像大小是 8*8 像素。好了，我们先来规划下整个 KNN 分类的流程：整个训练过程基本上都会包括三个阶段：数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8*8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。好了，按照上面的步骤，我们一起来实现下这个项目。首先是加载数据和对数据的探索：1234567891011121314# 加载数据digits = load_digits()data = digits.data# 数据探索print(data.shape)# 查看第5幅图像print(digits.images[4])# 第5幅图像代表的数字含义print(digits.target[4])# 将第5幅图像显示出来plt.gray()plt.imshow(digits.images[4])plt.show()结果：12345678910(1797, 64)[[ 0. 0. 0. 1. 11. 0. 0. 0.] [ 0. 0. 0. 7. 8. 0. 0. 0.] [ 0. 0. 1. 13. 6. 2. 2. 0.] [ 0. 0. 7. 15. 0. 9. 8. 0.] [ 0. 5. 16. 10. 0. 16. 6. 0.] [ 0. 4. 15. 16. 13. 16. 1. 0.] [ 0. 0. 0. 3. 15. 10. 0. 0.] [ 0. 0. 0. 2. 16. 4. 0. 0.]]4对应的手写图像数据：我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个 88 的像素矩阵，上面这幅图像是一个“4”，从训练集的分类标注中我们也可以看到分类标注为“4”。sklearn 自带的手写数字数据集一共包括了 1797 个样本，每幅图像都是 88 像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化，代码如下：1234567# 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# 采用Z-Score规范化ss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)注：上面代码中，在train的时候用到了：train_ss_x = ss.fit_transform(train_x)实际上：fit_transform是fit和transform两个函数都执行一次。所以ss是进行了fit拟合的。只有在fit拟合之后，才能进行transform，在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。另外，如果我们没有fit，直接进行transform会报错，因为需要先fit拟合，才可以进行transform。然后我们构造一个 KNN 分类器 knn，把训练集的数据传入构造好的 knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到 KNN 分类器准确率，代码如下：123456# 创建KNN分类器knn = KNeighborsClassifier() knn.fit(train_ss_x, train_y) predict_y = knn.predict(test_ss_x) print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))运行结果：12KNN准确率: 0.9756好了，这样我们就构造好了一个 KNN 分类器。之前我们还讲过 SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：1234567891011121314151617181920# 创建SVM分类器svm = SVC()svm.fit(train_ss_x, train_y)predict_y=svm.predict(test_ss_x)print('SVM准确率: %0.4lf' % accuracy_score(test_y, predict_y))# 采用Min-Max规范化mm = preprocessing.MinMaxScaler()train_mm_x = mm.fit_transform(train_x)test_mm_x = mm.transform(test_x)# 创建Naive Bayes分类器mnb = MultinomialNB()mnb.fit(train_mm_x, train_y) predict_y = mnb.predict(test_mm_x) print(\"多项式朴素贝叶斯准确率: %.4lf\" % accuracy_score(test_y, predict_y))# 创建CART决策树分类器dtc = DecisionTreeClassifier()dtc.fit(train_mm_x, train_y) predict_y = dtc.predict(test_mm_x) print(\"CART决策树准确率: %.4lf\" % accuracy_score(test_y, predict_y))结果：1234SVM准确率: 0.9867多项式朴素贝叶斯准确率: 0.8844CART决策树准确率: 0.8556这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为 Z-Score 会将数值规范化为一个标准的正态分布，即均值为 0，方差为 1，数值会包含负数。因此我们需要采用 Min-Max 规范化，将数据规范化到[0,1]范围内。你能看出来 KNN 的准确率还是不错的，和 SVM 不相上下。完整代码：123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*-# 手写数字分类from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as plt# 加载数据digits = load_digits()data = digits.data# 数据探索print(data.shape)# 查看第一幅图像print(digits.images[0])# 第一幅图像代表的数字含义print(digits.target[0])# 将第一幅图像显示出来plt.gray()plt.imshow(digits.images[0])plt.show()# 分割数据，将25%的数据作为测试集，其余作为训练集train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# 采用Z-Score规范化ss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# 创建KNN分类器knn = KNeighborsClassifier()knn.fit(train_ss_x, train_y) predict_y = knn.predict(test_ss_x) print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))总结手写数字分类识别的实战，分别用 KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在这个过程中你应该对数据探索、数据可视化、数据规范化、模型训练和结果评估的使用过程有了一定的体会。在数据量不大的情况下，使用 sklearn 还是方便的。如果数据量很大，比如 MNIST 数据集中的 6 万个训练数据和 1 万个测试数据，那么采用深度学习 +GPU 运算的方式会更适合。因为深度学习的特点就是需要大量并行的重复计算，GPU 最擅长的就是做大量的并行计算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"}]},{"title":"数据分析-KNN_1","slug":"数据分析-KNN-1","date":"2018-07-25T16:14:49.000Z","updated":"2020-05-01T14:14:47.958Z","comments":true,"path":"2018/07/26/数据分析-KNN-1/","link":"","permalink":"cpeixin.cn/2018/07/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-1/","excerpt":"","text":"数据分析 - KNN 原理KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行分类，统计了电影中打斗次数、接吻次数，当然还有其他的指标也可以被统计到，如下表所示。我们很容易理解《战狼》《红海行动》《碟中谍 6》是动作片，《前任 3》《春娇救志明》《泰坦尼克号》是爱情片，但是有没有一种方法让机器也可以掌握这个分类的规则，当有一部新电影的时候，也可以对它的类型自动分类呢？我们可以把打斗次数看成 X 轴，接吻次数看成 Y 轴，然后在二维的坐标轴上，对这几部电影进行标记，如下图所示。对于未知的电影 A，坐标为 (x,y)，我们需要看下离电影 A 最近的都有哪些电影，这些电影中的大多数属于哪个分类，那么电影 A 就属于哪个分类。实际操作中，我们还需要确定一个 K 值，也就是我们要观察离电影 A 最近的电影有多少个。KNN 的工作原理“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。整个计算过程分为三步：计算待分类物体与其他物体之间的距离；统计距离最近的 K 个邻居；对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。K 值如何选择你能看出整个 KNN 的分类过程，K 值的选择还是很重要的。那么问题来了，K 值选择多少是适合的呢？如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。距离如何计算在 KNN算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。关于距离的计算方式有下面五种方式：欧氏距离；曼哈顿距离；闵可夫斯基距离；切比雪夫距离；余弦距离。其中前三种距离是 KNN 中最常用的距离，我给你分别讲解下。欧氏距离是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：同理，我们也可以求得两点在 n 维空间中的距离：曼哈顿距离在几何空间中用的比较多。以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：闵可夫斯基距离不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。那么切比雪夫距离怎么计算呢？二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。其公式为p为极限无穷的情况：余弦距离实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。KD 树其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。用 KNN 做回归KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。那么 KNN 如何做回归呢？对于一个新电影 X，我们要预测它的某个属性值，比如打斗次数，具体特征属性和数值如下所示。此时，我们会先计算待测点（新电影 X）到已知点的距离，选择距离最近的 K 个点。假设 K=3，此时最近的 3 个点（电影）分别是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数就是这 3 个点的该属性值的平均值，即 (100+95+105)/3=100 次。总结今天我给你讲了 KNN 的原理，以及 KNN 中的几个关键因素。比如针对 K 值的选择，我们一般采用交叉验证的方式得出。针对样本点之间的距离的定义，常用的有 5 种表达方式，你也可以自己来定义两个样本之间的距离公式。不同的定义，适用的场景不同。比如在搜索关键词推荐中，余弦距离是更为常用的。另外你也可以用 KNN 进行回归，通过 K 个邻居对新的点的属性进行值的预测。KNN 的理论简单直接，针对 KNN 中的搜索也有相应的 KD 树这个数据结构。KNN 的理论成熟，可以应用到线性和非线性的分类问题中，也可以用于回归分析。不过 KNN 需要计算测试点与样本点之间的距离，当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间。另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。当然在实际工作中，我们需要考虑到各种可能存在的情况，比如针对某类样本少的情况，可以增加该类别的权重。同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"}]},{"title":"数据分析-SVM_2","slug":"数据分析-SVM-2","date":"2018-07-25T12:27:13.000Z","updated":"2020-05-01T14:14:15.085Z","comments":true,"path":"2018/07/25/数据分析-SVM-2/","link":"","permalink":"cpeixin.cn/2018/07/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-2/","excerpt":"","text":"SVM 是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。sklearn 中使用 SVM在 Python 的 sklearn 工具包中有 SVM 算法，首先需要引用工具包：1from sklearn import svmSVM 既可以做回归，也可以做分类器。当用 SVM 做回归的时候，我们可以使用 SVR 或 LinearSVR。SVR 的英文是 Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。当做分类器的时候，我们使用的是 SVC 或者 LinearSVC。SVC 的英文是 Support Vector Classification。我简单说一下这两者之前的差别。从名字上你能看出 LinearSVC 是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到 SVM 是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。如果是针对非线性的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。创建一个 SVM 分类器我们首先使用 SVC 的构造函数：1model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)这里有三个重要的参数 kernel、C 和 gamma。kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。linear：线性核函数poly：多项式核函数rbf：高斯核函数（默认）sigmoid：sigmoid 核函数这四种函数代表不同的映射方式，你可能会问，在实际工作中，如何选择这 4 种核函数呢？我来给你解释一下：线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。了解深度学习的同学应该知道 sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。上面介绍的 4 种核函数，除了第一种线性核函数外，其余 3 种都可以处理线性不可分的数据。参数 C 代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。参数 gamma 代表核函数的系数，默认为样本特征数的倒数，即 gamma = 1 / n_features。在创建 SVM 分类器之后，就可以输入训练集对它进行训练。我们使用 model.fit(train_X,train_y)，传入训练集中的特征值矩阵 train_X 和分类标识 train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用 prediction=model.predict(test_X) 来对结果进行预测，传入测试集中的样本特征矩阵 test_X，可以得到测试集的预测分类结果 prediction。同样我们也可以创建线性 SVM 分类器，使用 model=svm.LinearSVC()。在 LinearSVC 中没有 kernel 这个参数，限制我们只能使用线性核函数。由于 LinearSVC 对线性分类做了优化，对于数据量大的线性可分问题，使用 LinearSVC 的效率要高于 SVC。如果你不知道数据集是否为线性，可以直接使用 SVC 类创建 SVM 分类器。在训练和预测中，LinearSVC 和 SVC 一样，都是使用 model.fit(train_X,train_y) 和 model.predict(test_X)。SVM 进行乳腺癌检测在了解了如何创建和使用 SVM 分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集，点击这里进行下载 https://github.com/cystanford/breast_cancer_data/blob/master/data.csv。医疗人员采集了患者乳腺肿块经过细针穿刺 (FNA) 后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：数据表一共包括了 32 个字段，代表的含义如下：上面的表格中，mean 代表平均值，se 代表标准差，worst 代表最大值（3 个最大值的平均值）。每张图像都计算了相应的特征，得出了这 30 个特征值（不包括 ID 字段和分类标识结果字段 diagnosis），实际上是 10 个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry 和 fractal_dimension_mean）的 3 个维度，平均、标准差和最大值。这些特征值都保留了 4 位数字。字段中没有缺失的值。在 569 个患者中，一共有 357 个是良性，212 个是恶性。好了，我们的目标是生成一个乳腺癌诊断的 SVM 分类器，并计算这个分类器的准确率。首先设定项目的执行流程：首先我们需要加载数据源；在准备阶段，需要对加载的数据源进行探索，查看样本特征和特征值，这个过程你也可以使用数据可视化，它可以方便我们对数据及数据之间的关系进一步加深了解。然后按照“完全合一”的准则来评估数据的质量，如果数据质量不高就需要做数据清洗。数据清洗之后，你可以做特征选择，方便后续的模型训练；在分类阶段，选择核函数进行训练，如果不知道数据是否为线性，可以考虑使用 SVC(kernel=‘rbf’) ，也就是高斯核函数的 SVM 分类器。然后对训练好的模型用测试集进行评估。按照上面的流程，我们来编写下代码，加载数据并对数据做部分的探索：123456789# 加载数据集，你需要把数据放到目录中data = pd.read_csv(\"./data.csv\")# 数据探索# 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来pd.set_option('display.max_columns', None)print(data.columns)print(data.head(5))print(data.describe())运行结果中，你能看到 32 个字段里，id 是没有实际含义的，可以去掉。diagnosis 字段的取值为 B 或者 M，我们可以用 0 和 1 来替代。另外其余的 30 个字段，其实可以分成三组字段，下划线后面的 mean、se 和 worst 代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。12345678910# 将特征字段分成3组features_mean= list(data.columns[2:12])features_se= list(data.columns[12:22])features_worst=list(data.columns[22:32])# 数据清洗# ID列没有用，删除该列data.drop(\"id\",axis=1,inplace=True)# 将B良性替换为0，M恶性替换为1data['diagnosis']=data['diagnosis'].map(&#123;'M':1,'B':0&#125;)然后我们要做特征字段的筛选，首先需要观察下 features_mean 各变量之间的关系，这里我们可以用 DataFrame 的 corr() 函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。12345678910# 将肿瘤诊断结果可视化sns.countplot(data['diagnosis'],label=\"Count\")plt.show()# 用热力图呈现features_mean字段之间的相关性corr = data[features_mean].corr()plt.figure(figsize=(14,14))# annot=True显示每个方格的数据sns.heatmap(corr, annot=True)plt.show()图表展示：热力图中对角线上的为单变量自身的相关系数是 1。颜色越浅代表相关性越大。所以你能看出来 radius_mean、perimeter_mean 和 area_mean 相关性非常大，compactness_mean、concavity_mean、concave_points_mean 这三个字段也是相关的，因此我们可以取其中的一个作为代表。那么如何进行特征选择呢？特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。我们能看到 mean、se 和 worst 这三组特征是对同一组内容的不同度量方式，我们可以保留 mean 这组特征，在特征选择中忽略掉 se 和 worst。同时我们能看到 mean 这组特征中，radius_mean、perimeter_mean、area_mean 这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean 这三个属性相关性大。我们分别从这 2 类中选择 1 个属性作为代表，比如 radius_mean 和 compactness_mean。这样我们就可以把原来的 10 个属性缩减为 6 个属性，代码如下：123# 特征选择features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']对特征进行选择之后，我们就可以准备训练集和测试集：12345678# 抽取30%的数据作为测试集，其余作为训练集train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test# 抽取特征选择的数值作为训练和测试数据train_X = train[features_remain]train_y=train['diagnosis']test_X= test[features_remain]test_y =test['diagnosis']在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：12345# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()train_X = ss.fit_transform(train_X)test_X = ss.transform(test_X)最后我们可以让 SVM 做训练和预测了：12345678# 创建SVM分类器model = svm.SVC()# 用训练集做训练model.fit(train_X,train_y)# 用测试集做预测prediction=model.predict(test_X)print('准确率: ', metrics.accuracy_score(prediction,test_y))运行结果：12准确率: 0.9181286549707602乳腺癌诊断分类的 SVM 实战，从这个过程中整个执行的流程，包括数据加载、数据探索、数据清洗、特征选择、SVM 训练和结果评估等环节。sklearn 已经为我们提供了很好的工具，对上节课中讲到的 SVM 的创建和训练都进行了封装，让我们无需关心中间的运算细节。但正因为这样，我们更需要对每个流程熟练掌握，通过实战项目训练数据化思维和对数据的敏感度。全部代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pandasimport matplotlib.pyplot as pltfrom sklearn import svm, metricsimport seaborn as snsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerdata = pandas.read_csv(\"data.csv\")pandas.set_option('display.max_columns', None)# 将特征字段分成3组features_mean= list(data.columns[2:12])features_se= list(data.columns[12:22])features_worst=list(data.columns[22:32])# 数据清洗# ID列没有用，删除该列data.drop(\"id\",axis=1,inplace=True)# 将B良性替换为0，M恶性替换为1data['diagnosis']=data['diagnosis'].map(&#123;'M':1,'B':0&#125;)# # 将肿瘤诊断结果可视化# sns.countplot(data['diagnosis'],label=\"Count\")# plt.show()# # 用热力图呈现features_mean字段之间的相关性# corr = data[features_mean].corr()# plt.figure(figsize=(14,14))# # annot=True显示每个方格的数据# sns.heatmap(corr, annot=True)# plt.show()# 特征选择features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean','symmetry_mean', 'fractal_dimension_mean']# 抽取30%的数据作为测试集，其余作为训练集train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test# 抽取特征选择的数值作为训练和测试数据train_X = train[features_remain]train_y=train['diagnosis']test_X= test[features_remain]test_y =test['diagnosis']# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()train_X = ss.fit_transform(train_X)test_X = ss.transform(test_X)# 创建SVM分类器model = svm.SVC()# 用训练集做训练model.fit(train_X,train_y)# 用测试集做预测prediction=model.predict(test_X)print('准确率: ', metrics.accuracy_score(prediction,test_y))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"}]},{"title":"数据分析-SVM_1","slug":"数据分析-SVM-1","date":"2018-07-24T14:27:13.000Z","updated":"2020-05-01T14:14:27.095Z","comments":true,"path":"2018/07/24/数据分析-SVM-1/","link":"","permalink":"cpeixin.cn/2018/07/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-1/","excerpt":"","text":"SVM 支持向量机SVM 的英文叫 Support Vector Machine，中文名为支持向量机。它是常见的一种分类方法，在机器学习中，SVM 是有监督的学习模型。什么是有监督的学习模型呢？它指的是我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。桌子上我放了红色和蓝色两种球，请你用一根棍子将这两种颜色的球分开。你可以很快想到解决方案，在红色和蓝色球之间画条直线就好了，如下图所示：这次难度升级，桌子上依然放着红色、蓝色两种球，但是它们的摆放不规律，如下图所示。如何用一根棍子把这两种颜色分开呢？你可能想了想，认为一根棍子是分不开的。除非把棍子弯曲，像下面这样：所以这里直线变成了曲线。如果在同一个平面上来看，红蓝两种颜色的球是很难分开的。那么有没有一种方式，可以让它们自然地分开呢？这里你可能会灵机一动，猛拍一下桌子，这些小球瞬间腾空而起，如下图所示。在腾起的那一刹那，出现了一个水平切面，恰好把红、蓝两种颜色的球分开。在这里，二维平面变成了三维空间。原来的曲线变成了一个平面。这个平面，我们就叫做超平面。SVM 的工作原理用SVM 计算的过程就是帮我们找到那个超平面的过程，这个超平面就是我们的 SVM 分类器。我们再过头来看最简单的练习 1，其实我们可以有多种直线的划分，比如下图所示的直线 A、直线 B 和直线 C，究竟哪种才是更好的划分呢？很明显图中的直线 B 更靠近蓝色球，但是在真实环境下，球再多一些的话，蓝色球可能就被划分到了直线 B 的右侧，被认为是红色球。同样直线 A 更靠近红色球，在真实环境下，如果红色球再多一些，也可能会被误认为是蓝色球。所以相比于直线 A 和直线 B，直线 C 的划分更优，因为它的鲁棒性更强。那怎样才能寻找到直线 C 这个更优的答案呢？这里，我们引入一个 SVM 特有的概念：分类间隔。实际上，我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：如图中的决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。点到超平面的距离公式**在上面这个例子中，如果我们把红蓝两种颜色的球放到一个三维空间里，你发现决策面就变成了一个平面。这里我们可以用线性函数来表示，如果在一维空间里就表示一个点，在二维空间里表示一条直线，在三维空间中代表一个平面，当然空间维数还可以更多，这样我们给这个线性函数起个名称叫做“超平面”。超平面的数学表达可以写成：在这个公式里，w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。在这个过程中，支持向量就是离分类超平面最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。所以说， SVM 就是求解最大分类间隔的过程，我们还需要对分类间隔的大小进行定义。首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用 di 代表点 xi 到超平面 wxi+b=0 的欧氏距离。因此我们要求 di 的最小值，用它来代表这个样本到超平面的最短距离。di 可以用公式计算得出：其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导最大间隔的优化模型我们的目标就是找出所有分类间隔中最大的那个值对应的超平面。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开）。通过凸优化问题，最后可以求出最优的 w 和 b，也就是我们想要找的最优超平面。中间求解的过程会用到拉格朗日乘子，和 KKT（Karush-Kuhn-Tucker）条件。数学公式比较多，这里不进行展开。硬间隔、软间隔和非线性 SVM假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。我们知道，实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点。所以线性可分是个理想情况。这时，我们需要使用到软间隔 SVM（近似线性可分），比如下面这种情况：另外还存在一种情况，就是非线性支持向量机。比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状。那么这种情况下，不论是多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念：核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。用 SVM 如何解决多分类问题SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。一对多法假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：（1）样本 A 作为正集，B，C，D 作为负集；（2）样本 B 作为正集，A，C，D 作为负集；（3）样本 C 作为正集，A，B，D 作为负集；（4）样本 D 作为正集，A，B，C 作为负集。这种方法，针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。2. 一对一法一对一法的初衷是想在训练的时候更加灵活。我们可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：（1）分类器 1：A、B；（2）分类器 2：A、C；（3）分类器 3：B、C。当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。总结SVM 分类器，它在文本分类尤其是针对二分类任务性能卓越。同样，针对多分类的情况，我们可以采用一对多，或者一对一的方法，多个二值分类器组合成一个多分类器。另外关于 SVM 分类器的概念，我希望你能掌握以下的三个程度：完全线性可分情况下的线性分类器，也就是线性可分的情况，是最原始的 SVM，它最核心的思想就是找到最大的分类间隔；大部分线性可分情况下的线性分类器，引入了软间隔的概念。软间隔，就是允许一定量的样本分类错误；线性不可分情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到了一个高维的空间中，从而变得线性可分。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"}]},{"title":"数据分析-朴素贝叶斯（下）","slug":"数据分析-朴素贝叶斯（下）","date":"2018-07-21T02:34:38.000Z","updated":"2020-05-01T14:18:06.830Z","comments":true,"path":"2018/07/21/数据分析-朴素贝叶斯（下）/","link":"","permalink":"cpeixin.cn/2018/07/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。今天我带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是 sklearn 这个机器学习神器。sklearn机器学习包sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。TF-IDF 值我在多项式朴素贝叶斯中提到了“词的 TF-IDF 值”，如何理解这个概念呢？TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。词频 TF 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。TF-IDF 如何计算首先我们看下词频 TF 和逆向文档概率 IDF 的公式。为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。TF-IDF=TF*IDF你可以看到，TF-IDF 值就是 TF 与 IDF 的乘积, 这样可以更准确地对文档进行分类。比如“我”这样的高频单词，虽然 TF 词频高，但是 IDF 值很低，整体的 TF-IDF 也不高。我在这里举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。针对“this”，计算 TF-IDF 值：所以 TF-IDF=0.02(-0.0414)=-8.28e-4。针对“bayes”，计算 TF-IDF 值：TF-IDF=0.0050.5229=2.61e-3。很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。如何求 TF-IDF在 sklearn 中我们直接使用 TfidfVectorizer 类，它可以帮我们计算单词 TF-IDF 向量的值。在这个类中，取 sklearn 计算的对数 log 时，底数是 e，不是 10。下面我来讲下如何创建 TfidfVectorizer 类。创建 TfidfVectorizer 的方法是：1TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)我们在创建的时候，有两个构造参数，可以自定义停用词 stop_words 和规律规则 token_pattern。需要注意的是传递的数据结构，停用词 stop_words 是一个列表 List 类型，而过滤规则 token_pattern 是正则表达式。什么是停用词？停用词就是在分类中没有用的词，这些词一般词频 TF 高，但是 IDF 很低，起不到分类的作用。为了节省空间和计算时间，我们把这些词作为停用词 stop words，告诉机器这些词不需要帮我计算。当我们创建好 TF-IDF 向量类型时，可以用 fit_transform 帮我们计算，返回给我们文本矩阵，该矩阵表示了每个单词在每个文档中的 TF-IDF 值。在我们进行 fit_transform 拟合模型后，我们可以得到更多的 TF-IDF 向量属性，比如，我们可以得到词汇的对应关系（字典类型）和向量的 IDF 值，当然也可以获取设置的停用词 stop_words。如何对文档进行分类如果我们要对文档进行分类，有两个重要的阶段：基于分词的数据准备，包括分词、单词权重计算、去掉停用词；应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。整个流程将分成下面几个模块：模块 1：对文档进行分词在准备阶段里，最重要的就是分词。那么如果给文档进行分词呢？英文文档和中文文档所使用的分词工具不同。在英文文档中，最常用的是 NTLK 包。NTLK 包中包含了英文的停用词 stop words、分词和标注方法。1234import nltkword_list = nltk.word_tokenize(text) #分词nltk.pos_tag(word_list) #标注单词的词性在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。123import jiebaword_list = jieba.cut (text) #中文分词模块 2：加载停用词表我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。1stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]模块 3：计算单词的权重这里我们用到 sklearn 里的 TfidfVectorizer 类，上面我们介绍过它使用的方法。直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。123tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)features = tf.fit_transform(train_contents)这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。一般很少设置 min_df，因为 min_df 通常都会很小。模块 4：生成朴素贝叶斯分类器我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。当 0&lt; alpha &lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。1234# 多项式贝叶斯分类器from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)模块 5：使用生成的分类器做预测首先我们需要得到测试集的特征矩阵。方法是用训练集的分词创建一个 TfidfVectorizer 类，使用同样的 stop_words 和 max_df，然后用这个 TfidfVectorizer 类对测试集的内容进行 fit_transform 拟合，得到测试集的特征矩阵 test_features。123test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)test_features=test_tf.fit_transform(test_contents)然后我们用训练好的分类器对新数据做预测。方法是使用 predict 函数，传入测试集的特征矩阵 test_features，得到分类结果 predicted_labels。predict 函数做的工作就是求解所有后验概率并找出最大的那个。12predicted_labels=clf.predict(test_features)模块 6：计算准确率计算准确率实际上是对分类模型的评估。我们可以调用 sklearn 中的 metrics 包，在 metrics 中提供了 accuracy_score 函数，方便我们对实际结果和预测的结果做对比，给出模型的准确率。使用方法如下：123from sklearn import metricsprint metrics.accuracy_score(test_labels, predicted_labels)数据挖掘神器 sklearn从数据挖掘的流程来看，一般包括了获取数据、数据清洗、模型训练、模型评估和模型部署这几个过程。sklearn 中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。在文档分类的项目中，我们针对文档的特点，给出了基于分词的准备流程。一般来说 NTLK 包适用于英文文档，而 jieba 适用于中文文档。我们可以根据文档选择不同的包，对文档提取分词。这些分词就是贝叶斯分类中最重要的特征属性。基于这些分词，我们得到分词的权重，即特征矩阵。通过特征矩阵与分类结果，我们就可以创建出朴素贝叶斯分类器，然后用分类器进行预测，最后预测结果与实际结果做对比即可以得到分类器在测试集上的准确率。实战文档共有 4 种类型：女性、体育、文学、校园根据下面给定的训练数据和测试数据进行 朴素贝叶斯文本分类实战。数据地址：https://github.com/cystanford/text_classification/tree/master/text classification12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding: utf-8# 中文文本分类import osimport jiebaimport warningsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn import metricswarnings.filterwarnings('ignore')def cut_words(file_path): \"\"\" 对文本进行切词 :param file_path: txt文本路径 :return: 用空格分词的字符串 \"\"\" text_with_spaces = '' with open(file_path, 'r', encoding='gb18030', errors='ignore') as f: text = f.read() textcut = jieba.cut(text) for word in textcut: text_with_spaces += word + ' ' return text_with_spacestext_category = ['女性', '体育', '文学', '校园']def loadfile(data_path): \"\"\" 将路径下的所有文件加载 :param file_dir: 保存txt文件目录 :param label: 文档标签 :return: 分词后的文档列表和标签 \"\"\" words_list = [] labels = [] for category in text_category: file_path = data_path + category + '/' file_list = os.listdir(file_path) for file in file_list: words_list.append(cut_words(file_path+file)) labels.append(category) return words_list, labelsdef get_stop_words(stop_words_path): stop_words = open(stop_words_path, 'r', encoding='utf-8').read() stop_words = stop_words.encode('utf-8').decode('utf-8-sig') # 列表头部\\ufeff处理 stop_words = stop_words.split('\\n') # 根据分隔符分隔 return stop_wordsif __name__ == '__main__': train_data_path = 'train' + '/' test_data_path = 'test' + '/' stop_words_path = 'stop/stopword.txt' train_words_list, train_labels = loadfile(train_data_path) test_words_list, test_labels = loadfile(test_data_path) stop_words = get_stop_words(stop_words_path) tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5) train_features = tf.fit_transform(train_words_list) # 上面fit过了，这里transform test_features = tf.transform(test_words_list) # 多项式贝叶斯分类器 from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels) predicted_labels = clf.predict(test_features) # 计算准确率 print('准确率为：', metrics.accuracy_score(test_labels, predicted_labels))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"}]},{"title":"数据分析-朴素贝叶斯（上）","slug":"数据分析-朴素贝叶斯（上）","date":"2018-07-20T15:19:16.000Z","updated":"2020-05-01T14:18:19.882Z","comments":true,"path":"2018/07/20/数据分析-朴素贝叶斯（上）/","link":"","permalink":"cpeixin.cn/2018/07/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"贝叶斯原理贝叶斯原理是怎么来的呢？贝叶斯为了解决一个叫“逆向概率”问题写了一篇文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。什么是“逆向概率”呢？所谓“逆向概率”是相对“正向概率”而言。正向概率的问题很容易理解，比如我们已经知道袋子里面有 N 个球，不是黑球就是白球，其中 M 个是黑球，那么把手伸进去摸一个球，就能知道摸出黑球的概率是多少。但这种情况往往是上帝视角，即了解了事情的全貌再做判断。在现实生活中，我们很难知道事情的全貌。贝叶斯则从实际场景出发，提了一个问题：如果我们事先不知道袋子里面黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里面黑白球的比例么？正是这样的一个问题，影响了接下来近 200 年的统计学理论。这是因为，贝叶斯原理与其他统计学推断方法截然不同，它是建立在主观判断的基础上：在我们不了解所有客观事实的情况下，同样可以先估计一个值，然后根据实际结果不断进行修正。我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那是不是实际上患“贝叶死”的概率也是 99.9% 呢？实际上不是的。你自己想想，在 10000 个人中，还存在 0.1% 的误查的情况，也就是 10 个人没有患病但是被诊断成阳性。当然 10000 个人中，也确实存在一个患有贝叶死的人，他有 99.9% 的概率被检查出来。所以你可以粗算下，患病的这个人实际上是这 11 个人里面的一员，即实际患病比例是 1/11≈9%。上面这个例子中，实际上涉及到了贝叶斯原理中的几个概念：先验概率：通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。后验概率：后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。条件概率：事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。似然函数（likelihood function）：你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。介绍完贝叶斯原理中的这几个概念，我们再来看下贝叶斯原理，实际上贝叶斯原理就是求解后验概率，我们假设：A 表示事件 “测出为阳性”, 用 B1 表示“患有贝叶死”, B2 表示“没有患贝叶死”。根据上面那道题，我们可以得到下面的信息。患有贝叶死的情况下，测出为阳性的概率为 P(A|B1)=99.9%，没有患贝叶死，但测出为阳性的概率为 P(A|B2)=0.1%。另外患有贝叶死的概率为 P(B1)=0.01%，没有患贝叶死的概率 P(B2)=99.99%。那么我们检测出来为阳性，而且是贝叶死的概率 P(B1，A）=P(B1)_P(A|B1)=0.01%_99.9%=0.00999%。这里 P(B1,A) 代表的是联合概率，同样我们可以求得 P(B2,A)=P(B2)_P(A|B2)=99.99%_0.1%=0.09999%。然后我们想求得是检查为阳性的情况下，患有贝叶死的概率，也即是 P(B1|A)。所以检查出阳性，且患有贝叶死的概率为：检查出是阳性，但没有患有贝叶死的概率为：这里我们能看出来 0.01%+0.1% 均出现在了 P(B1|A) 和 P(B2|A) 的计算中作为分母。我们把它称之为论据因子，也相当于一个权值因子。其中 P(B1）、P(B2) 就是先验概率，我们现在知道了观测值，就是被检测出来是阳性，来求患贝叶死的概率，也就是求后验概率。求后验概率就是贝叶斯原理要求的，基于刚才求得的 P(B1|A)，P(B2|A)，我们可以总结出贝叶斯公式为：由此，我们可以得出通用的贝叶斯公式：朴素贝叶斯讲完贝叶斯原理之后，我们再来看下今天重点要讲的算法，朴素贝叶斯。它是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。朴素贝叶斯模型由两种类型的概率组成：每个类别的概率P(Cj)；每个属性的条件概率P(Ai|Cj)。我来举个例子说明下什么是类别概率和条件概率。假设我有 7 个棋子，其中 3 个是白色的，4 个是黑色的。那么棋子是白色的概率就是 3/7，黑色的概率就是 4/7，这个就是类别概率。假设我把这 7 个棋子放到了两个盒子里，其中盒子 A 里面有 2 个白棋，2 个黑棋；盒子 B 里面有 1 个白棋，2 个黑棋。那么在盒子 A 中抓到白棋的概率就是 1/2，抓到黑棋的概率也是 1/2，这个就是条件概率，也就是在某个条件（比如在盒子 A 中）下的概率。在朴素贝叶斯中，我们要统计的是属性的条件概率，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。另外我想告诉你的是，贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。朴素贝叶斯分类工作原理朴素贝叶斯分类是常用的贝叶斯分类方法。我们日常生活中看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。离散数据案例我们遇到的数据可以分为两种，一种是离散数据，另一种是连续数据。那什么是离散数据呢？离散就是不连续的意思，有明确的边界，比如整数 1，2，3 就是离散数据，而 1 到 3 之间的任何数，就是连续数据，它可以取在这个区间里的任何数值。我以下面的数据为例，这些是根据你之前的经验所获得的数据。然后给你一个新的数据：身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？针对这个问题，我们先确定一共有 3 个属性，假设我们用 A 代表属性，用 A1, A2, A3 分别为身高 = 高、体重 = 中、鞋码 = 中。一共有两个类别，假设用 C 代表类别，那么 C1,C2 分别是：男、女，在未知的情况下我们用 Cj 表示。那么我们想求在 A1、A2、A3 属性下，Cj 的概率，用条件概率表示就是 P(Cj|A1A2A3)。根据上面讲的贝叶斯的公式，我们可以得出：因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。在这个公式里，因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。我们假定 Ai 之间是相互独立的，那么：P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出 P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。我分别求下这些条件下的概率：P(A1|C1)=1/2, P(A2|C1)=1/2, P(A3|C1)=1/4，P(A1|C2)=0, P(A2|C2)=1/2, P(A3|C2)=1/2，所以 P(A1A2A3|C1)=1/16, P(A1A2A3|C2)=0。因为 P(A1A2A3|C1)P(C1)&gt;P(A1A2A3|C2)P(C2)，所以应该是 C1 类别，即男性。连续数据案例我们做了一个离散的数据案例，实际生活中我们得到的是连续的数值，比如下面这组数据：那么如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？公式还是上面的公式，这里的困难在于，由于身高、体重、鞋码都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办呢？这时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。比如，男性的身高是均值 179.5、标准差为 3.697 的正态分布。所以男性的身高为 180 的概率为 0.1069。怎么计算得出的呢? 你可以使用 EXCEL 的 NORMDIST(x,mean,standard_dev,cumulative) 函数，一共有 4 个参数：x：正态分布中，需要计算的数值；Mean：正态分布的平均值；Standard_dev：正态分布的标准差；Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE 时，函数结果为累积分布；为 False 时，函数结果为概率密度。这里我们使用的是 NORMDIST(180,179.5,3.697,0)=0.1069。同理我们可以计算得出男性体重为 120 的概率为 0.000382324，男性鞋码为 41 号的概率为 0.120304111。所以我们可以计算得出：P(A1A2A3|C1)=P(A1|C1)P(A2|C1)P(A3|C1)=0.10690.0003823240.120304111=4.9169e-6同理我们也可以计算出来该人为女的可能性：P(A1A2A3|C2)=P(A1|C2)P(A2|C2)P(A3|C2)=0.000001474890.0153541440.120306074=2.7244e-9很明显这组数据分类为男的概率大于分类为女的概率。当然在 Python 中，有第三方库可以直接帮我们进行上面的操作，这个我们会在下节课中介绍。这里主要是给你讲解下具体的运算原理。朴素贝叶斯分类器工作流程朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。流程可以用下图表示：从图片你也可以看出来，朴素贝叶斯分类器需要三个流程，我来给你一一讲解下这几个流程。第一阶段：准备阶段在这个阶段我们需要确定特征属性，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。第二阶段：训练阶段这个阶段就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。输入是特征属性和训练样本，输出是分类器。第三阶段：应用阶段这个阶段是使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"}]},{"title":"数据分析 - EM聚类","slug":"数据分析-EM聚类","date":"2018-07-15T14:18:39.000Z","updated":"2020-05-01T14:15:10.136Z","comments":true,"path":"2018/07/15/数据分析-EM聚类/","link":"","permalink":"cpeixin.cn/2018/07/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-EM%E8%81%9A%E7%B1%BB/","excerpt":"","text":"今天要来学习 EM 聚类。EM 的英文是 Expectation Maximization，所以 EM 算法也叫最大期望算法。我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子 A 中，然后再把剩余的分到碟子 B 中，再来观察碟子 A 和 B 里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子 A 和 B 里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是 EM 算法的过程。EM 算法的工作原理说到 EM 算法，我们先来看一个概念“最大似然”，英文是 Maximum Likelihood，Likelihood 代表可能性，所以最大似然也就是最大可能性的意思。什么是最大似然呢？举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是最大似然的概念。最大似然估计是什么呢？它指的就是一件事情已经发生了，然后反推更有可能是什么因素造成的。还是用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性。最大似然估计是一种通过已知结果，估计参数的方法。那么 EM 算法是什么？它和最大似然估计又有什么关系呢？EM 算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。再回过来看下开头我给你举的分菜的这个例子，实际上最终我们想要的是碟子 A 和碟子 B 中菜的份量，你可以把它们理解为想要求得的模型参数。然后我们通过 EM 算法中的 E 步来进行观察，然后通过 M 步来进行调整 A 和 B 的参数，最后让碟子 A 和碟子 B 的参数不再发生变化为止。实际我们遇到的问题，比分菜复杂。我再给你举个一个投掷硬币的例子，假设我们有 A 和 B 两枚硬币，我们做了 5 组实验，每组实验投掷 10 次，然后统计出现正面的次数，实验结果如下：投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是 A 还是 B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：我们现在想要求得硬币 A 和 B 出现正面次数的概率，可以直接求得：而实际情况是我不知道每次投掷的硬币是 A 还是 B，那么如何求得硬币 A 和硬币 B 出现正面的概率呢？这里就需要采用 EM 算法的思想。1. 初始化参数。我们假设硬币 A 和 B 的正面概率（随机指定）是θA=0.5 和θB=0.9。2. 计算期望值。假设实验 1 投掷的是硬币 A，那么正面次数为 5 的概率为：所以实验 1 更有可能投掷的是硬币 A。然后我们对实验 2~5 重复上面的计算过程，可以推理出来硬币顺序应该是{A，A，B，B，A}。这个过程实际上是通过假设的参数来估计未知参数，即“每次投掷是哪枚硬币”。3. 通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。然后一直重复第二步和第三步，直到参数不再发生变化。简单总结下上面的步骤，你能看出 EM 算法中的 E 步骤就是通过旧的参数来计算隐藏变量。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。EM 聚类的工作原理上面你能看到 EM 算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是 EM 聚类的原理。相比于 K-Means 算法，EM 聚类更加灵活，比如下面这两种情况，K-Means 会得到下面的聚类结果。因为 K-Means 是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。你可以把 EM 算法理解成为是一个框架，在这个框架中可以采用不同的模型来用 EM 进行求解。常用的 EM 聚类有 GMM 高斯混合模型和 HMM 隐马尔科夫模型。GMM（高斯混合模型）聚类就是 EM 聚类的一种。比如上面这两个图，可以采用 GMM 来进行聚类。和 K-Means 一样，我们事先知道聚类的个数，但是不知道每个样本分别属于哪一类。通常，我们可以假设样本是符合高斯分布的（也就是正态分布）。每个高斯分布都属于这个模型的组成部分（component），要分成 K 类就相当于是 K 个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是 E 步骤。再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即 M 步骤。反复 EM 步骤，直到每个组成部分的高斯分布参数不变为止。这样也就相当于将样本按照 GMM 模型进行了 EM 聚类。总结EM 算法相当于一个框架，你可以采用不同的模型来进行聚类，比如 GMM（高斯混合模型），或者 HMM（隐马尔科夫模型）来进行聚类。GMM 是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而 HMM 用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM 在自然语言处理和语音识别领域中有广泛的应用。在 EM 这个框架中，E 步骤相当于是通过初始化的参数来估计隐含变量。M 步骤就是通过隐含变量反推来优化参数。最后通过 EM 步骤的迭代得到模型参数。在这个过程里用到的一些数学公式这节课不进行展开。你需要重点理解 EM 算法的原理。通过上面举的炒菜的例子，你可以知道 EM 算法是一个不断观察和调整的过程。通过求硬币正面概率的例子，你可以理解如何通过初始化参数来求隐含数据的过程，以及再通过求得的隐含数据来优化参数。通过上面 GMM 图像聚类的例子，你可以知道很多 K-Means 解决不了的问题，EM 聚类是可以解决的。在 EM 框架中，我们将潜在类别当做隐藏变量，样本看做观察值，把聚类问题转化为参数估计问题，最终把样本进行聚类。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"}]},{"title":"数据分析-决策树实战-泰坦尼克号乘客生存预测","slug":"数据分析-决策树实战-泰坦尼克号乘客生存预测","date":"2018-07-15T14:12:49.000Z","updated":"2020-05-01T14:18:39.272Z","comments":true,"path":"2018/07/15/数据分析-决策树实战-泰坦尼克号乘客生存预测/","link":"","permalink":"cpeixin.cn/2018/07/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/","excerpt":"","text":"在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。今天我来带你用决策树进行项目的实战。决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。在了解决策树的原理后，今天我们用 sklearn 工具解决一个实际的问题：泰坦尼克号乘客的生存预测。sklearn 中的决策树模型首先，我们需要掌握 sklearn 中自带的决策树分类器 DecisionTreeClassifier，方法如下：1clf = DecisionTreeClassifier(criterion='entropy')到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 ginientropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。我们通过设置 criterion=’entropy’可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在 sklearn 中是个什么东西？123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。我整理了这些参数代表的含义：![1585397401268-c9ea5ebf-53aa-44e0-8588-13582a240b5d.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586919851176-2a57ac36-49da-475d-adc2-dc6f3f2434b3.png#align=left&display=inline&height=930&margin=%5Bobject%20Object%5D&name=1585397401268-c9ea5ebf-53aa-44e0-8588-13582a240b5d.png&originHeight=930&originWidth=620&size=366558&status=done&style=none&width=620)在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。下面这个表格是 fit 方法、predict 方法和 score 方法的作用:![1585397402135-ed1fff6c-1bee-49f9-bf07-9fb87d83e0f3.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586919851152-b2b5be44-9293-4e4d-998f-d1acd73a33da.png#align=left&display=inline&height=158&margin=%5Bobject%20Object%5D&name=1585397402135-ed1fff6c-1bee-49f9-bf07-9fb87d83e0f3.png&originHeight=158&originWidth=468&size=27331&status=done&style=none&width=468) ### Titanic 乘客生存预测 #### 问题描述 泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据，具体数据你可以从 GitHub 上下载：[点我](https://github.com/cystanford/Titanic_Data)其中数据集格式为 csv，一共有两个文件：train.csv 是训练数据集，包含特征信息和存活与否的标签；test.csv: 测试数据集，只包含特征信息。现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。在训练集中，包括了以下字段，它们具体为：![1585397401155-e052cf89-fe96-49b3-b3fe-37539d7a67ae.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586919895980-6bd3d51e-cf8d-4a92-a444-f540fcba423a.png#align=left&display=inline&height=370&margin=%5Bobject%20Object%5D&name=1585397401155-e052cf89-fe96-49b3-b3fe-37539d7a67ae.png&originHeight=370&originWidth=466&size=41457&status=done&style=none&width=466) #### 生存预测的关键流程 我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：![](https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397402088-8ce15112-7300-4b68-b34c-0bc6a0c960c0.png#align=left&display=inline&height=1470&margin=%5Bobject%20Object%5D&originHeight=1470&originWidth=3202&size=0&status=done&style=none&width=3202)**准备阶段**：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；**分类阶段**：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。下面，我分别对这些模块进行介绍。**模块 1**：数据探索数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。那么如何进行数据探索呢？这里有一些函数你需要了解：使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；使用 head 查看前几行数据（默认是前 5 行）；使用 tail 查看后几行数据（默认是最后 5 行）。我们可以使用 Pandas 便捷地处理这些问题：1234567891011121314151617181920import pandas as pdtrain_data = pd.read_csv('train.csv')print(train_data.info())print(\"===========================================\")print(train_data.describe())print(\"===========================================\")print(train_data.describe(include=['O']))print(\"===========================================\")print(train_data.head())print(\"===========================================\")print(train_data.tail())运行结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KBNone=========================================== PassengerId Survived Pclass ... SibSp Parch Farecount 891.000000 891.000000 891.000000 ... 891.000000 891.000000 891.000000mean 446.000000 0.383838 2.308642 ... 0.523008 0.381594 32.204208std 257.353842 0.486592 0.836071 ... 1.102743 0.806057 49.693429min 1.000000 0.000000 1.000000 ... 0.000000 0.000000 0.00000025% 223.500000 0.000000 2.000000 ... 0.000000 0.000000 7.91040050% 446.000000 0.000000 3.000000 ... 0.000000 0.000000 14.45420075% 668.500000 1.000000 3.000000 ... 1.000000 0.000000 31.000000max 891.000000 1.000000 3.000000 ... 8.000000 6.000000 512.329200[8 rows x 7 columns]=========================================== Name Sex Ticket Cabin Embarkedcount 891 891 891 204 889unique 891 2 681 147 3top Wick, Miss. Mary Natalie male 347082 B96 B98 Sfreq 1 577 7 4 644=========================================== PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]=========================================== PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]模块 2：数据清洗通过数据探索，我们发现 Age 和 Cabin 这三个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐；具体实现的代码如下：12train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。首先观察下 Embarked 字段的取值，方法如下：首先观察下 Embarked 字段的取值，方法如下：1print(train_data['Embarked'].value_counts())结果如下：123S 644C 168Q 77我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：1train_data['Embarked'].fillna('S', inplace=True)模块 3：特征选择特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？通过数据探索我们发现，PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；Cabin 字段缺失值太多，可以放弃； Ticket 字段为船票号码，杂乱无章且无规律，可以放弃。其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。12345# 特征选择features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']train_features = train_data[features]train_labels = train_data['Survived']test_features = test_data[features]特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。那该如何操作呢?我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下：123from sklearn.feature_extraction import DictVectorizerdvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record'))你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：12print(dvec.feature_names_)['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']你可以看到原本是一列的 Embarked，变成了“Embarked=C”“Embarked=Q”“Embarked=S”三列。Sex 列变成了“Sex=female”“Sex=male”两列。这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。模块 4：决策树模型刚才我们已经讲了如何使用 sklearn 中的决策树模型。现在我们使用 ID3 算法，即在创建 DecisionTreeClassifier 时，设置 criterion=‘entropy’，然后使用 fit 进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。12345from sklearn.tree import DecisionTreeClassifier# 构造ID3决策树clf = DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels)模块 5：模型预测 &amp; 评估在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labels：123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels = clf.predict(test_features)在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的 score 函数计算下得到的结果：123# 得到决策树准确率acc_decision_tree = round(clf.score(train_features, train_labels), 6)print(u'score准确率为 %.4lf' % acc_decision_tree)运行结果：1score准确率为 0.9820你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。这是为什么呢？因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比(test.csv数据中没有Survived)。如果我们使用 score 函数对训练集的准确率进行统计，正确率会接近于 100%（如上结果为 98.2%），无法对分类器的在实际环境下做准确率的评估。那么有什么办法，来统计决策树分类器的准确率呢？这里可以使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。K 折交叉验证的原理是这样的：将数据集平均分割成 K 个等份；使用 1 份数据作为测试数据，其余作为训练数据；计算测试准确率；使用不同的测试集，重复 2、3 步骤。在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score 和 cross_val_score 两种函数的正确率的评估结果：1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证 统计决策树准确率print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))1cross_val_score准确率为 0.7835你可以看到，score 函数的准确率为 0.9820，cross_val_score 准确率为 0.7835。这里很明显，对于不知道测试集实际结果的，要使用 K 折交叉验证才能知道模型的准确率。模块 6：决策树可视化sklearn 的决策树模型对我们来说，还是比较抽象的。我们可以使用 Graphviz 可视化工具帮我们把决策树呈现出来。安装 Graphviz 库需要下面的几步：安装 graphviz 工具，这里是它的下载地址；http://www.graphviz.org/download/将 Graphviz 添加到环境变量 PATH 中；需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。这样你就可以在程序里面使用 Graphviz 对决策树模型进行呈现，最后得到一个决策树可视化的 PDF 文件，可视化结果文件 Source.gv.pdf 你可以在 GitHub 上下载：https://github.com/cystanford/Titanic_Data决策树模型使用技巧总结今天我用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。我上面讲了泰坦尼克乘客生存预测的六个关键模块，请你用 sklearn 中的决策树模型独立完成这个项目，对测试集中的乘客是否生存进行预测，并给出模型准确率评估。数据从 GitHub 上下载即可。最后给你留一个思考题吧，我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，那么这两个函数有什么区别?项目完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import pandas as pdfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.feature_extraction import DictVectorizertrain_data = pd.read_csv('train.csv')test_data = pd.read_csv('test.csv')train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\"\"\"将空值用此列最多值来补齐index 获取索引值。 [num] 下标获取返回值 ，ascending=True 降序排列\"\"\"train_data['Embarked'].fillna(train_data['Embarked'].value_counts().index[0], inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().index[0], inplace=True)features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']train_features = train_data[features]train_labels = train_data['Survived']test_features = test_data[features]dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record'))# 构造ID3决策树clf = DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels)test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测# pred_labels = clf.predict(test_features)# # 得到决策树准确率# acc_decision_tree = round(clf.score(train_features, train_labels), 6)# print(u'score准确率为 %.4lf' % acc_decision_tree)import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证 统计决策树准确率print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树项目-用户流失预警","slug":"数据分析-决策树项目-用户流失预警","date":"2018-07-14T11:19:49.000Z","updated":"2020-05-01T14:18:27.501Z","comments":true,"path":"2018/07/14/数据分析-决策树项目-用户流失预警/","link":"","permalink":"cpeixin.cn/2018/07/14/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E9%A1%B9%E7%9B%AE-%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E8%AD%A6/","excerpt":"","text":"https://www.zhihu.com/question/340500005https://zhuanlan.zhihu.com/p/26332219https://zhuanlan.zhihu.com/p/45435103音乐网站用户流失 ** https://zhuanlan.zhihu.com/p/29598241","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-CART","slug":"数据分析-决策树分类算法-CART","date":"2018-07-13T15:12:49.000Z","updated":"2020-05-01T14:18:55.201Z","comments":true,"path":"2018/07/13/数据分析-决策树分类算法-CART/","link":"","permalink":"cpeixin.cn/2018/07/13/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-CART/","excerpt":"","text":"CART算法上节课我们讲了决策树，基于信息度量的不同方式，我们可以把决策树分为 ID3 算法、C4.5 算法和 CART 算法。今天我来带你学习 CART 算法。CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。那么你首先需要了解的是，什么是分类树，什么是回归树呢？我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。不管是分类，还是回归，其本质是一样的，都是对输入做出预测，并且都是监督学习。说白了，就是根据特征，分析输入的内容，判断它的类别，或者预测其值。分类问题输出的是物体所属的类别，回归问题输出的是物体的值分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。CART 分类树的工作流程通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。在属性选择上，我们是通过统计“不纯度”来做判断的，ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。我们接下来详解了解一下基尼系数。基尼系数不好懂，你最好跟着例子一起手动计算下。假设 t 为节点，那么该节点的 GINI 系数的计算公式为：这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：集合 1：6 个都去打篮球；集合 2：3 个去打篮球，3 个不去打篮球。针对集合 1，所有人都去打篮球，所以 p(Ck|t)=1，因此 GINI(t)=1-1=0。针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5_0.5+0.5_0.5）=0.5。通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到：所以在属性 A 的划分下，节点 D 的基尼系数为：节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。如何使用 CART 算法来创建分类树通过上面的讲解你可以知道，CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。下面，我们来用 CART 分类树，给 iris 数据集构造一棵分类决策树。iris 这个数据集，我在 Python 可视化中讲到过，实际上在 sklearn 中也自带了这个数据集。基于 iris 数据集，构造 CART 分类树的代码如下：1234567891011121314151617181920212223# encoding=utf-8from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_iris# 准备数据集iris=load_iris()# 获取特征集和分类标识features = iris.datalabels = iris.target# 随机抽取33%的数据作为测试集，其余为训练集train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)# 创建CART分类树clf = DecisionTreeClassifier(criterion='gini')# 拟合构造CART分类树clf = clf.fit(train_features, train_labels)# 用CART分类树做预测test_predict = clf.predict(test_features)# 预测结果与测试集结果作比对score = accuracy_score(test_labels, test_predict)print(\"CART分类树准确率 %.4lf\" % score)CART分类树准确率 0.9600如果我们把决策树画出来，可以得到下面的图示：首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。使用 clf = DecisionTreeClassifier(criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。使用 clf.fit(train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。使用 clf.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。最后使用 accuracy_score(test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。我们能看到 sklearn 帮我们做了 CART 分类树的使用封装，使用起来还是很方便的。CART 回归树的工作流程CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。其中差值的绝对值为样本值减去样本均值的绝对值：方差为每个样本值减去样本均值的平方和除以样本个数：所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。我们可以通过一个例子来看下如何创建一棵 CART 回归树来做预测。如何使用 CART 回归树做预测这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。根据这些指标，我们使用 CART 回归树对波士顿房价进行预测，代码如下：123456789101112131415161718192021222324# encoding=utf-8from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor# 准备数据集boston=load_boston()# 探索数据print(boston.feature_names)# 获取特征集和房价features = boston.dataprices = boston.target# 随机抽取33%的数据作为测试集，其余为训练集train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)# 创建CART回归树dtr=DecisionTreeRegressor()# 拟合构造CART回归树dtr.fit(train_features, train_price)# 预测测试集中的房价predict_price = dtr.predict(test_features)# 测试集的结果评价print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price))运行结果（每次运行结果可能会有不同）：[‘CRIM’ ‘ZN’ ‘INDUS’ ‘CHAS’ ‘NOX’ ‘RM’ ‘AGE’ ‘DIS’ ‘RAD’ ‘TAX’ ‘PTRATIO’ ‘B’ ‘LSTAT’]回归树二乘偏差均值: 23.80784431137724回归树绝对值偏差均值: 3.040119760479042如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）：我们来看下这个例子，首先加载了波士顿房价数据集，得到特征集和房价。然后通过 train_test_split 帮助我们把数据集抽取一部分作为测试集，其余作为训练集。使用 dtr=DecisionTreeRegressor() 初始化一棵 CART 回归树。使用 dtr.fit(train_features, train_price) 函数，将训练集的特征值和结果作为参数进行拟合，得到 CART 回归树。使用 dtr.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到预测结果 predict_price。最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。我们能看到 CART 回归树的使用和分类树类似，只是最后求得的预测值是个连续值。CART 决策树的剪枝CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。总结今天我给你讲了 CART 决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。最后我们来整理下三种决策树之间在属性选择标准上的差异：ID3 算法，基于信息增益做判断；C4.5 算法，基于信息增益率做判断；CART 算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。实际上这三个指标也是计算“不纯度”的三种计算方式。在工具使用上，我们可以使用 sklearn 中的 DecisionTreeClassifier 创建 CART 分类树，通过 DecisionTreeRegressor 创建 CART 回归树。你可以用代码自己跑一遍我在文稿中举到的例子。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-C4.5","slug":"数据分析-决策树分类算法-C4.5","date":"2018-07-12T06:12:39.000Z","updated":"2020-05-01T14:19:02.521Z","comments":true,"path":"2018/07/12/数据分析-决策树分类算法-C4.5/","link":"","permalink":"cpeixin.cn/2018/07/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-C4.5/","excerpt":"","text":"在 ID3 算法上进行改进的 C4.5 算法那么 C4.5 都在哪些方面改进了 ID3 呢？采用信息增益率因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。采用悲观剪枝ID3构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。离散化处理连续属性C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。处理缺失值针对数据集不完整的情况，C4.5 也可以进行处理。假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。温度 = 高：D1={2-,3+,4+} ；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。这里 + 号代表打篮球，- 号代表不打篮球。比如 ID=2 时，决策是不打篮球，我们可以记录为 2-。针对将属性选择为温度的信息增益为：Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208属性熵 =1.459, 信息增益率 Gain_ratio(D′, 温度)=0.208/1.459=0.1426。下图是计算过程：D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重比例为 6/7，所以 Gain(D′，温度) 所占权重比例为 6/7，所以：Gain_ratio(D, 温度)=6/7*0.1426=0.122。这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。Cart 算法在这里不做介绍，我会在下一讲给你讲解这个算法。现在我们总结下 ID3 和 C4.5 算法。首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。总结前面我们讲了两种决策树分类算法 ID3 和 C4.5，了解了它们的数学原理。你可能会问，公式这么多，在实际使用中该怎么办呢？实际上，我们可以使用一些数据挖掘工具使用它们，比如 Python 的 sklearn，或者是 Weka（一个免费的数据挖掘工作平台），它们已经集成了这两种算法。只是我们在了解了这两种算法之后，才能更加清楚这两种算法的优缺点。我们总结下，这次都讲到了哪些知识点呢？首先我们采用决策树分类，需要了解它的原理，包括它的构造原理、剪枝原理。另外在信息度量上，我们需要了解信息度量中的纯度和信息熵的概念。在决策树的构造中，一个决策树包括根节点、子节点、叶子节点。在属性选择的标准上，度量方法包括了信息增益和信息增益率。在算法上，我讲解了两种算法：ID3 和 C4.5，其中 ID3 是基础的决策树算法，C4.5 在它的基础上进行了改进，也是目前决策树中应用广泛的算法。然后在了解这些概念和原理后，强烈推荐你使用工具，具体工具的使用我会在后面进行介绍。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-ID3","slug":"数据分析-决策树分类算法-ID3","date":"2018-07-11T11:12:49.000Z","updated":"2020-05-01T14:18:48.874Z","comments":true,"path":"2018/07/11/数据分析-决策树分类算法-ID3/","link":"","permalink":"cpeixin.cn/2018/07/11/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-ID3/","excerpt":"","text":"在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，你会发现它实际上是一个树状图，这就是我们今天要讲的决策树决策树的工作原理决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去打篮球？还是不去？上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：构造和剪枝。构造什么是构造呢？构造就是生成一棵完整的决策树。简单来说，构造的过程就是选择什么属性作为节点的过程，那么在构造过程中，会存在三种节点：根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；叶节点：就是树最底部的节点，也就是决策结果。节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：选择哪个属性作为根节点；选择哪些属性作为子节点；什么时候停止并得到目标状态，即叶节点。剪枝决策树构造出来之后是不是就万事大吉了呢？也不尽然，我们可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止“过拟合”（Overfitting）现象的发生。“过拟合”这个概念你一定要理解，它指的就是模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果“太好“，反而在实际应用过程中会导致分类错误。造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。fitting:拟合，就是说这个曲线能不能很好的描述这个样本，有比较好的泛化能力过拟合（OverFititing）：太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平。欠拟合(UnderFitting)：样本不够或者算法不精确，测试样本特性没有学到，不具泛化性，拿到新样本后没有办法去准确的判断泛化能力指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。既然要对决策树进行剪枝，具体有哪些方法呢？一般来说，剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）**。预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。如何判断要不要去打篮球？我给你准备了打篮球的数据集，训练数据如下：我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：纯度和信息熵。先来说一下纯度。你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。我在这里举个例子，假设有 3 个集合：集合 1：6 次都去打篮球；集合 2：4 次去打篮球，2 次不去打篮球；集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1&gt; 集合 2&gt; 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。然后我们再来介绍信息熵（entropy）的概念，它表示了信息的不确定度**在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：大写Σ用于数学上的总和符号，比如：∑Pi，其中i=1,2,…,T，即为求P1 + P2 + … + PT的和。小写σ用于统计学上的标准差。这种写法表示的就是∑j=1+2+3+…+n。下图：其中i表示下界，n表示上界， k从i开始取数，一直取到n,全部加起来。p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。我举个简单的例子，假设有 2 个集合集合 1：5 次去打篮球，1 次不去打篮球；集合 2：3 次去打篮球，3 次不去打篮球。在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入上述信息熵公式可以计算得出：同样，集合 2 中，也是一共 6 次决策，其中类别 1 中“打篮球”的次数是 3，类别 2“不打篮球”的次数也是 3，那么信息熵为多少呢？我们可以计算得出：从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。ID3算法我们先看下 ID3 算法。ID3 算法计算的是信息增益，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。假设天气 = 晴的时候，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。那么 a 代表节点的属性，即天气 = 晴。你可以在下面的图例中直观地了解这几个概念。比如针对图上这个例子，D 作为节点的信息增益为：也就是 D 节点的信息熵 -2 个子节点的归一化信息熵。2 个子节点归一化信息熵 =3/10 的 D1 信息熵 +7/10 的 D2 信息熵。我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，于是我们可以用下面的方式来记录 D1，D2，D3：D1(天气 = 晴天)={1-,2-,6+}D2(天气 = 阴天)={3+,7-}D3(天气 = 小雨)={4+,5-}我们先分别计算三个叶子节点的信息熵：因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（父节点）中的概率是 3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是 2/7。那么作为子节点的归一化信息熵 = 3/70.918+2/71.0+2/7*1.0=0.965。因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。天气作为属性节点的信息增益为，Gain(D , 天气)=0.985-0.965=0.020。同理我们可以计算出其他属性作为根节点的信息增益，它们分别为 ：Gain(D , 温度)=0.128Gain(D , 湿度)=0.020Gain(D , 刮风)=0.020下图为计算Gain(D , 温度)过程：我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：然后我们要将上图中第一个叶节点，也就是 D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：Gain(D , 湿度)=1Gain(D , 天气)=1Gain(D , 刮风)=0.3115我们能看到湿度，或者天气为 D1 的节点都可以得到最大的信息增益，这里我们选取湿度作为节点的属性划分。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下：于是我们通过 ID3 算法得到了一棵决策树。ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3 算法倾向于选择取值比较多的属性。这样，如果我们把“编号”作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。所以 ID3 有一个缺陷就是，有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-使用sklearn优雅地进行数据挖掘","slug":"数据分析-使用sklearn优雅地进行数据挖掘","date":"2018-07-10T11:12:49.000Z","updated":"2020-05-01T14:17:46.353Z","comments":true,"path":"2018/07/10/数据分析-使用sklearn优雅地进行数据挖掘/","link":"","permalink":"cpeixin.cn/2018/07/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%BD%BF%E7%94%A8sklearn%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","excerpt":"","text":"https://www.cnblogs.com/jasonfreak/p/5448462.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"cpeixin.cn/tags/sklearn/"}]},{"title":"数据分析-特征选择","slug":"数据分析-特征选择","date":"2018-07-09T15:12:49.000Z","updated":"2020-05-01T14:17:11.676Z","comments":true,"path":"2018/07/09/数据分析-特征选择/","link":"","permalink":"cpeixin.cn/2018/07/09/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"","text":"数据分析-特征选择特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。根据特征选择的形式又可以将特征选择方法分为3种：Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。我们可以使用sklearn中的feature_selection库来进行特征选择。Filter方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：12345from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data)相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：123456789101112from sklearn.feature_selection import SelectKBestfrom minepy import MINE #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5 def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5)#选择K个最好的特征，返回特征选择后的数据SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)WrapperWrapper递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：1234567from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression#递归特征消除法，返回特征选择后的数据#参数estimator为基模型#参数n_features_to_select为选择的特征个数RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)Embedded基于惩罚项的特征选择法使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：12345from sklearn.feature_selection import SelectFromModelfrom sklearn.linear_model import LogisticRegression#带L1惩罚项的逻辑回归作为基模型的特征选择SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)实际上，L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn.linear_model import LogisticRegressionclass LR(LogisticRegression): def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1): #权值相近的阈值 self.threshold = threshold LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) #使用同样的参数创建L2逻辑回归 self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) def fit(self, X, y, sample_weight=None): #训练L1逻辑回归 super(LR, self).fit(X, y, sample_weight=sample_weight) self.coef_old_ = self.coef_.copy() #训练L2逻辑回归 self.l2.fit(X, y, sample_weight=sample_weight) cntOfRow, cntOfCol = self.coef_.shape #权值系数矩阵的行数对应目标值的种类数目 for i in range(cntOfRow): for j in range(cntOfCol): coef = self.coef_[i][j] #L1逻辑回归的权值系数不为0 if coef != 0: idx = [j] #对应在L2逻辑回归中的权值系数 coef1 = self.l2.coef_[i][j] for k in range(cntOfCol): coef2 = self.l2.coef_[i][k] #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0 if abs(coef1-coef2) &lt; self.threshold and j != k and self.coef_[i][k] == 0: idx.append(k) #计算这一类特征的权值系数均值 mean = coef / len(idx) self.coef_[i][idx] = mean return self 使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：from sklearn.feature_selection import SelectFromModel #带L1和L2惩罚项的逻辑回归作为基模型的特征选择#参数threshold为权值系数之差的阈值SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)基于树模型的特征选择法树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：123456from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifier#GBDT作为基模型的特征选择SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)降维当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。主成分分析法（PCA）使用decomposition库的PCA类选择特征的代码如下：12345from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(iris.data)线性判别分析法（LDA）使用lda库的LDA类选择特征的代码如下：12345from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(iris.data, iris.target)总结再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-特征预处理","slug":"数据分析-特征预处理","date":"2018-07-08T12:12:49.000Z","updated":"2020-05-01T14:17:03.540Z","comments":true,"path":"2018/07/08/数据分析-特征预处理/","link":"","permalink":"cpeixin.cn/2018/07/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"","text":"特征工程华盛顿大学教授、《终极算法》（The Master Algorithm）的作者佩德罗·多明戈斯曾在 Communications of The ACM 第 55 卷第 10 期上发表了一篇名为《机器学习你不得不知的那些事》（A Few Useful Things to Know about Machine Learning）的小文，介绍了 12 条机器学习中的“金科玉律”，其中的 7/8 两条说的就是对数据的作用的认识。多明戈斯的观点是：数据量比算法更重要。即使算法本身并没有什么精巧的设计，但使用大量数据进行训练也能起到填鸭的效果，获得比用少量数据训练出来的聪明算法更好的性能。这也应了那句老话：数据决定了机器学习的上限，而算法只是尽可能逼近这个上限。但多明戈斯嘴里的数据可不是硬件采集或者软件抓取的原始数据，而是经过特征工程处理之后的精修数据，在他看来，特征工程（feature engineering）才是机器学习的关键**。通常来说，原始数据并不直接适用于学习，而是特征筛选、构造和生成的基础。一个好的预测模型与高效的特征提取和明确的特征表示息息相关，如果通过特征工程得到很多独立的且与所属类别相关的特征，那学习过程就变成小菜一碟。特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码。机器学习中的很多具体算法都可以归纳到特征工程的范畴之中，比如使用 L1 正则化项的 LASSO 回归，就是通过将某些特征的权重系数缩小到 0 来实现特征的过滤；再比如主成分分析，将具有相关性的一组特征变换为另一组线性无关的特征。这些方法本质上完成的都是特征工程的任务。本质上讲，特征工程是一个表示和展现数据的过程；实际工作中，特征工程的目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。特征工程的重要性有以下几点：特征越好，灵活性越强。好的特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易和维护。特征越好，构建的模型越简单。好的特征可以在参数不是最优的情况，依然得到很好的性能，减少调参的工作量和时间，也就可以大大降低模型复杂度。特征越好，模型的性能越出色。特征工程的目的本来就是为了提升模型的性能。但是今天，我将不会讨论这些，而是把关注点放在算法之外，看一看在特征工程之前，数据的特征需要经过哪些必要的预处理（preprocessing）。特征预处理通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：不属于同一量纲即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。信息冗余对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。定性特征不能直接使用某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。存在缺失值：缺失值需要补充。信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。首先介绍数据预处理中，比较简单的部分：处理缺失值数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确。缺失值产生的原因信息暂时无法获取，或者获取信息的代价太大。信息被遗漏，人为的输入遗漏或者数据采集设备的遗漏。属性不存在，在某些情况下，缺失值并不意味着数据有错误，对一些对象来说某些属性值是不存在的，如未婚者的配偶姓名、儿童的固定收入等。缺失值的影响数据挖掘建模将丢失大量的有用信息。数据挖掘模型所表现出的不确定性更加显著，模型中蕴含的规律更难把握。包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。缺失值的处理方法直接使用含有缺失值的特征：当仅有少量样本缺失该特征的时候可以尝试使用；删除含有缺失值的特征：这个方法一般适用于大多数样本都缺少该特征，且仅包含少量有效值是有效的；插值补全缺失值插值补全缺失值最常使用的还是第三种插值补全缺失值的做法，这种做法又可以有多种补全方法。均值/中位数/众数补全如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。同类均值/中位数/众数补全对样本进行分类后，根据同类其他样本该属性的均值补全缺失值，当然同第一种方法类似，如果均值不可行，可以尝试众数或者中位数等统计数据来补全。固定值补全利用固定的数值补全缺失的属性值。建模预测利用机器学习方法，将缺失属性作为预测目标进行预测，具体为将样本根据是否缺少该属性分为训练集和测试集，然后采用如回归、决策树等机器学习算法训练模型，再利用训练得到的模型预测测试集中样本的该属性的数值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。高维映射将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。多重插补多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。压缩感知和矩阵补全压缩感知通过利用信号本身所具有的稀疏性，从部分观测样本中回复原信号。压缩感知分为感知测量和重构恢复两个阶段。感知测量：此阶段对原始信号进行处理以获得稀疏样本表示。常用的手段是傅里叶变换、小波变换、字典学习、稀疏编码等重构恢复：此阶段基于稀疏性从少量观测中恢复原信号。这是压缩感知的核心矩阵补全可以查看知乎上的问题手动补全除了手动补全方法，其他插值补全方法只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。但这种方法需要对问题领域有很高的认识和理解，要求比较高，如果缺失数据较多，会比较费时费力。最近邻补全寻找与该样本最接近的样本，使用其该属性数值来补全。处理异常值异常值分析是检验数据是否有录入错误以及含有不合常理的数据。忽视异常值的存在是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响。异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也称为离群点，异常值分析也称为离群点分析。异常值检测简单统计比如利用pandas库的describe()方法观察数据的统计性描述，或者简单使用散点图也能观察到异常值的存在，如下图所示：3∂原则这个原则有个条件：数据需要服从正态分布。在 3∂ 原则下，异常值如超过 3 倍标准差，那么可以将其视为异常值。正负3∂ 的概率是 99.7%，那么距离平均值 3∂ 之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。如下图所示：箱型图这种方法是利用箱型图的四分位距（IQR）对异常值进行检测，也叫Tukey‘s test。箱型图的定义如下：四分位距(IQR)就是上四分位与下四分位的差值。而我们通过IQR的1.5倍为标准，规定：超过上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离的点为异常值。下面是Python中的代码实现，主要使用了numpy的percentile方法。1234Percentile = np.percentile(df['length'],[0,25,50,75,100])IQR = Percentile[3] - Percentile[1]UpLimit = Percentile[3]+ageIQR*1.5DownLimit = Percentile[1]-ageIQR*1.5也可以使用seaborn的可视化方法boxplot来实现：123f,ax=plt.subplots(figsize=(10,8))sns.boxplot(y='length',data=df,ax=ax)plt.show()上面三种方法是比较简单的异常值检测方法，接下来是一些较复杂的异常值检测方法，因此这里简单介绍下这些方法的基本概念。基于模型预测顾名思义，该方法会构建一个概率分布模型，并计算对象符合该模型的概率，将低概率的对象视为异常点。如果模型是簇的组合，则异常点是不在任何簇的对象；如果模型是回归，异常点是远离预测值的对象(就是第一个方法的图示例子)。优缺点：有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。基于近邻度的离群点检测一个对象的离群点得分由到它的 k-最近邻（KNN）的距离给定。这里需要注意 k 值的取值会影响离群点得分，如果 k 太小，则少量的邻近离群点可能会导致较低的离群点得分；如果 k 太大，则点数少于 k 的簇中所有的对象可能都成了离群点。为了增强鲁棒性，可以采用 k 个最近邻的平均距离。优缺点：简单;基于邻近度的方法需要 O(m2) 时间，大数据集不适用；k 值的取值导致该方法对参数的选择也是敏感的；不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。基于密度的离群点检测一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。优缺点：给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；与基于距离的方法一样，这些方法必然具有 O(m2) 的时间复杂度。对于低维数据使用特定的数据结构可以达到 O(mlogm) ；参数选择是困难的。虽然 LOF 算法通过观察不同的 k 值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。基于聚类的离群点检测一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。这也是 k-means 算法的缺点，对离群点敏感。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。优缺点：基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；簇的定义通常是离群点的补集，因此可能同时发现簇和离群点；产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。专门的离群点检测除了以上提及的方法，还有两个专门用于检测异常点的方法比较常用：One Class SVM和Isolation Forest异常值处理删除含有异常值的记录：直接将含有异常值的记录删除；视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；平均值修正：可用前后两个观测值的平均值修正该异常值；不处理：直接在具有异常值的数据集上进行数据挖掘；将含有异常值的记录直接删除的方法简单易行，但缺点也很明显，在观测值很少的情况下，这种删除会造成样本量不足，可能会改变变量的原有分布，从而造成分析结果的不准确。视为缺失值处理的好处是可以利用现有变量的信息，对异常值（缺失值）进行填补。在很多情况下，要先分析异常值出现的可能原因，在判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。处理类别不平衡问题什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。那么如何解决类别不平衡问题呢？这里介绍八大解决办法。扩充数据集首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。尝试其他评价指标一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。其他可以推荐的评价指标有以下几种混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量；精确度(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，P=TP / (TP+FP)召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，R = TP / (TP+FN)F1 得分(F1 Score)：精确度和召回率的加权平均，F1=2PR / (P+R)Kappa (Cohen kappa)ROC 曲线(ROC Curves):常被用于评价一个二值分类器的优劣，而且对于正负样本分布变化的时候，ROC 曲线可以保持不变，即不受类别不平衡的影响。其中 TP、FP、TN、FN 分别表示正确预测的正类、错误预测的正类、预测正确的负类以及错误预测的负类。图例如下：对数据集进行重采样可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。对小类的数据样本进行采样来增加小类的数据样本个数，即过采样（over-sampling ，采样的个数大于该类样本的个数）。对大类的数据样本进行采样来减少该类数据样本的个数，即欠采样（under-sampling，采样的次数少于该类样本的个素）。采样算法往往很容易实现，并且其运行速度快，并且效果也不错。 一些经验法则：考虑对大类下的样本（超过 1 万、十万甚至更多）进行欠采样，即删除部分样本；考虑对小类下的样本（不足 1万甚至更少）进行过采样，即添加部分样本的副本；考虑尝试随机采样与非随机采样两种采样方法；考虑对各类别尝试不同的采样比例，比一定是 1:1，有时候 1:1 反而不好，因为与现实情况相差甚远；考虑同时使用过采样与欠采样。尝试人工生成数据样本一种简单的人工样本数据产生的方法便是，对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样。你可以使用基于经验对属性值进行随机采样而构造新的人工样本，或者使用类似朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。有一个系统的构造人工数据样本的方法 SMOTE(Synthetic Minority Over-sampling Technique)。SMOTE 是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。它基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本，然后对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。python 实现的 SMOTE 算法代码地址如下，它提供了多种不同实现版本，以及多个重采样算法。https://github.com/scikit-learn-contrib/imbalanced-learn尝试不同分类算法强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。决策树往往在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。尝试对模型进行惩罚你可以使用相同的分类算法，但使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 penalized-SVM 和 penalized-LDA 算法。如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用惩罚模型来解决不平衡性问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。尝试一个新的角度理解问题从一个新的角度来理解问题，比如我们可以将小类的样本作为异常点，那么问题就变成异常点检测与变化趋势检测问题。异常点检测：即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。变化趋势检测：类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。尝试创新仔细对问题进行分析和挖掘，是否可以将问题划分为多个更小的问题，可以尝试如下方法：将你的大类压缩成小类；使用 One Class 分类器（将小类作为异常点）；使用集成方式，训练多个分类器，然后联合这些分类器进行分类；对于类别不平衡问题，还是需要具体问题具体分析，如果有先验知识可以快速挑选合适的方法来解决，否则最好就是逐一测试每一种方法，然后挑选最好的算法。最重要的还是多做项目，多积累经验，这样遇到一个新的问题，也可以快速找到合适的解决方法。以上针对异常值，缺失数据，分类不均问题做了详细的描述，那么接下来，就要转化数据，使之成为有效的特征。常用的方法是标准化，归一化，特征的离散化等。无量纲化量纲：就是单位，特征的单位不一致，特征就不能放在一起比较。无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。标准化比方说有一些数字的单位是千克，有一些数字的单位是克，这个时候需要统一单位。如果没有标准化，两个变量混在一起搞，那么肯定就会不合适。0-1标准化是对原始数据进行线性变换，将特征值映射成区间为［0，1］的标准值中：Z标准化基于特征值的均值（mean）和标准差（standard deviation）进行数据的标准化。它的计算公式为：标准化的代码如下：123456789101112131415161718import pandasdata = pandas.read_csv('路径.csv')#（一）Min-Max 标准化from sklearn.preprocessing import MinMaxScaler#初始化一个scaler对象scaler = MinMaxScaler()#调用scaler的fit_transform方法，把我们要处理的列作为参数传进去data['标准化后的A列数据'] = scaler.fit_transform(data['A列数据'])data['标准化后的B列数据'] = scaler.fit_transform(data['B列数据'])#（二）Z-Score标准化 （可在scale中直接实现）from sklearn.preprocessing import scaledata['标准化后的A列数据'] = scale(data['A列数据'])data['标准化后的B列数据'] = scale(data['B列数据'])区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data)归一化归一化是因为在特征会在不同的尺度下有不同的表现形式，归一化会使得各个特征能够同时以恰当的方式表现。比方说某个专辑的点击播放率一般不会超过0.2，但是专辑的播放次数可能会达到几千次，所以说为了能够在模型里面得到更合适结果，需要先把一些特征在尺度上进行归一化，然后进行模型训练。与标准化的区别，简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：使用preproccessing库的Normalizer类对数据进行归一化的代码如下：12345from sklearn.preprocessing import Normalizerscaler = Normalizer()#归一化可以同时处理多个列，所以[0]第一个进行赋值data['归一化后的A列数据'] = scaler.fit_transform(data['A列数据'])[0]data['归一化后的B列数据'] = scaler.fit_transform(data['B列数据'])[0]对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：使用preproccessing库的Binarizer类对数据进行二值化的代码如下：1234from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data)特征的离散化在进行建模时，变量中经常会有一些变量为离散型变量，例如性别。这些变量我们一般无法直接放到模型中去训练模型。因此在使用之前，我们往往会对此类变量进行处理。一般是对离散变量进行one-hot编码。下面具体介绍通过python对离散变量进行one-hot的方法。离散化是把连续型的数值型特征分段，每一段内的数据都可以当做成一个新的特征。具体又可分为等步长方式离散化和等频率的方式离散化，等步长的方式比较简单，等频率的方式更加精准，会跟数据分布有很大的关系。 代码层面，可以用pandas中的cut方法进行切分。总之，离散化的特征能够提高模型的运行速度以及准确率。比方说年龄特征是一个连续的特征，但是把年龄层分成5－18岁（中小学生），19－23岁（大学生），24－29岁（工作前几年），30－40岁（成家立业），40－60岁（中年人）从某些层面来说比连续的年龄数据（比如说某人年龄是20岁1月3日之类的）更容易理解不同年龄层人的特性。典型的离散化步骤：对特征做排序－&gt; 选择合适的分割点－&gt; 作出区间的分割 －&gt; 作出区间分割－&gt; 查看是否能够达到停止条件。分桶1.离散化的常用方法是分桶：2.分桶的数量和边界通常需要人工指定。一般有两种方法：根据业务领域的经验来指定。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。年收入超过 260 万元，则为分桶 4 。根据模型指定。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。3.选择分桶大小时，有一些经验指导：分桶大小必须足够小，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。分桶大小必须足够大，使每个桶内都有足够的样本。如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。每个桶内的样本尽量分布均匀。离散特征的两种数据类型离散特征的取值之间有大小的意义：例如：尺寸（L、XL、XXL）离散特征的取值之间没有大小的意义：例如：颜色（Red、Blue、Green）离散特征值有大小意义的虚拟变量处理离散特征的取值之间有大小意义的处理函数，我们只需要把大小值以字典的方式，作为第一个参数传入即可；(1) dict映射的字典pandas.Series.map(dict)离散特征值没有大小意义的虚拟变量处理离散特征的取值之间没有大小意义的处理方法，我们可以使用get_dummies方法处理，它有6个常用的参数(1) data 要处理的DataFrame(2) prefix 列名的前缀，在多个列有相同的离散项时候使用(3) prefix_sep 前缀和离散值的分隔符，默认为下划线，默认即可(4) dummy_na 是否把NA值，作为一个离散值进行处理，默认不处理(5) columns 要处理的列名，如果不指定该列，那么默认处理所有列(6) drop_first 是否从备选项中删第一个，建模的时候为避免共线性使用pandas.getdummies(data,prefix=None,prefix_sep=’‘,dummy_na=False,columns=None,drop_first=False)举例：123456import pandas#有些朋友也可能是encoding='utf8'或其他data=pandas.read_csv('file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK')print(data)其实，虚拟变量的实质就是要把离散型的数据转化为连续型的数据，因为第1列年龄已经是连续值，所以我们就不需要处理了。我们看看如何处理学历和性别？因为不同学历之间是有高低之分的，因此我们要使用Map方法来处理这种类型的离散型数据；第1步： 首先我们要处理不同学历之间的大小值我们使用drop_duplicates方法，来看看数据列都有哪些学历12#查看学历去重之后的情况data['学历'].drop_duplicates()第2步：理解数据值背后的意义，作出我们自己的解析，对每个学历进行评分#构建学历字典123educationLevelDict=&#123;'博士':4,'硕士':3,'大学':2,'大专':1&#125;#调用Map方法进行虚拟变量的转换data['Education Level Map']=data['Education Level'].map(educationLevelDict)第3步 对于性别这种没有大小比较的离散变量，我们使用get_dummies方法，来进行调用处理即可；1234567dummies=pandas.get_dummies(data,columns=['性别'],prefix=['性别'],prefix_sep='_',dummy_na=False,drop_first=False)sklearn 处理离散变量preprocessing.LabelEncoder：用于标签，将分类转换为分类数值 一般就是一维的preprocessing.OrdinalEncoder ：特征专用，要求至少为二维矩阵preprocessing.OneHotEncoder虽然上面已经将文字类型转换成了数值类型，但是考虑下舱门这特征Embarked（S,C,Q），表示成了[0,1,2]这样合适吗?这三个数字在算法看来，是连续且可以计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所以算法会把舱门，学历这样的分类特征，都误会成是体重这样的分类特征。这是说，我们把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模。类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息：1234567#独热编码 创建哑变量from sklearn.preprocessing import OneHotEncoderdata_ = data.copy()encoder=OneHotEncoder()encoder.fit(data_.iloc[:,1:-1])res=encoder.transform(data_.iloc[:,1:-1]).toarray()#这里要转换成数组，否则得到的是一个对象resscikit DictVectorizer 热编码（只处理类别型变量）123456789101112# 数据预处理df.transpose().to_dict().values()feature = df.iloc[:, :-1]feature# 热编码from sklearn.feature_extraction import DictVectorizerdvec = DictVectorizer(sparse=False)X = dvec.fit_transform(feature.transpose().to_dict().values())pd.DataFrame(X, columns=dvec.get_feature_names())哑编码我们针对类别型的特征，通常采用哑编码（One_Hot Encodin）的方式。所谓的哑编码，直观的讲就是用N个维度来对N个类别进行编码，并且对于每个类别，只有一个维度有效，记作数字1 ；其它维度均记作数字0。但有时使用哑编码的方式，可能会造成维度的灾难，所以通常我们在做哑编码之前，会先对特征进行Hash处理，把每个维度的特征编码成词向量。以上为大家介绍了几种较为常见、通用的数据预处理方式，但只是浩大特征工程中的冰山一角。往往很多特征工程的方法需要我们在项目中不断去总结积累比如：针对缺失值的处理，在不同的数据集中，用均值填充、中位数填充、前后值填充的效果是不一样的；对于类别型的变量，有时我们不需要对全部的数据都进行哑编码处理；对于时间型的变量有时我们有时会把它当作是离散值，有时会当成连续值处理等。所以很多情况下，我们要根据实际问题，进行不同的数据预处理。参考：https://mp.weixin.qq.com/s/BnTXjzHSb5-4s0O0WuZYlg","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-特征提取","slug":"数据分析-特征提取","date":"2018-07-07T14:07:12.000Z","updated":"2020-05-01T14:17:20.871Z","comments":true,"path":"2018/07/07/数据分析-特征提取/","link":"","permalink":"cpeixin.cn/2018/07/07/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","excerpt":"","text":"特征工程当数据预处理完成后，我们就要开始进行特征工程了。主要包含以下几个方面：特征提取特征创造特征选择特征提取和特征选择的区别特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。特征分类下图就是将我们所能遇到的特征数据进行一个分类：首先是基本特征，而后统计和复杂特征层层递进。其中针对图像语音等抽提特征有专用的知识方法掌握了这套特征设计的思路，在复杂数据上几乎可以设计出无穷无尽的特征。而怎么在最短的时间内，把数据中最有价值的特征提炼出来，就要考验数据挖掘工程师的功底。特征提取特征抽取或者特征提取大概可以分为；字典特征抽取，应用DiceVectorizer实现对类别特征进行数值化、离散化文本特征抽取，应用CounterVertorize/TfIdfVectorize实现对文本特征数值化图像特征抽取(深度学习)对于以上不同类别数据的特征提取，这里不一一介绍，等以后遇到了对应问题，再详细的举例用哪些相应的算法来处理。特征提取也可以说是将任意数据（文本或者图像）转化成适用于机器学习的数字特征应用 sklearn.feature_extraction可以轻松完成上述任务","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-数据转换","slug":"数据分析-数据转换","date":"2018-07-06T15:26:15.000Z","updated":"2020-05-01T14:17:29.058Z","comments":true,"path":"2018/07/06/数据分析-数据转换/","link":"","permalink":"cpeixin.cn/2018/07/06/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2/","excerpt":"","text":"数据变换。如果一个人在百分制的考试中得了 95 分，你肯定会认为他学习成绩很好，如果得了 65 分，就会觉得他成绩不好。如果得了 80 分呢？你会觉得他成绩中等，因为在班级里这属于大部分人的情况。为什么会有这样的认知呢？这是因为我们从小到大的考试成绩基本上都会满足正态分布的情况。什么是正态分布呢？正态分布也叫作常态分布，就是正常的状态下，呈现的分布情况。比如你可能会问班里的考试成绩是怎样的？这里其实指的是大部分同学的成绩如何。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。正态分布的另一个好处就是，如果你知道了自己的成绩，和整体的正态分布情况，就可以知道自己的成绩在全班中的位置。另一个典型的例子就是，美国 SAT 考试成绩也符合正态分布。而且美国本科的申请，需要中国高中生的 GPA 在 80 分以上（百分制的成绩），背后的理由也是默认考试成绩属于正态分布的情况。为了让成绩符合正态分布，出题老师是怎么做的呢？他们通常可以把考题分成三类：第一类：基础题，占总分 70%，基本上属于送分题；第二类：灵活题，基础范围内 + 一定的灵活性，占 20%；第三类：难题，涉及知识面较广的难题，占 10%；那么，你想下，如果一个出题老师没有按照上面的标准来出题，而是将第三类难题比重占到了 70%，也就是我们说的“超纲”，结果会是怎样呢？你会发现，大部分人成绩都“不及格”，最后在大家激烈的讨论声中，老师会将考试成绩做规范化处理，从而让成绩满足正态分布的情况。因为只有这样，成绩才更具有比较性。所以正态分布的成绩，不仅可以让你了解全班整体的情况，还能了解每个人的成绩在全班中的位置。数据变换在数据分析中的角色我们再来举个例子，假设 A 考了 80 分，B 也考了 80 分，但前者是百分制，后者 500 分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。你现在可以理解为什么 80% 的工作时间会花在前期的数据准备上了吧。那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。我来介绍下这些常见的变换方法：数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。在这些变换方法中，最简单易用的就是对数据进行规范化处理。下面我来给你讲下如何对数据进行规范化处理。数据规范化的几种方法Min-max 规范化Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。Z-Score 规范化假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义。那么如何用相同的标准来比较 A 与 B 的成绩呢？Z-Score 就是用来可以解决这一问题的。我们定义：新数值 =（原数值 - 均值）/ 标准差。假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为 400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值 =(80-400)/100=-3.2。那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。我们能看到 Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。小数定标规范化小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。上面这三种是数值规范化中常用的几种方式。Python的SciKit-Learn库使用SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。我现在来讲下如何使用 SciKit-Learn 进行数据规范化。Min-max 规范化我们可以让原始数据投射到指定的空间[min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。默认情况下[min,max]是[0,1]，也就是把原始数据投放到[0,1]范围内。我们来看下下面这个例子：123456789101112# coding:utf-8from sklearn import preprocessingimport numpy as np# 初始化数据，每一行表示一个样本，每一列表示一个特征x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 将数据进行[0,1]规范化min_max_scaler = preprocessing.MinMaxScaler()minmax_x = min_max_scaler.fit_transform(x)print minmax_x结果：1234[[0. 0. 0.66666667] [1. 1. 1. ] [0. 1. 0. ]]Z-Score 规范化在 SciKit-Learn 库中使用 preprocessing.scale() 函数，可以直接将给定数据进行 Z-Score 规范化。12345678910from sklearn import preprocessingimport numpy as np# 初始化数据x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 将数据进行Z-Score规范化scaled_x = preprocessing.scale(x)print scaled_x结果：1234[[-0.70710678 -1.41421356 0.26726124] [ 1.41421356 0.70710678 1.06904497] [-0.70710678 0.70710678 -1.33630621]]1这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。我们看到 Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。小数定标规范化我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。这里我们看下运行代码：123456789101112# coding:utf-8from sklearn import preprocessingimport numpy as np# 初始化数据x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 小数定标规范化j = np.ceil(np.log10(np.max(abs(x))))scaled_x = x/(10**j)print scaled_x结果：1234[[ 0. -0.3 0.1] [ 0.3 0.1 0.2] [ 0. 0.1 -0.1]]数据挖掘中数据变换比算法选择更重要在考试成绩中，我们都需要让数据满足一定的规律，达到规范性的要求，便于进行挖掘。这就是数据变换的作用。如果不进行变换的话，要不就是维数过多，增加了计算的成本，要不就是数据过于集中，很难找到数据之间的特征。在数据变换中，重点是如何将数值进行规范化，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 Z-Score 规范化可以直接将数据转化为正态分布的情况，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。在最后我给大家推荐了 Python 的 sklearn 库，它和 NumPy, Pandas 都是非常有名的 Python 库，在数据统计工作中起了很大的作用。SciKit-Learn 不仅可以用于数据变换，它还提供了分类、聚类、预测等数据挖掘算法的 API 封装。后面我会详细给你讲解这些算法，也会教你如何使用 SciKit-Learn 工具来完成数据挖掘算法的工作。Q&amp;A数据规范化、归一化、标准化是同一个概念么？数据规范化是更大的概念，它指的是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。数据归一化和数据标准化都是数据规范化的方式。不同点在于数据归一化会让数据在一个[0,1]或者[-1,1]的区间范围内。而数据标准化会让规范化的数据呈现正态分布的情况，所以你可以这么记：归一化的“一”，是让数据在[0,1]的范围内。而标准化，目标是让数据呈现标准的正态分布。什么时候会用到数据规范化（Min-max、Z-Score 和小数定标）？刚才提到了，进行数据规范化有两个作用：一是让数据之间具有可比较性，二是加快后续算法的迭代收敛速度。实际上你能看到 Min-max、Z-Score 和小数定标规范化都是一种线性映射的关系，将原来的数值投射到新的空间中。这样变换的好处就是可以看到在特定空间内的数值分布情况，比如通过 Min-max 可以看到数据在[0,1]之间的分布情况，Z-Score 可以看到数值的正态分布情况等。不论是采用哪种数据规范化方法，规范化后的数值都会在同一个数量的级别上，这样方便后续进行运算。那么回过头来看，在数据挖掘算法中，是否都需要进行数据规范化呢？一般情况下是需要的，尤其是针对距离相关的运算，比如在 K-Means、KNN 以及聚类算法中，我们需要有对距离的定义，所以在做这些算法前，需要对数据进行规范化。另外还有一些算法用到了梯度下降作为优化器，这是为了提高迭代收敛的效率，也就是提升找到目标函数最优解的效率。我们也需要进行数据规范化，比如逻辑回归、SVM 和神经网络算法。在这些算法中都有目标函数，需要对目标函数进行求解。梯度下降的目标是寻找到目标函数的最优解，而梯度的方法则指明了最优解的方向，如下图所示。当然不是所有的算法都需要进行数据规范化。在构造决策树的时候，可以不用提前做数据规范化，因为我们不需要关心特征值的大小维度，也没有使用到梯度下降来做优化，所以数据规范化对决策树的构造结果和构造效率影响不大。除此之外，还是建议你在做数据挖掘算法前进行数据规范化。如何使用 Z-Score 规范化，将分数变成正态分布？假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。这里假设 A 和 B 的考试成绩都是成正态分布，可以直接采用 Z-Score 的线性化规范化方法。在专栏的讨论区中，有个同学提出了“Z-Score”的非线性计算方式，大家可以一起了解下：先按公式计算出百分等级。百分等级（年级）=100-(100x 年级名次 -50)/ 有效参加考试人数。这里百分等级是每个学生在该批学生中的相对位置，其中百分等级是按照正态分布图的所占面积比例求得的；按照百分等级数去标准正态分布表中查询得出 Z-Score 值，这样最终得出的 Z 分便是标准的正态分布，能够将偏态转化成标准正态。因为在很多情况下，数值如果不是正态分布，而是偏态分布，直接使用 Z-Score 的线性计算方式无法将分数转化成正态分布。采用以上的方法可以解决这一个问题，大家可以了解下。这里偏态分布指的是非对称分布的偏斜状态，包括了负偏态，也就是左偏态分布，以及正偏态，也就是右偏态分布。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-数据清洗","slug":"数据分析-数据清洗","date":"2018-07-05T15:53:22.000Z","updated":"2020-05-01T14:17:36.976Z","comments":true,"path":"2018/07/05/数据分析-数据清洗/","link":"","permalink":"cpeixin.cn/2018/07/05/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/","excerpt":"","text":"数据分析80%时间都花费在了数据清洗任务上？做完数据采集就可以直接进行挖掘了吗？肯定不是的。就拿做饭打个比方吧，对于很多人来说，热油下锅、掌勺翻炒一定是做饭中最过瘾的环节，但实际上炒菜这个过程只占做饭时间的 20%，剩下 80% 的时间都是在做准备，比如买菜、择菜、洗菜等等。在数据挖掘中，数据清洗就是这样的前期准备工作。对于数据科学家来说，我们会遇到各种各样的数据，在分析前，要投入大量的时间和精力把数据“整理裁剪”成自己想要或需要的样子。为什么呢？因为我们采集到的数据往往有很多问题。我们先看一个例子，假设老板给你以下的数据，让你做数据分析，你看到这个数据后有什么感觉呢？你刚看到这些数据可能会比较懵，因为这些数据缺少标注。我们在收集整理数据的时候，一定要对数据做标注，数据表头很重要。比如这份数据表，就缺少列名的标注，这样一来我们就不知道每列数据所代表的含义，无法从业务中理解这些数值的作用，以及这些数值是否正确。我简单解释下上图这些数据代表的含义。这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。列坐标中，第 0 列代表的是序号，第 1 列代表的会员的姓名，第 2 列代表年龄，第 3 列代表体重，第 46 列代表男性会员的三围尺寸，第 79 列代表女性会员的三围尺寸。了解含义以后，我们再看下中间部分具体的数据，你可能会想，这些数据怎么这么“脏乱差”啊，有很多值是空的（NaN），还有空行的情况。是的，这还仅仅是一家商店的部分会员数据，我们一眼看过去就能发现一些问题。日常工作中的数据业务会复杂很多，通常我们要统计更多的数据维度，比如 100 个指标，数据量通常都是超过 TB、EB 级别的，所以整个数据分析的处理难度是呈指数级增加的。这个时候，仅仅通过肉眼就很难找到问题所在了。我举了这样一个简单的例子，带你理解在数据分析之前为什么要有数据清洗这个重要的准备工作。有经验的数据分析师都知道，好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。但在实际工作中，也可能像这个案例一样，数据是缺少标注的。但是大多数的情况下，数据表头都是存在的，即使不存在，我们也会首先的想倒要去处理表头信息，例如：如果数据来源是python爬虫获取的，那么在爬取之前，我们就会定义好，解析获得到的每个字段是什么意思。如果数据源是日志采集而来的，那么在我们接收到前端传输过来数据流，一般都是带有字段信息的，例如，前端埋点数据打到 kafka消息队 列中，是json格式，那么字段信息则为key值，如果埋点数据并不是标准的json格式，那么前端人员也是会给一份数据字典与之对应的。数据源来自某个业务数据库，那么，数据表头信息则是数据库中数据的列名。数据质量的准则在上面这个服装店会员数据的案例中，一看到这些数据，你肯定能发现几个问题。你是不是想知道，有没有一些准则来规范这些数据的质量呢？准则肯定是有的。不过如果数据存在七八种甚至更多的问题，我们很难将这些规则都记住。有研究说一个人的短期记忆，最多可以记住 7 条内容或信息，超过 7 条就记不住了。而数据清洗要解决的问题，远不止 7 条，我们万一漏掉一项该怎么办呢？有没有一种方法，我们既可以很方便地记住，又能保证我们的数据得到很好的清洗，提升数据质量呢？在这里，我将数据清洗规则总结为以下 4 个关键点，统一起来叫“完全合一”，下面我来解释下。完整性：单条数据是否存在空值，统计的字段是否完善。全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。拿接触过的实际项目举例，检查数据质量是有多么的重要，工作初期，经常会使用hive，spark做一些数据报表，面对几百GB的数据，经常是直接忽略过数据质量审查的这一步骤，所以导致的结果就是，在程序计算过程中，会出现很对关于数值的错误异常，还有空值异常，极大异常值等，例如 前端生日栏位出现bug,用户自定义输入生日可以为‘3000-14-03’,用户性别值，前端居然给了‘未知选项’等等让人哭笑不得的数值错误，反而就是因为没有去检查数据质量，所以会导致在后面的编程过程中，要花费更多的时间去DEBUG，去写更多的容错逻辑。在很多数据挖掘的教学中，数据准则通常会列出来 7~8 项，在这里我们归类成了“完全合一” 4 项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。清洗数据一一击破了解了数据质量准则之后，我们针对上面服装店会员数据案例中的问题进行一一击破。这里你就需要 Python 的 Pandas 工具了。这个工具我们之前介绍过。它是基于 NumPy 的工具，专门为解决数据分析任务而创建。Pandas 纳入了大量库，我们可以利用这些库高效地进行数据清理工作。这里我补充说明一下，如果你对 Python 还不是很熟悉，但是很想从事数据挖掘、数据分析相关的工作，那么花一些时间和精力来学习一下 Python 是很有必要的。Python 拥有丰富的库，堪称数据挖掘利器。当然了，数据清洗的工具也还有很多，这里我们只是以 Pandas 为例，帮你应用数据清洗准则，带你更加直观地了解数据清洗到底是怎么回事儿。下面，我们就依照“完全合一”的准则，使用 Pandas 来进行清洗。完整性问题 1：缺失值在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：删除：删除数据缺失的记录；均值：使用当前列的均值；高频：使用当前列出现频率最高的数据。问题 2：空行我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。全面性问题：列数据的单位不统一观察 weight 列的数值，我们能发现 weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）合理性问题：非 ASCII 字符我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法唯一性问题 1：一列有多个参数在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname 两个字段。我们使用 Python 的 split 方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name 列删除。问题 2：重复数据我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。养成数据审核的习惯现在，你是不是能感受到数据问题不是小事，上面这个简单的例子里都有 6 处错误。所以我们常说，现实世界的数据是“脏的”，需要清洗。第三方的数据要清洗，自有产品的数据，也需要数据清洗。比如美团自身做数据挖掘的时候，也需要去除爬虫抓取，作弊数据等。可以说没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有“数据审核”的“职业病”。这就好比编辑非常在意文章中的错别字、语法一样。数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。当然了，这也是一门需要不断修炼的功夫。终有一天，你会进入这样一种境界：看一眼数据，差不多 7 秒钟的时间，就能知道这个数据是否存在问题。为了这一眼的功力，我们要做很多练习。刚开始接触数据科学工作的时候，一定会觉得数据挖掘是件很酷、很有价值的事。确实如此，不过今天我还要告诉你，再酷炫的事也离不开基础性的工作，就像我们今天讲的数据清洗工作。对于这些基础性的工作，我们需要耐下性子，一个坑一个坑地去解决。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据清洗","slug":"数据清洗","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"}]},{"title":"数据分析-如何采集数据","slug":"数据分析-如何采集数据","date":"2018-07-04T15:12:49.000Z","updated":"2020-05-01T14:17:54.345Z","comments":true,"path":"2018/07/04/数据分析-如何采集数据/","link":"","permalink":"cpeixin.cn/2018/07/04/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A6%82%E4%BD%95%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE/","excerpt":"","text":"如何对用户画像建模，而建模之前我们都要进行数据采集。数据采集是数据挖掘的基础，没有数据，也没有下一阶段的数据挖掘。很多时候，我们拥有多少数据源，多少数据量，以及数据质量如何，将决定我们挖掘产出的成果会怎样。有一句话 垃圾进，垃圾出， 指的就是你利用一批垃圾数据，那么挖掘出来的结论，也是如同垃圾一样，毫无价值。举个例子，你做量化投资，基于大数据预测未来股票的波动，根据这个预测结果进行买卖。你当前能够拿到以往股票的所有历史数据，是否可以根据这些数据做出一个预测率高的数据分析系统呢？实际上，如果你只有股票历史数据，你仍然无法理解股票为什么会产生大幅的波动。比如，当时可能是爆发了 SARS 疫情，或者某地区发生了战争等。这些重大的社会事件对股票的影响也是巨大的。因此我们需要考虑到，一个数据的走势，是由多个维度影响的。我们需要通过多源的数据采集，收集到尽可能多的数据维度，同时保证数据的质量，这样才能得到高质量的数据挖掘结果。数据源分类那么，从数据采集角度来说，都有哪些数据源呢？我将数据源分成了以下的四类这四类数据源包括了：开放数据源、爬虫抓取、传感器和日志采集。开放数据源一般是针对行业的数据库。比如美国人口调查局开放了美国的人口信息、地区分布和教育情况数据。除了政府外，企业和高校也会开放相应的大数据，这方面北美相对来说做得好一些。国内，贵州做了不少大胆尝试，搭建了云平台，逐年开放了旅游、交通、商务等领域的数据量。要知道很多研究都是基于开放数据源进行的，否则每年不会有那么多论文发表，大家需要相同的数据集才能对比出算法的好坏。爬虫抓取，一般是针对特定的网站或 App。如果我们想要抓取指定的网站数据，比如购物网站上的购物评价, 以及微博实时热点等，就需要我们做特定的爬虫抓取。第三类数据源是传感器，也就是目前比较火的IOT， 例如小米智能手环，智能体重秤，街边摄像头录下来的录像，它基本上采集的是物理信息。最后是日志采集，这个是统计用户的操作。我们可以在前端进行埋点，在后端进行脚本收集、统计，来分析网站的访问情况，以及使用瓶颈等。数据采集知道了有四类数据源，那如何采集到这些数据呢？收集开源数据我们先来看下开放数据源，教你个方法，开放数据源可以从两个维度来考虑，一个是单位的维度，比如政府、企业、高校；一个就是行业维度，比如交通、金融、能源等领域。这方面，国外的开放数据源比国内做得好一些，当然近些年国内的政府和高校做开放数据源的也越来越多。一方面服务社会，另一方面自己的影响力也会越来越大。比如，下面这张表格列举的就是单位维度的数据源。所以如果你想找某个领域的数据源，比如金融领域，你基本上可以看下政府、高校、企业是否有开放的数据源。当然你也可以直接搜索金融开放数据源。爬虫采集使用爬虫做抓取爬虫抓取应该属于最常见的需求，也是我最常用的获取数据的方式。比如你想要餐厅的评价数据。当然这里要注重版权问题，而且很多网站也是有反爬机制的。最直接的方法就是使用 Python 编写爬虫代码，当然前提是你需要会 Python 的基本语法。除此之外，PHP 也可以做爬虫，只是功能不如 Python 完善，尤其是涉及到多线程的操作。在 Python 爬虫中，基本上会经历三个过程。发请求，获取内容。使用 Requests 爬取内容。我们可以使用 Requests 库来抓取网页信息。Requests 库可以说是 Python 爬虫的利器，也就是 Python 的 HTTP 库，通过这个库爬取网页中的数据，非常方便，可以帮我们节约大量的时间。同时也可以使用目前比较成熟的爬虫框架 scrapy 来进行数据爬取。解析内容BeautifulSoup 是爬虫必学的技能。BeautifulSoup最主要的功能是从网页解析数据,其次还可以使用 XPath 解析内容。XPath 是 XML Path 的缩写，也就是 XML 路径语言。它是一种用来确定 XML 文档中某部分位置的语言，在开发中经常用来当作小型查询语言。XPath 可以通过元素和属性进行位置索引。保存数据根据你所爬取的数据量，可以选择不同的存储方式。如果你爬取的只是单次小型数据集，那么可以直接存储到txt,csv等文件中，也可以存储到，mysql数据库中，方便以后分析使用。如果你爬取的是大型数据集，可以使用 Pandas 保存数据。Pandas 是让数据分析工作变得更加简单的高级数据结构，我们可以用 Pandas 保存爬取的数据。最后通过 Pandas 再写入到 XLS 或者 MySQL 等数据库中。如果你的爬虫任务是长期执行的，那么你就要考虑好你要用什么存储工具来存储数据了 （eg: mongoDB??? elasticsearch ????）当然做 Python 爬虫还有很多利器，比如 Selenium，PhantomJS，或者用 Puppteteer 这种无头模式, 以上所说的三个工具，都是对付那些反爬虫比较复杂的数据源。日志采集为什么要做日志采集呢？日志采集最大的作用，就是通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，也可以方便技术人员基于用户实际的访问情况进行优化。日志采集也是运维人员的重要工作之一，那么日志都包括哪些呢，又该如何对日志进行采集呢？日志就是日记的意思，它记录了用户访问网站的全过程：哪些人在什么时间，通过什么渠道（比如搜索引擎、网址输入）来过，都执行了哪些操作；系统是否产生了错误；甚至包括用户的 IP、HTTP 请求的时间，用户代理等。这些日志数据可以被写在一个日志文件中，也可以分成不同的日志文件，比如访问日志、错误日志等。日志采集可以分两种形式通过 Web 服务器采集，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的 Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。自定义采集用户行为，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。埋点是什么？？埋点是日志采集的关键步骤，那什么是埋点呢？埋点就是在有需要的位置采集相应的信息，进行上报。比如某页面的访问情况，包括用户信息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。那我们要如何进行埋点呢？埋点就是在你需要统计数据的地方植入统计代码，当然植入代码可以自己写，也可以使用第三方统计工具。我之前讲到“不重复造轮子”的原则，一般来说需要自己写的代码，一般是主营核心业务，对于埋点这类监测性的工具，市场上已经比较成熟，这里推荐你使用第三方的工具，比如友盟、Google Analysis、Talkingdata 等。他们都是采用前端埋点的方式，然后在第三方工具里就可以看到用户的行为数据。但如果我们想要看到更深层的用户操作行为，就需要进行自定义埋点。总结一下，日志采集有助于我们了解用户的操作数据，适用于运维监控、安全审计、业务数据分析等场景。一般 Web 服务器会自带日志功能，也可以使用 Flume 从不同的服务器集群中采集、汇总和传输大容量的日志数据。当然我们也可以使用第三方的统计工具或自定义埋点得到自己想要的统计内容。传感器采集基本上是基于特定的设备，将设备采集的信息通过进行收集即可总结数据采集是数据分析的关键，很多时候我们会想到 Python 网络爬虫，实际上数据采集的方法、渠道很广，有些可以直接使用开放的数据源，比如想获取比特币历史的价格及交易数据，可以直接从 Kaggle 上下载，不需要自己爬取。另一方面根据我们的需求，需要采集的数据也不同，比如交通行业，数据采集会和摄像头或者测速仪有关。对于运维人员，日志采集和分析则是关键。所以我们需要针对特定的业务场景，选择适合的采集工具。今天我讲了数据采集的不同渠道以及相关的工具。给你留一个思考题，假如你想预测比特币的未来走势，都需要哪些维度的数据源呢？怎样收集到它们呢？","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据采集","slug":"数据采集","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"}]},{"title":"数据分析-用户画像项目","slug":"数据分析-用户画像项目","date":"2018-07-03T14:32:06.000Z","updated":"2020-05-01T14:16:35.499Z","comments":true,"path":"2018/07/03/数据分析-用户画像项目/","link":"","permalink":"cpeixin.cn/2018/07/03/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"用户标签： 性别，年龄，职业，星座，生活城市，家乡，用户注册渠道，邀请码消费标签： 存款，领取优惠金额，参与活动，会员星级，使用信用卡次数，储蓄卡使用次数， 数字货币使用次数，购买装备金额行为标签： 登录次数，游戏次数，在线时长，游戏时长，行为路径，用户点击热点，（搜索补充）内容标签： 论坛留言，浏览优惠活动查看，游戏聊天（搜索补充）针对单个用户的画像分析。针对某一属性群体的画像分析。平均值，最大值，最小值，众数，中位数，四分之三位数，六分之一位数 等数值在在实际中代表的意义。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"}]},{"title":"数据分析-用户画像标签","slug":"数据分析-用户画像标签","date":"2018-07-02T14:31:58.000Z","updated":"2020-05-01T14:16:55.619Z","comments":true,"path":"2018/07/02/数据分析-用户画像标签/","link":"","permalink":"cpeixin.cn/2018/07/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E6%A0%87%E7%AD%BE/","excerpt":"","text":"王兴说过，我们已经进入到互联网的下半场。在上半场，也就是早期的互联网时代，你永远不知道在对面坐的是什么样的人。那个年代大部分人还是 QQ 的早期用户。在下半场，互联网公司已经不新鲜了，大部分公司已经互联网化。他们已经在用网络进行产品宣传，使用电商销售自己的商品。这两年引领下半场发展的是那些在讲 “大数据”“赋能”的企业，他们有数据，有用户。通过大数据告诉政府该如何智慧地管理交通，做城市规划。通过消费数据分析，告诉企业该在什么时间生产什么产品，以最大化地满足用户的需求。通过生活大数据告诉我们餐饮企业，甚至房地产企业该如何选址。如果说互联网的上半场是粗狂运营，因为有流量红利不需要考虑细节。那么在下半场，精细化运营将是长久的主题。有数据，有数据分析能力才能让用户得到更好的体验。所以，用户是根本，也是数据分析的出发点。假如你进入到一家卖羊肉串的餐饮公司，老板说现在竞争越来越激烈，要想做得好就要明白顾客喜欢什么。于是上班第一天，老板问你：“你能不能分析下用户数据，给咱们公司的业务做个赋能啊？”听到这，你会怎么想？你说：“老板啊，咱们是卖羊肉串的，做数据挖掘没用啊。”估计老板听后，晚上就把你给开了。那该怎么办呢？如果你感觉一头懵，没关系，我们今天就来讲讲怎么一步步分析用户数据。用户画像的准则首先就是将自己企业的用户画像做个白描，告诉他这些用户“都是谁”“从哪来”“要去哪”。你可以这么和老板说：“老板啊，用户画像建模是个系统的工程，我们要解决三个问题。第一呢，就是用户从哪里来，这里我们需要统一标识用户 ID，方便我们对用户后续行为进行跟踪。我们要了解这些羊肉串的用户从哪里来，也就是所谓的用户来源渠道。他们是为了聚餐，还是自己吃宵夜，这些场景我们都要做统计分析。第二呢，这些用户是谁？这里指的是 用户生日，职业，性别，年龄，收入等基本信息， 我们需要对这些用户进行标签化，方便我们对用户行为进行理解。第三呢，就是用户要到哪里去？我们要将这些用户画像与我们的业务相关联，提升我们的转化率，或者降低我们的流失率。”听到这，老板给你竖起了大拇指，说：“不错，都需要什么资源，随时找我就行。”刚才说的这三个步骤，下面我一一给你做个梳理。首先，为什么要设计唯一标识？用户唯一标识是整个用户画像的核心。我们以一个 App 为例，它把“从用户开始使用 APP 到下单到售后整个所有的用户行为”进行串联，因为一个用户在这个APP中没操作一项，都可能会产生一条单独的数据，所以在后续的分析过程中，需要利用用户的唯一标识去抓数据，统计数据。这样就可以更好地去跟踪和分析一个用户的特征。设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。其次，给用户打标签。你可能会想，标签有很多，且不同的产品，标签的选择范围也不同，这么多的标签，怎样划分才能既方便记忆，又能保证用户画像的全面性呢？这里我总结了八个字，叫“用户消费行为分析”。我们可以从这 4 个维度来进行标签划分。用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。最后，当你有了用户画像，可以为企业带来什么业务价值呢？我们可以从用户生命周期的三个阶段来划分业务价值，包括：拉新、用户粘度和留存：如何进行拉新，通过更精准的营销获取客户。用户粘度：个性化推荐，搜索排序，场景运营等。留存：流失率预测，分析关键节点降低流失率。如果按照数据流处理的阶段来划分用户画像建模的过程，可以分为数据层、算法层和业务层。你会发现在不同的层，都需要打上不同的标签。数据层指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录。算法层指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识。业务层指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果。所以这个标签化的流程，就是通过数据层的“事实标签”，在算法层进行计算，打上“模型标签”的分类结果，最后指导业务层，得出“预测标签”。美团外卖的用户画像该设计刚才讲的是用户画像的三个阶段，以及每个阶段的准则。下面，我们来使用这些准则做个练习。如果你是美团外卖的数据分析师，你该如何制定用户标识 ID，制定用户画像，以及基于用户画像可以做哪些业务关联？首先，我们先回顾下美团外卖的产品背景。美团已经和大众点评进行了合并，因此在大众点评和美团外卖上都可以进行外卖下单。另外美团外卖针对的是高频 O2O 的场景，美团外卖是美团的核心产品，基本上有一半的市值都是由外卖撑起来的。基于用户画像实施的三个阶段，我们首先需要统一用户的唯一标识，那么究竟哪个字段可以作为用户标识呢？我们先看下美团和大众点评都是通过哪些方式登录的。我们看到，美团采用的是手机号、微信、微博、美团账号的登录方式。大众点评采用的是手机号、微信、QQ、微博的登录方式。这里面两个 APP 共同的登录方式都是手机号、微信和微博。那么究竟哪个可以作为用户的唯一标识呢？当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。当然，大家知道在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，尤其是多条产品线，在数据整合的过程中，会出现各种各样的数据格式问题，数据精度，数据的空值处理等。所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。然后我们思考下，有了用户，用户画像都可以统计到哪些标签。我们按照“用户消费行为分析”的准则来进行设计。用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。当你有了“用户消费行为分析”的标签之后，你就可以更好地理解业务了。比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。有了这些数据，我们就可以预测用户的行为。比如一个用户购买了“月子餐”后，更有可能购买婴儿水，同样婴儿相关的产品比如婴儿湿巾等的购买概率也会增大。具体在业务层上，我们都可以基于标签产生哪些业务价值呢？在拉新上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。在用户粘度上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。在留存上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。锻炼自己的抽象能力，将繁杂的事务简单化上面我们讲到的“用户消费行为标签”都是基于一般情况考虑的，除此之外，用户的行为也会随着营销的节奏产生异常值，比如双十一的时候，如果商家都在促销就会产生突发的大量订单。因此在做用户画像的时候，还要考虑到异常值的处理。总之，数据量是庞大的，会存在各种各样的使用情况。光是分析 EB 级别的大数据，我们就要花很长的时间。但我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。对数据的标签化能让我们快速理解一个用户，一个商品，乃至一个视频内容的特征，从而方便我们去理解和使用数据。对数据的标签化其实考验的是我们的抽象能力，在日常工作中，我们也要锻炼自己的抽象能力，它可以让我们很快地将一个繁杂的事物简单化，不仅方便理解，还有益后续的使用。我们今天讲了用户画像的流程，其中很重要的一个步骤就是给用户打标签，那么你不妨想想，如果给羊肉串连锁店进行用户画像分析，都可以从哪些角度进行标签化？最后，我们从现实生活中出发，打开你的手机，翻翻看你的微信通讯录，分析下你的朋友圈，都有哪些用户画像？如果你来给它设计标签，都有哪些种类需要统计呢。为了方便后续使用，你是如何将他们归类分组的？关于朋友圈的用户画像，我们按照 “用户消费行为习惯” 来设定标签的话，大体可以参考如下：用户标签:用户性别、用户年龄、用户所处位置、用户家乡、用户学历、用户角色、用户来源渠道消费标签：朋友圈广告点击情况、用户参与活动、使用小程序的类型行为标签：朋友圈发布频次、朋友圈可见设置、朋友圈点赞次数、朋友圈评论次数、朋友圈文字浏览次数、朋友圈权限设置内容分析：浏览文字类型、文字转发类型","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"}]},{"title":"数据分析-基本概念","slug":"数据分析-基本概念","date":"2018-07-01T14:31:58.000Z","updated":"2020-05-01T14:19:10.225Z","comments":true,"path":"2018/07/01/数据分析-基本概念/","link":"","permalink":"cpeixin.cn/2018/07/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"前言美国明尼苏达州一家 Target 百货被客户投诉，这名客户指控 Target 将婴儿产品优惠券寄给他的女儿，而他女儿还是一名高中生。但没多久这名客户就来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。Target 百货寄送婴儿产品优惠券绝非偶然之举，他们发现妇女在怀孕的情况下，购买的物品会发生变化，比如护手霜会从有香味的改成无味的，此外还会购买大量维生素等保健品。通过类似的关联分析，Target 构建了一个“怀孕预测指数”，通过这个指数预测到了顾客已经怀孕的情况，并把优惠券寄送给她。那么顾客怀孕与商品之间的关联关系是如何被发现的呢？实际上他们都是用的 Apriori 算法，该算法是由美国学者 Agrawal 在 1994 年提出的。他通过分析购物篮中的商品集合，找出商品之间的关联关系。利用这种隐性关联关系，商家就可以强化这类购买行为，从而提升销售额。这就是数据分析的力量，人们总是从数据分析中得到有价值的信息，啤酒和尿布的故事也是个经典的案例。如今在超市中，我们还能看到不少组合的套装打包在一起卖，比如宝洁的产品：飘柔洗发水 + 玉兰油沐浴露、海飞丝洗发水 + 舒肤佳沐浴露等等。商品的捆绑销售是个很有用的营销方式，背后都是数据分析在发挥作用。商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系开头中的百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。所以我们能在这个场景里看到三个重要的概念：商业智能、数据仓库和数据挖掘。商业智能的英文是 Business Intelligence，缩写是 BI。相比于数据仓库、数据挖掘，它是一个更大的概念。商业智能可以说是基于数据仓库，经过了数据挖掘后，得到了商业价值的过程。所以说数据仓库是个金矿，数据挖掘是炼金术，而商业报告则是黄金。数据仓库的英文是 Data Warehouse，缩写是 DW。它可以说是 BI 这个房子的地基，搭建好 DW 这个地基之后，才能进行分析使用，最后产生价值。数据仓库可以说是数据库的升级概念。数据仓库是企业级别的，而数据库则是业务系统和单个项目中所应用到的技术，从逻辑上理解，数据库和数据仓库没有什么区别，都是通过数据库技术来存储数据的。不过从数量上来讲，数据仓库的量更庞大，适用于数据挖掘和数据分析。数据库可以理解是一项技术。数据仓库将原有的多个数据来源中的数据进行汇总、整理而得。数据进入数据仓库前，必须消除数据中的不一致性，方便后续进行数据分析和挖掘。数据挖掘的英文是 Data Mining，缩写是 DM。在商业智能 BI 中经常会使用到数据挖掘技术。数据挖掘的核心包括分类、聚类、预测、关联分析等任务，通过这些炼金术，我们可以从数据仓库中得到宝藏，比如商业报告。很多时候，企业老板总是以结果为导向，他们认为商业报告才是他们想要的，但是这也是需要经过地基 DW、搬运工 ETL、科学家 DM 等共同的努力才得到的。元数据 VS 数据元我们前面提到了数据仓库，在数据仓库中，还有一类重要的数据是元数据，那么它和数据元有什么区别呢？元数据（MetaData）：描述其它数据的数据，也称为“中介数据”。数据元（Data Element）：就是最小数据单元。在生活中，只要有一类事物，就可以定义一套元数据。举个例子，比如一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。在图书这个元数据中，书名、作者、出版社就是数据元。你可以理解是最小的数据单元。元数据最大的好处是使信息的描述和分类实现了结构化，让机器处理起来很方便。元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息。而且整个数据仓库的运行都是基于元数据的，比如抽取调度数据、获取历史数据等。通过元数据，可以很方便地帮助我们管理数据仓库。数据挖掘的流程聊完了数据仓库，我们再来谈谈数据挖掘。数据挖掘不是凭空产生的，它与数据库技术的发展分不开。数据挖掘的一个英文解释叫 Knowledge Discovery in Database，简称 KDD，也就是数据库中的知识发现。在数据挖掘中，有几个非常重要的任务，就是分类、聚类、预测和关联分析。我来解释下这些概念。分类就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。这里需要说明下训练集和测试集的概念。一般来说数据可以划分为训练集和测试集。训练集是用来给机器做训练的，通常是人们整理好训练数据，以及这些数据对应的分类标识。通过训练，机器就产生了自我分类的模型，然后机器就可以拿着这个分类模型，对测试集中的数据进行分类预测。同样如果测试集中，人们已经给出了测试结果，我们就可以用测试结果来做验证，从而了解分类器在测试环境下的表现。聚类人以群分，物以类聚。聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。预测顾名思义，就是通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。关联分析就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。比如我们开头提到的那个案例。数据挖掘要怎么完成这些任务它需要将数据库中的数据经过一系列的加工计算，最终得出有用的信息。这个过程可以用以下步骤来描述。首先，输入我们收集到的数据，然后对数据进行预处理。预处理通常是将数据转化成我们想要的格式，然后我们再对数据进行挖掘，最后通过后处理得到我们想要的信息。那你可能想问，为什么不直接进行数据挖掘，还要进行数据预处理呢？因为在这个过程中，输入的数据通常是从不同渠道采集而来的，所以数据的格式以及质量是参差不齐的，所以我们需要对数据进行预处理。数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。数据清洗主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。数据集成是将多个数据源中的数据存放在一个统一的数据存储中。数据变换就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 01 之间。我会在后面的几节课给你讲解如何对数据进行预处理。数据后处理是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到的是 01 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"NLP word vector - word2vec","slug":"NLP-word-vector-word2vec","date":"2018-06-25T14:07:35.000Z","updated":"2020-04-04T11:05:38.947Z","comments":true,"path":"2018/06/25/NLP-word-vector-word2vec/","link":"","permalink":"cpeixin.cn/2018/06/25/NLP-word-vector-word2vec/","excerpt":"","text":"word vectorFirstly, in recent years, word vector is the basic knowledge of NLP, which is also very important.Among them, words or phrases from vocabulary are mapped to vectors of real Numbers, which is to translate human language symbols into Numbers that can be calculated by machines, which can improve the quality of machine translation.For example, word_1 is expressed as a vector [0,0,1] in articles and statements. There are many models or tools for word-to-vector transformation. Eg: one-hot, n-gram, word2vecword2vecWord2vec is a toolWord2vec contains two models skp-gram and CBOW, as well as two efficient training methods negative sampling and hiearchical softmax. Why introduce word2vec separately, because it can well express the similarity and analogy between different words顺便说说这两个语言模型。统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。","categories":[{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"cpeixin.cn/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"}]},{"title":"python - 优雅的程序设计","slug":"优雅的程序设计-python「工匠」","date":"2018-05-24T15:01:20.000Z","updated":"2020-04-04T12:00:22.701Z","comments":true,"path":"2018/05/24/优雅的程序设计-python「工匠」/","link":"","permalink":"cpeixin.cn/2018/05/24/%E4%BC%98%E9%9B%85%E7%9A%84%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-python%E3%80%8C%E5%B7%A5%E5%8C%A0%E3%80%8D/","excerpt":"","text":"今天是优秀博客的搬运工😂记录两篇看完很爽的文章，编程也是一门手艺，也可以说是一门艺术，编程的人要把自己当成工匠，对自己的作品不断的打磨，打磨成一个可以供人观赏的艺术品。Python 工匠：善用变量来改善代码质量Python 工匠：编写条件分支代码的技巧","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"Flink 数据输出 Sink","slug":"Flink-数据输出-Sink","date":"2018-05-09T18:07:42.000Z","updated":"2020-05-22T15:23:46.924Z","comments":true,"path":"2018/05/10/Flink-数据输出-Sink/","link":"","permalink":"cpeixin.cn/2018/05/10/Flink-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA-Sink/","excerpt":"","text":"在Flink中，主要分为source，transform，sink三个部分，前面我们使用Flink利用封装好的方法对接Kafka，就是属于source部分，那么在sink阶段也是一样，我们只能通过定义好的sink方法，将数据落地到我们想存储的地方，这里虽看看起来没有Spark那么灵活，要么foreachRDD，foreachPartition遍历输出，或者是第三方组件方法进行输出，但是Flink这样设计更有利于工程师降低代码复杂度，更多精力去关心业务开发就可以。一些比较基本的 Sink 已经内置在 Flink 里。数据接收器使用DataStream，并将其转发到文件，套接字，外部系统或打印它们。Flink带有各种内置的输出格式，这些格式封装在DataStream的操作后面：writeAsText()/ TextOutputFormat-将元素按行写为字符串。通过调用每个元素的toString（）方法获得字符串。writeAsCsv(…)/ CsvOutputFormat-将元组写为逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的toString（）方法。print()/ printToErr() - 在标准输出/标准错误流上打印每个元素的toString（）值。可选地，可以提供前缀（msg），该前缀在输出之前。这可以帮助区分不同的打印调用。如果并行度大于1，则输出之前还将带有产生输出的任务的标识符。writeUsingOutputFormat()/ FileOutputFormat-自定义文件输出的方法和基类。支持自定义对象到字节的转换。writeToSocket -根据以下内容将元素写入套接字 SerializationSchemaaddSink-调用自定义接收器功能。Flink捆绑有连接到其他系统（例如Apache Kafka）的连接器，这些连接器实现为接收器功能。请注意，上的write*()方法DataStream主要用于调试目的。它们不参与Flink的检查点，这意味着这些功能通常具有至少一次的语义。刷新到目标系统的数据取决于OutputFormat的实现。这意味着并非所有发送到OutputFormat的元素都立即显示在目标系统中。同样，在失败的情况下，这些记录可能会丢失。为了将流可靠，准确地一次传输到文件系统中，请使用flink-connector-filesystem。同样，通过该.addSink(…)方法的自定义实现可以参与Flink一次精确语义的检查点。Sink原理SinkFunction 是一个接口，类似于SourceFunction接口。SinkFunction中主要包含一个方法，那就是用于数据输出的invoke 方法,每条记录都会执行一次invoke方法，用于执行输出操作。123456789101112131415// Writes the given value to the sink. This function is called for every record.default void invoke(IN value) throws Exception default void invoke(IN value, Context context) throws Exception // Context接口中返回关于时间的信息interface Context&lt;T&gt; &#123; /** Returns the current processing time. */ long currentProcessingTime(); /** Returns the current event-time watermark. */ long currentWatermark(); /** * Returns the timestamp of the current input record or &#123;@code null&#125; if the element does not * have an assigned timestamp. */ Long timestamp(); &#125;我们一般自定义Sink的时候，都是继承AbstractRichFunction，他是一个抽象类，实现了RichFunction接口。1public abstract class AbstractRichFunction implements RichFunction, Serializable并且提供了关于RuntimContext的操作和open，clone方法。AbstractRichFunction 有很多实现类，比如关于msyql操作的JDBCSinkFunction，比如直接输出结果的 PrintSinkFunction，在我们开发的过程中，我们进程用print语句来打印结果，但是print函数中就是讲PrintSinkFunction类传递到addSink方法中。1234public DataStreamSink&lt;T&gt; print() &#123; PrintSinkFunction&lt;T&gt; printFunction = new PrintSinkFunction&lt;&gt;(); return addSink(printFunction).name(\"Print to Std. Out\");&#125;PrintSinkFunction我们这里分析一下PrintSinkFunction这个类，这个类就是将没个元素输出到标准输出或者是标准错误输出流中。1234567891011121314151617181920212223242526272829303132333435363738394041public class PrintSinkFunction&lt;IN&gt; extends RichSinkFunction&lt;IN&gt; &#123; private static final long serialVersionUID = 1L; private final PrintSinkOutputWriter&lt;IN&gt; writer; /** * Instantiates a print sink function that prints to standard out. */ public PrintSinkFunction() &#123; writer = new PrintSinkOutputWriter&lt;&gt;(false); &#125; /** * Instantiates a print sink function that prints to standard out. * * @param stdErr True, if the format should print to standard error instead of standard out. */ public PrintSinkFunction(final boolean stdErr) &#123; writer = new PrintSinkOutputWriter&lt;&gt;(stdErr); &#125; /** * Instantiates a print sink function that prints to standard out and gives a sink identifier. * * @param stdErr True, if the format should print to standard error instead of standard out. * @param sinkIdentifier Message that identify sink and is prefixed to the output of the value */ public PrintSinkFunction(final String sinkIdentifier, final boolean stdErr) &#123; writer = new PrintSinkOutputWriter&lt;&gt;(sinkIdentifier, stdErr); &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext(); writer.open(context.getIndexOfThisSubtask(), context.getNumberOfParallelSubtasks()); &#125; @Override public void invoke(IN record) &#123; writer.write(record); &#125; @Override public String toString() &#123; return writer.toString(); &#125;&#125;分析：1、调用构造函数来创建一个PrintSinkOutputWriter2、调用open方法中在调用PrintSinkOutputWriter 的open方法，进行初始化3、调用invoke方法，通过PrintSinkOutputWriter 的writer方法吧record输出第三方Sink下面我们将使用 Elasticsearch Connector 作为Sink 为例示范Sink的使用。Elasticsearch Connector 提供了 *at least once *语义支持，at lease once 支持需要用到Flink的checkpoint 机制。要使用Elasticsearch Connector 需要根据Elasticsearch 版本添加依赖，版本参考12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;ES版本6.7， 写入示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package data_streamimport java.net.&#123;InetAddress, InetSocketAddress&#125;import java.utilimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.api.common.functions.RuntimeContextimport org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.elasticsearch.util.IgnoringFailureHandlerimport org.apache.flink.streaming.connectors.elasticsearch.&#123;ActionRequestFailureHandler, ElasticsearchSinkFunction, RequestIndexer&#125;import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.ExceptionUtilsimport org.apache.http.HttpHostimport org.elasticsearch.ElasticsearchParseExceptionimport org.elasticsearch.action.ActionRequestimport org.elasticsearch.action.index.IndexRequestimport org.elasticsearch.client.Requestsimport org.elasticsearch.common.util.concurrent.EsRejectedExecutionExceptionimport utils.KafkaUtilobject datastream_2_es_HandFail &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val httpHosts = new java.util.ArrayList[HttpHost] httpHosts.add(new HttpHost(\"127.0.0.1\", 9200, \"http\")) val esSinkBuilder = new ElasticsearchSink.Builder[Raw] ( httpHosts, new ElasticsearchSinkFunction[Raw] &#123; override def process(data: Raw, runtimeContext: RuntimeContext, requestIndexer: RequestIndexer): Unit = &#123; print(\"saving data\" + data) //包装成一个Map或者JsonObject val hashMap = new util.HashMap[String, String]() hashMap.put(\"date_time\", data.date_time) hashMap.put(\"keyword_list\", data.keywordList) hashMap.put(\"rowkey\", \"i am rowkey haha\") //创建index request,准备发送数据 val indexRequest: IndexRequest = Requests.indexRequest().index(\"weibo_keyword-2018-04-30\").`type`(\"default\").source(hashMap) //发送请求,写入数据 requestIndexer.add(indexRequest) println(\"data saved successfully\") &#125; &#125; ) esSinkBuilder.setBulkFlushMaxActions(2) esSinkBuilder.setBulkFlushInterval(1000L) // 自定义异常处理 esSinkBuilder.setFailureHandler(new ActionRequestFailureHandler &#123; override def onFailure(actionRequest: ActionRequest, throwable: Throwable, i: Int, requestIndexer: RequestIndexer): Unit = &#123; println(\"@@@@@@@On failure from ElasticsearchSink:--&gt;\" + throwable.getMessage) &#125; &#125;) word_stream.addSink(esSinkBuilder.build()) env.execute(\"Flink Streaming—————es sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;自定义Sinkmysql在向MySQL中写数据的时候，千万不要忘记引入驱动依赖：12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt;&lt;/dependency&gt;示例代码：自定义Sink123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.sql.&#123;Connection, DriverManager, PreparedStatement, SQLException&#125;import data_stream.datastream_2_mysql.Rawimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;import org.slf4j.&#123;Logger, LoggerFactory&#125;class MysqlSink_v2 extends RichSinkFunction[Raw] &#123; val logger: Logger = LoggerFactory.getLogger(\"MysqlSink\") var conn: Connection = _ var ps: PreparedStatement = _ val jdbcUrl = \"jdbc:mysql://localhost:3306?useSSL=false&amp;allowPublicKeyRetrieval=true\" val username = \"root\" val password = \"password\" val driverName = \"com.mysql.jdbc.Driver\" override def open(parameters: Configuration): Unit = &#123; Class.forName(driverName) try &#123; Class.forName(driverName) conn = DriverManager.getConnection(jdbcUrl, username, password) // close auto commit conn.setAutoCommit(false) &#125; catch &#123; case e@(_: ClassNotFoundException | _: SQLException) =&gt; logger.error(\"init mysql error\") e.printStackTrace() System.exit(-1); &#125; &#125; /** * 吞吐量不够话，可以将数据暂存在状态中，批量提交的方式提高吞吐量（如果oom，可能就是数据量太大，资源没有及时释放导致的） * @param raw * @param context */ override def invoke(raw: Raw, context: SinkFunction.Context[_]): Unit = &#123; println(\"data : \" + raw.toString) ps = conn.prepareStatement(\"insert into test.t_weibo_keyword(date_time,keywordList) values(?,?)\") ps.setString(1, raw.date_time) ps.setString(2, raw.keywordList) ps.execute() conn.commit() &#125; override def close(): Unit = &#123; if (conn != null)&#123; conn.commit() conn.close() &#125; &#125;&#125;主程序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport utils.KafkaUtilobject datastream_2_mysql &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val sink = new MysqlSink_v2() word_stream.addSink(sink) env.execute(\"Flink Streaming—————mysql sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;HBase自定义sink1234567891011121314151617181920212223242526272829303132333435363738package data_streamimport data_stream.datastream_2_hbase.Rawimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;import org.apache.hadoop.hbase.&#123;HBaseConfiguration, HConstants, TableName&#125;import org.apache.hadoop.hbase.client._import org.apache.hadoop.hbase.util.Bytesclass HBaseSink_v4(tableName: String, family: String) extends RichSinkFunction[Raw] &#123; var conn: Connection = _ override def open(parameters: Configuration): Unit = &#123; super.open(parameters) val conf = HBaseConfiguration.create() conf.set(HConstants.ZOOKEEPER_QUORUM, \"localhost:2181\") conn = ConnectionFactory.createConnection(conf) &#125; override def invoke(value: Raw, context: SinkFunction.Context[_]): Unit = &#123; println(value) val t: Table = conn.getTable(TableName.valueOf(tableName)) val put: Put = new Put(Bytes.toBytes(value.date_time)) put.addColumn(Bytes.toBytes(family), Bytes.toBytes(\"name\"), Bytes.toBytes(value.date_time.toString)) put.addColumn(Bytes.toBytes(family), Bytes.toBytes(\"id_no\"), Bytes.toBytes(value.keywordList.toString)) t.put(put) t.close() &#125; override def close(): Unit = &#123; super.close() conn.close() &#125;&#125;主程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.datastream.DataStreamSinkimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport utils.KafkaUtilobject datastream_2_hbase &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer_hbase\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) env.enableCheckpointing(5000) env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource/hbase\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic).setStartFromLatest() val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val value: DataStreamSink[Raw] = word_stream.addSink(new HBaseSink_v4(\"t_weibo_keyword_2\",\"cf1\")).name(\"write_2_hbase\") env.execute(\"Flink Streaming—————hbase sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;Redis12345&lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;创建Sink工具类123456789101112131415161718192021222324package utilsimport data_stream.datastream_2_redis.Rawimport org.apache.flink.streaming.connectors.redis.RedisSinkimport org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfigimport org.apache.flink.streaming.connectors.redis.common.mapper.&#123;RedisCommand, RedisCommandDescription, RedisMapper&#125;object RedisUtil &#123; private val config: FlinkJedisPoolConfig = new FlinkJedisPoolConfig.Builder().setHost(\"127.0.0.1\").setPort(6379).build() def getRedisSink(): RedisSink[Raw] = &#123; new RedisSink[Raw](config, new MyRedisMapper) &#125; class MyRedisMapper extends RedisMapper[Raw] &#123; override def getCommandDescription: RedisCommandDescription = &#123; new RedisCommandDescription(RedisCommand.HSET, \"weibo_keyword\") &#125; // value override def getValueFromData(t: Raw): String = t.keywordList //key override def getKeyFromData(t: Raw): String = t.date_time &#125;&#125;主程序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.streaming.connectors.redis.RedisSinkimport utils.&#123;KafkaUtil, RedisUtil&#125;object datastream_2_redis &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) word_stream.print(\"2_redis\").setParallelism(1) val redis_sink: RedisSink[Raw] = RedisUtil.getRedisSink() word_stream.addSink(redis_sink).name(\"write_2_redis\") env.execute(\"Flink Streaming—————redis sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Mac 安装 redis","slug":"Mac-安装-redis","date":"2018-04-30T13:53:24.000Z","updated":"2020-05-22T15:22:30.207Z","comments":true,"path":"2018/04/30/Mac-安装-redis/","link":"","permalink":"cpeixin.cn/2018/04/30/Mac-%E5%AE%89%E8%A3%85-redis/","excerpt":"","text":"Redis安装下载软件包访问 https://redis.io/download 下载 Stable 版本或者1wget http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-6.0.3.tar.gz解压软件包1tar -zxvf redis-6.0.3.tar.gz移动到指定目录1sudo mv redis-6.0.3 &#x2F;usr&#x2F;local&#x2F;编译redis1sudo make编译test1sudo make test安装1sudo make install运行1redis-server配置后台启动找到redis.conf 并修改 daemonize no 为 daemonize yes ，这样就可以默认启动就后台运行1redis-server &#x2F;usr&#x2F;local&#x2F;redis-6.0.3&#x2F;redis.confRedis Desktop Manager安装下载 Redis Desktop Manager可以去官网下载，也可以去RedisDesktopManager下载免费tar包自己编译，而我选择了百度网盘下载😄https://pan.baidu.com/s/1tpnvkE9R63U9VVMfw5xodQ提取码：zd7y","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"}]},{"title":"Flink 合并流 connect & union","slug":"Flink-合并流-connect-union","date":"2018-04-25T13:30:11.000Z","updated":"2020-05-17T05:17:59.071Z","comments":true,"path":"2018/04/25/Flink-合并流-connect-union/","link":"","permalink":"cpeixin.cn/2018/04/25/Flink-%E5%90%88%E5%B9%B6%E6%B5%81-connect-union/","excerpt":"","text":"Flink 合并流 connect &amp; union* connect &amp; union(合并流)**connect之后生成ConnectedStreams，会对两个流的数据应用不同的处理方法，并且双流 之间可以共享状态(比如计数)。这在第一个流的输入会影响第二个流时, 会非常有用;union 合并多个流，新的流包含所有流的数据。union是DataStream → DataStream。connect是DataStream* → ConnectedStreams。connect只能连接两个流，而union可以连接多于两个流 。connect连接的两个流类型可以不一致，但是合并数据的数据类型要一致，而union连接的流的类型必须一致。connect算子后面跟的map 和 flatMap和普通的map, flatmap类似，只不过作用在ConnectedStreams上会改变流的类型，由ConnectedStreams → DataStream下面的实例，是以sideoutput那篇文章的基础上，对主数据流和侧出流进行connect，代码中也给出了union合并流方法，unino方法使用起来则很简单，并且和spark streaming中union的用法是一样的1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.datastream.DataStreamSinkimport org.apache.flink.streaming.api.functions.ProcessFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.Collectorimport utils.KafkaUtilobject connect_datastream &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val processStream: DataStream[Raw] = word_stream.process(new SideOutput()) val dirty_stream: DataStream[Raw] = processStream.getSideOutput(new OutputTag[Raw](\"dirty_data\"))// val connect_datastream: DataStream[String] = processStream.connect(dirty_stream)// .map(// (originalRaw: Raw) =&gt; originalRaw.keywordList,// (dirtyRaw: Raw) =&gt; dirtyRaw.keywordList// )//// connect_datastream.print(\"ALL \") val union_datastream: DataStream[Raw] = processStream.union(dirty_stream) union_datastream.print(\"union_datastream \") env.execute(\"connect stream test\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125; class SideOutput() extends ProcessFunction[Raw, Raw] &#123; //定义一个侧输出流标签 lazy val dirty_data: OutputTag[Raw] = new OutputTag[Raw](\"dirty_data\") override def processElement(value: Raw, ctx: ProcessFunction[Raw, Raw]#Context, out: Collector[Raw]): Unit = &#123; if (value.keywordList == \"dirty_data\") &#123; ctx.output(dirty_data, value) &#125; else &#123; out.collect(value) &#125; &#125; &#125;&#125;打印结果：1234567891011121314151617181920212223242526ALL :8&gt; dirty_dataALL :7&gt; 网购翻车的经历,经历ALL :1&gt; 彭昱畅的自拍和他拍,彭昱畅ALL :7&gt; dirty_dataALL :1&gt; 彭昱畅的自拍和他拍,彭昱畅ALL :7&gt; dirty_dataALL :1&gt; 网购翻车的经历,经历ALL :7&gt; dirty_dataALL :1&gt; 网购翻车的经历,经历ALL :8&gt; dirty_dataALL :7&gt; 痛仰演唱会,演唱会ALL :1&gt; 痛仰演唱会,演唱会ALL :8&gt; dirty_dataALL :1&gt; dirty_dataALL :7&gt; dirty_dataALL :1&gt; 痛仰演唱会,演唱会ALL :1&gt; dirty_dataALL :8&gt; dirty_dataALL :7&gt; 痛仰演唱会,演唱会ALL :8&gt; dirty_dataALL :1&gt; 痛仰演唱会,演唱会ALL :8&gt; dirty_dataALL :1&gt; dirty_dataALL :7&gt; dirty_dataALL :1&gt; 幸福触手可及片花,片花ALL :7&gt; 幸福触手可及片花,片花","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 侧输出流SideOutput","slug":"Flink-侧输出流SideOutput","date":"2018-04-20T13:30:11.000Z","updated":"2020-05-15T15:51:24.179Z","comments":true,"path":"2018/04/20/Flink-侧输出流SideOutput/","link":"","permalink":"cpeixin.cn/2018/04/20/Flink-%E4%BE%A7%E8%BE%93%E5%87%BA%E6%B5%81SideOutput/","excerpt":"","text":"在Spark和Flink的流式处理方面，有很多相似之处，例如map()，flatmap()等算子使用方法都是相似的。那这里我举出一个在Spark Streaming中没有的一个功能，侧出流SideOutput。在Flink以前的版本中，是使用split()算子来实现这个功能的，但是目前1.4官方不推荐使用，在编译器中也被标示过时方法，推荐使用sideoutput。具体能实现的功能是将一个流数据，按照你自定义的规则，在流数据内部来分成一个或者多个流，并且同时输出给你。在Spark Streaming中，如果我们想要对一个流数据进行分割，我们需要对一个流数据分别做两次filter算子，这样就会进行数据复制，相对来说耗用资源更高一些。flink中的侧输出就是将数据流进行分割，而不对流进行复制的一种分流机制。flink的侧输出的另一个作用就是对延时迟到的数据进行处理，这样就可以不必丢弃迟到的数据。实例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package data_streamimport java.util.Propertiesimport org.apache.flink.streaming.api.scala._import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.functions.ProcessFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.Collectorimport utils.KafkaUtilobject sideoutput_datastream &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val processStream: DataStream[Raw] = word_stream.process(new SideOutput()) processStream.print(\"original data\") //通过getSideOutput获取侧输出流，并打印输出 processStream.getSideOutput(new OutputTag[String](\"dirty_data\")).print(\"side output data\") env.execute(\"side output test\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125; class SideOutput() extends ProcessFunction[Raw, Raw] &#123; //定义一个侧输出流标签 lazy val dirty_data: OutputTag[String] = new OutputTag[String](\"dirty_data\") override def processElement(value: Raw, ctx: ProcessFunction[Raw, Raw]#Context, out: Collector[Raw]): Unit = &#123; if (value.keywordList == \"dirty_data\") &#123; ctx.output(dirty_data, s\"$&#123;value.date_time&#125;+$&#123;value.keywordList&#125; is dirty data\") &#125; else &#123; out.collect(value) &#125; &#125; &#125;&#125;结果输出","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"OLTP与OLAP：有何区别？","slug":"OLTP与OLAP：有何区别？","date":"2018-04-10T14:25:18.000Z","updated":"2020-05-11T05:14:44.588Z","comments":true,"path":"2018/04/10/OLTP与OLAP：有何区别？/","link":"","permalink":"cpeixin.cn/2018/04/10/OLTP%E4%B8%8EOLAP%EF%BC%9A%E6%9C%89%E4%BD%95%E5%8C%BA%E5%88%AB%EF%BC%9F/","excerpt":"","text":"什么是OLAP？联机分析处理，一类软件工具，可为业务决策提供数据分析。OLAP系统允许用户一次分析来自多个数据库系统的数据库信息。主要目标是数据分析而不是数据处理。什么是OLTP？在线事务处理（简称OLTP）在3层体系结构中支持面向事务的应用程序。OLTP管理组织的日常事务。主要目标是数据处理而不是数据分析OLAP示例任何数据仓库系统都是OLAP系统。OLAP的用途如下公司可能会将9月的手机销售与10月的销售进行比较，然后将这些结果与可能存储在正确数据库中的另一个位置进行比较。亚马逊分析其客户的购买情况，以提供个性化的主页，其中包含其客户可能感兴趣的产品。OLTP系统示例OLTP系统的一个示例是ATM中心。假设一对夫妇在银行有一个联名账户。一天，两者都同时在精确的同一时间到达不同的ATM中心，并希望提取其银行帐户中的总金额。但是，首先完成身份验证过程的人将能够赚钱。在这种情况下，OLTP系统确保提款金额永远不会超过银行中存在的金额。这里要注意的关键是OLTP系统针对事务优势进行了优化，而不是数据分析。OLTP系统的其他示例是：网上银行业务网上机票预订发送短信订单输入将书添加到购物车使用OLAP服务的好处OLAP为所有类型的业务分析需求（包括计划，预算，预测和分析）创建一个平台。OLAP的主要好处是信息和计算的一致性。轻松对用户和对象施加安全限制，以符合法规并保护敏感数据。OLTP方法的好处它管理组织的日常交易。OLTP通过简化单个流程来扩大组织的客户基础。OLAP服务的缺点实施和维护依赖于IT专业人员，因为传统的OLAP工具需要复杂的建模过程。OLAP工具需要各个部门人员之间的合作才能有效，而这通常是不可能的。OLTP方法的缺点如果OLTP系统面临硬件故障，则在线交易会受到严重影响。OLTP系统允许多个用户同时访问和更改同一数据，这多次创造了前所未有的局面。OLTP和OLAP之间的区别参量OLTPOLAP处理这是一个在线交易系统。它管理数据库修改。OLAP是一个在线分析和数据检索过程。特性它的特点是大量的短期在线交易。它的特点是数据量大。功能性OLTP是一个在线数据库修改系统。OLAP是一个在线数据库查询管理系统。方法OLTP使用传统的DBMS。OLAP使用数据仓库。询问从数据库中插入，更新和删除信息。主要是选择操作表OLTP数据库中的表已标准化。OLAP数据库中的表未规范化。资源OLTP及其事务是数据源。不同的OLTP数据库成为OLAP的数据源。数据的完整性OLTP数据库必须维护数据完整性约束。OLAP数据库不会经常修改。因此，数据完整性不是问题。响应时间它的响应时间以毫秒为单位。响应时间以秒为单位。资料品质OLTP数据库中的数据始终是详细和组织的。OLAP流程中的数据可能没有组织。有用性它有助于控制和运行基本业务任务。它有助于计划，问题解决和决策支持。运作方式允许读/写操作。只读，很少写。听众这是一个面向市场的过程。这是一个以客户为导向的过程。查询类型此过程中的查询是标准化且简单的。涉及聚合的复杂查询。后备完整的数据备份与增量备份相结合。OLAP仅需要不时备份。与OLTP相比，备份并不重要设计数据库设计是面向应用程序的。示例：数据库设计随零售，航空公司，银行等行业的变化而变化数据库设计是面向主题的。示例：数据库设计随销售，市场营销，采购等主题而变化。用户类型数据关键用户（如业务员，DBA和数据库专业人员）使用它。由数据知识用户（例如工人，经理和CEO）使用。目的专为实时业务运营而设计。设计用于按类别和属性分析业务度量。绩效指标事务吞吐量是性能指标查询吞吐量是性能指标。用户数这种数据库用户允许成千上万的用户。这种数据库仅允许数百个用户。生产率它有助于提高用户的自助服务和生产率帮助提高业务分析师的生产率。挑战从历史上看，数据仓库一直是一个开发项目，可能证明构建成本很高。OLAP多维数据集不是开放的SQL Server数据仓库。因此，技术知识和经验对于管理OLAP服务器至关重要。处理它为日常使用的数据提供了快速的结果。它确保对查询的响应更快，更一致。特性它易于创建和维护。它使用户可以在电子表格的帮助下创建视图。样式OLTP被设计为具有快速响应时间，低数据冗余并已标准化。数据仓库是唯一创建的，因此它可以集成不同的数据源以构建统一的数据库关键区别在线分析处理（OLAP）是一类软件工具，可以分析存储在数据库中的数据，而在线事务处理（OLTP）支持3层体系结构中面向事务的应用程序。OLAP为所有类型的业务分析需求（包括计划，预算，预测和分析）创建一个平台，而OLTP对管理组织的日常事务很有用。OLAP的特点是数据量大，而OLTP的特点是大量的短时间在线交易。在OLAP中，数据仓库是唯一创建的，因此它可以集成不同的数据源以构建统一的数据库，而OLTP使用传统的DBMS。","categories":[{"name":"DataBase","slug":"DataBase","permalink":"cpeixin.cn/categories/DataBase/"}],"tags":[{"name":"OLAP","slug":"OLAP","permalink":"cpeixin.cn/tags/OLAP/"}]},{"title":"Flink 数据流容错","slug":"Flink-数据流容错","date":"2018-03-30T10:09:11.000Z","updated":"2020-05-13T13:30:19.746Z","comments":true,"path":"2018/03/30/Flink-数据流容错/","link":"","permalink":"cpeixin.cn/2018/03/30/Flink-%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99/","excerpt":"","text":"介绍Apache Flink提供了一种容错机制，可以一致地恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态最终也将exactly once反映出数据流中的每个记录。请注意，有一个开关可以至少_将担保_降级一次容错机制连续绘制分布式流数据流的快照。对于状态小的流应用程序，这些快照非常轻巧，可以在不影响性能的情况下频繁绘制。流应用程序的状态存储在可配置的位置（例如主节点或HDFS）。如果发生程序故障（由于机器，网络或软件故障），Flink将停止分布式流数据流。然后，系统重新启动operators，并将其重置为最新的成功检查点。输入流将重置为状态快照的点。确保作为重新启动的并行数据流的一部分处理的任何记录都不属于先前的检查点状态。_注意：_默认情况下，检查点是禁用的。有关如何启用和配置检查点的详细信息，请参见检查点。_注意：_为了使该机制实现其全部保证，数据源（例如消息队列或代理）必须能够将流后退到定义的最近点。Apache Kafka具有此功能(kafka 0.11版本后支持)，Flink与Kafka的连接器利用了此功能。有关Flink连接器提供的保证的更多信息，请参见数据源和接收器的容错保证。_注意：_由于Flink的checkpoints是通过分布式快照实现的，因此我们可以交替使用_snapshot_和checkpoint一词。CheckpointFlink容错机制的核心部分是绘制分布式数据流和operator state的一致快照。这些快照充当一致的检查点，如果发生故障，系统可以回退到这些检查点。Flink绘制这些快照的机制在“ 分布式数据流的轻量级异步快照 ”中进行了介绍。它受用于分布式快照的标准Chandy-Lamport算法的启发，并且专门针对Flink的执行模型进行了量身定制。Barriers流屏障(barriers)是Flink分布式快照中的核心元素。这些barriers将注入到数据流中，并与记录一起作为数据流的一部分流动。壁垒从不超越记录，它们严格按照顺序排列。屏障将数据流中的记录分为进入当前快照的记录集和进入下一个快照的记录集。每个屏障都带有快照的ID，快照的记录已推送到快照的前面。屏障不会中断流的流动，因此非常轻便。来自不同快照的多个障碍可以同时出现在流中，这意味着各种快照可能同时发生。_流屏障(barriers)在流源处注入并行数据流中。快照n的屏障被注入的点（我们称其为_S _）是快照中覆盖数据的源流中的位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。该位置_S _被报告给_检查点协调器_（Flink的JobManager）。然后，屏障向下游流动。当中间operator从其所有输入流中收到快照n的屏障时，它会将快照n的屏障发射到其所有输出流中。接收器运算符（流式DAG的末尾）从其所有输入流接收到屏障n后，便将快照n确认给检查点协调器。所有接收器都确认快照后，就认为快照已完成。一旦快照n完成，该作业将再也不会向源询问_S _之前的记录，因为此时这些记录（及其后代记录）将通过整个数据流拓扑。接收多个输入流的operator需要在快照barriers上_对齐_输入流。上图说明了这一点：operator一旦从传入流接收到快照屏障n，就无法处理该流中的任何其他记录，直到它也从其他输入接收到屏障n为止。否则，它将混合属于快照_n的_记录和属于快照_n + 1的记录_。报告屏障_n的_流被暂时搁置。从这些流接收到的记录将不进行处理，而是放入输入缓冲区中。一旦最后一个流接收到屏障n，操作员将发出所有未决的传出记录，然后自身发出快照n屏障。此后，它将恢复处理所有输入流中的记录，处理输入缓冲中的记录，然后再处理流中的记录。State当运算符包含任何形式的_状态时_，该状态也必须是快照的一部分。operator状态以不同的形式出现：_用户定义的状态_：这是由转换功能（如map()或filter()）直接创建和修改的状态。有关详细信息，请参见流应用程序中的状态。_系统状态_：此状态是指作为操作员计算的一部分的数据缓冲区。这种状态的一个典型示例是_窗口缓冲区_，系统在其中收集（并汇总）窗口记录，直到评估并逐出窗口为止。操作员在从输入流接收到所有快照barriers的时间点，以及在将barriers发送到其输出流之前，对其状态进行快照。届时，将对进行障碍之前的记录进行的所有状态更新，并且不依赖于应用障碍后的记录进行的任何更新。由于快照的状态可能很大，因此将其存储在可配置state _backend中_。默认情况下，这是JobManager的内存，但对于生产用途，应配置分布式可靠存储（例如HDFS）。存储状态后，操作员确认检查点，将快照屏障发送到输出流中，然后继续。现在生成的快照包含：对于每个并行流数据源，快照启动时流中的偏移量/位置对于每个运算符，指向作为快照一部分存储的状态的指针Exactly Once vs. At Least Once对准步骤可以向流传输程序增加等待时间。通常，这种额外的延迟大约是几毫秒，但是我们看到一些异常值的延迟显着增加的情况。对于要求所有记录始终具有超低延迟（几毫秒）的应用程序，Flink可以进行切换以在检查点期间跳过流对齐。一旦操作员从每个输入看到检查点障碍，仍然会绘制检查点快照。跳过对齐后，即使到达检查点n的_某些检查点障碍，操作员仍会继续处理所有输入。这样，操作员还可以在获取检查点n的状态快照之前处理属于检查点_n + 1的_元素。在还原时，这些记录将作为重复记录出现，因为它们都包含在检查点n的状态快照中，并将在检查点n之后作为数据的一部分重播。_注意_：对齐仅适用于具有多个前任（联接）的运算符以及具有多个发件人的运算符（在流重新分区/混洗之后）。正因为如此，有数据流只有尴尬的并行流操作（map()，flatMap()，filter()，…）实际上给_正好一次_甚至在保证_至少一次_的模式。Asynchronous State Snapshots注意，上述机制意味着操作员在将输入状态的快照存储在_状态后端时_停止处理输入记录。每次拍摄快照时，此_同步_状态快照都会带来延迟。可以让operator在存储其状态快照时继续进行处理，从而有效地使状态快照在后台_异步_发生。为此，operator必须能够产生状态对象，该状态对象的存储方式应确保对操作员状态的进一步修改不会影响该状态对象。例如，在RocksDB中使用_的写时复制_数据结构具有此行为。在其输入上收到检查点屏障后，操作员将开始对其状态进行异步快照复制。它立即对输出发出障碍，并继续进行常规流处理。后台复制过程完成后，它将向检查点协调器（JobManager）确认检查点。现在，只有在所有接收器都接收到障碍并且所有有状态操作员都已确认完成备份后（可能是在障碍到达接收器之后），检查点才完成。有关状态快照的详细信息，请参见状态后端。Recovery在这种机制下的恢复非常简单：失败时，Flink选择最新完成的检查点k。然后，系统重新部署整个分布式数据流，并为每个操作员提供作为检查点_k的_一部分快照的状态。设置源以开始从位置_S _读取流。例如，在Apache Kafka中，这意味着告诉使用者开始从偏移量_S _获取。如果状态是增量快照，则操作员将从最新完整快照的状态开始，然后对该状态应用一系列增量快照更新。有关更多信息，请参见重新启动策略。Operator Snapshot Implementation拍摄操作员快照时，有两个部分：同步部分和异步部分。运算符和状态后端以Java形式提供其快照FutureTask。该任务包含_同步_部分已完成而_异步_部分未决的状态。然后，异步部分由该检查点的后台线程执行。纯粹检查点的操作员将同步返回已完成的操作FutureTask。如果需要执行异步操作，则以run()that 的方法执行FutureTask。这些任务是可取消的，因此可以释放流和其他消耗资源的句柄。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink DataStream - Event Time","slug":"Flink-DataStream-Event-Time","date":"2018-03-25T10:09:11.000Z","updated":"2020-05-12T14:53:18.624Z","comments":true,"path":"2018/03/25/Flink-DataStream-Event-Time/","link":"","permalink":"cpeixin.cn/2018/03/25/Flink-DataStream-Event-Time/","excerpt":"","text":"Event Time / Processing Time / Ingestion TimeFlink 在流式传输程序中支持不同的_时间_概念。处理时间(Processing Time)：处理时间是指执行相应操作的机器的系统时间。当流式程序按处理时间运行时，所有基于时间的操作（如时间窗口）都将使用运行相应操作员的计算机的系统时钟。每小时处理时间窗口将包括系统时钟指示整小时的时间之间到达特定操作员的所有记录。例如，如果应用程序在9:15 am开始运行，则第一个每小时处理时间窗口将包括在9:15 am和10:00 am之间处理的事件，下一个窗口将包括在10:00 am和11:00 am之间处理的事件，依此类推。处理时间是最简单的时间概念，不需要流和机器之间的协调。它提供了最佳的性能和最低的延迟。但是，在分布式和异步环境中，处理时间不能提供确定性，因为它容易受到记录到达系统（例如从消息队列）到达系统的速度，记录在系统内部操作员之间流动的速度的影响。 以及中断（计划的或其他方式）。事件时间(Event time)：事件时间是每个事件在其生产设备上发生的时间。该时间通常在它们进入Flink之前嵌入到记录中，并且 可以从每个记录中提取_事件时间戳_。在事件时间中，时间的进度取决于数据，而不取决于任何挂钟。事件时间程序必须指定如何生成“ _事件时间水印”_，这是一种表示事件时间进展的机制。在理想情况下，事件时间处理将产生完全一致且确定的结果，而不管事件何时到达或它们的顺序如何。但是，除非已知事件是按时间戳（按时间戳）到达的，否则事件时间处理会在等待无序事件时产生一定的延迟。由于只能等待有限的时间，因此这限制了确定性事件时间应用程序的可用性。假设所有数据都已到达，事件时间操作将按预期方式运行，即使在处理无序或迟到的事件或重新处理历史数据时，也会产生正确且一致的结果。例如，每小时事件时间窗口将包含所有带有落入该小时事件时间戳的记录，无论它们到达的顺序或处理的时间。请注意，有时当事件时间程序实时处理实时数据时，它们将使用一些_处理时间_操作，以确保它们及时进行。摄取时间(Ingestion Time)：摄取时间是事件进入Flink的时间。在源操作员处，每个记录都将源的当前时间作为时间戳记，并且基于时间的操作（如时间窗口）引用该时间戳记。_摄取时间_从概念上讲介于_事件时间和处理时间之间_。与_处理时间_相比 ，它稍微贵一些，但结果却更可预测。由于 _摄取时间_使用稳定的时间戳（在源处分配了一次），因此对记录的不同窗口操作将引用相同的时间戳，而在_处理时间中，_每个窗口操作员都可以将记录分配给不同的窗口（基于本地系统时钟和任何运输延误）。与_事件时间_相比，_提取时间_程序无法处理任何乱序事件或迟到的数据，但是程序不必指定如何生成_水印_。在内部，_摄取时间与事件时间_非常相似，但是具有自动时间戳分配和自动水印生成功能。设置时间特征（Setting a Time Characteristic）Flink DataStream程序的第一部分通常设置基准_时间特征_。该设置定义了数据流源的行为方式（例如，是否分配时间戳），以及诸如的窗口操作应使用什么时间概念KeyedStream.timeWindow(Time.seconds(30))。以下示例显示了一个Flink程序，该程序在每小时的时间窗口中汇总事件。窗口的行为与时间特征相适应。1234567891011val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)// alternatively:// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))stream .keyBy( _.getUser ) .timeWindow(Time.hours(1)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...)请注意，为了在_事件时间中_运行此示例，程序需要使用直接为数据定义事件时间并自己发出水印的源，或者程序必须在源之后注入_Timestamp Assigner＆Watermark Generator_。这些功能描述了如何访问事件时间戳，以及事件流呈现出何种程度的乱序。以下部分描述了_时间戳和水印_背后的一般机制。有关如何在Flink DataStream API中使用时间戳分配和水印生成的指南，请参阅 生成时间戳/水印。现实世界中的时间是不一致的，在 flink 中被划分为事件时间，提取时间，处理时间三种。如果以 EventTime 为基准来定义时间窗口那将形成 EventTimeWindow,要求消息本身就应该携带 EventTime 如果以 IngesingtTime 为基准来定义时间窗口那将形成 IngestingTimeWindow,以 source 的 systemTime 为准。 如果以 ProcessingTime 基准来定义时间窗口那将形成 ProcessingTimeWindow，以 operator 的 systemTime 为准。以上三种选择是需要根据不同业务需求进行选择的，例如，在数据计算的过程中，需要考虑到网络延迟的原因，那么就应该选择setStreamTimeCharacteristic(TimeCharacteristic.EventTime)，相反，不需要考虑延迟原因的话，只考虑处理数据的时间，则可以选择另两个。Event Time and Watermarks支持事件时间的_流处理器需要一种测量事件时间进度的方法。例如，当事件时间超过一个小时结束时，需要通知构建每小时窗口的窗口操作员，以便该操作员可以关闭正在进行的窗口。_事件时间_可以独立于_处理时间_（由挂钟测量）进行。例如，在一个程序中，操作员的当前_事件时间_可能会稍微落后于_处理时间 （考虑到事件接收的延迟），而两者均以相同的速度进行。另一方面，另一个流媒体程序可以通过快速转发已经在Kafka主题（或另一个消息队列）中缓存的一些历史数据来在数周的事件时间内进行处理，而处理时间仅为几秒钟。Flink中衡量事件时间进度的机制是水印。水印作为数据流的一部分流动，并带有时间戳t。甲_水印（T）_宣布事件时间达到时间 吨该流，这意味着应该有从该流没有更多的元素与时间戳_T” &lt;= T_（即，具有时间戳的事件较旧的或等于水印）。下图显示了带有（逻辑）时间戳记的事件流，以及串联的水印。在此示例中，事件是按顺序排列的（相对于其时间戳），这意味着水印只是流中的周期性标记。水印对于_乱序_流至关重要，如下图所示，其中事件不是按其时间戳排序的。通常，水印是一种声明，即到流中的那个点，直到某个时间戳的所有事件都应该到达。一旦水印到达操作员，操作员就可以将其内部_事件时钟_提前到水印的值。请注意，事件时间是由新创建的一个（或多个）流元素从产生它们的事件或触发了创建这些元素的水印中继承的。并行流中的水印(Watermarks in Parallel Streams)水印在源函数处或源函数之后直接生成。源函数的每个并行子任务通常独立生成其水印。这些水印定义了该特定并行源处的事件时间。随着水印在流式传输程序中的流动，它们会提前到达其到达的运营商的事件时间。每当操作员提前其事件时间时，都会为其后续操作员在下游生成新的水印。一些运算符消耗多个输入流；例如，并集，或遵循_keyBy（…）或partition（…）_函数的运算符。该操作员的当前事件时间是其输入流的事件时间中的最小值。随着其输入流更新其事件时间，操作员也将更新。下图显示了流过并行流的事件和水印的示例，操作员跟踪事件时间。请注意，Kafka源支持按分区添加水印，您可以在此处阅读更多信息。后期元素某些元素可能会违反水印条件，这意味着即使在发生_水印（t）_之后，也会出现更多时间戳为_t’&lt;= t的_元素。实际上，在许多现实世界的设置中，某些元素可以任意延迟，从而无法指定某个事件时间戳记的所有元素都将发生的时间。此外，即使可以限制延迟，通常也不希望将水印延迟太多，因为这会导致事件时间窗的评估延迟过多。由于这个原因，流式传输程序可能会明确期望某些_后期_元素。延迟元素是在系统的事件时间时钟（由水印指示）已经经过延迟元素时间戳的时间之后到达的元素。有关如何在事件时间窗口中使用延迟元素的更多信息，请参见允许延迟。闲置来源当前，使用纯事件时间水印生成器，如果没有要处理的元素，则水印将无法进行。这意味着在输入数据存在间隙的情况下，事件时间将不会继续进行，例如不会触发窗口操作符，因此现有窗口将无法生成任何输出数据。为了避免这种情况，可以使用周期性的水印分配器，这些分配器不仅基于元素时间戳进行分配。一个示例解决方案可能是一个分配器，该分配器在一段时间内未观察到新事件之后切换为使用当前处理时间作为时间基础。可以使用将源标记为空闲SourceFunction.SourceContext#markAsTemporarilyIdle。有关详细信息，请参考此方法的Javadoc以及StreamStatus。调试水印请参阅“ 调试Windows和事件时间”部分以在运行时调试水印。运营商如何处理水印通常，要求操作员在将给定水印转发到下游之前对其进行完全处理。例如， WindowOperator将首先评估应触发哪个窗口，只有在产生了所有由水印触发的输出之后，水印本身才会被发送到下游。换句话说，由于水印的出现而产生的所有元素将在水印之前发出。相同的规则适用于TwoInputStreamOperator。但是，在这种情况下，操作员的当前水印被定义为其两个输入的最小值。生成时间戳/水印分配时间戳带有时间戳和水印的源函数时间戳记分配器/水印生成器每个Kafka分区的时间戳本节与在事件时间运行的程序有关。有关事件时间_， _处理时间和摄取时间的简介_，请参阅事件时间的简介。为了处理**_事件时间，流式传输程序需要相应地设置时间特征**。12val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)分配时间戳为了使用_事件时间_，Flink需要知道事件的_时间戳_，这意味着流中的每个元素都需要_分配_其事件时间戳。这通常是通过从元素的某个字段访问/提取时间戳来完成的。时间戳分配与生成水印齐头并进，水印告诉系统事件时间的进展。有两种分配时间戳和生成水印的方法：直接在数据流源中通过时间戳分配器/水印生成器：在Flink中，时间戳分配器还定义要发送的水印注意自1970年1月1日T00：00：00Z的Java时代以来，时间戳和水印都指定为毫秒。带有时间戳和水印的源函数流源可以将时间戳直接分配给它们产生的元素，并且它们还可以发出水印。完成此操作后，无需时间戳分配器。请注意，如果使用时间戳分配器，则源所提供的任何时间戳和水印都将被覆盖。要将时间戳直接分配给源中的元素，源必须使用上的collectWithTimestamp(...) 方法SourceContext。要生成水印，源必须调用该emitWatermark(Watermark)函数。下面是一个简单的示例_（非检查点）_源，该源分配时间戳并生成水印：12345678910override def run(ctx: SourceContext[MyType]): Unit &#x3D; &#123; while (&#x2F;* condition *&#x2F;) &#123; val next: MyType &#x3D; getNext() ctx.collectWithTimestamp(next, next.eventTimestamp) if (next.hasWatermarkTime) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime)) &#125; &#125;&#125;时间戳记分配器/水印生成器时间戳记分配器获取流并产生带有时间戳记的元素和水印的新流。如果原始流已经具有时间戳和/或水印，则时间戳分配器将覆盖它们。时间戳记分配器通常在数据源之后立即指定，但并非严格要求这样做。例如，一种常见的模式是在时间戳分配器之前解析（_MapFunction_）和筛选器（_FilterFunction_）。无论如何，都需要在事件时间的第一个操作（例如第一个窗口操作）之前指定时间戳分配器。作为一种特殊情况，当使用Kafka作为流作业的源时，Flink允许在源（或使用者）本身内部指定时间戳分配器/水印发射器。有关如何执行此操作的更多信息，请参见 Kafka Connector文档。注意：本节的其余部分介绍了程序员为了创建自己的时间戳提取器/水印发射器而必须实现的主要接口。要查看Flink附带的预实现提取器，请参阅“ 预定义时间戳提取器/水印发射器”页面。爪哇斯卡拉12345678910111213141516val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter())val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())withTimestampsAndWatermarks .keyBy( _.getGroup ) .timeWindow(Time.seconds(10)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...)**带有定期水印AssignerWithPeriodicWatermarks 分配时间戳并定期生成水印（可能取决于流元素，或仅基于处理时间）。通过定义生成水印的间隔（每n毫秒） ExecutionConfig.setAutoWatermarkInterval(...)。分配器的getCurrentWatermark()方法每次都会被调用，如果返回的水印非空且大于前一个水印，则将发出新的水印。在这里，我们显示了两个使用定期水印生成的时间戳分配器的简单示例。请注意，Flink随附与以下所示BoundedOutOfOrdernessTimestampExtractor类似的内容BoundedOutOfOrdernessGenerator，您可以在此处阅读有关内容。12345678910111213141516171819202122232425262728293031323334353637383940/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxOutOfOrderness = 3500L // 3.5 seconds var currentMaxTimestamp: Long = _ override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; val timestamp = element.getCreationTime() currentMaxTimestamp = max(timestamp, currentMaxTimestamp) timestamp &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound new Watermark(currentMaxTimestamp - maxOutOfOrderness) &#125;&#125;/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxTimeLag = 5000L // 5 seconds override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current time minus the maximum time lag new Watermark(System.currentTimeMillis() - maxTimeLag) &#125;&#125;**带标点的水印要在特定事件表明可能会生成新的水印时生成水印，请使用 AssignerWithPunctuatedWatermarks。对于此类，Flink将首先调用该extractTimestamp(...)方法为元素分配时间戳，然后立即checkAndGetNextWatermark(...)在该元素上调用该 方法。该checkAndGetNextWatermark(...)方法会传递该方法中分配的时间戳extractTimestamp(...) ，并可以决定是否要生成水印。每当该checkAndGetNextWatermark(...) 方法返回一个非空水印，并且该水印大于最新的先前水印时，就会发出新的水印。爪哇斯卡拉12345678910class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123; if (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null &#125;&#125;_注意：_可以在每个事件上生成水印。但是，由于每个水印都会在下游引起一些计算，因此过多的水印会降低性能。每个Kafka分区的时间戳当使用Apache Kafka作为数据源时，每个Kafka分区可能都有一个简单的事件时间模式（时间戳升序或有界乱序）。但是，在使用来自Kafka的流时，通常会并行使用多个分区，从而交错插入分区中的事件并破坏每个分区的模式（这是Kafka的客户客户端工作方式所固有的）。在这种情况下，您可以使用Flink的Kafka分区感知水印生成。使用该功能，将在Kafka使用者内部针对每个Kafka分区生成水印，并且按与合并水印在流shuffle上相同的方式合并每个分区的水印。例如，如果事件时间戳严格按照每个Kafka分区递增，则使用递增时间戳水印生成器生成按分区的水印 将产生完美的整体水印。下图显示了如何使用按kafka分区的水印生成，以及在这种情况下水印如何通过流数据流传播。123456val kafkaSource = new FlinkKafkaConsumer09[MyType](\"myTopic\", schema, props)kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123; def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp&#125;)val stream: DataStream[MyType] = env.addSource(kafkaSource)预定义的时间戳提取器/水印发射器如时间戳和水印处理中所述，Flink提供了抽象，允许程序员分配自己的时间戳并发出自己的水印。更具体地说，根据使用情况，可以通过实现AssignerWithPeriodicWatermarks和AssignerWithPunctuatedWatermarks接口之一来实现。简而言之，第一个将定期发出水印，而第二个则根据传入记录的某些属性发出水印，例如，每当流中遇到特殊元素时。为了进一步简化此类任务的编程工作，Flink附带了一些预先实现的时间戳分配器。本节提供了它们的列表。除了开箱即用的功能外，它们的实现还可以作为自定义实现的示例。时间戳递增的分配者_定期_生成水印的最简单的特殊情况是给定源任务看到的时间戳以升序出现的情况。在这种情况下，当前时间戳始终可以充当水印，因为没有更早的时间戳会到达。请注意，_每个并行数据源任务_只需要增加时间戳记即可。例如，如果在一个特定的设置中，一个并行数据源实例读取一个Kafka分区，则只需要在每个Kafka分区内将时间戳记递增。每当对并行流进行混洗，合并，连接或合并时，Flink的水印合并机制都会生成正确的水印。12val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )**分配器允许固定的延迟时间周期性水印生成的另一个示例是水印在流中看到的最大（事件时间）时间戳落后固定时间量的情况。这种情况包括预先知道流中可能遇到的最大延迟的场景，例如，当创建包含时间戳的元素的自定义源时，该时间戳在固定的时间段内传播以进行测试。对于这些情况，Flink提供了BoundedOutOfOrdernessTimestampExtractor，将用作参数maxOutOfOrderness，即在计算给定窗口的最终结果时允许元素延迟到被忽略之前的最长时间。延迟对应于的结果t - t_w，其中t元素的（事件时间）时间戳和t_w前一个水印的时间戳。如果lateness &gt; 0那么该元素将被认为是较晚的元素，默认情况下，在为其相应窗口计算作业结果时将其忽略。有关 使用延迟元素的更多信息，请参见有关允许延迟的文档。12val stream: DataStream[MyEvent] &#x3D; ...val withTimestampsAndWatermarks &#x3D; stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink DataStream API","slug":"Flink-DataStream-API","date":"2018-03-20T14:44:42.000Z","updated":"2020-05-12T14:53:29.761Z","comments":true,"path":"2018/03/20/Flink-DataStream-API/","link":"","permalink":"cpeixin.cn/2018/03/20/Flink-DataStream-API/","excerpt":"","text":"Flink中的DataStream程序是常规程序，可对数据流实施转换（例如，过滤，更新状态，定义窗口，聚合）。最初从各种来源（例如，消息队列，套接字流，文件）创建数据流。结果通过接收器返回，接收器可以例如将数据写入文件或标准输出（例如命令行终端）。Flink程序可以在各种上下文中运行，独立运行或嵌入其他程序中。执行可以在本地JVM或许多计算机的群集中进行。范例程序以下程序是流式窗口单词计数应用程序的一个完整的工作示例，该应用程序在5秒的窗口中对来自Web套接字的单词进行计数。您可以复制并粘贴代码以在本地运行。123456789101112131415import org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Timeobject WindowWordCount &#123; def main(args: Array[String]) &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val text = env.socketTextStream(\"localhost\", 9999) val counts = text.flatMap &#123; _.toLowerCase.split(\"\\\\W+\") filter &#123; _.nonEmpty &#125; &#125; .map &#123; (_, 1) &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1) counts.print() env.execute(\"Window Stream WordCount\") &#125;&#125;要运行示例程序，请首先从终端使用netcat启动输入流：1nc -lk 9999只需输入一些单词，然后按回车键即可获得一个新单词。这些将作为单词计数程序的输入。如果您想看到计数大于1，请在5秒钟内一次又一次地键入相同的单词（如果您不能快速键入☺，则将窗口大小从5秒钟增加）。数据源源是程序读取其输入的位置。您可以使用将源附加到程序StreamExecutionEnvironment.addSource(sourceFunction)。Flink附带了许多预先实现的源函数，但是您始终可以通过实现SourceFunction for非并行源，实现ParallelSourceFunction接口或扩展 RichParallelSourceFunctionfor并行源来编写自己的自定义源。可从以下位置访问几个预定义的流源StreamExecutionEnvironment：基于文件：readTextFile(path)- TextInputFormat逐行读取文本文件，即符合规范的文件，并将其作为字符串返回。readFile(fileInputFormat, path) -根据指定的文件输入格式读取（一次）文件。readFile(fileInputFormat, path, watchType, interval, pathFilter)-这是前两个内部调用的方法。它path根据给定的读取文件fileInputFormat。根据提供的内容watchType，此源可以定期（每intervalms）监视路径中的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或处理一次路径中当前的数据并退出（FileProcessingMode.PROCESS_ONCE）。使用pathFilter，用户可以进一步从文件中排除文件。实施：在后台，Flink将文件读取过程分为两个子任务，即目录监视和数据读取。这些子任务中的每一个都是由单独的实体实现的。监视由单个非并行（并行度= 1）任务实现，而读取由并行运行的多个任务执行。后者的并行性等于作业并行性。单个监视任务的作用是扫描目录（根据定期扫描或仅扫描一次watchType），找到要处理的文件，将它们分成多个部分，并将这些拆分分配给下游阅读器。读者将是阅读实际数据的人。每个拆分只能由一个阅读器读取，而阅读器可以一一阅读多个拆分。重要笔记：如果将watchType设置为FileProcessingMode.PROCESS_CONTINUOUSLY，则在修改文件时，将完全重新处理其内容。这可能会破坏“完全一次”的语义，因为在文件末尾附加数据将导致重新处理其所有内容。如果将watchType设置为FileProcessingMode.PROCESS_ONCE，则源将扫描路径一次并退出，而无需等待读取器完成文件内容的读取。当然，读者将继续阅读，直到读取了所有文件内容。关闭源将导致在该点之后没有更多检查点。这可能导致节点故障后恢复速度变慢，因为作业将从上一个检查点恢复读取。基于套接字：socketTextStream-从套接字读取。元素可以由定界符分隔。基于集合：fromCollection(Seq)-从Java Java.util.Collection创建数据流。集合中的所有元素必须具有相同的类型。fromCollection(Iterator)-从迭代器创建数据流。该类指定迭代器返回的元素的数据类型。fromElements(elements: _*)-从给定的对象序列创建数据流。所有对象必须具有相同的类型。fromParallelCollection(SplittableIterator)-从迭代器并行创建数据流。该类指定迭代器返回的元素的数据类型。generateSequence(from, to) -并行生成给定间隔中的数字序列。自定义：addSource-附加新的源功能。例如，要阅读Apache Kafka，可以使用 addSource(new FlinkKafkaConsumer08&lt;&gt;(…))。有关更多详细信息，请参见连接器。Flink 连接 kafka实例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.api.common.serialization.SimpleStringSchemaimport org.apache.flink.streaming.api.scala._import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.connectors.kafka.&#123;FlinkKafkaConsumer, FlinkKafkaConsumerBase&#125;object wordcount_stream &#123; private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val dataSource: FlinkKafkaConsumerBase[String] = new FlinkKafkaConsumer( KAFKA_TOPIC, new SimpleStringSchema(), properties)// .setStartFromEarliest() // 指定从最早offset开始消费 .setStartFromLatest() // 指定从最新offset开始消费 env.addSource(dataSource) .flatMap(get_value(_: String).split(\",\")) .map((_: String, 1)) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1) .print()// .setParallelism(1) 设置并行度 env.execute(\"Flink Streaming—————KafkaSource\") &#125; def get_value(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;数据接收器数据接收器使用DataStream，并将其转发到文件，套接字，外部系统或打印它们。Flink带有各种内置的输出格式，这些格式封装在DataStream的操作后面：writeAsText()/ TextOutputFormat-将元素按行写为字符串。通过调用每个元素的_toString（）_方法获得字符串。writeAsCsv(...)/ CsvOutputFormat-将元组写为逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的_toString（）_方法。print()/ printToErr() - 在标准输出/标准错误流上打印每个元素的_toString（）_值。可选地，可以提供前缀（msg），该前缀在输出之前。这可以帮助区分不同的_打印_调用。如果并行度大于1，则输出之前还将带有产生输出的任务的标识符。writeUsingOutputFormat()/ FileOutputFormat-自定义文件输出的方法和基类。支持自定义对象到字节的转换。writeToSocket -根据以下内容将元素写入套接字 SerializationSchemaaddSink-调用自定义接收器功能。Flink捆绑有连接到其他系统（例如Apache Kafka）的连接器，这些连接器实现为接收器功能。请注意，上的write*()方法DataStream主要用于调试目的。它们不参与Flink的检查点，这意味着这些功能通常具有至少一次的语义。刷新到目标系统的数据取决于OutputFormat的实现。这意味着并非所有发送到OutputFormat的元素都立即显示在目标系统中。同样，在失败的情况下，这些记录可能会丢失。为了将流可靠，准确地一次传输到文件系统中，请使用flink-connector-filesystem。同样，通过该.addSink(...)方法的自定义实现可以参与Flink一次精确语义的检查点。执行参数在StreamExecutionEnvironment包含了ExecutionConfig允许用于运行组工作的具体配置值。请参考执行配置 以获取大多数参数的说明。这些参数专门与DataStream API有关：setAutoWatermarkInterval(long milliseconds)：设置自动水印发射的间隔。您可以使用获取当前值long getAutoWatermarkInterval()容错能力状态和检查点描述了如何启用和配置Flink的检查点机制。控制延迟默认情况下，元素不会在网络上一对一传输（这会导致不必要的网络通信），但是会进行缓冲。缓冲区的大小（实际上是在计算机之间传输的）可以在Flink配置文件中设置。尽管此方法可以优化吞吐量，但是当传入流不够快时，它可能会导致延迟问题。为了控制吞吐量和延迟，您可以env.setBufferTimeout(timeoutMillis)在执行环境（或各个运算符）上使用来设置缓冲区填充的最大等待时间。在此时间之后，即使缓冲区未满，也会自动发送缓冲区。此超时的默认值为100毫秒。用法:123val env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironmentenv.setBufferTimeout(timeoutMillis)env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis)为了最大化吞吐量，请设置set setBufferTimeout(-1)来消除超时，并且仅在缓冲区已满时才刷新它们。为了使延迟最小化，请将超时设置为接近0的值（例如5或10 ms）。应避免将缓冲区超时设置为0，因为它可能导致严重的性能下降。调试在分布式群集中运行流式程序之前，最好确保已实现的算法按预期工作。因此，实施数据分析程序通常是检查结果，调试和改进的增量过程。Flink提供的功能可通过在IDE中支持本地调试，注入测试数据和收集结果数据来大大简化数据分析程序的开发过程。本节提供一些提示，说明如何简化Flink程序的开发。本地执行环境A LocalStreamEnvironment在创建该JVM的同一JVM进程内启动Flink系统。如果从IDE启动LocalEnvironment，则可以在代码中设置断点并轻松调试程序。创建并按如下方式使用LocalEnvironment：1234val env = StreamExecutionEnvironment.createLocalEnvironment()val lines = env.addSource(/* some source */)// build your programenv.execute()收集数据源Flink提供了由Java集合支持的特殊数据源，以简化测试。一旦测试了程序，就可以轻松地将源和接收器替换为可读取/写入外部系统的源和接收器。收集数据源可以按如下方式使用：123456789val env = StreamExecutionEnvironment.createLocalEnvironment()// Create a DataStream from a list of elementsval myInts = env.fromElements(1, 2, 3, 4, 5)// Create a DataStream from any Collectionval data: Seq[(String, Int)] = ...val myTuples = env.fromCollection(data)// Create a DataStream from an Iteratorval longIt: Iterator[Long] = ...val myLongs = env.fromCollection(longIt)注意：当前，集合数据源要求数据类型和迭代器实现 Serializable。此外，收集数据源不能并行执行（并行度= 1）。迭代器数据接收器Flink还提供接收器以收集DataStream结果以进行测试和调试。可以如下使用：1234import org.apache.flink.streaming.experimental.DataStreamUtilsimport scala.collection.JavaConverters.asScalaIteratorConverterval myResult: DataStream[(String, Int)] = ...val myOutput: Iterator[(String, Int)] = DataStreamUtils.collect(myResult.javaStream).asScala注意： flink-streaming-contrib模块已从Flink 1.5.0中删除。已移入flink-streaming-java和flink-streaming-scala。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 基础","slug":"Flink-基础","date":"2018-03-15T14:44:42.000Z","updated":"2020-05-12T14:53:28.007Z","comments":true,"path":"2018/03/15/Flink-基础/","link":"","permalink":"cpeixin.cn/2018/03/15/Flink-%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Flink 程序是实现了分布式集合转换（例如过滤、映射、更新状态、join、分组、定义窗口、聚合）的规范化程序。集合初始创建自 source（例如读取文件、kafka 主题，或本地内存中的集合）。结果通过 sink 返回，例如，它可以将数据写入（分布式）文件，或标准输出（例如命令行终端）。Flink 程序可以在多种环境中运行，独立运行或嵌入到其他程序中。可以在本地 JVM 中执行，也可以在多台机器的集群上执行。针对有界和无界两种数据 source 类型，你可以使用 DataSet API 来编写批处理程序或使用 DataStream API 来编写流处理程序。本篇指南将介绍这两种 API 通用的基本概念，关于使用 API 编写程序的具体信息请查阅 流处理指南 和 批处理指南。请注意：** 当展示如何使用 API 的实际示例时我们使用 StreamingExecutionEnvironment 和 DataStream API。对于批处理，将他们替换为 ExecutionEnvironment 和 DataSet API 即可，概念是完全相同的。DataSet 和 DataStreamFlink 用特有的 DataSet 和 DataStream 类来表示程序中的数据。你可以将他们视为可能包含重复项的不可变数据集合。对于 DataSet，数据是有限的，而对于 DataStream，元素的数量可以是无限的。这些集合与标准的 Java 集合有一些关键的区别。首先它们是不可变的，也就是说它们一旦被创建你就不能添加或删除元素了。你也不能简单地检查它们内部的元素。在 Flink 程序中，集合最初通过添加数据 source 来创建，通过使用诸如 map、filter 等 API 方法对数据 source 进行转换从而派生新的集合。剖析一个 Flink 程序Flink 程序看起来像是转换数据集合的规范化程序。每个程序由一些基本的部分组成：获取执行环境，加载/创建初始数据，指定对数据的转换操作，指定计算结果存放的位置，触发程序执行我们现在将概述每个步骤，详细信息请参阅相应章节。请注意所有 Scala DataSet API 可以在这个包 org.apache.flink.api.scala 中找到，同时，所有 Scala DataStream API 可以在这个包 org.apache.flink.streaming.api.scala 中找到。StreamExecutionEnvironment 是所有 Flink 程序的基础。你可以使用它的这些静态方法获取：123getExecutionEnvironment()createLocalEnvironment()createRemoteEnvironment(host: String, port: Int, jarFiles: String*)通常你只需要使用 getExecutionEnvironment()，因为它会根据上下文环境完成正确的工作，如果你在 IDE 中执行程序或者作为标准的 Java 程序来执行，它会创建你的本机执行环境。如果你将程序封装成 JAR 包，然后通过命令行调用，Flink 集群管理器会执行你的 main 方法并且 getExecutionEnvironment() 会返回在集群上执行程序的执行环境。针对不同的数据 source，执行环境有若干不同的读取文件的方法：你可以逐行读取 CSV 文件，或者使用完全自定义的输入格式。要将文本文件作为一系列行读取，你可以使用：12val env = StreamExecutionEnvironment.getExecutionEnvironment()val text: DataStream[String] = env.readTextFile(\"file:///path/to/file\")如此你会得到一个 DataStream 然后对其应用转换操作从而创建新的派生 DataStream。通过调用 DataStream 的转换函数来进行转换。下面是一个映射转换的实例：12val input: DataSet[String] &#x3D; ...val mapped &#x3D; input.map &#123; x &#x3D;&gt; x.toInt &#125;这会通过把原始数据集合的每个字符串转换为一个整数创建一个新的 DataStream。一旦你得到了包含最终结果的 DataStream，就可以通过创建 sink 将其写入外部系统。如下是一些创建 sink 的示例：12writeAsText(path: String)print()当设定好整个程序以后你需要调用 StreamExecutionEnvironment 的 execute() 方法触发程序执行。至于在你的本机触发还是提交到集群运行取决于 ExecutionEnvironment 的类型。execute() 方法返回 JobExecutionResult，它包括执行耗时和一个累加器的结果。如果你不需要等待作业的结束，只是想要触发程序执行，你可以调用 StreamExecutionEnvironment 的 executeAsync() 方法。这个方法将返回一个 JobClient 对象，通过 JobClient 能够与程序对应的作业进行交互。作为例子，这里介绍通过 executeAsync() 实现与 execute() 相同行为的方法。12final JobClient jobClient = env.executeAsync();final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult(userClassloader).get();有关流数据的 source 和 sink 以及有关 DataStream 支持的转换操作的详细信息请参阅流处理指南。有关批数据的 source 和 sink 以及有关 DataSet 支持的转换操作的详细信息请参阅批处理指南。延迟计算无论在本地还是集群执行，所有的 Flink 程序都是延迟执行的：当程序的 main 方法被执行时，并不立即执行数据的加载和转换，而是创建每个操作并将其加入到程序的执行计划中。当执行环境调用 execute() 方法显式地触发执行的时候才真正执行各个操作。延迟计算允许你构建复杂的程序，Flink 将其作为整体计划单元来执行。指定键一些转换操作（join, coGroup, keyBy, groupBy）要求在元素集合上定义键。另外一些转换操作 （Reduce, GroupReduce, Aggregate, Windows）允许在应用这些转换之前将数据按键分组。如下对 DataSet 分组1234DataSet&lt;...&gt; input = // [...]DataSet&lt;...&gt; reduced = input .groupBy(/*在这里定义键*/) .reduceGroup(/*一些处理操作*/);如下对 DataStream 指定键1234DataStream&lt;...&gt; input = // [...]DataStream&lt;...&gt; windowed = input .keyBy(/*在这里定义键*/) .window(/*指定窗口*/);Flink 的数据模型不是基于键值对的。因此你不需要将数据集类型物理地打包到键和值中。键都是“虚拟的”：它们的功能是指导分组算子用哪些数据来分组。请注意：*下面的讨论中我们将以 *DataStream** 和 **keyby** 为例。 对于 DataSet API 你只需要用 **DataSet** 和 **groupBy** 替换即可。**为 Tuple 定义键最简单的方式是按照 Tuple 的一个或多个字段进行分组：12val input: DataStream[(Int, String, Long)] = // [...]val keyed = input.keyBy(0)按照第一个字段（整型字段）对 Tuple 分组。12val input: DataSet[(Int, String, Long)] = // [...]val grouped = input.groupBy(0,1)这里我们用第一个字段和第二个字段组成的组合键对 Tuple 分组对于嵌套 Tuple 请注意： 如果你的 DataStream 是嵌套 Tuple，例如：1DataStream&lt;Tuple3&lt;Tuple2&lt;Integer, Float&gt;,String,Long&gt;&gt; ds;指定 keyBy(0) 将导致系统使用整个 Tuple2 作为键（一个整数和一个浮点数）。 如果你想“进入”到 Tuple2 的内部，你必须使用如下所述的字段表达式键。使用字段表达式定义键可以使用基于字符串的字段表达式来引用嵌套字段，并定义用于分组、排序、join 或 coGrouping 的键。字段表达式可以很容易地选取复合（嵌套）类型中的字段，例如 Tuple 和 POJO 类型。下例中，我们有一个包含“word”和“count”两个字段的 POJO：WC。要用 word 字段分组，我们只需要把它的名字传给 keyBy() 函数即可。12345678910// 普通的 POJO（简单的 Java 对象）class WC(var word: String, var count: Int) &#123; def this() &#123; this(\"\", 0L) &#125;&#125;val words: DataStream[WC] = // [...]val wordCounts = words.keyBy(\"word\").window(/*指定窗口*/)// 或者，代码少一点的 case classcase class WC(word: String, count: Int)val words: DataStream[WC] = // [...]val wordCounts = words.keyBy(\"word\").window(/*指定窗口*/)字段表达式语法**：根据字段名称选择 POJO 的字段。例如 “user” 就是指 POJO 类型的“user”字段。根据 1 开始的字段名称或 0 开始的字段索引选择 Tuple 的字段。例如 “_1” 和 “5” 分别指 Java Tuple 类型的第一个和第六个字段。可以选择 POJO 和 Tuple 的嵌套字段。 例如，一个 POJO 类型有一个“user”字段还是一个 POJO 类型，那么 “user.zip” 即指这个“user”字段的“zip”字段。任意嵌套和混合的 POJO 和 Tuple都是支持的，例如 “_2.user.zip” 或 “user._4.1.zip”。可以使用 &quot;*&quot; 通配符表达式选择完整的类型。这也适用于非 Tuple 或 POJO 类型。字段表达式示例**:12345678910class WC(var complex: ComplexNestedClass, var count: Int) &#123; def this() &#123; this(null, 0) &#125;&#125;class ComplexNestedClass( var someNumber: Int, someFloat: Float, word: (Long, Long, String), hadoopCitizen: IntWritable) &#123; def this() &#123; this(0, 0, (0, 0, \"\"), new IntWritable(0)) &#125;&#125;这些字段表达式对于以上代码示例都是合法的：&quot;count&quot;：WC 类的 count 字段。&quot;complex&quot;：递归选择 POJO 类型 ComplexNestedClass 的 complex 字段的全部字段。&quot;complex.word._3&quot;：选择嵌套 Tuple3 类型的最后一个字段。&quot;complex.hadoopCitizen&quot;：选择 hadoop 的 IntWritable 类型。使用键选择器函数定义键定义键的另一种方法是“键选择器”函数。键选择器函数将单个元素作为输入并返回元素的键。键可以是任意类型，并且可以由确定性计算得出。下例展示了一个简单返回对象字段的键选择器函数：1234&#x2F;&#x2F; 普通的 case classcase class WC(word: String, count: Int)val words: DataStream[WC] &#x3D; &#x2F;&#x2F; [...]val keyed &#x3D; words.keyBy( _.word )指定转换函数大多数转换操作需要用户定义函数。本节列举了指定它们的不同方法。Lambda 函数正如前面的例子中所见，所有操作都接受 lambda 函数来描述操作：12val data: DataSet[String] = // [...]data.filter &#123; _.startsWith(\"http://\") &#125;1234val data: DataSet[Int] = // [...]data.reduce &#123; (i1,i2) =&gt; i1 + i2 &#125;// 或者data.reduce &#123; _ + _ &#125;富函数所有需要用户定义函数的转换操作都可以将富函数作为参数。例如，对于1data.map &#123; x =&gt; x.toInt &#125;你可以替换成123class MyMapFunction extends RichMapFunction[String, Int] &#123; def map(in: String):Int = &#123; in.toInt &#125;&#125;;并像往常一样将函数传递给 map 转换操作：1data.map(new MyMapFunction())富函数也可以被定义为匿名类：123data.map (new RichMapFunction[String, Int] &#123; def map(in: String):Int &#x3D; &#123; in.toInt &#125;&#125;)富函数为用户定义函数（map、reduce 等）额外提供了 4 个方法： open、close、getRuntimeContext 和 setRuntimeContext。这些方法有助于向函数传参（请参阅 向函数传递参数）、 创建和终止本地状态、访问广播变量（请参阅 广播变量）、访问诸如累加器和计数器等运行时信息（请参阅 累加器和计数器）和迭代信息（请参阅 迭代）。支持的数据类型Flink 对于 DataSet 或 DataStream 中可以包含的元素类型做了一些限制。这么做是为了使系统能够分析类型以确定有效的执行策略。有七种不同的数据类型：Java Tuple 和 Scala Case ClassJava POJO基本数据类型常规的类值Hadoop Writable特殊类型Tuple 和 Case ClassScala Case Class（以及作为 Case Class 的特例的 Scala Tuple）是复合类型，包含固定数量的各种类型的字段。Tuple 的字段从 1 开始索引。例如 _1 指第一个字段。Case Class 字段用名称索引。123456789case class WordCount(word: String, count: Int)val input = env.fromElements( WordCount(\"hello\", 1), WordCount(\"world\", 2)) // Case Class 数据集input.keyBy(\"word\")// 以字段表达式“word”为键val input2 = env.fromElements((\"hello\", 1), (\"world\", 2)) // Tuple2 数据集input2.keyBy(0, 1) // 以第 0 和第 1 个字段为键POJOFlink 将满足如下条件的 Java 和 Scala 的类作为特殊的 POJO 数据类型处理：类必须是公有的。它必须有一个公有的无参构造器（默认构造器）。所有的字段要么是公有的要么必须可以通过 getter 和 setter 函数访问。例如一个名为 foo 的字段，它的 getter 和 setter 方法必须命名为 getFoo() 和 setFoo()。字段的类型必须被已注册的序列化程序所支持。POJO 通常用 PojoTypeInfo 表示，并使用 PojoSerializer（Kryo 作为可配置的备用序列化器）序列化。 例外情况是 POJO 是 Avro 类型（Avro 指定的记录）或作为“Avro 反射类型”生成时。 在这种情况下POJO 由 AvroTypeInfo 表示，并且由 AvroSerializer 序列化。 如果需要，你可以注册自己的序列化器；更多信息请参阅 序列化。Flink 分析 POJO 类型的结构，也就是说，它会推断出 POJO 的字段。因此，POJO 类型比常规类型更易于使用。此外，Flink 可以比一般类型更高效地处理 POJO。下例展示了一个拥有两个公有字段的简单 POJO。123456789class WordWithCount(var word: String, var count: Int) &#123; def this() &#123; this(null, -1) &#125;&#125;val input = env.fromElements( new WordWithCount(\"hello\", 1), new WordWithCount(\"world\", 2)) // Case Class 数据集input.keyBy(\"word\")// 以字段表达式“word”为键基本数据类型Flink 支持所有 Java 和 Scala 的基本数据类型如 Integer、 String、和 Double。常规的类Flink 支持大部分 Java 和 Scala 的类（API 和自定义）。 除了包含无法序列化的字段的类，如文件指针，I / O流或其他本地资源。遵循 Java Beans 约定的类通常可以很好地工作。Flink 对于所有未识别为 POJO 类型的类（请参阅上面对于的 POJO 要求）都作为常规类处理。 Flink 将这些数据类型视为黑盒，并且无法访问其内容（为了诸如高效排序等目的）。常规类使用 Kryo 序列化框架进行序列化和反序列化。值值 类型手工描述其序列化和反序列化。它们不是通过通用序列化框架，而是通过实现org.apache.flinktypes.Value 接口的 read 和 write 方法来为这些操作提供自定义编码。当通用序列化效率非常低时，使用值类型是合理的。例如，用数组实现稀疏向量。已知数组大部分元素为零，就可以对非零元素使用特殊编码，而通用序列化只会简单地将所有数组元素都写入。org.apache.flinktypes.CopyableValue 接口以类似的方式支持内部手工克隆逻辑。Flink 有与基本数据类型对应的预定义值类型。（ByteValue、 ShortValue、 IntValue、LongValue、 FloatValue、DoubleValue、 StringValue、CharValue、 BooleanValue）。这些值类型充当基本数据类型的可变变体：它们的值可以改变，允许程序员重用对象并减轻垃圾回收器的压力。Hadoop Writable可以使用实现了 org.apache.hadoop.Writable 接口的类型。它们会使用 write() 和 readFields() 方法中定义的序列化逻辑。特殊类型可以使用特殊类型，包括 Scala 的 Either、Option 和 Try。 Java API 有对 Either 的自定义实现。 类似于 Scala 的 Either，它表示一个具有 Left 或 Right 两种可能类型的值。 Either 可用于错误处理或需要输出两种不同类型记录的算子。类型擦除和类型推断_Java 编译器在编译后抛弃了大量泛型类型信息。这在 Java 中被称作 _类型擦除_。它意味着在运行时，对象的实例已经不知道它的泛型类型了。例如 DataStream&lt;String&gt; 和 DataStream&lt;Long&gt; 的实例在 JVM 看来是一样的。Flink 在准备程序执行时（程序的 main 方法被调用时）需要类型信息。Flink Java API 尝试重建以各种方式丢弃的类型信息，并将其显式存储在数据集和算子中。你可以通过 DataStream.getType() 获取数据类型。此方法返回 TypeInformation 的一个实例，这是 Flink 内部表示类型的方式。类型推断有其局限性，在某些情况下需要程序员的“配合”。 这方面的示例是从集合创建数据集的方法，例如 ExecutionEnvironment.fromCollection()，你可以在这里传递一个描述类型的参数。 像MapFunction&lt;I, O&gt; 这样的泛型函数同样可能需要额外的类型信息。可以通过输入格式和函数实现 ResultTypeQueryable 接口，以明确告知 API 其返回类型。 被调函数的_输入类型_通常可以通过先前操作的结果类型来推断。累加器和计数器累加器简单地由 加法操作 和 最终累加结果构成，可在作业结束后使用。最简单的累加器是一个 计数器：你可以使用 Accumulator.add(V value) 方法递增它。作业结束时 Flink 会合计（合并）所有的部分结果并发送给客户端。累加器在 debug 或者你想快速了解数据的时候非常有用。Flink 目前有如下 内置累加器。它们每一个都实现了 Accumulator 接口。IntCounter, LongCounter 和 DoubleCounter： 有关使用计数器的示例，请参见下文。Histogram: 离散数量桶的直方图实现。在内部，它只是一个从整数到整数的映射。你可以用它计算值的分布，例如一个词频统计程序中每行词频的分布。如何使用累加器：**首先你必须在要使用它的用户定义转换函数中创建累加器对象（下例为计数器）。1private IntCounter numLines = new IntCounter();其次，你必须注册累加器对象，通常在富函数的 open() 方法中。在这里你还可以定义名称。1getRuntimeContext().addAccumulator(\"num-lines\", this.numLines);你现在可以在算子函数中的任何位置使用累加器，包括 open() 和 close() 方法。1this.numLines.add(1);总体结果将存储在 JobExecutionResult 对象中，该对象是从执行环境的 execute() 方法返回的 （目前这仅在执行等待作业完成时才有效）。1myJobExecutionResult.getAccumulatorResult(&quot;num-lines&quot;)每个作业的所有累加器共享一个命名空间。 这样你就可以在作业的不同算子函数中使用相同的累加器。Flink 会在内部合并所有同名累加器。关于累加器和迭代请注意： 目前，累加器的结果只有在整个作业结束以后才可用。我们还计划实现在下一次迭代中使前一次迭代的结果可用。你可以使用 Aggregators 计算每次迭代的统计信息，并根据这些信息确定迭代何时终止。自定义累加器：要实现你自己的累加器，只需编写累加器接口的实现即可。如果你认为 Flink 应该提供你的自定义累加器，请创建 pull request。你可以选择实现 Accumulator 或者 SimpleAccumulator。Accumulator&lt;V,R&gt; 最灵活：它为要递增的值定义类型 V，为最终结果定义类型 R。例如对于 histogram，V 是数字而 R 是 histogram。SimpleAccumulator 则适用于两个类型相同的情况，例如计数器。Scala API扩展为了在Scala和Java API之间保持相当程度的一致性，用于批处理和流传输的标准API省略了一些允许在Scala中进行高水平表达的功能。如果您想_享受完整的Scala体验_，则可以选择加入通过隐式转换来增强Scala API的扩展。要使用所有可用的扩展，您只需import为DataSet API 添加一个简单的1import org.apache.flink.api.scala.extensions._或DataStream API1import org.apache.flink.streaming.api.scala.extensions._另外，您也可以导入单个扩展_一个点菜_只使用那些你喜欢。通常，DataSet和DataStream API都不接受匿名模式匹配函数来解构元组，案例类或集合，如下所示：123456val data: DataSet[(Int, String, Double)] = // [...]data.map &#123; case (id, name, temperature) =&gt; // [...] // The previous line causes the following compilation error: // \"The argument types of an anonymous function must be fully known. (SLS 8.5)\"&#125;此扩展在DataSet和DataStream Scala API中引入了新方法，这些新方法在扩展API中具有一对一的对应关系。这些委托方法确实支持匿名模式匹配功能。DataSet APIMethodOriginalExamplemapWithmap (DataSet)```data.mapWith {case (_, value) =&gt; value.toString}12 || **mapPartitionWith** | **mapPartition (DataSet)** |data.mapPartitionWith {case head #:: _ =&gt; head}12 || **flatMapWith** | **flatMap (DataSet)** |data.flatMapWith {case (_, name, visitTimes) =&gt; visitTimes.map(name -&gt; _)}12 || **filterWith** | **filter (DataSet)** |data.filterWith {case Train(_, isOnTime) =&gt; isOnTime}12 || **reduceWith** | **reduce (DataSet, GroupedDataSet)** |data.reduceWith {case ((, amount1), (, amount2)) =&gt; amount1 + amount2}12 || **reduceGroupWith** | **reduceGroup (GroupedDataSet)** |data.reduceGroupWith {case id #:: value #:: _ =&gt; id -&gt; value}12 || **groupingBy** | **groupBy (DataSet)** |data.groupingBy {case (id, _, _) =&gt; id}12 || **sortGroupWith** | **sortGroup (GroupedDataSet)** |grouped.sortGroupWith(Order.ASCENDING) {case House(_, value) =&gt; value}12 || **combineGroupWith** | **combineGroup (GroupedDataSet)** |grouped.combineGroupWith {case header #:: amounts =&gt; amounts.sum}12 || **projecting** | **apply (JoinDataSet, CrossDataSet)** |data1.join(data2).whereClause(case (pk, _) =&gt; pk).isEqualTo(case (_, fk) =&gt; fk).projecting {case ((pk, tx), (products, fk)) =&gt; tx -&gt; products}data1.cross(data2).projecting {case ((a, ), (, b) =&gt; a -&gt; b}12 || **projecting** | **apply (CoGroupDataSet)** |data1.coGroup(data2).whereClause(case (pk, _) =&gt; pk).isEqualTo(case (_, fk) =&gt; fk).projecting {case (head1 #:: _, head2 #:: _) =&gt; head1 -&gt; head2}}123456789 |&lt;a name&#x3D;&quot;datastream-api&quot;&gt;&lt;&#x2F;a&gt;#### &lt;a name&#x3D;&quot;n1Myp&quot;&gt;&lt;&#x2F;a&gt;#### DataStream API| Method | Original | Example || :--- | :--- | :---: || **mapWith** | **map (DataStream)** |data.mapWith {case (_, value) =&gt; value.toString}12 || **flatMapWith** | **flatMap (DataStream)** |data.flatMapWith {case (_, name, visits) =&gt; visits.map(name -&gt; _)}12 || **filterWith** | **filter (DataStream)** |data.filterWith {case Train(_, isOnTime) =&gt; isOnTime}12 || **keyingBy** | **keyBy (DataStream)** |data.keyingBy {case (id, _, _) =&gt; id}12 || **mapWith** | **map (ConnectedDataStream)** |data.mapWith(map1 = case (_, value) =&gt; value.toString,map2 = case (_, _, value, _) =&gt; value + 1)12 || **flatMapWith** | **flatMap (ConnectedDataStream)** |data.flatMapWith(flatMap1 = case (_, json) =&gt; parse(json),flatMap2 = case (_, _, json, _) =&gt; parse(json))12 || **keyingBy** | **keyBy (ConnectedDataStream)** |data.keyingBy(key1 = case (_, timestamp) =&gt; timestamp,key2 = case (id, _, _) =&gt; id)12 || **reduceWith** | **reduce (KeyedStream, WindowedStream)** |data.reduceWith {case ((, sum1), (, sum2) =&gt; sum1 + sum2}12 || **foldWith** | **fold (KeyedStream, WindowedStream)** |data.foldWith(User(bought = 0)) {case (User(b), (_, items)) =&gt; User(b + items.size)}12 || **applyWith** | **apply (WindowedStream)** |data.applyWith(0)(foldFunction = case (sum, amount) =&gt; sum + amountwindowFunction = case (k, w, sum) =&gt; // […])12 || **projecting** | **apply (JoinedStream)** |data1.join(data2).whereClause(case (pk, _) =&gt; pk).isEqualTo(case (_, fk) =&gt; fk).projecting {case ((pk, tx), (products, fk)) =&gt; tx -&gt; products}1234567 |&lt;br &#x2F;&gt;有关每种方法的语义的更多信息，请参考 [DataSet](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;batch&#x2F;index.html)和[DataStream](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;datastream_api.html) API文档。&lt;br &#x2F;&gt;&lt;br &#x2F;&gt;要专门使用此扩展，可以添加以下内容&#96;import&#96;：&#96;&#96;&#96;scalaimport org.apache.flink.api.scala.extensions.acceptPartialFunctions用于数据集扩展和1import org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions以下代码片段显示了如何一起使用这些扩展方法（与DataSet API一起使用）的最小示例：12345678910111213141516171819object Main &#123; import org.apache.flink.api.scala.extensions._ case class Point(x: Double, y: Double) def main(args: Array[String]): Unit = &#123; val env = ExecutionEnvironment.getExecutionEnvironment val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6)) ds.filterWith &#123; case Point(x, _) =&gt; x &gt; 1 &#125;.reduceWith &#123; case (Point(x1, y1), (Point(x2, y2))) =&gt; Point(x1 + y1, x2 + y2) &#125;.mapWith &#123; case Point(x, y) =&gt; (x, y) &#125;.flatMapWith &#123; case (x, y) =&gt; Seq(\"x\" -&gt; x, \"y\" -&gt; y) &#125;.groupingBy &#123; case (id, value) =&gt; id &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"MySQL中如何实现事务","slug":"MySQL中如何实现事务","date":"2018-03-10T14:04:36.000Z","updated":"2020-05-10T14:06:56.435Z","comments":true,"path":"2018/03/10/MySQL中如何实现事务/","link":"","permalink":"cpeixin.cn/2018/03/10/MySQL%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"事务（Transaction），一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。事务其实就是并发控制的基本单位；特性事务是恢复和并发控制的基本单位。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。原子性（atomicity）。一个事务是一个不可分割的工作单位，事务中包括的操作要么都做，要么都不做。一致性（consistency）。事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。隔离性（isolation）。一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。持久性（durability）。持久性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。MySQL怎么实现事务的？事务是MySQL等关系型数据库区别于NoSQL的重要方面，是保证数据一致性的重要手段。MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你既需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！MySQL在架构方面分为三层：第一层：处理客户端连接、授权认证等。第二层：服务器层，负责查询语句的解析、优化、缓存以及内置函数的实现、存储过程等。第三层：存储引擎，负责MySQL中数据的存储和提取。MySQL中服务器层不管理事务，事务是由存储引擎实现的。**MySQL支持事务的存储引擎有InnoDB、BDB Cluster等，其中InnoDB的使用最为广泛；其他存储引擎不支持事务，如MyIsam、Memory等。实现原理 #### 原子性如果事务不具备原子性，那么就没办法保证同一个事务中的所有操作都被执行或者未被执行了，整个数据库系统就既不可用也不可信。想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚。在 MySQL 中，恢复机制是通过_回滚日志_（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。回滚日志除了能够在发生错误或者用户执行 ROLLBACK 时提供回滚相关的信息，它还能够在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志逻辑地将数据库中的修改撤销掉看，可以理解为，我们在事务中使用的每一条 INSERT 都对应了一条 DELETE，每一条 UPDATE 也都对应一条相反的 UPDATE 语句。持久性事务的持久性就体现在，一旦事务被提交，那么数据一定会被写入到数据库中并持久存储起来。与原子性一样，事务的持久性也是通过日志来实现的，MySQL 使用重做日志（redo log）实现事务的持久性，重做日志由两部分组成，一是内存中的重做日志缓冲区，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在磁盘上的重做日志文件，它是持久的。当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上，在 InnoDB 中，重做日志都是以 512 字节的块的形式进行存储的，同时因为块的大小与磁盘扇区大小相同，所以重做日志的写入可以保证原子性，不会由于机器断电导致重做日志仅写入一半并留下脏数据。在MySQL中还存在binlog(二进制日志)也可以记录写操作并用于数据的恢复，但二者是有着根本的不同的：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。隔离性与原子性、持久性侧重于研究事务本身不同，隔离性研究的是不同事务之间的相互影响。**隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们仅考虑最简单的读操作和写操作(暂时不考虑带锁读等特殊操作)，那么隔离性的探讨，主要可以分为两个方面：(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性概括来说，InnoDB实现的重复读，通过锁机制、数据的隐藏列、undo log和类next-key lock，实现了一定程度的隔离性，可以满足大多数场景的需要。一致性一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。**可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。实现一致性的措施包括：保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致","categories":[{"name":"DataBase","slug":"DataBase","permalink":"cpeixin.cn/categories/DataBase/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"cpeixin.cn/tags/mysql/"}]},{"title":"Flink 名词解释","slug":"Flink-名词解释","date":"2018-03-10T07:32:00.000Z","updated":"2020-05-11T09:45:22.543Z","comments":true,"path":"2018/03/10/Flink-名词解释/","link":"","permalink":"cpeixin.cn/2018/03/10/Flink-%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/","excerpt":"","text":"Flink Application ClusterFlink Application Cluster 是一个专用的 Flink Cluster，它仅用于执行单个 Flink Job。Flink Cluster的生命周期与 Flink Job的生命周期绑定在一起。以前，Flink Application Cluster 也称为_job mode_的 Flink Cluster。和 Flink Session Cluster 作对比。Flink Cluster一般情况下，Flink 集群是由一个 Flink Master 和一个或多个 Flink TaskManager 进程组成的分布式系统。EventEvent 是对应用程序建模的域的状态更改的声明。它可以同时为流或批处理应用程序的 input 和 output，也可以单独是 input 或者 output 中的一种。Event 是特殊类型的 Record。ExecutionGraph见 Physical Graph。FunctionFunction 是由用户实现的，并封装了 Flink 程序的应用程序逻辑。大多数 Function 都由相应的 Operator 封装。InstanceInstance 常用于描述运行时的特定类型(通常是 Operator 或者 Function)的一个具体实例。由于 Apache Flink 主要是用 Java 编写的，所以，这与 Java 中的 Instance 或 Object 的定义相对应。在 Apache Flink 的上下文中，parallel instance 也常用于强调同一 Operator 或者 Function 的多个 instance 以并行的方式运行。Flink JobFlink Job 代表运行时的 Flink 程序。Flink Job 可以提交到长时间运行的 Flink Session Cluster，也可以作为独立的 Flink Application Cluster 启动。JobGraph见 Logical Graph。Flink JobManagerJobManager 是在 Flink Master 运行中的组件之一。JobManager 负责监督单个作业 Task 的执行。以前，整个 Flink Master 都叫做 JobManager。Logical GraphLogical Graph 是一种描述流处理程序的高阶逻辑有向图。节点是Operator，边代表输入/输出关系、数据流和数据集中的之一。Managed StateManaged State 描述了已在框架中注册的应用程序的托管状态。对于托管状态，Apache Flink 会负责持久化和重伸缩等事宜。Flink MasterFlink Master 是 Flink Cluster 的主节点。它包含三个不同的组件：Flink Resource Manager、Flink Dispatcher、运行每个 Flink Job 的 Flink JobManager。OperatorLogical Graph 的节点。算子执行某种操作，该操作通常由 Function 执行。Source 和 Sink 是数据输入和数据输出的特殊算子。Operator Chain算子链由两个或多个连续的 Operator 组成，两者之间没有任何的重新分区。同一算子链内的算子可以彼此直接传递 record，而无需通过序列化或 Flink 的网络栈。Partition分区是整个数据流或数据集的独立子集。通过将每个 Record 分配给一个或多个分区，来把数据流或数据集划分为多个分区。在运行期间，Task 会消费数据流或数据集的分区。改变数据流或数据集分区方式的转换通常称为重分区。Physical GraphPhysical graph 是一个在分布式运行时，把 Logical Graph 转换为可执行的结果。节点是 Task，边表示数据流或数据集的输入/输出关系或 partition。RecordRecord 是数据集或数据流的组成元素。Operator 和 Function接收 record 作为输入，并将 record 作为输出发出。Flink Session Cluster长时间运行的 Flink Cluster，它可以接受多个 Flink Job 的执行。此 Flink Cluster 的生命周期不受任何 Flink Job 生命周期的约束限制。以前，Flink Session Cluster 也称为 session mode 的 Flink Cluster，和 Flink Application Cluster 相对应。State Backend对于流处理程序，Flink Job 的 State Backend 决定了其 state 是如何存储在每个 TaskManager 上的（ TaskManager 的 Java 堆栈或嵌入式 RocksDB），以及它在 checkpoint 时的写入位置（ Flink Master 的 Java 堆或者 Filesystem）。Sub-TaskSub-Task 是负责处理数据流 Partition 的 Task。”Sub-Task”强调的是同一个 Operator 或者 Operator Chain 具有多个并行的 Task 。TaskTask 是 Physical Graph 的节点。它是基本的工作单元，由 Flink 的 runtime 来执行。Task 正好封装了一个 Operator 或者 Operator Chain 的 _parallel instance_。Flink TaskManagerTaskManager 是 Flink Cluster 的工作进程。Task 被调度到 TaskManager 上执行。TaskManager 相互通信，只为在后续的 Task 之间交换数据。TransformationTransformation 应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。Transformation 可能会在每个记录的基础上更改数据流或数据集，但也可以只更改其分区或执行聚合。虽然 Operator 和 Function 是 Flink API 的“物理”部分，但 Transformation 只是一个 API 概念。具体来说，大多数（但不是全部）Transformation 是由某些 Operator 实现的。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 运行架构","slug":"Flink-运行架构","date":"2018-03-05T07:32:00.000Z","updated":"2020-05-12T14:53:23.279Z","comments":true,"path":"2018/03/05/Flink-运行架构/","link":"","permalink":"cpeixin.cn/2018/03/05/Flink-%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/","excerpt":"","text":"任务提交流程客户端提交流程1.执行命令**:123bin/flink run -d -m yarn-cluster 或者bin/yarn-session.sh提交per-job运行模式或session运行模式的应用；2.解析命令参数项并初始化，启动指定运行模式，如果是per-job运行模式将根据命令行参数指定的Job主类创建job graph；如果可以从命令行参数(-yid )或YARN properties临时文件(${java.io.tmpdir}/.yarn-properties-${user.name})中获取应用ID，向指定的应用提交Job；否则当命令行参数中包含 -d（表示detached模式）和 -m yarn-cluster（表示指定YARN集群模式），启动per-job运行模式；否则当命令行参数项不包含 -yq（表示查询YARN集群可用资源）时，启动session运行模式；3.获取YARN集群信息、新应用ID并启动运行前检查；**通过YarnClient向YARN ResourceManager(下文缩写为：YARN RM，YARN Master节点，负责整个集群资源的管理和调度)请求创建一个新应用（YARN RM收到创建应用请求后生成新应用ID和container申请的资源上限后返回），并且获取YARN Slave节点报告（YARN RM返回全部slave节点的ID、状态、rack、http地址、总资源、已使用资源等信息）；运行前检查：(1) 简单验证YARN集群能否访问；(2) 最大node资源能否满足flink JobManager/TaskManager vcores资源申请需求；(3) 指定queue是否存在(不存在也只是打印WARN信息，后续向YARN提交时排除异常并退出)；(4)当预期应用申请的Container资源会超出YARN资源限制时抛出异常并退出；(5) 当预期应用申请不能被满足时（例如总资源超出YARN集群可用资源总量、Container申请资源超出NM可用资源最大值等）提供一些参考信息。4.将应用配置(flink-conf.yaml、logback.xml、log4j.properties)和相关文件(flink jars、ship files、user jars、job graph等)上传至分布式存储(例如HDFS)的应用暂存目录(/user/${user.name}/.flink/)；5.准备应用提交上下文(ApplicationSubmissionContext，包括应用的名称、类型、队列、标签等信息和应用Master的container的环境变量、classpath、资源大小等)，注册处理部署失败的shutdown hook（清理应用对应的HDFS目录），然后通过YarnClient向YARN RM提交应用；6.循环等待直到应用状态为RUNNING，包含两个阶段：循环等待应用提交成功（SUBMITTED）：默认每隔200ms通过YarnClient获取应用报告，如果应用状态不是NEW和NEW_SAVING则认为提交成功并退出循环，每循环10次会将当前的应用状态输出至日志：”Application submission is not finished, submitted application is still in “，提交成功后输出日志：”Submitted application “循环等待应用正常运行（RUNNING）：每隔250ms通过YarnClient获取应用报告，每轮循环也会将当前的应用状态输出至日志：”Deploying cluster, current state “。应用状态成功变为RUNNING后将输出日志”YARN application has been deployed successfully.” 并退出循环，如果等到的是非预期状态如FAILED/FINISHED/KILLED,就会在输出YARN返回的诊断信息（”The YARN application unexpectedly switched to state during deployment. Diagnostics from YARN: …”）之后抛出异常并退出。Flink Cluster启动流程1.YARN RM中的ClientRMService（为普通用户提供的RPC服务组件，处理来自客户端的各种RPC请求，比如查询YARN集群信息，提交、终止应用等）接收到应用提交请求，简单校验后将请求转交RMAppManager（YARN RM内部管理应用生命周期的组件）；2.RMAppManager根据应用提交上下文内容创建初始状态为NEW的应用，将应用状态持久化到RM状态存储服务（例如ZooKeeper集群，RM状态存储服务用来保证RM重启、HA切换或发生故障后集群应用能够正常恢复，后续流程中的涉及状态存储时不再赘述），应用状态变为NEW_SAVING；3.应用状态存储完成后，应用状态变为SUBMITTED；RMAppManager开始向ResourceScheduler（YARN RM可拔插资源调度器，YARN自带三种调度器FifoScheduler/FairScheduler/CapacityScheduler，其中CapacityScheduler支持功能最多使用最广泛，FifoScheduler功能最简单基本不可用，今年社区已明确不再继续支持FairScheduler，建议已有用户迁至CapacityScheduler）提交应用，如果无法正常提交（例如队列不存在、不是叶子队列、队列已停用、超出队列最大应用数限制等）则抛出拒绝该应用，应用状态先变为FINAL_SAVING触发应用状态存储流程并在完成后变为FAILED；如果提交成功，应用状态变为ACCEPTED；4.开始创建应用运行实例(ApplicationAttempt，由于一次运行实例中最重要的组件是ApplicationMaster，下文简称AM，它的状态代表了ApplicationAttempt的当前状态，所以ApplicationAttempt实际也代表了AM)，初始状态为NEW；5.初始化应用运行实例信息，并向ApplicationMasterService（AM&amp;RM协议接口服务，处理来自AM的请求，主要包括注册和心跳）注册，应用实例状态变为SUBMITTED；6.RMAppManager维护的应用实例开始初始化AM资源申请信息并重新校验队列，然后向ResourceScheduler申请AM Container（Container是YARN中资源的抽象，包含了内存、CPU等多维度资源），应用实例状态变为ACCEPTED；7.ResourceScheduler会根据优先级（队列/应用/请求每个维度都有优先级配置）从根队列开始层层递进，先后选择当前优先级最高的子队列、应用直至具体某个请求，然后结合集群资源分布等情况作出分配决策，AM Container分配成功后，应用实例状态变为ALLOCATED_SAVING，并触发应用实例状态存储流程，存储成功后应用实例状态变为ALLOCATED；8.RMAppManager维护的应用实例开始通知ApplicationMasterLauncher（AM生命周期管理服务，负责启动或清理AM container）启动AM container，ApplicationMasterLauncher与YARN NodeManager（下文简称YARN NM，与YARN RM保持通信，负责管理单个节点上的全部资源、Container生命周期、附属服务等，监控节点健康状况和Container资源使用）建立通信并请求启动AM container；9.ContainerManager（YARN NM核心组件，管理所有Container的生命周期）接收到AM container启动请求，YARN NM开始校验Container Token及资源文件，创建应用实例和Container实例并存储至本地，结果返回后应用实例状态变为LAUNCHED；10.ResourceLocalizationService（资源本地化服务，负责Container所需资源的本地化。它能够按照描述从HDFS上下载Container所需的文件资源，并尽量将它们分摊到各个磁盘上以防止出现访问热点）初始化各种服务组件、创建工作目录、从HDFS下载运行所需的各种资源至Container工作目录（路径为: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache//）；11.ContainersLauncher（负责container的具体操作，包括启动、重启、恢复和清理等）将待运行Container所需的环境变量和运行命令写到Container工作目录下的launch_container.sh脚本中，然后运行该脚本启动Container；12.Container进程加载并运行ClusterEntrypoint**(Flink JobManager入口类，每种集群部署模式和应用运行模式都有相应的实现，例如在YARN集群部署模式下，per-job应用运行模式实现类是YarnJobClusterEntrypoint，session应用运行模式实现类是YarnSessionClusterEntrypoint)，首先初始化相关运行环境：输出各软件版本及运行环境信息、命令行参数项、classpath等信息；注册处理各种SIGNAL的handler:记录到日志注册JVM关闭保障的shutdown hook：避免JVM退出时被其他shutdown hook阻塞打印YARN运行环境信息：用户名从运行目录中加载flink conf初始化文件系统创建并启动各类内部服务（包括RpcService、HAService、BlobServer、HeartbeatServices、MetricRegistry、ExecutionGraphStore等）将RPC address和port更新到flink conf配置13.启动ResourceManager（Flink资源管理核心组件，包含YarnResourceManager和SlotManager两个子组件，YarnResourceManager负责外部资源管理，与YARN RM建立通信并保持心跳，申请或释放TaskManager资源，注销应用等；SlotManager则负责内部资源管理，维护全部Slot信息和状态）及相关服务，创建异步AMRMClient，开始注册AM，注册成功后每隔一段时间（心跳间隔配置项：${yarn.heartbeat.interval}，默认5s）向YARN RM发送心跳来发送资源更新请求和接受资源变更结果。YARN RM内部该应用和应用运行实例的状态都变为RUNNING，并通知AMLivelinessMonitor服务监控AM是否存活状态，当心跳超过一定时间（默认10分钟）触发AM failover流程；14.启动**Dispatcher（负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager）及相关服务（包括REST endpoint等），在per-job运行模式下，Dispatcher将直接从Container工作目录加载JobGraph文件；在session运行模式下，Dispatcher将在接收客户端提交的Job（_通过BlockServer接收job graph文件）后再进行后续流程；15.根据JobGraph启动JobManager（负责作业调度、管理Job和Task的生命周期），构建 ExecutionGraph（JobGraph的并行化版本，调度层最核心的数据结构）；16.JobManager开始执行ExecutionGraph，向ResourceManager申请资源；17.ResourceManager将资源请求加入等待请求队列，并通过心跳向YARN RM申请新的Container资源来启动TaskManager进程；后续流程如果有空闲Slot资源，SlotManager将其分配给等待请求队列中匹配的请求，不用再通过18. YarnResourceManager申请新的TaskManager；18.YARN ApplicationMasterService接收到资源请求后，解析出新的资源请求并更新应用请求信息；19.YARN ResourceScheduler成功为该应用分配资源后更新应用信息，ApplicationMasterService接收到Flink JobManager的下一次心跳时返回新分配资源信息；20.Flink ResourceManager接收到新分配的Container资源后，准备好TaskManager启动上下文（ContainerLauncherContext，生成TaskManager配置并上传至分布式存储，配置其他依赖和环境变量等），然后向YARN NM申请启动TaskManager进程，YARN NM启动Container的流程与AM Container启动流程基本类似，区别在于应用实例在NM上已存在并未RUNNING状态时则跳过应用实例初始化流程，这里不再赘述；21.TaskManager进程加载并运行YarnTaskExecutorRunner（Flink TaskManager入口类），初始化流程完成后启动TaskExecutor（负责执行Task相关操作）；22.TaskExecutor启动后先向ResourceManager注册，成功后再向SlotManager汇报自己的Slot资源与状态；SlotManager接收到Slot空闲资源后主动触发Slot分配，从等待请求队列中选出合适的资源请求后，向TaskManager请求该Slot资源23.TaskManager收到请求后检查该Slot是否可分配（不存在则返回异常信息）、Job是否已注册（没有则先注册再分配Slot），检查通过后将Slot分配给JobManager；24.JobManager检查Slot分配是否重复**，通过后通知Execution执行部署task流程，向TaskExecutor提交task；TaskExecutor启动新的线程运行Task。任务和算子链分布式计算中，Flink 将算子（operator）的 subtask _链接（chain）_成 task。每个 task 由一个线程执行。把算子链接成 tasks 能够减少线程间切换和缓冲的开销，在降低延迟的同时提高了整体吞吐量。链接操作的配置详情可参考：chaining docs下图的 dataflow 由五个 subtasks 执行，因此具有五个并行线程。Job Managers、Task Managers、客户端（Clients）Flink 运行时包含两类进程：JobManagers （也称为 masters_）协调分布式计算。它们负责调度任务、协调 checkpoints、协调故障恢复等。每个 Job 至少会有一个 JobManager。高可用部署下会有多个 JobManagers，其中一个作为 _leader_，其余处于 _standby 状态。TaskManagers（也称为 _workers_）执行 dataflow 中的 _tasks_（准确来说是 subtasks ），并且缓存和交换数据 _streams_。每个 Job 至少会有一个 TaskManager。JobManagers 和 TaskManagers 有多种启动方式：直接在机器上启动（该集群称为 standalone cluster），在容器或资源管理框架，如 YARN 或 Mesos，中启动。TaskManagers 连接到 JobManagers，通知后者自己可用，然后开始接手被分配的工作。客户端**虽然不是运行时（runtime）和作业执行时的一部分，但它是被用作准备和提交 dataflow 到 JobManager 的。提交完成之后，客户端可以断开连接，也可以保持连接来接收进度报告。客户端既可以作为触发执行的 Java / Scala 程序的一部分，也可以在命令行进程中运行./bin/flink run ...。Task Slots 和资源每个 worker（TaskManager）都是一个 JVM 进程_，并且可以在不同的线程中执行一个或多个 subtasks。为了控制 worker 接收 task 的数量，worker 拥有所谓的 task slots （至少一个）。每个 _task slots 代表 TaskManager 的一份固定资源子集。例如，具有三个 slots 的 TaskManager 会将其管理的内存资源分成三等份给每个 slot。 划分资源意味着 subtask 之间不会竞争资源，但是也意味着它们只拥有固定的资源。注意这里并没有 CPU 隔离，当前 slots 之间只是划分任务的内存资源。通过调整 slot 的数量，用户可以决定 subtasks 的隔离方式。每个 TaskManager 有一个 slot 意味着每组 task 在一个单独的 JVM 中运行（例如，在一个单独的容器中启动）。拥有多个 slots 意味着多个 subtasks 共享同一个 JVM。 Tasks 在同一个 JVM 中共享 TCP 连接（通过多路复用技术）和心跳信息（heartbeat messages）。它们还可能共享数据集和数据结构，从而降低每个 task 的开销。默认情况下，Flink 允许 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，只要它们来自同一个 job。因此，一个 slot 可能会负责这个 job 的整个管道（pipeline）。允许 slot sharing 有两个好处：Flink 集群需要与 job 中使用的最高并行度一样多的 slots。这样不需要计算作业总共包含多少个 tasks（具有不同并行度）。更好的资源利用率。在没有 slot sharing 的情况下，简单的 subtasks（_source/map()_）将会占用和复杂的 subtasks （_window_）一样多的资源。通过 slot sharing，将示例中的并行度从 2 增加到 6 可以充分利用 slot 的资源，同时确保繁重的 subtask 在 TaskManagers 之间公平地获取资源。APIs 还包含了 resource group 机制，它可以用来防止不必要的 slot sharing。根据经验，合理的 slots 数量应该和 CPU 核数相同。在使用超线程（hyper-threading）时，每个 slot 将会占用 2 个或更多的硬件线程上下文（hardware thread contexts）。State Backendskey/values 索引存储的数据结构取决于 state backend 的选择。一类 state backend 将数据存储在内存的哈希映射中，另一类 state backend 使用 RocksDB 作为键/值存储。除了定义保存状态（state）的数据结构之外， state backend 还实现了获取键/值状态的时间点快照的逻辑，并将该快照存储为 checkpoint 的一部分。Savepoints用 Data Stream API 编写的程序可以从 savepoint 继续执行。Savepoints 允许在不丢失任何状态的情况下升级程序和 Flink 集群。Savepoints 是手动触发的 checkpoints，它依靠常规的 checkpoint 机制获取程序的快照并将其写入 state backend。在执行期间，程序会定期在 worker 节点上创建快照并生成 checkpoints。对于恢复，Flink 仅需要最后完成的 checkpoint，而一旦完成了新的 checkpoint，旧的就可以被丢弃。Savepoints 类似于这些定期的 checkpoints，除了它们是由用户触发并且在新的 checkpoint 完成时不会自动过期。你可以通过命令行 或在取消一个 job 时通过 REST API 来创建 Savepoints。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 编程模型","slug":"Flink-编程模型","date":"2018-03-01T07:32:00.000Z","updated":"2020-05-11T09:45:19.925Z","comments":true,"path":"2018/03/01/Flink-编程模型/","link":"","permalink":"cpeixin.cn/2018/03/01/Flink-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"抽象级别Flink提供了不同级别的抽象来开发流/批处理应用程序。最低级别的抽象仅提供状态流。它 通过Process Function嵌入到DataStream API中。它允许用户自由地处理一个或多个流中的事件，并使用一致的容错_状态_。此外，用户可以注册事件时间和处理时间回调，从而允许程序实现复杂的计算。实际上，大多数应用程序不需要上述低级抽象，而是针对Core API进行编程， 例如DataStream API（有界/无界流）和DataSet API （有界数据集）。这些流畅的API为数据处理提供了通用的构建块，例如各种形式的用户指定的转换，联接，聚合，窗口，状态等。这些API中处理的数据类型以相应编程语言中的类表示。低级_Process Function_与_DataStream API_集成在一起，从而使得仅对某些操作进行低级抽象成为可能。该_数据集API_提供的有限数据集的其他原语，如循环/迭代。该Table API是为中心的声明性DSL 表，其可被动态地改变的表（表示流时）。该Table API遵循（扩展）关系模型：表有一个模式连接（类似于在关系数据库中的表）和API提供可比的操作，如select, project, join, group-by, aggregate等表API程序以声明的方式定义应该进行哪些逻辑运算，_而 不确切地定义_运算的代码外观_。尽管Table API可以通过各种类型的用户定义函数进行扩展，但它的表达性不如_Core API。_，但使用起来更简洁（无需编写代码）。此外，Table API程序还经过优化程序，该优化程序在执行之前应用优化规则。可以在表和_DataStream / DataSet_之间无缝转换，从而允许程序将_Table API_以及_DataStream 和DataSet API混合使用。Flink提供的最高级别的抽象是SQL。这种抽象在语义和表达方式上均类似于_Table API_，但是将程序表示为SQL查询表达式。在SQL抽象与表API SQL查询紧密地相互作用，并且可以在中定义的表执行_表API_。程序和数据流Flink程序的基本构建部分是流和转换。（请注意，Flink的DataSet API中使用的DataSet也是内部流）从概念上讲，流是数据记录流（可能永无止境），而转换_是将一个或多个流作为一个操作的操作。输入，并产生一个或多个输出流。执行时，Flink程序将映射到由流和转换运算符组成的流数据流。每个数据流都以一个或多个源开始，并以一个或多个接收器结束。数据流类似于任意**有向无环图_（DAG）**。尽管可以通过_迭代_构造来允许特殊形式的循环 ，但是在大多数情况下，为简单起见，我们将对其进行介绍。程序中的转换与数据流中的运算符之间通常存在一一对应的关系。但是，有时，一个转换可能包含多个转换运算符。源和接收器记录在流连接器和批处理连接器文档中。转换记录在DataStream运算符和DataSet转换中。回到顶部并行数据流Flink中的程序本质上是并行的和分布式的。在执行期间，一个流具有一个或多个流分区，并且每个_运算符_具有一个或多个运算符子任务。操作员子任务彼此独立，并在不同的线程中执行，并且可能在不同的机器或容器上执行。_operator subtask_的数量是该特定操作员的并行性。流的并行性始终是其生产运营商的并行性。同一程序的不同运算符可能具有不同的并行度。流可以_按一对一_（或_转发_）模式或_重新分配_模式在两个运算符之间传输数据：一对一的流（例如，上图中的_Source_和_map（）_运算符之间）保留元素的分区和排序。这意味着_map（）_运算符的subtask [1] 将以与_Source_运算符的subtask [1]产生的相同顺序看到相同的元素。重新分配流（如上面的map（）和keyBy（） / window（）_之间以及 _keyBy（） / window（）和Sink之间_）会更改流的分区。每个_operator subtask都_将数据发送到不同的目标子任务，具体取决于所选的转换。实例是 _keyBy（） _（其重新分区通过散列键），_broadcast() 或者 rebalance() _（其重新分区随机地）。在_重新分配_交换中，元素之间的顺序仅保留在每对发送和接收子任务中（例如_map（）的 subtask [1]和map（）的 subtask [2]_keyBy / window_）。因此，在此示例中，保留了每个键内的顺序，但是并行性确实引入了不确定性，即不同键的聚合结果到达接收器的顺序。有关配置和控制并行性的详细信息，请参见并行执行文档。视窗汇总事件（例如，计数，总和）在流上的工作方式与批处理中的不同。例如，不可能计算流中的所有元素，因为流通常是无限的（无界）。相反，流上的聚合（计数，总和等）由窗口确定范围，例如_“过去5分钟内的计数”或“最近100个元素的总和”_。Windows可以是_时间驱动的_（例如：每30秒）或_数据驱动的_（例如：每100个元素）。通常可以区分不同类型的窗口，例如_滚动窗口_（无重叠）， _滑动窗口_（有重叠）和_会话窗口_（由不活动的间隙打断）。可以在此博客文章中找到更多窗口示例。更多详细信息在docs窗口中。时间在流式传输程序中引用时间（例如，定义窗口）时，可以引用不同的时间概念：事件时间是创建事件的时间。通常用事件中的时间戳记来描述，例如由生产传感器或生产服务附加。Flink通过时间戳分配器访问事件时间戳。接收时间是事件在源操作员进入Flink数据流的时间。处理时间是每个执行基于时间的操作的操作员的本地时间。有关如何处理时间的更多详细信息，请参见事件时间文档。有状态的操作尽管数据流中的许多操作一次仅查看一个_事件_（例如事件解析器），但某些操作会记住多个事件的信息（例如窗口运算符）。这些操作称为有状态。有状态操作的状态以可以被认为是嵌入式键/值存储的方式维护。严格将状态与有状态运算符读取的流一起进行分区和分发。因此，只有在_keyBy（）_函数之后，才可以在_键控流_上访问键/值状态，并且仅限于与当前事件的键关联的值。对齐流键和状态键可确保所有状态更新都是本地操作，从而确保了一致性而没有事务开销。这种对齐方式还允许Flink重新分配状态并透明地调整流分区。有关更多信息，请参阅关于state的文档。容错检查点Flink通过结合stream replay and checkpointing.来实现容错。检查点与每个输入流中的特定点以及每个运算符的对应状态有关。通过恢复operators的状态并从检查点开始重放事件，可以从检查点恢复流数据流，同时保持一致性_（一次处理语义）_。检查点间隔是在执行过程中权衡容错开销与恢复时间（需要重播的事件数）的一种方法。容错内部的描述提供了有关Flink如何管理检查点和相关主题的更多信息。有关启用和配置检查点的详细信息，请参见checkpointing API文档。串流处理Flink执行批处理程序，这是流程序的特例，其中流是有界的（元素数量有限）。甲_数据集_在内部视为数据流。因此，以上概念以同样的方式适用于批处理程序，也适用于流式程序，但有少量例外：批处理程序的容错功能不使用检查点。通过完全重播流来进行恢复。这是可能的，因为输入是有界的。这将成本更多地推向了恢复，但由于避免了检查点，因此使常规处理的成本降低了。DataSet API中的状态操作使用简化的内存中/核外数据结构，而不是键/值索引。DataSet API引入了特殊的同步（基于超步）迭代，仅在有限流上才有可能。有关详细信息，请查看迭代文档。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 应用场景","slug":"Flink-应用场景","date":"2018-02-22T09:00:58.000Z","updated":"2020-05-10T13:06:31.078Z","comments":true,"path":"2018/02/22/Flink-应用场景/","link":"","permalink":"cpeixin.cn/2018/02/22/Flink-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"Apache Flink 功能强大，支持开发和运行多种不同种类的应用程序。它的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。Flink 不仅可以运行在包括 YARN、 Mesos、Kubernetes 在内的多种资源管理框架上，还支持在裸机集群上独立部署。在启用高可用选项的情况下，它不存在单点失效问题。事实证明，Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。接下来我们将介绍 Flink 常见的几类应用并给出相关实例链接。事件驱动型应用数据分析应用数据管道应用事件驱动型应用什么是事件驱动型应用？事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。事件驱动型应用是在计算存储分离的传统应用基础上进化而来。在传统架构中，应用需要读写远程事务型数据库。相反，事件驱动型应用是基于状态化流处理来完成。在该设计中，数据和计算不会分离，应用只需访问本地（内存或磁盘）即可获取数据。系统容错性的实现依赖于定期向远程持久化存储写入 checkpoint。下图描述了传统应用和事件驱动型应用架构的区别。事件驱动型应用的优势？事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少。Flink 如何支持事件驱动型应用？事件驱动型应用会受制于底层流处理系统对时间和状态的把控能力，Flink 诸多优秀特质都是围绕这些方面来设计的。它提供了一系列丰富的状态操作原语，允许以精确一次的一致性语义合并海量规模（TB 级别）的状态数据。此外，Flink 还支持事件时间和自由度极高的定制化窗口逻辑，而且它内置的 ProcessFunction 支持细粒度时间控制，方便实现一些高级业务逻辑。同时，Flink 还拥有一个复杂事件处理（CEP）类库，可以用来检测数据流中的模式。Flink 中针对事件驱动应用的明星特性当属 savepoint。Savepoint 是一个一致性的状态映像，它可以用来初始化任意状态兼容的应用。在完成一次 savepoint 后，即可放心对应用升级或扩容，还可以启动多个版本的应用来完成 A/B 测试。典型的事件驱动型应用实例反欺诈异常检测基于规则的报警业务流程监控（社交网络）Web 应用数据分析应用什么是数据分析应用？数据分析任务需要从原始数据中提取有价值的信息和指标。传统的分析方式通常是利用批查询，或将事件记录下来并基于此有限数据集构建应用来完成。为了得到最新数据的分析结果，必须先将它们加入分析数据集并重新执行查询或运行应用，随后将结果写入存储系统或生成报告。借助一些先进的流处理引擎，还可以实时地进行数据分析。和传统模式下读取有限数据集不同，流式查询或应用会接入实时事件流，并随着事件消费持续产生和更新结果。这些结果数据可能会写入外部数据库系统或以内部状态的形式维护。仪表展示应用可以相应地从外部数据库读取数据或直接查询应用的内部状态。如下图所示，Apache Flink 同时支持流式及批量分析应用。流式分析应用的优势？和批量分析相比，由于流式分析省掉了周期性的数据导入和查询过程，因此从事件中获取指标的延迟更低。不仅如此，批量查询必须处理那些由定期导入和输入有界性导致的人工数据边界，而流式查询则无须考虑该问题。另一方面，流式分析会简化应用抽象。批量查询的流水线通常由多个独立部件组成，需要周期性地调度提取数据和执行查询。如此复杂的流水线操作起来并不容易，一旦某个组件出错将会影响流水线的后续步骤。而流式分析应用整体运行在 Flink 之类的高端流处理系统之上，涵盖了从数据接入到连续结果计算的所有步骤，因此可以依赖底层引擎提供的故障恢复机制。Flink 如何支持数据分析类应用？Flink 为持续流式分析和批量分析都提供了良好的支持。具体而言，它内置了一个符合 ANSI 标准的 SQL 接口，将批、流查询的语义统一起来。无论是在记录事件的静态数据集上还是实时事件流上，相同 SQL 查询都会得到一致的结果。同时 Flink 还支持丰富的用户自定义函数，允许在 SQL 中执行定制化代码。如果还需进一步定制逻辑，可以利用 Flink DataStream API 和 DataSet API 进行更低层次的控制。此外，Flink 的 Gelly 库为基于批量数据集的大规模高性能图分析提供了算法和构建模块支持。典型的数据分析应用实例电信网络质量监控移动应用中的产品更新及实验评估分析消费者技术中的实时数据即席分析大规模图分析数据管道应用什么是数据管道？提取-转换-加载（ETL）是一种在存储系统之间进行数据转换和迁移的常用方法。ETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库。数据管道和 ETL 作业的用途相似，都可以转换、丰富数据，并将其从某个存储系统移动到另一个。但数据管道是以持续流模式运行，而非周期性触发。因此它支持从一个不断生成数据的源头读取记录，并将它们以低延迟移动到终点。例如：数据管道可以用来监控文件系统目录中的新文件，并将其数据写入事件日志；另一个应用可能会将事件流物化到数据库或增量构建和优化查询索引。下图描述了周期性 ETL 作业和持续数据管道的差异。数据管道的优势？和周期性 ETL 作业相比，持续数据管道可以明显降低将数据移动到目的端的延迟。此外，由于它能够持续消费和发送数据，因此用途更广，支持用例更多。Flink 如何支持数据管道应用？很多常见的数据转换和增强操作可以利用 Flink 的 SQL 接口（或 Table API）及用户自定义函数解决。如果数据管道有更高级的需求，可以选择更通用的 DataStream API 来实现。Flink 为多种数据存储系统（如：Kafka、Kinesis、Elasticsearch、JDBC数据库系统等）内置了连接器。同时它还提供了文件系统的连续型数据源及数据汇，可用来监控目录变化和以时间分区的方式写入文件。典型的数据管道应用实例电子商务中的实时查询索引构建电子商务中的持续 ETL","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 是什么？","slug":"Flink-是什么？","date":"2018-02-20T09:21:58.000Z","updated":"2020-05-10T13:06:33.408Z","comments":true,"path":"2018/02/20/Flink-是什么？/","link":"","permalink":"cpeixin.cn/2018/02/20/Flink-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"","text":"架构Apache Flink 是一个框架和分布式处理引擎，用于在_无边界和有边界_数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。接下来，我们来介绍一下 Flink 架构中的重要方面。处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。有界流 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。部署应用到任意地方Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。Flink 集成了所有常见的集群资源管理器，例如 Hadoop YARN、 Apache Mesos 和 Kubernetes，但同时也可以作为独立集群运行。Flink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。部署 Flink 应用程序时，Flink 会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。运行任意规模应用Flink 旨在任意规模上运行有状态流式应用。因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO。而且 Flink 很容易维护非常大的应用程序状态。其异步和增量的检查点算法对处理延迟产生最小的影响，同时保证精确一次状态的一致性。Flink 用户报告了其生产环境中一些令人印象深刻的扩展性数字处理每天处理数万亿的事件,应用维护几TB大小的状态, 和应用在数千个内核上运行。利用内存性能有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。应用Apache Flink 是一个针对无界和有界数据流进行有状态计算的框架。Flink 自底向上在不同的抽象级别提供了多种 API，并且针对常见的使用场景开发了专用的扩展库。流处理应用的基本组件可以由流处理框架构建和执行的应用程序类型是由框架对 流、状态、时间 的支持程度来决定的。在下文中，我们将对上述这些流处理应用的基本组件逐一进行描述，并对 Flink 处理它们的方法进行细致剖析。**流显而易见，（数据）流是流处理的基本要素。然而，流也拥有着多种特征。这些特征决定了流如何以及何时被处理。Flink 是一个能够处理任何类型数据流的强大处理框架。有界 和 无界 的数据流：流可以是无界的；也可以是有界的，例如固定大小的数据集。Flink 在无界的数据流处理上拥有诸多功能强大的特性，同时也针对有界的数据流开发了专用的高效算子。实时 和 历史记录 的数据流：所有的数据都是以流的方式产生，但用户通常会使用两种截然不同的方法处理数据。或是在数据生成时进行实时的处理；亦或是先将数据流持久化到存储系统中——例如文件系统或对象存储，然后再进行批处理。Flink 的应用能够同时支持处理实时以及历史记录数据流。状态只有在每一个单独的事件上进行转换操作的应用才不需要状态，换言之，每一个具有一定复杂度的流处理应用都是有状态的。任何运行基本业务逻辑的流处理应用都需要在一定时间内存储所接收的事件或中间结果，以供后续的某个时间点（例如收到下一个事件或者经过一段特定时间）进行访问并进行后续处理。应用状态是 Flink 中的一等公民，Flink 提供了许多状态管理相关的特性支持，其中包括：多种状态基础类型：Flink 为多种不同的数据结构提供了相对应的状态基础类型，例如原子值（value），列表（list）以及映射（map）。开发者可以基于处理函数对状态的访问方式，选择最高效、最适合的状态基础类型。插件化的State Backend：State Backend 负责管理应用程序状态，并在需要的时候进行 checkpoint。Flink 支持多种 state backend，可以将状态存在内存或者 RocksDB。RocksDB 是一种高效的嵌入式、持久化键值存储引擎。Flink 也支持插件式的自定义 state backend 进行状态存储。精确一次语义：Flink 的 checkpoint 和故障恢复算法保证了故障发生后应用状态的一致性。因此，Flink 能够在应用程序发生故障时，对应用程序透明，不造成正确性的影响。超大数据量状态：Flink 能够利用其异步以及增量式的 checkpoint 算法，存储数 TB 级别的应用状态。可弹性伸缩的应用：Flink 能够通过在更多或更少的工作节点上对状态进行重新分布，支持有状态应用的分布式的横向伸缩。时间时间是流处理应用另一个重要的组成部分。因为事件总是在特定时间点发生，所以大多数的事件流都拥有事件本身所固有的时间语义。进一步而言，许多常见的流计算都基于时间语义，例如窗口聚合、会话计算、模式检测和基于时间的 join。流处理的一个重要方面是应用程序如何衡量时间，即区分事件时间（event-time）和处理时间（processing-time）。Flink 提供了丰富的时间语义支持。事件时间模式：使用事件时间语义的流处理应用根据事件本身自带的时间戳进行结果的计算。因此，无论处理的是历史记录的事件还是实时的事件，事件时间模式的处理总能保证结果的准确性和一致性。Watermark 支持：Flink 引入了 watermark 的概念，用以衡量事件时间进展。Watermark 也是一种平衡处理延时和完整性的灵活机制。迟到数据处理：当以带有 watermark 的事件时间模式处理数据流时，在计算完成之后仍会有相关数据到达。这样的事件被称为迟到事件。Flink 提供了多种处理迟到数据的选项，例如将这些数据重定向到旁路输出（side output）或者更新之前完成计算的结果。处理时间模式：除了事件时间模式，Flink 还支持处理时间语义。处理时间模式根据处理引擎的机器时钟触发计算，一般适用于有着严格的低延迟需求，并且能够容忍近似结果的流处理应用。分层 APIFlink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。下文中，我们将简要描述每一种 API 及其应用，并提供相关的代码示例。ProcessFunctionProcessFunction 是 Flink 所提供的最具表达力的接口。ProcessFunction 可以处理一或两条输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件驱动应用所需要的基于单个事件的复杂业务逻辑。下面的代码示例展示了如何在 KeyedStream 上利用 KeyedProcessFunction 对标记为 START 和 END 的事件进行处理。当收到 START 事件时，处理函数会记录其时间戳，并且注册一个时长4小时的计时器。如果在计时器结束之前收到 END 事件，处理函数会计算其与上一个 START 事件的时间间隔，清空状态并将计算结果返回。否则，计时器结束，并清空状态。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 将相邻的 keyed START 和 END 事件相匹配并计算两者的时间间隔 * 输入数据为 Tuple2&lt;String, String&gt; 类型，第一个字段为 key 值， * 第二个字段标记 START 和 END 事件。 */public static class StartEndDuration extends KeyedProcessFunction&lt;String, Tuple2&lt;String, String&gt;, Tuple2&lt;String, Long&gt;&gt; &#123; private ValueState&lt;Long&gt; startTime; @Override public void open(Configuration conf) &#123; // obtain state handle startTime = getRuntimeContext() .getState(new ValueStateDescriptor&lt;Long&gt;(\"startTime\", Long.class)); &#125; /** Called for each processed event. */ @Override public void processElement( Tuple2&lt;String, String&gt; in, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; switch (in.f1) &#123; case \"START\": // set the start time if we receive a start event. startTime.update(ctx.timestamp()); // register a timer in four hours from the start event. ctx.timerService() .registerEventTimeTimer(ctx.timestamp() + 4 * 60 * 60 * 1000); break; case \"END\": // emit the duration between start and end event Long sTime = startTime.value(); if (sTime != null) &#123; out.collect(Tuple2.of(in.f0, ctx.timestamp() - sTime)); // clear the state startTime.clear(); &#125; default: // do nothing &#125; &#125; /** Called when a timer fires. */ @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) &#123; // Timeout interval exceeded. Cleaning up the state. startTime.clear(); &#125;&#125;这个例子充分展现了 KeyedProcessFunction 强大的表达力，也因此是一个实现相当复杂的接口。DataStream APIDataStream API 为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如map()、reduce()、aggregate() 等函数。你可以通过扩展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。下面的代码示例展示了如何捕获会话时间范围内所有的点击流事件，并对每一次会话的点击量进行计数。123456789101112131415161718// 网站点击 Click 的数据流DataStream&lt;Click&gt; clicks = ...DataStream&lt;Tuple2&lt;String, Long&gt;&gt; result = clicks // 将网站点击映射为 (userId, 1) 以便计数 .map( // 实现 MapFunction 接口定义函数 new MapFunction&lt;Click, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; map(Click click) &#123; return Tuple2.of(click.userId, 1L); &#125; &#125;) // 以 userId (field 0) 作为 key .keyBy(0) // 定义 30 分钟超时的会话窗口 .window(EventTimeSessionWindows.withGap(Time.minutes(30L))) // 对每个会话窗口的点击进行计数，使用 lambda 表达式定义 reduce 函数 .reduce((a, b) -&gt; Tuple2.of(a.f0, a.f1 + b.f1));SQL &amp; Table APIFlink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API 和 SQL 借助了 Apache Calcite 来进行查询的解析，校验以及优化。它们可以与 DataStream 和 DataSet API 无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。Flink 的关系型 API 旨在简化数据分析、数据流水线和 ETL 应用的定义。下面的代码示例展示了如何使用 SQL 语句查询捕获会话时间范围内所有的点击流事件，并对每一次会话的点击量进行计数。此示例与上述 DataStream API 中的示例有着相同的逻辑。123SELECT userId, COUNT(*)FROM clicksGROUP BY SESSION(clicktime, INTERVAL &#39;30&#39; MINUTE), userId库Flink 具有数个适用于常见数据处理应用场景的扩展库。这些库通常嵌入在 API 中，且并不完全独立于其它 API。它们也因此可以受益于 API 的所有特性，并与其他库集成。复杂事件处理(CEP)：模式检测是事件流处理中的一个非常常见的用例。Flink 的 CEP 库提供了 API，使用户能够以例如正则表达式或状态机的方式指定事件模式。CEP 库与 Flink 的 DataStream API 集成，以便在 DataStream 上评估模式。CEP 库的应用包括网络入侵检测，业务流程监控和欺诈检测。DataSet API：DataSet API 是 Flink 用于批处理应用程序的核心 API。DataSet API 所提供的基础算子包括map_、_reduce_、(outer) join_、_co-group_、_iterate_等。所有算子都有相应的算法和数据结构支持，对内存中的序列化数据进行操作。如果数据大小超过预留内存，则过量数据将存储到磁盘。Flink 的 DataSet API 的数据处理算法借鉴了传统数据库算法的实现，例如混合散列连接（hybrid hash-join）和外部归并排序（external merge-sort）。Gelly: Gelly 是一个可扩展的图形处理和分析库。Gelly 是在 DataSet API 之上实现的，并与 DataSet API 集成。因此，它能够受益于其可扩展且健壮的操作符。Gelly 提供了内置算法，如 label propagation、triangle enumeration 和 page rank 算法，也提供了一个简化自定义图算法实现的 Graph API。运维Apache Flink 是一个针对无界和有界数据流进行有状态计算的框架。由于许多流应用程序旨在以最短的停机时间连续运行，因此流处理器必须提供出色的故障恢复能力，以及在应用程序运行期间进行监控和维护的工具。Apache Flink 非常注重流数据处理的可运维性。因此在这一小节中，我们将详细介绍 Flink 的故障恢复机制，并介绍其管理和监控应用的功能。7 * 24小时稳定运行在分布式系统中，服务故障是常有的事，为了保证服务能够7*24小时稳定运行，像Flink这样的流处理器故障恢复机制是必须要有的。显然这就意味着，它(这类流处理器)不仅要能在服务出现故障时候能够重启服务，而且还要当故障发生时，保证能够持久化服务内部各个组件的当前状态，只有这样才能保证在故障恢复时候，服务能够继续正常运行，好像故障就没有发生过一样。Flink通过几下多种机制维护应用可持续运行及其一致性:检查点的一致性: Flink的故障恢复机制是通过建立分布式应用服务状态一致性检查点实现的，当有故障产生时，应用服务会重启后，再重新加载上一次成功备份的状态检查点信息。结合可重放的数据源，该特性可保证_精确一次（exactly-once）_的状态一致性。高效的检查点: 如果一个应用要维护一个TB级的状态信息，对此应用的状态建立检查点服务的资源开销是很高的，为了减小因检查点服务对应用的延迟性（SLAs服务等级协议）的影响，Flink采用异步及增量的方式构建检查点服务。端到端的精确一次: Flink 为某些特定的存储支持了事务型输出的功能，及时在发生故障的情况下，也能够保证精确一次的输出。集成多种集群管理服务: Flink已与多种集群管理服务紧密集成，如 Hadoop YARN, Mesos, 以及 Kubernetes。当集群中某个流程任务失败后，一个新的流程服务会自动启动并替代它继续执行。内置高可用服务: Flink内置了为解决单点故障问题的高可用性服务模块，此模块是基于Apache ZooKeeper 技术实现的，Apache ZooKeeper是一种可靠的、交互式的、分布式协调服务组件。Flink能够更方便地升级、迁移、暂停、恢复应用服务驱动关键业务服务的流应用是经常需要维护的。比如需要修复系统漏洞，改进功能，或开发新功能。然而升级一个有状态的流应用并不是简单的事情，因为在我们为了升级一个改进后版本而简单停止当前流应用并重启时，我们还不能丢失掉当前流应用的所处于的状态信息。而Flink的 Savepoint 服务就是为解决升级服务过程中记录流应用状态信息及其相关难题而产生的一种唯一的、强大的组件。一个 Savepoint，就是一个应用服务状态的一致性快照，因此其与checkpoint组件的很相似，但是与checkpoint相比，Savepoint 需要手动触发启动，而且当流应用服务停止时，它并不会自动删除。Savepoint 常被应用于启动一个已含有状态的流服务，并初始化其（备份时）状态。Savepoint 有以下特点：便于升级应用服务版本: Savepoint 常在应用版本升级时使用，当前应用的新版本更新升级时，可以根据上一个版本程序记录的 Savepoint 内的服务状态信息来重启服务。它也可能会使用更早的 Savepoint 还原点来重启服务，以便于修复由于有缺陷的程序版本导致的不正确的程序运行结果。方便集群服务移植: 通过使用 Savepoint，流服务应用可以自由的在不同集群中迁移部署。方便Flink版本升级: 通过使用 Savepoint，可以使应用服务在升级Flink时，更加安全便捷。增加应用并行服务的扩展性: Savepoint 也常在增加或减少应用服务集群的并行度时使用。便于A/B测试及假设分析场景对比结果: 通过把同一应用在使用不同版本的应用程序，基于同一个 Savepoint 还原点启动服务时，可以测试对比2个或多个版本程序的性能及服务质量。暂停和恢复服务: 一个应用服务可以在新建一个 Savepoint 后再停止服务，以便于后面任何时间点再根据这个实时刷新的 Savepoint 还原点进行恢复服务。归档服务: Savepoint 还提供还原点的归档服务，以便于用户能够指定时间点的 Savepoint 的服务数据进行重置应用服务的状态，进行恢复服务。监控和控制应用服务如其它应用服务一样，持续运行的流应用服务也需要监控及集成到一些基础设施资源管理服务中，例如一个组件的监控服务及日志服务等。监控服务有助于预测问题并提前做出反应，日志服务提供日志记录能够帮助追踪、调查、分析故障发生的根本原因。最后，便捷易用的访问控制应用服务运行的接口也是Flink的一个重要的亮点特征。Flink与许多常见的日志记录和监视服务集成得很好，并提供了一个REST API来控制应用服务和查询应用信息。具体表现如下：Web UI方式: Flink提供了一个web UI来观察、监视和调试正在运行的应用服务。并且还可以执行或取消组件或任务的执行。日志集成服务:Flink实现了流行的slf4j日志接口，并与日志框架log4j或logback集成。指标服务: Flink提供了一个复杂的度量系统来收集和报告系统和用户定义的度量指标信息。度量信息可以导出到多个报表组件服务，包括 JMX, Ganglia, Graphite, Prometheus, StatsD, Datadog, 和 Slf4j.标准的WEB REST API接口服务: Flink提供多种REST API接口，有提交新应用程序、获取正在运行的应用程序的Savepoint服务信息、取消应用服务等接口。REST API还提供元数据信息和已采集的运行中或完成后的应用服务的指标信息。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Docker three elements","slug":"Docker-three-elements","date":"2018-01-02T13:01:05.000Z","updated":"2020-04-04T12:00:32.855Z","comments":true,"path":"2018/01/02/Docker-three-elements/","link":"","permalink":"cpeixin.cn/2018/01/02/Docker-three-elements/","excerpt":"","text":"whatbuild, ship and run any app anywhereDocker is designed to avoid a series of problems caused by inconsistencies between development and production environments during the process of program delivery and deployment between development engineers and operations engineers. Through docker, the development engineer packages the program code, running system, software version, description document and so on in a image file, and passes the image file to the operation and maintenance engineer for deployment. For example, if you buy a fish tank with water in it from a pet store, the fish can live in your home without any other operation.At present, there is no need to discuss the struggle between docker and virtual machine, because major Internet companies are using docker technology, and can expand capacity in a short time, we can know that in the current high concurrency business scenario, from practicality and convenience, virtual machine is unable to defeat docker.Simply, docker’s centos image is only 200MB, while the virtual machine image is about 2GB. Docker makes use of the resources of the host and only copies the Linux kernel, and docker is no need Hypervisor ,which can be said to be a stripped-down version of the virtual machinesoftware architectureFocus on understanding the three elementsHowDocker has many commands, which are not complicated. Just like Linux commands, it is recommended to find them on the official website and practice them a lotdocker imagesdocker run [options]docker build [options]and so on …Docker imagebase on UnionFSimage = fs1 + fs2 + … + fnIt looks like an image is a file system, but it’s actually an image that’s made up of layers of file systemsAn available image file is made up of multiple base image layers superimposed, all read-onlyFor example,Tomcat image = kernel image + centos image + jdk image + tomcat imageDocker containerdocker container = docker run -it imageWhen docker image is launched, a docker container will be instantiated. In principle, a layer of writable files is added to the top layer of multiple read-only docker images superimposed together, thus generating the running docker containerDocker volumes is very important and you can check the official website for detailed usageDockerfileDockerfile is similar to a key/value configuration file, which is composed of the construction word in the following figure. Starting From ‘From’, other construction words are listed according to requirements to tell the Dockerfile what to do and what image to generate.Note the difference between CMD and ENTRYPOINT build wordsDockerfile Docker image Docker container， So what is the relationship between these threeDockerfile–&gt;(build)–&gt;Docker image–&gt;(run)–&gt;Docker containerDocker RegistryA Repository is a place to store images. Like Node’s NPM; Python PyPi. Currently, Docker officially maintains a public repository, and most of the requirements can be implemented by directly downloading the image from the Docker Hub. Similar to GitHub, in the process of making image or Dockerfile, we draw the base image, i.e. From [base image], From the public library.","categories":[{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"}]},{"title":"Docker install and first Dockerfile","slug":"Docker-install-and-first-Dockerfile","date":"2018-01-01T12:00:05.000Z","updated":"2020-04-04T12:00:27.785Z","comments":true,"path":"2018/01/01/Docker-install-and-first-Dockerfile/","link":"","permalink":"cpeixin.cn/2018/01/01/Docker-install-and-first-Dockerfile/","excerpt":"","text":"prefaceThis article only explains the installation of Docker and the creation of Dockerfile. Dockerfile is composed of different commands in different requirements, which can be understood as a configuration file. However, only a demo is shown here to show the basic usage of DockerServer environmentlinux centos 7.6installstep 1If you have installed an older version of docker, you need to uninstall it first12345678910sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-enginestep 2install dependencies123sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2step 3set stable Repository123sudo yum-config-manager \\ --add-repo \\ https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;centos&#x2F;docker-ce.repostep 4install docker1sudo yum install docker-ce docker-ce-cli containerd.iostep 5start docker service1sudo systemctl start dockerstep 6Run a demo to see if the installation was successful: The hello-world image is not in your local, but when you run the command, docker will pulls the helloworld image from the remote repository1sudo docker run hello-worldcreate a demostep 1create dockerfileChoose any directory and create a dockerfile.It is suggested to name ‘Dockerfile’, because by default docker will run the file called ‘Dockerfile’ in the current directory. If you give it a different name, add the -f parameter and the path of the dockerfile12345678vim DockerfileFROM nginxMAINTAINER author &lt;email&gt;RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;&#x2F;h1&gt;&#39; &gt; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html:wqstep 2build image1docker build -t test&#x2F;hello-world: .-t is to set the repository and imagetest is repository namehello-world as the image namestep 3run image1docker run --name hello -d -p 8080:80 test&#x2F;hellostep 4Browser input http://localhost:8080/finalThis is a simple example of using Dockerfile to build the image and run the container!","categories":[{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"}]},{"title":"数据仓库 - 元数据管理","slug":"数据仓库-元数据管理","date":"2017-11-30T13:20:15.000Z","updated":"2020-04-04T11:23:20.355Z","comments":true,"path":"2017/11/30/数据仓库-元数据管理/","link":"","permalink":"cpeixin.cn/2017/11/30/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/","excerpt":"","text":"好文分享https://www.jianshu.com/p/9fe3ff2bbe99","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 数据分层","slug":"数据仓库数据分层","date":"2017-11-25T15:26:15.000Z","updated":"2020-05-06T16:56:35.700Z","comments":true,"path":"2017/11/25/数据仓库数据分层/","link":"","permalink":"cpeixin.cn/2017/11/25/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E5%88%86%E5%B1%82/","excerpt":"","text":"简述在上篇博客‘数据仓库建模方法’中我们探讨过数据模型的相关问题，实际场景中，我们团队在确定了数据模型使用经典的星型模型和3NF之后，接下来要进行讨论的问题则是，每天大批量的数据，应该怎样的放进库中，怎么存放合理。在数据对接业务之前，要经过哪些提炼的过程，在整个提炼的过程中，要分成几个步骤，还是一步到位的直接从原始数据ETL给业务人员。其中提到的这个提炼过程，也就是数据分层的过程，分层设计的好，数据分布合理，节约计算资源，避免多余的数据开发。所以，数据分层是数据仓库建设中，重要的一环。各种重复计算，严重浪费了计算资源，需要优化性能。为什么要数据分层我们对数据进行分层的一个主要原因就是希望在管理数据的时候，能对数据有一个更加清晰的掌控，详细来讲，主要有下面几个原因：清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。数据血缘追踪：简单来讲可以这样理解，我们最终给业务诚信的是一能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。把复杂问题简单化。讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。屏蔽原始数据的异常。屏蔽业务的影响，不必改一次业务就需要重新接入数据。怎么分层在这里，就拿我们团队对我们公司设计的数据仓库来举例。首先呢，我们公司的数据量级和数据主题复杂度，相当于一个二线互联网公司，数据主题目前这一版数据仓库可以分为用户行为，订单，游戏活动等7个主题数据，业务并不是很复杂。所以在数据分层上，总共分成三层。源数据层 （dw = staging）宽表数据层 （dw = dw）指标数据层 （dw = subjectName）源数据层源数据层拉取业务表有20张左右，使用spark读取底层RDBMS来获取数据，写入到Hive中，在这一层中，除了选择合适的分区字段，我们的分析业务中，时间粒度多为天为单位，业务数据来源于多个产品线，所以这一层，会以snapshot_date（‘YYYY-MM-DD’）和product_id为分区字段，对数据表进行分区外，不做任何数据处理，不做脏数据处理，不做合并，保持数据的原始性，随时可以做到追本溯源。我们对于近源数据层的定位是可以”快速”的构建基础数据平台. 不做业务相关的处理可以让这部分的工作专注在大数据架构正确性和稳定性的问题，近源数据层出现以后, 实际上我们已经可以开始主要的数据分析工作了。宽表数据层宽表数据层的数据是从源数据层经过ETL字段清洗，计算并且相关主题表，维度表进行join而来的。例如订单表：Orders表order_*为订单主题（order_id,order_number,order_amount,create_date…）t_users_dim 用户维度表(user_id,city,last_login_time,device_type…)t_activity_dim 活动维度表(activity_id,activity_name,create_date…)t_promotion_dim 优惠维度表(promotion_id,promotion_name,promotion_type…)t_deposit 充值表(deposit_id,deposit_type,channal,deposit_time…)以上五张表经过相关键join就组成了订单宽表 如下图所示：指标数据层指标数据层是从宽表数据层计算而来，这一层我们采用的策略是由总到分，关联时间维度表，缩小时间粒度到小时级别。可以对运营人员提供小时级别的数据指标。对于一些金额相关数据表，也会经过计算添加一些常用的数据模型，例如某用户近20次充值的max值和min值，中位数，四分位数等字段。这一层也是最不稳定的一层，经常要根据业务的变化增加字段或者新加指标数据表。由于的依赖Hive建设的数据仓库，那么对于增加字段的这件事，要小心对数据的影响，注意选择合适的Hive表格式。这层就不画图了 有点困了 困了～～～～这篇数据仓库数据分层设计，是根据我们公司的业务数据和主题来设计的，也是我们大数据工程团队的这几个渣渣工程师和架构师一起商讨的结果方案，对于其他行业和大规模海量数据肯定是不能全部适用的。在写这篇文章前，我有看过美团技术团队美团点评酒旅数据仓库建设实践这篇文章，发现业务复杂度和数据规模对数据建模和数据分层的影响很大，像我们公司的数据量量级要完全按照美团数仓去做，是完全没有必要的，而且会弄的更加复杂，所以在这里想表明的就是，对于数仓的建设，要选择适合自己的，对自己量身定制，照搬某个公司的，是没有意义的，反而可能会增加使用的难度。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 建模方法","slug":"数据仓库建模方法","date":"2017-11-24T12:26:15.000Z","updated":"2020-04-04T11:23:02.106Z","comments":true,"path":"2017/11/24/数据仓库建模方法/","link":"","permalink":"cpeixin.cn/2017/11/24/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/","excerpt":"","text":"背景接着上个文章数据仓库架构设计，想写一篇数据仓库常用模型的文章，但是自己对数据仓库模型的理解程度和建设架构并没有下面这个技术专家理解的深刻，并且自己去组织语言，可能会有不准确的地方，怕影响大家对数据仓库建模的理解，数据仓库属于一个工程学科，在设计上要体验出工程严谨性，所以这次向大家推荐这篇文章，毕竟IBM在数据仓库和数据集市方面已经做得很成熟了，已经有成型的商业数据仓库组件，这篇文章写的很好，可以让大家很好的理解数据仓库。版权作者 周三保(zhousb@cn.ibm.com) IBM 软件部信息技术专家.原文地址本文主要的主线就是回答下面三个问题：什么是数据模型为什么需要数据模型如何建设数据模型什么是数据模型数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图 1 所示。通过上面的图形，我们能够很容易的看出在整个数据仓库得建模过程中，我们需要经历一般四个过程：业务建模，生成业务模型，主要解决业务层面的分解和程序化。领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。为什么需要数据模型在数据仓库的建设中，我们一再强调需要数据模型，那么数据模型究竟为什么这么重要呢？首先我们需要了解整个数据仓库的建设的发展史。数据仓库的发展大致经历了这样的三个过程：简单报表阶段：这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。数据集市阶段：这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。数据仓库阶段：这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题：进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。如何建设数据模型建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。数据仓库数据模型架构数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。图 3. 数据仓库数据模型架构从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分：系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。数据仓库建模阶段划分我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容：图 4. 数据仓库建模阶段划分从上图我们可以清楚地看出，数据仓库的数据建模大致分为四个阶段：业务建模，这部分建模工作，主要包含以下几个部分：划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。深入了解各个业务部门的内具体业务流程并将其程序化。提出修改和改进业务部门工作流程的方法并程序化。数据建模的范围界定，整个数据仓库项目的目标和阶段划分。领域概念建模，这部分得建模工作，主要包含以下几个部分：抽取关键业务概念，并将之抽象化。将业务概念分组，按照业务主线聚合类似的分组概念。细化分组概念，理清分组概念内的业务流程并抽象化。理清分组概念之间的关联，形成完整的领域概念模型。逻辑建模，这部分的建模工作，主要包含以下几个部分：业务概念实体化，并考虑其具体的属性事件实体化，并考虑其属性内容说明实体化，并考虑其属性内容物理建模，这部分得建模工作，主要包含以下几个部分：针对特定物理化平台，做出相应的技术调整针对模型的性能考虑，对特定平台作出相应的调整针对管理的需要，结合特定的平台，做出相应的调整生成最后的执行脚本，并完善之。从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。数据仓库建模方法大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库得建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法，维度建模法，实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。范式建模法（Third Normal Form，3NF）范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :每个属性值唯一，不具有多义性 ;每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。根据 Inmon 的观点，数据仓库模型得建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例话。从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于：数据仓库的域模型应该包含企业数据模型得域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。以笔者的观点来看，Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。维度建模法维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模型 和 雪花模型。这里做一下 星型模型 和 雪花模型的扩充：星型模型星型模是一种多维的数据关系，它由一个事实表和一组维表组成。每个维表都有一个维作为主键，所有这些维的主键组合成事实表的主键。强调的是对维度进行预处理，将多个维度集合到一个事实表，形成一个宽表。这也是我们在使用hive时，经常会看到一些大宽表的原因，大宽表一般都是事实表，包含了维度关联的主键和一些度量信息，而维度表则是事实表里面维度的具体信息，使用时候一般通过join来组合数据，相对来说对OLAP的分析比较方便。雪花模型当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。雪花模型更加符合数据库范式，减少数据冗余，但是在分析数据的时候，操作比较复杂，需要join的表比较多所以其性能并不一定比星型模型高。星型模型和雪花模型的优劣对比应用场景星型模型的设计方式主要带来的好处是能够提升查询效率，因为生成的事实表已经经过预处理，主要的数据都在事实表里面，所以只要扫描实时表就能够进行大量的查询，而不必进行大量的join，其次维表数据一般比较少，在join可直接放入内存进行join以提升效率，除此之外，星型模型的事实表可读性比较好，不用关联多个表就能获取大部分核心信息，设计维护相对比较简答。雪花模型的设计方式是比较符合数据库范式的理念，设计方式比较正规，数据冗余少，但在查询的时候可能需要join多张表从而导致查询效率下降，此外规范化操作在后期维护比较复杂。总结通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。图 6. 维度建模法上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。实体建模法实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示：图 7. 实体建模法上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分：实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。说明，主要是针对实体和事件的特殊说明。由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。数据仓库数据模型与业务系统数据模型设计的区别数据仓库建模样例上面介绍得是一些抽象得建模方法和理论，可能理解起来相对有些难度，因此，笔者在这里举一个例子，读者可以跟着我们的这个样例，来初步了解整个数据仓库建模的大概过程。背景介绍熟悉社保行业的读者可以知道，目前我们国家的社保主要分为养老，失业，工伤，生育，医疗保险和劳动力市场这 6 大块主要业务领域。在这 6 大业务领域中，目前的状况养老和事业的系统已经基本完善，已经有一部分数据开始联网检测。而，对于工伤，生育，医疗和劳动力市场这一块业务，有些地方发展的比较成熟，而有些地方还不够成熟。1.业务建模阶段基于以上的背景介绍，我们在业务建模阶段，就很容易来划分相应的业务。因此，在业务建模阶段，我们基本上确定我们本次数据仓库建设的目标，建设的方法，以及长远规划等。如下图：图 8. 业务建模阶段在这里，我们将整个业务很清楚地划分成了几个大的业务主线，例如：养老，失业，工伤，生育，医疗，劳动力等着几个大的部分，然后我们可以根据这些大的模块，在每个业务主线内，考虑具体的业务主线内需要分析的业务主题。因此，业务建模阶段其实是一次和业务人员梳理业务的过程，在这个过程中，不仅能帮助我们技术人员更好的理解业务，另一方面，也能够发现业务流程中的一些不合理的环节，加以改善和改进。同时，业务建模阶段的另一个重要工作就是确定我们数据建模的范围，例如：在某些数据准备不够充分的业务模块内，我们可以考虑先不建设相应的数据模型。等到条件充分成熟的情况下，我们可以再来考虑数据建模的问题。领域概念建模阶段领域概念建模阶段是数据仓库数据建模的一个重要阶段，由于我们在业务建模阶段已经完全理清相应的业务范围和流程，因此，我们在这个领域概念建模阶段的最主要的工作就是进行概念的抽象，整个领域概念建模的工作层次如下图所示：图 9. 领域概念建模阶段从上图我们可以清楚地看到，领域概念建模就是运用了实体建模法，从纷繁的业务表象背后通过实体建模法，抽象出实体，事件，说明等抽象的实体，从而找出业务表象后抽象实体间的相互的关联性，保证了我们数据仓库数据按照数据模型所能达到的一致性和关联性。从图上看，我们可以把整个抽象过程分为四个层次，分别为：抽象方法层，整个数据模型的核心方法，领域概念建模的实体的划分通过这种抽象方法来实现。领域概念层，这是我们整个数据模型的核心部分，因为不同程度的抽象方法，决定了我们领域概念的不同。例如：在这里，我们可以使用“参与方”这个概念，同时，你也可以把他分成三个概念：“个人”，“公司”，和“经办机构”这三个概念。而我们在构建自己的模型的时候，可以参考业务的状况以及我们自己模型的需要，选择抽象程度高的概念或者是抽象程度低的概念。相对来说，抽象程度高的概念，理解起来较为复杂，需要专业的建模专家才能理解，而抽象程度低的概念，较适合于一般业务人员的理解，使用起来比较方便。笔者在这里建议读者可以选用抽象概念较低的实体，以方便业务人员和技术人员之间的交流和沟通。具体业务层，主要是解决具体的业务问题，从这张图我们可以看出，具体的业务层，其实只是领域概念模型中实体之间的一些不同组合而已。因此，完整的数据仓库的数据模型应该能够相应灵活多变的前端业务的需求，而其本身的模型架构具有很强的灵活性。这也是数据仓库模型所具备的功能之一。业务主线层，这个层次主要划分大的业务领域，一般在业务建模阶段即已经完成这方面的划分。我们一般通过这种大的业务主线来划分整个业务模型大的框架。通过领域概念建模，数据仓库的模型已经被抽象成一个个的实体，模型的框架已经搭建完毕，下面的工作就是给这些框架注入有效的肌体。逻辑建模阶段通过领域概念建模之后，虽然模型的框架已经完成，但是还有很多细致的工作需要完成。一般在这个阶段，我们还需要做非常多的工作，主要包括：实例话每一个抽象的实体，例如：在上面的概念模型之后，我们需要对“人”和“公司”等这些抽象实体进行实例化。主要是，我们需要考虑“人”的属性包括那些，在业务模块中，用到的所有跟“人”相关的属性是哪些，我们都需要将这些属性附着在我们数据模型的“人”这个实体上，例如“人”得年龄，性别，受教育程度等等。同理，我们对其他属性同样需要做这个工作。找出抽象实体间的联系，并将其实例话。这里，我们主要考虑是“事件”这个抽象概念的实例话，例如：对于养老金征缴这个“事件”的属性得考虑，对于失业劳动者培训这个“事件”的属性得考虑等等。找出抽象事件的关系，并对其进行说明。在这里我们主要是要针对“事件”进行完善的“说明”。例如：对于“事件”中的地域，事件等因素的考量等等。总而言之，在逻辑建模阶段，我们主要考虑得是抽象实体的一些细致的属性。通过逻辑建模阶段，我们才能够将整个概念模型完整串联成一个有机的实体，才能够完整的表达出业务之间的关联性。在这个阶段，笔者建议大家可以参考 3NF 的建模方法，表达出实体的属性，以及实体与实体之间的联系。例如：在这个阶段，我们可以通过采用 ERWIN 等建模工具等作出符合 3NF 的关系型数据模型来。物理建模阶段物理建模阶段是整个数据建模的最后一个过程，这个过程其实是将前面的逻辑数据模型落地的一个过程。考虑到数据仓库平台的不同，因此，数据模型得物理建模过程可能会稍微有一些不同，在这个阶段我们主要的工作是：生成创建表的脚本。不同的数据仓库平台可能生成不同的脚本。针对不同的数据仓库平台，进行一些相应的优化工作，例如对于 DB2 数据仓库来说，创建一些 MQT 表，来加速报表的生成等等。针对数据集市的需要，按照维度建模的方法，生成一些事实表，维表等工作。针对数据仓库的 ETL 车和元数据管理的需要，生成一些数据仓库维护的表，例如：日志表等。经过物理建模阶段，整个数据仓库的模型已经全部完成，我们可以按照自己的设计来针对当前的行业创建满足自己需要的数据模型来。这里，笔者通过一个数据建模的样例，希望能够给读者一个关于数据仓库建模的感性的认识。希望读者在利用这些数据仓库得建模方法创建自己的数据模型的时候，可以根据业务实际的需要和自己对抽象能力的把握来创建适合自己的数据模型。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 架构设计","slug":"数据仓库架构设计","date":"2017-11-22T15:26:15.000Z","updated":"2020-04-04T11:23:13.660Z","comments":true,"path":"2017/11/22/数据仓库架构设计/","link":"","permalink":"cpeixin.cn/2017/11/22/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"业务数据背景我所在的公司处理的数据主要是游戏过程数据，用户行为数据，游戏币消费数据还有其他业务部门维护的例如用户优惠数据，游戏活动等业务数据。其中对于业务系统DBMS中的数据，基本按照“天”作为时间粒度来采集到数据仓库中，对于实时性要求较强的游戏数据，则是需要采用埋点实时采集和处理的。需求的多样化和复杂性，运营人员希望做到精细化运营和数据指标的实时响应, 这要求数据仓库具有提供高效明细数据能力,也能随时提供最新的数据调取，为了满足不同层次的数据提出和分析, 就要求着数据仓库在设计之初，就要考虑到要同时支持离线和实时的情况。数据仓库架构理念设计数据仓库架构之初，我们大数据工程组团队开会讨论的第一件事就是：Who is the User of Data in Data Warehouse？这件事情还是很重要的，弄清楚了这件事，对于接下来架构设计和数仓数据主题的确定都是有影响的，其中对于架构设计来说，这决定了我们架构上层需要提供哪些接口，对接不同的业务系统，我们应该设计怎样的查询和索引，不同的使用者，能够接受的数据粒度或者是数据响应时间，整个数据仓库的数据流大致应该怎样流向，这都与使用用户有关。在架构设计方面，基本上都是由下到上的去设计，但是呢，也不妨从上层需求开始，向下层实现设计开始✌️架构图话不多说，po图👽架构层次&amp;技术选型数据源由于公司各业务部门的数据都由运维DBA统一管理和做权限控制，RDBMS中的数据的采集，需要由运维部门给出备库URL，避免直接来取主库数据，对线上业务产生影响。这部份的数据采集，主要涉及Sqoop，NIFI，Spark 这三个工具。Sqoop在上一版的数据仓库中，Sqoop是作为RDBMS -&gt; HDFS中主要的搬运工。主要原因是开发成本小，数据量小，使用简单。但是随着数仓原始层数据量激增了原来的三倍，Sqoop计算能力就捉襟见肘了，每天的第一个任务就是拉取原始层数据，如果继续使用Sqoop，会严重影响接下来其他例行任务的按时进行。所以，我们决定抛弃这个使用MapReduce作为计算框架的数据同步工具。Spark提升数仓原始数据层拉取速度，就要选择合适的拉去工具。面对拉去原始数据，基本上不需要对任何数据字段进行清洗，基本上直接下 select columns就可以满足要求， 所以选用Spark SQL， read jdbc -&gt; dataframe -&gt; writeInto -&gt; Hive。这样运行的速度相对Sqoop来说，提升了四倍✌️NIFINIFI这个工具，并没有作为主要的数据工具来使用，起初只是作为团队的技术调研对象，在几次的分享会上，大家对NIFI都比较感兴趣，可能是大家都想解放双手，用拖拽的形式来拒绝写代码 哈哈哈哈。就这样，NIFI作为一个技术储备，承担了test库中测试数据的同步，等以后团队人手充足了，有维护多框架的能力时可以考虑发挥NIFI的更多用处。另外对于埋点数据，产品方会将数据打到运维的kafka中，随后我们会使用fluentd将运维kafka中的数据实时转发到我们大数据平台中的kafka中，做接下来的处理Fluentd运维的kafka中，会收集这我们想要的日志数据和埋点数据，埋点数据一般为json格式，将数据解析和打到大数据平台kafka的这一段中，我们在Logstash,Fluentd两个工具中进行选择，Logstash是非常出名的ELK中的 ‘L’ , 成熟度和稳定性不用说。Fluentd则是我们第一次接触，我们并没有亲自对Fluentd和Logstash进行效率和资源消耗上做测试，但是根据网上几篇对比测评文章，Fluentd的支持度虽没有Logstash那么完整，但是Fluentd中的插件完全能满足我们的开发需求，最重要的，Fluentd在CPU和内存上的消耗，都要优于Logstash。所以最后我们选择了FluentdFlink &amp; Spark StreamingFlink，Spark Streaming这两个实时计算框架就不多介绍了，家喻户晓。在这里说一下，在实时计算的这一层面，团队为什么选择了两个计算框架。主要是因为对待不同的需求，使用更适合的工具，团队成员对Spark Streaming更加熟悉，在对实时性要求更高的项目中，例如游戏监控系统，对于数据状态管理，watermark的需求都是有要求的，在这方面，Flink的实现就是要比Spark Streaming好。数据存储层数据存储层分为离线和实时HiveHive是Hadoop的一个数据仓库工具，底层存储为HDFS，可以将我们按照主题设计好的结构化数据映射成表，提供SQL接口来分析数据。从存储层到共享层中，大部分的计算任务，都是在Hive表中，使用Spark作为计算框架，ETL将数据清洗到HBase,MySQL等结果库中。HBase&amp;Elasticsearch&amp;GrafanaHBaseHBase的用途基本上是存储流式ETL后的结果数据，或者是直接存入原始数据作为备份ElasticsearchES的用途基本上是搭配Fluentd和Kibana，组成EFK，做数据展示，多数情况下会存储日志数据，做详情的检索。还有一个常用的功能就是结合HBase做二级索引，会在ES索引里面存储一个hbase的关键字rowkey，根据查询条件定位到某个rowkey，直接去hbase里面get数据，秒级返回GrafanaGrafana是团队后期引入的，主要是被Grafana的UI所吸引，Grafana和Kibana对比起来，UI展示要强大的很多，对于Grafana，我们正在调研，使用Grafana来做数据质量的监控，等使用成熟了，也会分享出来。数据共享层共享层的数据，都是经过ETL清洗的，都是即拿即用的。业务系统 ，对接数据仓库的，都是通过GoApi结构进行调取的。报表数据 ，工程师会通过Spark写成CSV文件，提供给运营人员即席查询 ，我们还开放了Presto接口（presto的查询速度比hive快5-10倍），同时对数据仓库做了仅查权限控制，希望业务分析人员可以自主的通过presto进行数据调取，但是理想是丰满的，现实是骨感的。根本没人用 哈哈哈哈哈哈哈数据应用层略数据用户层略","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 实际作用与需求","slug":"数据仓库实际作用与需求","date":"2017-11-20T15:26:15.000Z","updated":"2020-04-04T11:22:56.342Z","comments":true,"path":"2017/11/20/数据仓库实际作用与需求/","link":"","permalink":"cpeixin.cn/2017/11/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AE%9E%E9%99%85%E4%BD%9C%E7%94%A8%E4%B8%8E%E9%9C%80%E6%B1%82/","excerpt":"","text":"数据仓库的产生背景数据仓库的起源可以追溯到计算机的发展初期，并且数据仓库是信息技术长期发展的产物，在以后也会一直发展。从工程角度来讲，在计算机领域出现了大型在线事务处理系统后，对于数据之间的交互和使用就逐渐的多了起来。随后出现的抽取程序，可以通过设置参数，在文件中搜索满足条件的数据，然后把这些数据传送到其他文件或者数据库中。慢慢的人们发现在抽取结果中，加上一些条件限制可以更方便的得到想要的数据，于是就出现了基于抽取之上的抽取。这样就造成了如下问题：无休止的抽取带来诸多问题，抽取程序过多，功能单一，不可复用，无公共使用数据源，数据之间会产生差异等，所以开始思考是否可以建立成体系的机构化环境，以减少数据的差异。这也就是数据仓库出现的原因。数据仓库从操作型数据库中抽取数据，通过规范的加工过程，得到粒度化数据，并且这些数据时面向主题、集成、不易失、随时间变化的从商业角度来讲，在一次大会上，马云爸爸谈到当今时代，是IT时代到DT时代的变革。数据资源是比石油还要重要的资源，数据就是生产资料。随着企业多年的经营，业务数据量的不断增大，如何能够更好地利用数据，将数据转化成商业价值，已经成为人们越来越关心的问题。举例来说，数据库系统可以很好地解决事务处理，实现对数据的“增删改查”等功能，但是却不能提供很好的决策分析支持。因为事务处理首先考虑响应的及时性，多数情况都是在处理当前数据，而决策分析需要考虑的是数据的集成性和历史性，可能对分析处理的时效性要求不高。所以为了提高决策分析的有效性和完整性，人们逐渐将一部分或者大部分数据从联机事物处理系统中剥离出来，形成今天的数据仓库。数据团队眼中的数据仓库从实习工作，到目前为止，我参与了两次数仓的建设，实习公司当时大数据团队刚刚成立，那时并没有建立数仓，而是依据当时的各个项目需求，直接建立数据集市，将每个审计局的业务需求，分别建立不同的业务表，数据来源有来自Excel文件的，也有来自Oracle数据库的。再一个就是当前公司，这次的数仓建设，是我完整的一次参与整体流程，建立数仓的目的就是来应对各个产品所会提出的各种数据需求和统一各个产品的数据标准，现在的产品方，确实具有一定的数据头脑，不再像以前只追求盈利，不去思考数据背后的价值和数据指标对经营带来的影响。我所在的大数据团队负责公司层面和下面8个产品线的数据驱动服务，公司层面经常会调取盈利相关指标，产品方的需求则是各种各样，时间长度也是从年到天都有。对于公司和产品方，他们并不关心数据的来源和数仓的重要性，因为他们是使用数据结果的人，但是对于大数据团队，建立数据仓库是一个必要的事情，数据仓库对企业的价值数据仓库对企业带来的价值是很难估算的，但是数据仓库的成本倒是可以估算出大概的。就我们公司而言，数据仓库数据主题分成两个大类来说，就是用户数据与业务数据，那么产品方最想看到的就是用户数据与业务数据联合起来，这就是数据仓库对公司产品运营方带来的价值。数据仓库具有历史性，其中存储的数据大多是结构化数据，这些数据并非企业全量数据，而是根据需求针对性抽取的，因此数据仓库对于业务的价值是各种各样的报表，但这些报表又无法实时产生。数据仓库报表虽然能够提供部分业务价值，但不能直接影响业务，需要数据开发人员对数据仓库中的数据进行ETL开发，给出指导性的数据分析结果。数据仓库中数据与数据库中数据的不同这里呢，简单的说，应该就是OLTP和OLAP的区别数据仓库需求分析阶段数据仓库的需求分析，在很多公司中，都是由数据团队或者数据仓库工程师几个技术人员协定下来的。但是一个合格的数据仓库，应该是由业务方，产品方，运营方，数据团队技术方一同来商定的。一个完整的流程应该是去了解数据的背景，细节，去了解用户需要用到哪些主题的数据和数据粒度的大小，去了解主题数据中数据的维度，最后才是数据仓库设计人员开始拿出数据仓库的架构方案和ETL方案。数据仓库作用整合公司所有业务数据，建立统一的数据中心产生业务报表，用于作出决策为网站运营提供运营上的数据支持可以作为各个业务的数据源，形成业务数据互相反馈的良性循环分析用户行为数据，通过数据挖掘来降低投入成本，提高投入效果开发数据产品，直接或间接地为公司盈利","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 简述","slug":"数据仓库简述","date":"2017-11-19T15:26:15.000Z","updated":"2020-04-04T11:23:08.091Z","comments":true,"path":"2017/11/19/数据仓库简述/","link":"","permalink":"cpeixin.cn/2017/11/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0/","excerpt":"","text":"前言说到数据仓库，在一年前，我的答案是非常模糊的，在两年前，我根本不知道数据仓库的存在。相信大多数的人都是经常接触数据库，上大学的时候学过Oracle，MySQL这两个关系型数据库，操作着各种增删改查，现在人们对这种类型的数据库都称作传统数据库，数据量偏小，每一张表都是为了某一系统功能而设计的。在之后从事大数据行业了，数据仓库这几个字也就总出现在各种博客中，各种技术方案中。现在正在和同事主导公司数据仓库的设计和搭建工作，所以准备把我这段工程经验，拿出来分享一下。数据库与数据仓库的区别数据库这里我们所指的数据库，也就是一系列经典的RDBMS，如Oracle，MySQL，SQL Server等关系型数据库。其中的一些设计过程如 ER图的设计，逻辑模型的设计，物理模型设计，还有规范化设计 如 至少要符合第一范式，尽可量的去符合第二范式，第三范式等设计细节，这里就不赘述了，但是以上所提到的都是关系型数据库的精华。关系型数据库的用途，现在也会被分成两大类（1）操作型数据库，主要用于业务支撑。一个公司往往会使用并维护若干个数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等；（2）分析型数据库，主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析；数据仓库接下来先给大家看百度百科给出的数据仓库概念数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。从上面的定义来看，数据仓库的主要功能是用于做企业各个业务层面的分析，企业层面的数据总集，曾经看过一个博主对数据仓库的定义是“面向分析的存储系统”，这个定义是很接地气的，如果按照这个思路往下去思考，数据仓库存储的数据是很庞大的，不是一个单一的业务数据集，不用精准到每一条数据，例如，淘宝在存储每一条下单记录的时候，在关系型数据库中，这条记录是不会存储到其他业务表里面，只会存储在下单记录表里面，并且下单记录表里面不会存储两条相同的订单数据，这是因为关系型数据库要严格满足完整性/参照性约束以及范式设计要求，但是在数据仓库中，这条订单数据可能会存在用户行为表中，也可能存在下单数据表中。也就是说，数据仓库不应让传统关系数据库来实现，因为关系数据库最少也要求满足第1范式，而数据仓库里的关系表可以不满足第1范式。也就是说，同样的记录在一个关系表里可以出现N次。但由于大多数数据仓库内的表的统计分析还是用SQL，因此很多人把它和关系数据库搞混了。数据仓库的特点面向主题面向主题特性是数据仓库和操作型数据库的根本区别。操作型数据库是为了支撑各种业务而建立，而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；集成性集成性是指数据仓库会将不同源数据库中的数据汇总到一起；企业范围数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来；历史性较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年；时变性时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告；数据仓库组件数据仓库的核心组件有四个：各源数据库，ETL，数据仓库，前端应用。如下图所示：业务系统业务系统包含各种源数据库或者网站前端埋点实时数据，数据源可以是离线的，也可以是实时的（时间粒度为小时）。比如我们数据仓库中的离线数据来源是公司业务系统的Oracle,这些源数据库既为业务系统提供数据支撑，同时也作为数据仓库的数据源，部分实时数据是通过kafka，flume，NIFI收集来的埋点数据。ETLETL分别代表：提取extraction、转换transformation、加载load。其中提取过程表示操作型数据库搜集指定数据，转换过程表示将数据转化为指定格式并进行数据清洗保证数据质量，加载过程表示将转换过后满足指定格式的数据加载进数据仓库。数据仓库会周期不断地从源数据库提取清洗好了的数据。（ETL应该是数据工程师必会的技能😂）前端应用数据仓库的数据，肯定是为了前端应用而准备的，前端应用则是为了使用数据的用户而准备的。目前我搭建的数仓一般不会直接对接前端应用，而是将数仓中的数据，按照用户方需求进行计算聚合到HBase或者ES中。中间加一层API,API可以是Go或者Python开发的，这样前端应用直接调用API,也有一些特殊场景，会直接调用presto来访问Hive数据集市数据集市是数据仓库下面衍生出来的概念，在这里，我举一个例子来帮助大家理解，我们可以把数据仓库理解成万达购物中心，其中每层都卖着不同种类的商品，其中，一层卖的服装，二层买的家电…这样就可以分成一层是一个服装主题的数据集市，二层是家电主题的数据集市。同时可以理解为是一种”小型数据仓库”，它只包含单个主题，且关注范围也非全局。集市可以分为两种，一种是独立数据集市(independent data mart)，这类数据集市有自己的源数据库和ETL架构；另一种是非独立数据集市(dependent data mart)，这种数据集市没有自己的源系统，它的数据来自数据仓库。当用户或者应用程序不需要/不必要/不允许用到整个数据仓库的数据时，非独立数据集市就可以简单为用户提供一个数据仓库的”子集”。数据仓库搭建流程大家先看这个流程图，其中的各个步骤，我都会去介绍，其中着重介绍的会是数据仓库建模和ETL工程，因为，建模是整个数据仓库的核心，ETL工程师整个数据仓库中最耗时耗力的。今天呢，先简单介绍一下，铺垫一下","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"Spark Streaming offset 管理","slug":"Spark-Streaming-offset-管理","date":"2017-08-02T08:03:44.000Z","updated":"2020-05-07T08:13:24.491Z","comments":true,"path":"2017/08/02/Spark-Streaming-offset-管理/","link":"","permalink":"cpeixin.cn/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/","excerpt":"","text":"越来越多的实时项目需求，感觉好像各个业务线，产品都想让自己的数据动起来，并且配合上数据可视化展示出来。那么在使用Spark Streaming过程中，肯定不能避免一个问题，那就是，你全天24小时运行的实时程序，如果在某一时刻因为各种原因，停掉。当你发现实时程序已经因为故障停止运行了1个小时，或者产品运营中数据的使用者打电话给你，通知你最近一个小时的数据没有显示（尴尬😅），或者实时程序要临时升级，需要添加新的业务逻辑重新部署。那么此时，在你还没有做offset管理的时候，你准备怎么办呢？是”auto.offset.reset” -&gt; “latest” 从最新的offset位移数据读起，放弃那1个小时未读取到的数据，还是”auto.offset.reset” -&gt; “earliest”, 从最早的位移数据读取，重新处理一遍所有topic数据。思考一下，这两种情况都不是一个好的办法，所以，我们要在有这个需求的程序中，来进行offset的管理，避免数据的丢失以及重复计算的问题。将offset存储在外部数据存储中CheckpointsHBaseZooKeeperKafkaredis不管理offset上图描述了在Spark Streaming应用程序中管理offset的一般流程。offset可以通过几种方式进行管理，但通常遵循以下通用步骤。在Direct DStream初始化后，可以指定每个主题分区的offset映射，以了解Direct DStream应该从哪个分区开始读取。指定的偏移量与下面的第4步写入的位置相同。然后可以读取和处理这批消息。处理后，结果和offset都可以存储。_存储结果和提交偏移量_动作周围的虚线只是突出显示了一系列步骤，如果需要特殊的交付语义更严格的情况，用户可能需要进一步检查。这可能包括检查幂等运算或将结果及其偏移量存储在原子运算中。最后，任何外部持久数据存储（例如HBase，Kafka，HDFS和ZooKeeper）都可以用来跟踪已处理的消息。根据业务需求，可以将不同的方案合并到上述步骤中。Spark的编程灵活性允许用户进行细粒度的控制，以在处理的周期性阶段之前或之后存储offset。考虑发生以下情况的应用程序：Spark Streaming应用程序正在从Kafka读取消息，针对HBase数据执行转换操作，然后将操作后的消息发布到另一个topic中或单独的系统（例如，其他消息传递系统，到HBase，Solr，DBMS等）。在这种情况下，只有将消息成功发布到辅助系统后，我们才将其视为已处理。外部存储offset在本节中，我们探索用于在持久数据存储区中将offset持久保存在外部的不同选项。对于本节中提到的方法，如果使用spark-streaming-kafka-0-10_2.**库，建议用户将enable.auto.commit 设置为false。此配置仅适用于此版本，将enable.auto.commit 设置为true意味着offset将以config auto.commit.interval.ms控制的频率自动提交。在Spark Streaming中，将此值设置为true会在从Kafka读取消息时自动向Kafka提交偏移量，这不一定意味着Spark已完成对这些消息的处理。要启用精确的偏移量控制，请将Kafka参数enable.auto.commit 设置为 false。Spark Streaming checkpoints启用Spark Streaming的是存储checkpoints的最简单方法，因为它在Spark的框架中很容易获得。checkpoint是专门为保存应用程序的状态（一般情况下保存在HDFS）而设计的，以便可以在出现故障时将其恢复。对Kafka流进行检查点将导致偏移范围存储在检查点中。如果出现故障，Spark Streaming应用程序可以开始从检查点偏移范围读取消息。但是，Spark Streaming检查点无法在Spark应用程序升级之后恢复，因此不是很可靠，尤其是当您将这种机制用于关键的生产应用程序时。我们不建议通过Spark检查点管理偏移量。在HBase中存储offsetHBase可用作外部数据存储，以可靠的方式保留偏移范围。通过在外部存储偏移量范围，它使Spark Streaming应用程序能够从任意时间点重新启动和重播消息，只要消息在Kafka中仍然有效。借助HBase的通用设计，该应用程序能够利用rowkey和column family 来处理跨同一表中的多个Spark Streaming应用程序和Kafka topic 存储偏移范围。在此示例中，是使用包含topic，group id和Spark Streaming 的 batchTime.milliSeconds 组合作为行键来区分写入表的每个条目。新记录将累积在我们在以下设计中配置的表格中，以在30天后自动过期。下面是HBase表的DDL和结构。ddl1create &#39;stream_kafka_offsets&#39;, &#123;NAME&#x3D;&gt;&#39;offsets&#39;, TTL&#x3D;&gt;2592000&#125;RowKey 设计1234row: &lt;TOPIC_NAME&gt;:&lt;GROUP_ID&gt;:&lt;EPOCH_BATCHTIME_MS&gt;column family: offsetsqualifier: &lt;PARTITION_ID&gt;value: &lt;OFFSET_ID&gt;下面直接给出在HBase中，offset的管理设计流程代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162import kafka.utils.ZkUtilsimport org.apache.hadoop.hbase.filter.PrefixFilterimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.&#123;TableName, HBaseConfiguration&#125;import org.apache.hadoop.hbase.client.&#123;Scan, Put, ConnectionFactory&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.kafka010.ConsumerStrategies._import org.apache.spark.streaming.kafka010.&#123;OffsetRange, HasOffsetRanges, KafkaUtils&#125;import org.apache.spark.streaming.kafka010.LocationStrategies._import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkContext, SparkConf&#125;/** * Created by gmedasani on 6/10/17. */object KafkaOffsetsBlogStreamingDriver &#123; def main(args: Array[String]) &#123; if (args.length &lt; 6) &#123; System.err.println(\"Usage: KafkaDirectStreamTest &lt;batch-duration-in-seconds&gt; &lt;kafka-bootstrap-servers&gt; \" + \"&lt;kafka-topics&gt; &lt;kafka-consumer-group-id&gt; &lt;hbase-table-name&gt; &lt;kafka-zookeeper-quorum&gt;\") System.exit(1) &#125; val batchDuration = args(0) val bootstrapServers = args(1).toString val topicsSet = args(2).toString.split(\",\").toSet val consumerGroupID = args(3) val hbaseTableName = args(4) val zkQuorum = args(5) val zkKafkaRootDir = \"kafka\" val zkSessionTimeOut = 10000 val zkConnectionTimeOut = 10000 val sparkConf = new SparkConf().setAppName(\"Kafka-Offset-Management-Blog\") .setMaster(\"local[4]\")//Uncomment this line to test while developing on a workstation val sc = new SparkContext(sparkConf) val ssc = new StreamingContext(sc, Seconds(batchDuration.toLong)) val topics = topicsSet.toArray val topic = topics(0) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; bootstrapServers, \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; consumerGroupID, \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) /* Create a dummy process that simply returns the message as is. */ def processMessage(message:ConsumerRecord[String,String]):ConsumerRecord[String,String]=&#123; message &#125; /* Save Offsets into HBase */ def saveOffsets(TOPIC_NAME:String,GROUP_ID:String,offsetRanges:Array[OffsetRange],hbaseTableName:String, batchTime: org.apache.spark.streaming.Time) =&#123; val hbaseConf = HBaseConfiguration.create() hbaseConf.addResource(\"src/main/resources/hbase-site.xml\") val conn = ConnectionFactory.createConnection(hbaseConf) val table = conn.getTable(TableName.valueOf(hbaseTableName)) val rowKey = TOPIC_NAME + \":\" + GROUP_ID + \":\" + String.valueOf(batchTime.milliseconds) val put = new Put(rowKey.getBytes) for(offset &lt;- offsetRanges)&#123; put.addColumn(Bytes.toBytes(\"offsets\"),Bytes.toBytes(offset.partition.toString), Bytes.toBytes(offset.untilOffset.toString)) &#125; table.put(put) conn.close() &#125; /* Returns last committed offsets for all the partitions of a given topic from HBase in following cases. - CASE 1: SparkStreaming job is started for the first time. This function gets the number of topic partitions from Zookeeper and for each partition returns the last committed offset as 0 - CASE 2: SparkStreaming is restarted and there are no changes to the number of partitions in a topic. Last committed offsets for each topic-partition is returned as is from HBase. - CASE 3: SparkStreaming is restarted and the number of partitions in a topic increased. For old partitions, last committed offsets for each topic-partition is returned as is from HBase as is. For newly added partitions, function returns last committed offsets as 0 */ def getLastCommittedOffsets(TOPIC_NAME:String,GROUP_ID:String,hbaseTableName:String,zkQuorum:String, zkRootDir:String, sessionTimeout:Int,connectionTimeOut:Int):Map[TopicPartition,Long] =&#123; val hbaseConf = HBaseConfiguration.create() hbaseConf.addResource(\"src/main/resources/hbase-site.xml\") val zkUrl = zkQuorum+\"/\"+zkRootDir val zkClientAndConnection = ZkUtils.createZkClientAndConnection(zkUrl,sessionTimeout,connectionTimeOut) val zkUtils = new ZkUtils(zkClientAndConnection._1, zkClientAndConnection._2,false) val zKNumberOfPartitionsForTopic = zkUtils.getPartitionsForTopics(Seq(TOPIC_NAME)).get(TOPIC_NAME).toList.head.size //Connect to HBase to retrieve last committed offsets val conn = ConnectionFactory.createConnection(hbaseConf) val table = conn.getTable(TableName.valueOf(hbaseTableName)) val startRow = TOPIC_NAME + \":\" + GROUP_ID + \":\" + String.valueOf(System.currentTimeMillis()) val stopRow = TOPIC_NAME + \":\" + GROUP_ID + \":\" + 0 val scan = new Scan() val scanner = table.getScanner(scan.setStartRow(startRow.getBytes).setStopRow(stopRow.getBytes).setReversed(true)) val result = scanner.next() var hbaseNumberOfPartitionsForTopic = 0 //Set the number of partitions discovered for a topic in HBase to 0 if (result != null)&#123; //If the result from hbase scanner is not null, set number of partitions from hbase to the number of cells hbaseNumberOfPartitionsForTopic = result.listCells().size() &#125; val fromOffsets = collection.mutable.Map[TopicPartition,Long]() if(hbaseNumberOfPartitionsForTopic == 0)&#123; // initialize fromOffsets to beginning for (partition &lt;- 0 to zKNumberOfPartitionsForTopic-1)&#123; fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; 0)&#125; &#125; else if(zKNumberOfPartitionsForTopic &gt; hbaseNumberOfPartitionsForTopic)&#123; // handle scenario where new partitions have been added to existing kafka topic for (partition &lt;- 0 to hbaseNumberOfPartitionsForTopic-1)&#123; val fromOffset = Bytes.toString(result.getValue(Bytes.toBytes(\"offsets\"),Bytes.toBytes(partition.toString))) fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; fromOffset.toLong)&#125; for (partition &lt;- hbaseNumberOfPartitionsForTopic to zKNumberOfPartitionsForTopic-1)&#123; fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; 0)&#125; &#125; else &#123; //initialize fromOffsets from last run for (partition &lt;- 0 to hbaseNumberOfPartitionsForTopic-1 )&#123; val fromOffset = Bytes.toString(result.getValue(Bytes.toBytes(\"offsets\"),Bytes.toBytes(partition.toString))) fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; fromOffset.toLong)&#125; &#125; scanner.close() conn.close() fromOffsets.toMap &#125; val fromOffsets= getLastCommittedOffsets(topic,consumerGroupID,hbaseTableName,zkQuorum,zkKafkaRootDir, zkSessionTimeOut,zkConnectionTimeOut) val inputDStream = KafkaUtils.createDirectStream[String, String](ssc,PreferConsistent,Assign[String, String]( fromOffsets.keys,kafkaParams,fromOffsets)) /* For each RDD in a DStream apply a map transformation that processes the message. */ inputDStream.foreachRDD((rdd,batchTime) =&gt; &#123; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges offsetRanges.foreach(offset =&gt; println(offset.topic, offset.partition, offset.fromOffset,offset.untilOffset)) val newRDD = rdd.map(message =&gt; processMessage(message)) newRDD.count() saveOffsets(topic,consumerGroupID,offsetRanges,hbaseTableName,batchTime) //save the offsets to HBase &#125;) println(\"Number of messages processed \" + inputDStream.count()) ssc.start() ssc.awaitTermination() &#125;&#125;在ZooKeeper中存储offset用户可以将偏移范围存储在ZooKeeper中，这可以类似地提供一种可靠的方法，以在最后停止的Kafka流上开始流处理。在这种情况下，启动时，Spark Streaming作业将从ZooKeeper中检索每个主题分区的最新处理过的偏移量。如果找到了一个以前在ZooKeeper中未管理过的新分区，则默认将其最新处理的偏移量从头开始。处理完每批后，用户可以存储第一个或最后一个处理过的偏移量。此外，在ZooKeeper中存储偏移量的znode位置使用与旧Kafka使用者API相同的格式。因此，用于跟踪或监视存储在ZooKeeper中的Kafka偏移量的任何工具仍然可以使用。初始化ZooKeeper连接，以获取和存储到ZooKeeper的偏移量：12val zkClientAndConnection = ZkUtils.createZkClientAndConnection(zkUrl, sessionTimeout, connectionTimeout)val zkUtils = new ZkUtils(zkClientAndConnection._1, zkClientAndConnection._2, false)检索存储在使用者组和主题列表的ZooKeeper中的最后偏移量的方法：1234567891011121314151617181920212223242526272829303132def readOffsets(topics: Seq[String], groupId:String): Map[TopicPartition, Long] = &#123; val topicPartOffsetMap = collection.mutable.HashMap.empty[TopicPartition, Long] val partitionMap = zkUtils.getPartitionsForTopics(topics) // /consumers/&lt;groupId&gt;/offsets/&lt;topic&gt;/ partitionMap.foreach(topicPartitions =&gt; &#123; val zkGroupTopicDirs = new ZKGroupTopicDirs(groupId, topicPartitions._1) topicPartitions._2.foreach(partition =&gt; &#123; val offsetPath = zkGroupTopicDirs.consumerOffsetDir + \"/\" + partition try &#123; val offsetStatTuple = zkUtils.readData(offsetPath) if (offsetStatTuple != null) &#123; LOGGER.info(\"retrieving offset details - topic: &#123;&#125;, partition: &#123;&#125;, offset: &#123;&#125;, node path: &#123;&#125;\", Seq[AnyRef](topicPartitions._1, partition.toString, offsetStatTuple._1, offsetPath): _*) topicPartOffsetMap.put(new TopicPartition(topicPartitions._1, Integer.valueOf(partition)), offsetStatTuple._1.toLong) &#125; &#125; catch &#123; case e: Exception =&gt; LOGGER.warn(\"retrieving offset details - no previous node exists:\" + \" &#123;&#125;, topic: &#123;&#125;, partition: &#123;&#125;, node path: &#123;&#125;\", Seq[AnyRef](e.getMessage, topicPartitions._1, partition.toString, offsetPath): _*) topicPartOffsetMap.put(new TopicPartition(topicPartitions._1, Integer.valueOf(partition)), 0L) &#125; &#125;) &#125;) topicPartOffsetMap.toMap&#125;使用特定的偏移量初始化Kafka Direct Dstream以开始处理：1val inputDStream = KafkaUtils.createDirectStream(ssc, PreferConsistent, ConsumerStrategies.Subscribe[String,String](topics, kafkaParams, fromOffsets))将一组可恢复的偏移量持久保存到ZooKeeper的方法。注意：_offsetPath_是一个ZooKeeper位置，表示为/ consumers / [groupId] / offsets / topic / [partitionId]，用于存储偏移值123456789101112131415161718def persistOffsets(offsets: Seq[OffsetRange], groupId: String, storeEndOffset: Boolean): Unit = &#123; offsets.foreach(or =&gt; &#123; val zkGroupTopicDirs = new ZKGroupTopicDirs(groupId, or.topic); val acls = new ListBuffer[ACL]() val acl = new ACL acl.setId(ANYONE_ID_UNSAFE) acl.setPerms(PERMISSIONS_ALL) acls += acl val offsetPath = zkGroupTopicDirs.consumerOffsetDir + \"/\" + or.partition; val offsetVal = if (storeEndOffset) or.untilOffset else or.fromOffset zkUtils.updatePersistentPath(zkGroupTopicDirs.consumerOffsetDir + \"/\" + or.partition, offsetVal + \"\", JavaConversions.bufferAsJavaList(acls)) LOGGER.debug(\"persisting offset details - topic: &#123;&#125;, partition: &#123;&#125;, offset: &#123;&#125;, node path: &#123;&#125;\", Seq[AnyRef](or.topic, or.partition.toString, offsetVal.toString, offsetPath): _*) &#125;)&#125;在kafka中管理offset在Apache Spark 2.1.x的Cloudera发行版中，spark-streaming-kafka-0-10使用了新的Consumer api，它公开了commitAsync API。使用commitAsync API，使用方可以在知道输出已存储后将偏移量提交给Kafka。新的使用者api根据使用者的_group.id_唯一地将偏移提交回Kafka 。Persist Offsets in Kafka123456789101112131415161718192021222324252627282930313233343536def createKafkaRDD(ssc: StreamingContext, config: Source) = &#123; var SparkDStream: InputDStream[ConsumerRecord[String, String]] = null try &#123; SparkDStream = &#123; val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; config.servers, \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; config.group, \"auto.offset.reset\" -&gt; config.offset )/* \"enable.auto.commit\" -&gt; config.getString(\"kafkaSource.enable.auto.commit\"))*/ // val subscribeTopics = config.getStringList(\"kafkaSource.topics\").toIterable import scala.collection.JavaConversions._ val kafkaStream = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](config.topic.toList, kafkaParams) ) kafkaStream &#125; &#125; catch &#123; case e: Throwable =&gt; &#123; throw new Exception(\"Couldn't init Spark stream processing\", e) &#125; &#125; SparkDStream &#125;var inputDStream: InputDStream[ConsumerRecord[String, String]] = createKafkaRDD（）inputDStream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // 更新 Offset 值 inputDStream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges) &#125;有关详细信息，请访问– http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#kafka-itself注意：commitAsync（）是Spark Streaming和Kafka Integration的kafka-0-10版本的一部分。如Spark文档所述，此集成仍处于试验阶段，API可能会发生变化。在Redis中存储offset123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010._import scala.collection.JavaConverters._import scala.util.Tryobject KafkaOffsetsBlogStreamingDriver &#123; /** * 根据groupId保存offset * @param ranges * @param groupId */ def storeOffset(ranges: Array[OffsetRange], groupId: String): Unit = &#123; for (o &lt;- ranges) &#123; val key = s\"bi_kafka_offset_$&#123;groupId&#125;_$&#123;o.topic&#125;_$&#123;o.partition&#125;\" val value = o.untilOffset JedisUtil.set(key, value.toString) &#125; &#125; /** * 根据topic，groupid获取offset * @param topics * @param groupId * @return */ def getOffset(topics: Array[String], groupId: String): (Map[TopicPartition, Long], Int) = &#123; val fromOffSets = scala.collection.mutable.Map[TopicPartition, Long]() topics.foreach(topic =&gt; &#123; val keys = JedisUtil.getKeys(s\"bi_kafka_offset_$&#123;groupId&#125;_$&#123;topic&#125;*\") if (!keys.isEmpty) &#123; keys.asScala.foreach(key =&gt; &#123; val offset = JedisUtil.get(key) val partition = Try(key.split(s\"bi_kafka_offset_$&#123;groupId&#125;_$&#123;topic&#125;_\").apply(1)).getOrElse(\"0\") fromOffSets.put(new TopicPartition(topic, partition.toInt), offset.toLong) &#125;) &#125; &#125;) if (fromOffSets.isEmpty) &#123; (fromOffSets.toMap, 0) &#125; else &#123; (fromOffSets.toMap, 1) &#125; &#125; /** * 创建InputDStream，如果auto.offset.reset为latest则从redis读取 * @param ssc * @param topic * @param kafkaParams * @return */ def createStreamingContextRedis(ssc: StreamingContext, topic: Array[String], kafkaParams: Map[String, Object]): InputDStream[ConsumerRecord[String, String]] = &#123; var kafkaStreams: InputDStream[ConsumerRecord[String, String]] = null val groupId = kafkaParams.get(\"group.id\").get val (fromOffSet, flag) = getOffset(topic, groupId.toString) val offsetReset = kafkaParams.get(\"auto.offset.reset\").get if (flag == 1 &amp;&amp; offsetReset.equals(\"latest\")) &#123; kafkaStreams = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe(topic, kafkaParams, fromOffSet)) &#125; else &#123; kafkaStreams = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe(topic, kafkaParams)) &#125; kafkaStreams &#125; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"offSet Redis\").setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(60)) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"group.id\" -&gt; \"binlog.test.rpt_test_1min\", \"auto.offset.reset\" -&gt; \"latest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean), \"session.timeout.ms\" -&gt; \"20000\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer] ) val topic = Array(\"weibo_keyword\") val groupId = \"test\" val lines = createStreamingContextRedis(ssc, topic, kafkaParams) lines.foreachRDD(rdds =&gt; &#123; if (!rdds.isEmpty()) &#123; println(\"##################:\" + rdds.count()) &#125; storeOffset(rdds.asInstanceOf[HasOffsetRanges].offsetRanges, groupId) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;import java.utilimport com.typesafe.config.ConfigFactoryimport org.apache.kafka.common.serialization.StringDeserializerimport redis.clients.jedis.&#123;HostAndPort, JedisCluster, JedisPool, JedisPoolConfig&#125;object JedisUtil &#123; private val config = ConfigFactory.load(\"realtime-etl.conf\") private val redisHosts: String = config.getString(\"redis.server\") private val port: Int = config.getInt(\"redis.port\") private val hostAndPortsSet: java.util.Set[HostAndPort] = new util.HashSet[HostAndPort]() redisHosts.split(\",\").foreach(host =&gt; &#123; hostAndPortsSet.add(new HostAndPort(host, port)) &#125;) private val jedisConf: JedisPoolConfig = new JedisPoolConfig() jedisConf.setMaxTotal(5000) jedisConf.setMaxWaitMillis(50000) jedisConf.setMaxIdle(300) jedisConf.setTestOnBorrow(true) jedisConf.setTestOnReturn(true) jedisConf.setTestWhileIdle(true) jedisConf.setMinEvictableIdleTimeMillis(60000l) jedisConf.setTimeBetweenEvictionRunsMillis(3000l) jedisConf.setNumTestsPerEvictionRun(-1) lazy val redis = new JedisCluster(hostAndPortsSet, jedisConf) def get(key: String): String = &#123; try &#123; redis.get(key) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() null &#125; &#125; def set(key: String, value: String) = &#123; try &#123; redis.set(key, value) &#125; catch &#123; case e: Exception =&gt; &#123; e.printStackTrace() &#125; &#125; &#125; def hmset(key: String, map: java.util.Map[String, String]): Unit = &#123; // val redis=pool.getResource try &#123; redis.hmset(key, map) &#125;catch &#123; case e:Exception =&gt; e.printStackTrace() &#125; &#125; def hset(key: String, field: String, value: String): Unit = &#123; // val redis=pool.getResource try &#123; redis.hset(key, field, value) &#125; catch &#123; case e: Exception =&gt; &#123; e.printStackTrace() &#125; &#125; &#125; def hget(key: String, field: String): String = &#123; try &#123; redis.hget(key, field) &#125;catch &#123; case e:Exception =&gt; e.printStackTrace() null &#125; &#125; def hgetAll(key: String): java.util.Map[String, String] = &#123; try &#123; redis.hgetAll(key) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() null &#125; &#125;&#125; #### 其他方法 值得一提的是，您还可以将偏移量存储在HDFS之类的存储系统中。与上述选项相比，在HDFS中存储偏移量不太受欢迎，因为与其他系统（如ZooKeeper和HBase）相比，HDFS具有更高的延迟。此外，如果管理不当，则在HDFS中为每个批次编写offsetRanges可能会导致文件较小的问题。不管理offset当然，Spark Streaming应用程序并不是必须的去管理offset。对当前业务考虑好是否需要对offset进行保存。本文参考如下：Offset Management For Apache Kafka With Apache Spark Streaming","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"python - 协程","slug":"python-协程","date":"2017-05-31T12:16:32.000Z","updated":"2020-04-04T11:03:54.388Z","comments":true,"path":"2017/05/31/python-协程/","link":"","permalink":"cpeixin.cn/2017/05/31/python-%E5%8D%8F%E7%A8%8B/","excerpt":"","text":"深入理解Python异步编程推荐推荐推荐！！！！协程，Python asyncio异步编程 只此一篇足矣，一览众山小！链接","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 生成器 2","slug":"python-生成器-2","date":"2017-05-30T11:08:35.000Z","updated":"2020-04-04T17:14:25.836Z","comments":true,"path":"2017/05/30/python-生成器-2/","link":"","permalink":"cpeixin.cn/2017/05/30/python-%E7%94%9F%E6%88%90%E5%99%A8-2/","excerpt":"","text":"生成器进化成协程生成器是由迭代器进化而来，所以生成器对象有 iter 和 next 方法，可以使用 for 循环获得值，注意这里所说的 “获得值” 指的是下文代码块里 yield 语句中 yield 关键字后面的 i 。这是在 Python 2.5 时出现的特性，在 Python 3.3 中出现 yield from 语法之前，生成器没有太大用途。但此时 yield 关键字还是实现了一些特性，且至关重要，就是生成器对象有 send 、throw 和 close 方法。这三个方法的作用分别是发送数据给生成器并赋值给 yield 语句、向生成器中抛入异常由生成器内部处理、终止生成器。这三个方法使得生成器进化成协程。协程有四种存在状态：GEN_CREATED 创建完成，等待执行GEN_RUNNING 解释器正在执行（这个状态在下面的示例程序中无法看到）GEN_SUSPENDED 在 yield 表达式处暂停GEN_CLOSE 执行结束，生成器停止可以使用 inspect.getgeneratorstate 方法查看协程的当前状态，举例如下：inspect模块用于收集python对象的信息，可以获取类或函数的参数的信息，源码，解析堆栈，对对象进行类型检查等等12345678910111213141516171819202122import inspectdef generator(): i = '激活生成器' while True: try: value = yield i except ValueError: print('OVER') i = valueg = generator() # 1inspect.getgeneratorstate(g) # 2print(next(g)) # 3inspect.getgeneratorstate(g)print(g.send('Hello Shiyanlou')) # 4g.throw(ValueError) # 5g.close() # 6inspect.getgeneratorstate(g)代码说明如下：1、创建生成器2、查看生成器状态3、这步操作叫做预激生成器（或协程），这是必须做的。在生成器创建完成后，需要将其第一次运行到 yield 语句处暂停4、暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值，也就是 yield 表达式等号前面的 value 变量的值变成 ‘Hello Shiyanlou’，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i5、向生成器抛入异常，异常会被 try except 捕获，作进一步处理6、close 方法终止生成器，异常不会被抛出因为生成器的调用方也就是程序员自己可以控制生成器的启动、暂停、终止，而且可以向生成器内部传入数据，所以这种生成器又叫做协程，generator 函数既可以叫做生成器函数，也可以叫协程函数，这是生成器向协程的过渡阶段。yield -&gt; yield from在 Python 3.3 中新增了 yield from 语法，如果将yield理解成“返回”，那么yield from就是“从什么（生成器）里面返回”,这是全新的语言结构，是 yield 的升级版。相比 yield ，该语法有两大优势，我们来举例说明它的用法。区别示例123456789101112def generator(): yield 'a' yield 'b' yield 'c' yield from generator1() #yield from iterable本质上等于 for item in iterable: yield item的缩写版 yield from [11,22,33,44] yield from (12,23,34) yield from range(3) for i in generator(): print(i,end=' , ')避免潜逃循环yield:12345678910def chain(*args): for iter_obj in args: for i in iter_obj: yield ichain(&#123;'one', 'two'&#125;, list('ace'))for i in c: print(i)yield from:123456789def chain(*args): for iter_obj in args: yield from iter_objc = chain(&#123;'one', 'two'&#125;, list('ace'))for i in c: print(i)可以看到 yield from 语句可以替代 for 循环，避免了嵌套循环。同 yield 一样，yield from 语句也只能出现在函数体内部，有 yield from 语句的函数叫做协程函数或生成器函数。yield from 后面接收一个可迭代对象，例如上面代码中的 iter_obj 变量，在协程中，可迭代对象往往是协程对象，这样就形成了嵌套协程。转移控制权123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import timefrom faker import Fakerfrom functools import wraps# 预激协程装饰器def coroutine(func): @wraps(func) def wrapper(*args, **kw): g = func(*args, **kw) next(g) return g return wrapper# 子生成器函数，这个生成器是真正做事的生成器def sub_coro(): l = [] # 创建空列表 while True: # 无限循环 value = yield # 调用方使用 send 方法发生数据并赋值给 value 变量 if value == 'CLOSE': # 如果调用方发生的数据是 CLOSE ，终止循环 break l.append(value) # 向列表添加数据 return sorted(l) # 返回排序后的列表# 使用预激协程装饰器# 带有 yield from 语句的父生成器函数@coroutinedef dele_coro(): # while True 可以多次循环，每次循环会创建一个新的子生成器 sub_coro() # 这里 while 只循环一次，这是由调用方，也就是 main 函数决定的 # while 循环可以捕获函数本身创建的父生成器终止时触发的 StopIteration 异常 while True: # yield from 会自动预激子生成器 sub_coro() # 所以 sub_coro 在定义时不可以使用预激协程装饰器 # yield from 将捕获子生成器终止时触发的 StopIteration 异常 # 并将异常的 value 属性值赋值给等号前面的变量 l # 也就是 l 变量的值等于 sub_coro 函数的 return 值 # yield from 还实现了一个重要功能 # 就是父生成器的 send 方法将发送值给子生成器 # 并赋值给子生成器中 yield 语句等号前面的变量 value l = yield from sub_coro() print('排序后的列表：', l) print('------------------')# 调用父生成器的函数，也叫调用方def main(): # 生成随机国家代号的方法 fake = Faker().country_code # 嵌套列表，每个子列表中有三个随机国家代号(字符串) nest_country_list = [[fake() for i in range(3)] for j in range(3)] for country_list in nest_country_list: print('国家代号列表：', country_list) c = dele_coro() # 创建父生成器 for country in country_list: c.send(country) # 父生成器的 send 方法将国家代号发送给子生成器 # CLOSE 将终止子生成器中的 while 循环 # 子生成器的 return 值赋值给父生成器 yield from 语句中等号前面的变量 l c.send('CLOSE')if __name__ == '__main__': main()所谓 “转移控制权” 就是 yield from 语法可以将子生成器的控制权交给调用方 main 函数，在 main 函数内部创建父生成器 c ，控制 c.send 方法传值给子生成器。这是一个巨大的进步，在此基础上，Python 3.4 新增了创建协程的装饰器，这样非生成器函数的协程函数就正式出现了。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 生成器","slug":"python-生成器","date":"2017-05-28T12:07:35.000Z","updated":"2020-04-04T17:13:37.299Z","comments":true,"path":"2017/05/28/python-生成器/","link":"","permalink":"cpeixin.cn/2017/05/28/python-%E7%94%9F%E6%88%90%E5%99%A8/","excerpt":"","text":"前言在开始讲解协程和生成器之前，说明一下协程和生成器之间的关系是很有必要的，生成器在学习python的时候，并没有深入的学习，所以感觉很抽象很难懂，和普通函数单向调用的逻辑思维并不一样。协程则是和线程，进程是同一类别的概念。首先要认识yield关键字，是yield关键词，yield放在函数中可以使得函数变成生成器，也可以变成协程。在生成器中, yield 只对外产出值，在协程中，yield能对外产出值，而且能接收通过send()方法传入值yielld构造的生成器可以作为协程使用，协程是指一个过程，这个过程与调用方协作，由调用方提供的值，来计算并产出。纯粹的生，这样可以交接给for调用。成器只输出值，和迭代有关协程与函数的区别，函数是一种上下级调用关系，而协程是通过_yield_方式转移执行权，对称而平级的调用对方，典型的有生产者和消费者。从调试过程中理解区别于其他教程，前几段都是云里雾里的讲概念，但是对于生成器，一上来就看概念，真的很难懂。所以我准备了三个实例，建议上来先通过打断点，分步调试过程中，通过看调用顺序和返回值来初步了解生成器的原理，接下来在看概念解释，则比较容易理解。1234567891011121314151617# encoding:UTF-8def yield_test(n): for i in range(n): yield call(i) print(\"i=\", i) # 做一些其它的事情 print(\"do something.\") print(\"end.\")def call(i): return i * 2# 使用for循环for i in yield_test(5): print(i, \",\")下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c)下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示12345678910def h(): print 'Wen Chuan', m = yield 5 print m d = yield 12 print 'We are together!'c = h()m = c.__next__() c.send('Fighting!')下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示经过上面的三个程序的调试步骤后，下面开始带着心中初步了解的程序运行顺序来看生成器相关的原理生成器what生成器是一次生成一个值的特殊类型函数。可以将其视为可恢复函数。调用该函数将返回一个可用于生成连续 x 值的生成【Generator】，简单的说就是在函数的执行过程中，yield语句会把你需要的值返回给调用生成器的地方，然后退出函数，下一次调用生成器函数的时候又从上次中断的地方开始执行，而生成器内的所有变量参数都会被保存下来供下一次使用。why列表所有数据都在内存中，如果有海量数据的话将会非常耗内存。如：仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。如果列表元素按照某种算法推算出来，那我们就可以在循环的过程中不断推算出后续的元素，这样就不必创建完整的list，从而节省大量的空间。简单一句话：我又想要得到庞大的数据，又想让它占用空间少，那就用生成器！How方法一123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;方法二如果一个函数中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。调用函数就是创建了一个生成器（generator）对象。yield关键字yield 的用法起源于对一般 function 中 return 的扩展。在一个 function 中，必须有一个返回值列于 return 之后，可以返回数字也可以返回空值，但是必须要有一个返回值，标志着这个 function 的结束。一旦它结束，那么这个 function 中产生的一切变量将被统统抛弃，有什么可以使一个 function 暂停下来，并且返回当前所在地方的值，当接收到继续的命令时可以继续前进呢？换个说法，就是 return 返回一个值，并且记住这个返回的位置。这个操作有点儿像Python2.5以前，yield是一个语句，但现在2.5中，yield是一个表达式(Expression)，比如：1m = yield 5大家不要认为，m值为5，而是表达式(yield 5)的返回值将赋值给m，那么yield 5的返回值是什么呢？yield 5的返回值是下面将要提到的send(msg)方法传递过来的msg参数。生成器的工作原理生成器(generator)能够迭代的关键是它有一个 next() 方法 。 工作原理就是通过重复调用next()方法，直到捕获一个异常。带有 yield 的函数不再是一个普通函数，而是一个生成器generator。可用next()调用生成器对象来取值。next 两种方式 t.next() | next(t)。 可用for 循环获取返回值（每执行一次，取生成器里面一个值） （基本上不会用next()来获取下一个返回值，而是直接使用for循环来迭代）。yield相当于 return 返回一个值，_迭代一次遇到yield时就返回yield后面(右边)的值_。并且记住这个返回的位置，下次迭代时，代码从yield的_下一条_语句开始执行。send() 和next()一样，都能让生成器继续往下走一步（下次遇到yield停），send()能传一个值，这个值作为yield表达式等号左边的值。——换句话说，就是send可以强行修改上一个yield表达式值。比如函数中有一个yield赋值，a = yield第一次迭代到这里会返回5，a还没有赋值。第二次迭代时，使用.send(10)，那么，就是强行修改yield 5表达式的值为10，本来是5的，那么a=10send(msg)与next()都有返回值，它们的返回值是当前迭代遇到yield时，yield后面表达式的值，其实就是当前迭代中yield后面的参数。感受下yield返回值的过程（关注点：每次停在哪，下次又开始在哪）及send()传参的通讯过程生成器还可以使用 next 方法迭代。生成器会在 yield 语句处暂停，这是至关重要的，未来协程中的 IO 阻塞就出现在这里。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 多进程","slug":"python-多进程","date":"2017-05-26T15:10:35.000Z","updated":"2020-04-04T11:05:03.269Z","comments":true,"path":"2017/05/26/python-多进程/","link":"","permalink":"cpeixin.cn/2017/05/26/python-%E5%A4%9A%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"多线程： 同一进程中，创建多个线程，执行添加的任务列表多进程： 创建任意个进程，各自执行添加的任务列表假如cpu只有一个（早期的计算机确实如此），也能保证支持（伪）并发的能力，将一个单独的cpu变成多个虚拟的cpu（多道技术：时间多路复用和空间多路复用+硬件上支持隔离），没有进程的抽象，现代计算机将不复存在。对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。如果我们并没有在程序中加入进程，线程，协程等，程序都是执行单任务的进程，也就是只有一个线程。如果我们要同时执行多个任务怎么办？有两种解决方案：一种是启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。还有一种方法是启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。当然还有第三种方法，就是启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。总结一下就是，多任务的实现有3种方式：多进程模式；多线程模式；多进程+多线程模式。同时执行多个任务通常各个任务之间并不是没有关联的，而是需要相互通信和协调，有时，任务1必须暂停等待任务2完成后才能继续执行，有时，任务3和任务4又不能同时执行，所以，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。什么时候用多进程1.多线程使用场景：IO密集型2.多进程使用场景：CPU密集型涉及并发的场景，大家想到使用多线程或多进程解决并发问题;一般情况下，解决多并发场景问题，多数语言采用多线程编程模式(线程是轻量级的进程，共用一份进程空间)。也同样适用于Python多并发处理吗? 不是的，针对并发处理，Python多线程和多进程是有很大差异的!Python多线程和多进程差异Python多线程不能使用CPU多核资源，即同一时刻，只有一个线程使用CPU资源，所以使用Python多线程不能算是并发。如果想要充分利用CPU多核资源，做到多并发，这就需要Python多进程的了!也就是说：只有Python多进程才能利用CPU多核资源，做到真正的多并发!Python多线程和多进程应用场景Python多线程适用于I/O密集型场景，如解决网络IO、磁盘IO阻塞问题，例如文件读写、网络数据传输等;而Python多进程更适用于计算密集型场景，多并发，大量计算任务等。注意：Python多线程和多进程在平时开发过程中，需要注意使用，如果使用Python多线程方式处理计算密集型任务，它比实际单进程处理性能还要慢!所以要注意，看场景类型。多进程-fork()Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程：123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid))结果：123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876.由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的，推荐大家用Mac学Python！有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。多进程-multiprocessing进程基础版本：1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.')执行结果如下：1234Parent process 928.Process will start.Run child process test (929)...Process end.进程池：如果要启动大量的子进程，可以用进程池的方式批量创建子进程：12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.')代码解读：对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：p = Pool(5)就可以同时跑5个进程。由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。进程池实例方法：apply(func[, args[, kwds]])：同步进程池apply_async(func[, args[, kwds[, callback[, error_callback]]]]) ：异步进程池Lock互斥锁当多个进程需要访问共享资源的时候，Lock可以用来避免访问的冲突进程之间数据隔离，但是共享一套文件系统，因而可以通过文件来实现进程直接的通信，但问题是必须自己加锁处理。注意：加锁的目的是为了保证多个进程修改同一块数据时，同一时间只能有一个修改，即串行的修改，没错，速度是慢了，牺牲了速度而保证了数据安全1234567891011121314151617181920212223242526272829303132333435363738import jsonimport timeimport randomimport osfrom multiprocessing import Process,Lockdef chakan(): dic = json.load(open('piao',)) # 先查看票数，也就是打开那个文件 print('剩余票数：%s' % dic['count']) # 查看剩余的票数def buy(): dic = json.load(open('piao',)) if dic['count']&gt;0: #如果还有票 dic['count']-=1 #就修改里面的值-1 time.sleep(random.randint(1,3)) #执行里面买票的一系列操作就先不执行了，让睡一会代替（并且随机的睡） json.dump(dic,open('piao','w')) print('%s 购票成功' % os.getpid()) # 当前的那个id购票成功def task(mutex): #抢票 # 第一种加锁： # mutex.acquire() #加锁 # chakan() # 因为查看的时候大家都可以看到，不需要加锁 # buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买 # mutex.release() #取消锁 # 第二种加锁： #with表示自动打开自动释放锁 with mutex: chakan() # 因为查看的时候大家都可以看到，不需要加锁 buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买if __name__ == '__main__': mutex = Lock() for i in range(50):#让50个人去访问那个票数 p = Process(target=task,args=(mutex,)) p.start()进程池加锁还是上面抢票的例子，这次使用了进程池，由于进程之间不共享内存，所以进程之间的通信不能像线程之间直接引用，使用进程池异步对共享变量进行操作，异步操作lock锁，会引起冲突，因而需要采取一些策略来完成进程之间的数据通信。所以要引入进程Manager来完成进程间通信的方式这里举两个例子：大家自行创建 piao 文件，内容为 {“count”:320}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import jsonimport timeimport randomimport osfrom multiprocessing import Process,Lock,Pooldef chakan(): dic = json.load(open('piao',)) # 先查看票数，也就是打开那个文件 print('剩余票数：%s' % dic['count']) # 查看剩余的票数def buy(): dic = json.load(open('piao',)) if dic['count']&gt;0: #如果还有票 dic['count']-=1 #就修改里面的值-1 time.sleep(random.randint(1,3)) #执行里面买票的一系列操作就先不执行了，让睡一会代替（并且随机的睡） json.dump(dic,open('piao','w')) print('%s 购票成功' % os.getpid()) # 当前的那个id购票成功def task(mutex): #抢票 # mutex.acquire() #加锁 # chakan() # 因为查看的时候大家都可以看到，不需要加锁 # buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买 # mutex.release() #取消锁 with mutex: chakan() # 因为查看的时候大家都可以看到，不需要加锁 buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买if __name__ == '__main__': mutex = Lock() pool = Pool(20) \"\"\"进程池加锁 \"\"\" from multiprocessing import Pool, Manager, Lock manager = Manager() mutex = manager.Lock() for i in range(50):#让50个人去访问那个票数 pool.apply_async(task,args=(mutex, )) pool.close() pool.join()12345678910111213141516171819202122232425from multiprocessing import Process,Managerimport os# 这里实现的就是多个进程之间共享内存，并修改数据# 这里不需要加锁，因为manager已经默认给你加锁了def f(d,l): d[1] = '1' d['2'] = 2 d[0.25] = None l.append(os.getpid()) print(l)if __name__ == '__main__': with Manager() as manager: d = manager.dict() #生成一个字典 l = manager.list(range(5)) #生成一个列表 p_list = [] for i in range(10): p = Process(target=f,args=(d,l)) p.start() p_list.append(p) for res in p_list: res.join() print(d) print(l)","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 多线程","slug":"python-多线程","date":"2017-05-24T15:08:35.000Z","updated":"2020-04-04T11:04:52.986Z","comments":true,"path":"2017/05/24/python-多线程/","link":"","permalink":"cpeixin.cn/2017/05/24/python-%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"巴拉巴拉最近在搞爬虫项目，架构和程序设计都是由我来决定和设计，所以发挥空间还是很自由的。程序语言选择的是python，在语言选型上也考虑过golang，golang的效率和速度肯定是要比python好很多的。但是在爬虫领域，python的易用性，超多的第三方扩展库，而且python支持协程后，效率速度方面上的表现，也是很不错的。那么在python做爬虫的过程中，多线程，多进程，协程，异步等肯定都是逃不过的。所以抽出一些时间，写一写线程，进程，协程，异步等方法在爬虫中的表现。多进程和多线程我们常见的 Linux、Windows、Mac OS 操作系统，都是支持多进程的多核操作系统。所谓多进程，就是系统可以同时运行多个任务。例如我们的电脑上运行着 QQ、浏览器、音乐播放器、影音播放器等。在操作系统中，每个任务就是一个进程。每个进程至少做一件事，多数进程会做很多事，例如影音播放器，要播放画面，同时要播放声音，在一个进程中，就有很多线程，每个线程做一件事，在一个进程中有多个线程运行就是多线程。可以在实验环境终端执行 ps -ef 命令来查看当前系统中正在运行的进程。计算机的两大核心为运算器和存储器。常说的手机配置四核、八核，指的就是 CPU 的数量，它决定了手机的运算能力；128G、256G 超大存储空间，指的就是手机存储数据的能力。当我们运行一个程序来计算 3 + 5，计算机操作系统会启动一个进程，并要求运算器派过来一个 CPU 来完成任务；当我们运行一个程序来打开文件，操作系统会启动存储器的功能将硬盘中的文件数据导入到内存中。一个 CPU 在某一时刻只能做一项任务，即在一个进程（或线程）中工作，当它闲置时，会被系统派到其它进程中。单核计算机也可以实现多进程，原理是第 1 秒的时间段内运行 A 进程，其它进程等待：第 2 秒的时间段内运行 B 进程，其它进程等待。。。第 5 秒的时间段内又运行 A 进程，往复循环。当然实际上 CPU 在各个进程间的切换是极快的，在毫秒（千分之一）、微秒（百万分之一）级，以至于我们看起来这些程序就像在同时运行。现代的计算机都是多核配置，四核八核等，但计算机启动的瞬间，往往就有几十上百个进程在运行了，所以进程切换是一定会发生的，CPU 在忙不迭停地到处赶场。注意，什么时候进行进程 、线程切换是由操作系统决定的，无法人为干预。线程安全我们都知道在 MySQL 中有 “原子操作” 的概念，打个比方：韩梅向李红转账 100 块钱，在 MySQL 中需要两步操作：韩梅账户减少 100 元，李红账户增加 100 元。如果第一步操作完成后，意外情况导致第二步没有做，这是不允许发生的，如何保证其不允许发生呢？将两步操作设计成一个事务，事务里可以有多个步骤，其中任何一步出现问题，事务都将失败，前面的步骤全部回滚，就像什么事都没发生。这种操作就叫做原子操作，这种特性就叫做原子性。在 Python 多线程中，变量是共享的，这也是相较多进程的一个优点，线程占用资源要少得多，但也导致多个 CPU 同时操作多个线程时会引起结果无法预测的问题，也就是说 Python 的线程不安全。在多线程与多进程的时候，因为一般情况下都是各自完成各自的任务，各个子线程或者各个子进程之前并没有太多的联系，如果需要通信的话我会使用队列或者数据库来完成，但是最近我在写一些多线程与多进程的代码时，发现如果它们需要用到共享变量的话，需要有一些注意的地方接下来用实例讲解一下线程共享的问题标准数据类型在线程间共享下面的实例，在主线程中创建变量d,在5个子线程中引用。123456789101112131415# coding:utf-8import threadingdef test(name, data): print(\"in thread &#123;&#125; name is &#123;&#125;\".format(threading.current_thread(), name)) print(\"data is &#123;&#125; id(data) is &#123;&#125;\".format(data, id(data)))if __name__ == '__main__': d = 5 name = \"cpeixin\" for i in range(5): th = threading.Thread(target=test, args=(name, d)) th.start()下面的结果中显示，5个子线程中打印出变量d的id相同，表示引用的同一个变量，所以说明在主线程中创建了变量d，在子线程中是可以共享的，在子线程中对共享元素的改变是会影响到其它线程的，所以如果要对共享变量进行修改时，也就是线程不安全的，需要加锁。1234567891011/Users/cpeixin/venv/pythonCode/bin/python /Users/cpeixin/PycharmProjects/pythonCode/thread/variable_thread.pyin thread &lt;Thread(Thread-1, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-2, started 123145524641792)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-3, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-4, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-5, started 123145524641792)&gt; name is cpeixindata is 5 id(data) is 4304846080自定义类型对象在线程间共享如果我们要自定义一个类呢，将一个对象作为变量在子线程中传递呢？会是什么效果呢？123456789101112131415161718192021222324252627# coding:utf-8import threadingclass Data: def __init__(self, data=None): self.data = data def get(self): return self.data def set(self, data): self.data = datadef test(name, data): print(\"in thread &#123;&#125; name is &#123;&#125;\".format(threading.current_thread(), name)) print(\"data is &#123;&#125; id(data) is &#123;&#125;\".format(data.get(), id(data)))if __name__ == '__main__': d = Data(10) name = \"cpeixin\" print(\"in main thread id(data) is &#123;&#125;\".format(id(d))) for i in range(5): th = threading.Thread(target=test, args=(name, d)) th.start()1234567891011in main thread id(data) is 4348194152in thread &lt;Thread(Thread-1, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-2, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-3, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-4, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-5, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152我们看到，在主线程和子线程中，这个对象的id是一样的，说明它们用的是同一个对象。无论是标准数据类型还是复杂的自定义数据类型，它们在多线程之间是共享同一个的以上就是在多线程中，变量共享的码上说明GIL 全局解释器锁如何解决线程安全问题？CPython 解释器使用了加锁的方法。每个进程有一把锁，启动线程先加锁，结束线程释放锁。打个比方，进程是一个厂房，厂房大门是开着的，门内有锁，工人进入大门后可以在内部上锁。厂房里面有 10 个车间对应 10 个线程，每个 CPU 就是一个工人。GIL（Global Interpreter Lock）全局锁就相当于厂房规定：工人要到车间工作，从厂房大门进去后要在里面反锁，完成工作后开锁出门，下一个工人再进门上锁。也就是说，任意时刻厂房里只能有一个工人，但这样就保证了工作的安全性，这就是 GIL 的原理。当然了，GIL 的存在有很多其它益处，包括简化 CPython 解释器和大量扩展的实现。根据上面的例子可以看出 GIL 实现了线程操作的安全性，但多线程的效率被大打折扣，一个工厂里只能有一个工人干活，很难想象。这也是 David Beazley（《Python 参考手册》和《Python Cookbook》的作者）说 “Python 线程毫无用处” 的原因。注意，GIL 不是语言特性，而是解释器的设计特点，有些 Python 解释器例如 JPython 就没有 GIL ，除了 Python 其它语言也有 GIL 设计，例如 Ruby 。线程锁为什么需要线程锁?多个线程对同一个数据进行修改时， 可能会出现不可预料的情况.例如实现银行转账功能，money += 1 这句其实有三个步骤 money; money+1; money=money+1;假如这三步骤还没完成money-=1的线程就开始执行了，后果可想而知，money的值肯定时乱的如何实现线程锁?实例化一个锁对象;lock = threading.Lock()操作变量之前进行加锁lock.acquire()操作变量之后进行解锁lock.release()12345678910111213141516171819202122232425262728293031323334353637import threading# 银行存钱和取钱def add(lock): global money # 生命money为全局变量 for i in range(1000000): # 2. 操作变量之前进行加锁 lock.acquire() money += 1 # money; money+1; money=money+1; # 3. 操作变量之后进行解锁 lock.release()def reduce(lock): global money for i in range(1000000): # 2. 操作变量之前进行加锁 lock.acquire() money -= 1 # 3. 操作变量之后进行解锁 lock.release()if __name__ == '__main__': money = 0 # 1. 实例化一个锁对象; lock = threading.Lock() t1 = threading.Thread(target=add, args=(lock,)) t2 = threading.Thread(target=reduce, args=(lock,)) t1.start() t2.start() t1.join() t2.join() print(\"当前金额:\", money)多线程提高工作效率实际情况并非上面讲得那么惨，Python 多线程可以成倍提高程序的运行速度，而且在多数情况下都是有效的。接着上面的例子说，一个工厂里同一时刻只能有一个工人在工作，如果这个工厂里各个车间的自动化程度极高且任务耦合度极低，工人进去只是按几下按钮，就可以等待机器完成其余工作，那情况就不一样了，这种场景下一个工人可以管理好多个车间，而且大多数时间都是等，甚至还能抽空打打羽毛球看场电影。比如爬虫程序爬取页面数据这个场景中，CPU 做的事就是发起页面请求和处理响应数据，这两步是极快的，中间网络传输数据的过程是耗时且不占用 CPU 的。一个工人可以在吃完早饭后一分钟内快速到 1000 个车间按下发起请求的按钮，吃完午饭睡一觉，日薄西山时差不多收到网络传回的数据，又用一分钟处理数据，整个程序完成。上面的场景中，CPU 再多也没有用处，一个 CPU 抽空就能完成整个任务了，毕竟程序中需要 CPU 做的事并不多。这就涉及复杂程序的分类：CPU 密集型和 IO 密集型。爬虫程序就是 IO 密集型程序。CPU 密集型程序全是手工操作，工人一刻也不能停歇，这种情况下 Python 多线程就真可以说是毫无用处了。我们可以使用 time.sleep 方法模拟 IO 操作来写一段程序证明多线程可以提高程序的运行效率：12345678910111213141516171819202122232425262728293031323334353637# File Name: thread.pyimport threadingimport timeimport requestsdef crawl_url(): # 假设这是爬虫程序，爬取一个 URL time.sleep(0.02) # 模拟 IO 操作 # res = requests.get(\"http://www.ip111.cn\").status_code # print(res)def main1(): # 单线程程序 for i in range(100): crawl_url()def main2(): # 多线程程序 thread_list = [] for i in range(100): t = threading.Thread(target=crawl_url) t.start() thread_list.append(t) for t in thread_list: t.join()if __name__ == '__main__': start = time.time() main1() end = time.time() print('单线程耗时：&#123;:.4f&#125;s'.format(end - start)) start = time.time() main2() end = time.time() print('多线程耗时：&#123;:.4f&#125;s'.format(end - start))12单线程耗时：2.4027s多线程耗时：0.0323s理论上，main1 的耗时是 main2 的 100 倍，考虑到 main2 创建多线程、线程切换的开销，这个结果也是相当可观的，IO 操作耗时越长，多线程的威力越大。线程池ThreadPool在使用多线程处理任务时也不是线程越多越好，由于在切换线程的时候，需要切换上下文环境，依然会造成cpu的大量开销。为解决这个问题，线程池的概念被提出来了。预先创建好一个较为优化的数量的线程，让过来的任务立刻能够使用，就形成了线程池。1234567891011121314# coding: utf-8from concurrent.futures import ThreadPoolExecutorimport timedef spider(page): time.sleep(page) print(f\"crawl task&#123;page&#125; finished\") return pagewith ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 task1 = t.submit(spider, 1) task2 = t.submit(spider, 2) # 通过submit提交执行的函数到线程池中 task3 = t.submit(spider, 3)使用 with 语句 ，通过 ThreadPoolExecutor 构造实例，同时传入 max_workers 参数来设置线程池中最多能同时运行的线程数目。使用 submit 函数来提交线程需要执行的任务到线程池中，并返回该任务的句柄（类似于文件、画图），注意 submit() 不是阻塞的，而是立即返回。multiprocessing.dummy.Poolmultiprocessing.dummy.Pool 是个什么东东？看起来像一个进程池的样子啊～～看了源码中的代码和注释123456789# Support for the API of the multiprocessing package using threads## multiprocessing/dummy/__init__.pyclass DummyProcess(threading.Thread)def Pool(processes=None, initializer=None, initargs=()): from ..pool import ThreadPool return ThreadPool(processes, initializer, initargs)这只是是以multiprocessing相同API实现的多线程模块。继承了Thread，内部是封装调用了ThreadPool，使用起来更加方便。异步和同步，阻塞和非阻塞上文的模拟爬虫示例代码中，main1 中的 for 循环运行 100 次爬取网页的操作，前一个完成后才能运行下一个，这就是同步的概念，在 crawl_url 函数内部的 IO 操作为阻塞操作，线程无法向下执行。main2 中的第一个 for 循环，_创建 100 个线程并启动，这步操作是非阻塞的_，不会等一个线程运行完成才创建下一个线程，它会一气儿创建 100 个线程；第二个 for 循环将主线程挂起，直到全部子线程完成，此时的主线程就是阻塞的。这种程序运行方式叫做异步，CPU 在遇到 IO 阻塞时不会站在那儿傻等，而是被操作系统派往其它线程中看看有什么事可做。所谓的异步，就是 CPU 在当前线程阻塞时可以去其它线程中工作，不管怎么设计，在一个线程内部代码都是顺序执行的，遇到 IO 都得阻塞，所谓的非阻塞，是遇到当前线程阻塞时，CPU 去其它线程工作。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"Spark 单元测试","slug":"Spark-单元测试","date":"2017-05-19T09:52:52.000Z","updated":"2020-05-19T09:57:56.873Z","comments":true,"path":"2017/05/19/Spark-单元测试/","link":"","permalink":"cpeixin.cn/2017/05/19/Spark-%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","excerpt":"","text":"Spark 单元测试相比于传统代码，Spark是比较难调试的。程序运行在集群中，每次修改代码后，都要上传到集群进行测试，代价非常大，所以优先在本地进行单元测试，可以减少小模块的逻辑错误。ScalaTest 测试框架ScalaTest是比JUnit和TestNG更加高阶的测试编写工具，这个Scala应用在JVM上运行，可以测试Scala以及Java代码。ScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。FunSuite的方式较为灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包。而FlatSpec和FunSpec则通过提供诸如it、should、describe等方法，来规定书写测试的一种模式。Maven中引入：1234567&lt;!-- Test Dependency --&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalatest&lt;/groupId&gt; &lt;artifactId&gt;scalatest_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;首先定义好我们要测试的函数，下面的程序中，我们需要对 count(rdd:RDD[String]): RDD[(String,Int)] 进行测试1234567891011121314151617181920212223242526package unit_testimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDobject WordCount extends Serializable&#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setAppName(\"test_rdd\").setMaster(\"local\") val sc = new SparkContext(sparkConf) val make_rdd: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\")) val result_rdd: RDD[(String, Int)] = count(make_rdd) result_rdd.foreach(println) &#125; def count(rdd:RDD[String]): RDD[(String,Int)]=&#123; val wordcount_rdd: RDD[(String, Int)] =rdd.map((word: String) =&gt;(word,1)) .reduceByKey((_: Int) + (_: Int)) wordcount_rdd &#125;&#125;创建测试类：12345678910111213141516171819202122232425262728293031323334353637383940import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.scalatest.&#123;BeforeAndAfter, FlatSpec&#125;import unit_test.WordCount//引入scalatest建立一个单元测试类，混入特质BeforeAndAfter，在before和after中分别初始化sc和停止sc，//初始化SparkContext时只需将Master设置为local(local[N],N表示线程)即可，无需本地配置或搭建集群，class WordCountTests extends FlatSpec with BeforeAndAfter&#123; val master=\"local\" //sparkcontext的运行master var sc:SparkContext=_ \"wordcount_class\" should \"map word ,1\" in&#123; //其中参数为rdd或者dataframe可以通过通过简单的手动构造即可 val seq=Seq(\"Brent\",\"HayLee\",\"Henry\") val rdd=sc.parallelize(seq) val wordCounts=WordCount.count(rdd) wordCounts.map(p=&gt;&#123; p._1 match &#123; case \"Brent\"=&gt; assert(p._2==1)// 断言 case \"HayLee\"=&gt; assert(p._2==1) case \"Henry\"=&gt; assert(p._2==1) case _=&gt; None &#125; &#125;).foreach(_=&gt;()) &#125; //这里before和after中分别进行sparkcontext的初始化和结束，如果是SQLContext也可以在这里面初始化 before&#123; val conf=new SparkConf() .setAppName(\"test\").setMaster(master) sc=new SparkContext(conf) &#125; after&#123; if(sc!=null)&#123; sc.stop() &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Streaming + ELK + HBase","slug":"Spark-Streaming-ELK-HBase","date":"2017-04-22T15:26:15.000Z","updated":"2020-05-06T16:57:17.655Z","comments":true,"path":"2017/04/22/Spark-Streaming-ELK-HBase/","link":"","permalink":"cpeixin.cn/2017/04/22/Spark-Streaming-ELK-HBase/","excerpt":"","text":"在Spark Streaming的业务场景中，大多数的业务需求是针对实时数据做数据统计，网站数据，App数据的监控分析，或者是对抓取的实时数据做ETL，再进行展示，以及实时的推荐系统等。目前公司正在做的项目，针对公司内部运营部门的需求，一方面统计时尚，教育，科技等行业的最新动态，二则是对公司编辑人员提供目前行业热点素材，以热点图的方式展示，点击相应的关键词，并提取出相应的素材。那么在整体的架构上：数据源来自Python的定向爬虫消息队列采用Kafka数据处理采用Spark Streaming，对搜集的素材做ETL以及关键词提取等数据存储采用HBase，Elasticsearch， HBase存储全量数据，做历史数据的备份，方便于后续的数据分析，Elasticsearch搭配HBase做二级索引方案，同时存储部分数据，对戒Kibana做数据可视化。下面对上面的业务场景做一个简单的代码实例，其中没有写入项目的数据逻辑，那自己电脑中的数据进行了模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package writeimport java.security.MessageDigestimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.hadoop.hbase.TableNameimport org.apache.hadoop.hbase.client.&#123;Connection, Put, Table&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.elasticsearch.spark.streaming.EsSparkStreamingimport utils.HBaseUtilimport scala.util.Tryobject streaming_to_hbase_1 &#123; val logger:Logger = Logger.getRootLogger Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"spark streaming window\") .setMaster(\"local[2]\") .set(\"spark.es.nodes\", \"localhost\") .set(\"spark.es.port\", \"9200\") .set(\"es.index.auto.create\", \"true\") val ssc = new StreamingContext(conf, Seconds(5)) val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest，latest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: DStream[(String, String)] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) .map((x: ConsumerRecord[String, String]) =&gt; &#123; val json_data: JSONObject = JSON.parseObject(x.value()) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;) // ES 数据写入部分 val es_dstream: DStream[String] = kafkaStream.map((x: (String, String)) =&gt; &#123; val date_time: String = x._1 val keywordList: String = x._2 val data_es_json: JSONObject = new JSONObject() data_es_json.put(\"date_time\", date_time) data_es_json.put(\"keyword_list\", keywordList) data_es_json.put(\"rowkey\", MD5Encode(date_time)) data_es_json.toJSONString &#125;) EsSparkStreaming.saveJsonToEs(es_dstream,\"weibo_keyword-2017-04-25/default\") //HBase 数据写入部分 kafkaStream.foreachRDD((rdd: RDD[(String, String)]) =&gt; &#123; rdd.foreachPartition((partitionRecords: Iterator[(String, String)]) =&gt; &#123;//循环分区 try &#123; val connection: Connection = HBaseUtil.getHBaseConn //获取HBase连接,分区创建一个连接，分区不跨节点，不需要序列化 partitionRecords.foreach((s: (String, String)) =&gt; &#123; val tableName: TableName = TableName.valueOf(\"t_weibo_keyword\") val table: Table = connection.getTable(tableName)//获取表连接 var date_time: String = s._1 val keywordList: String = s._2 val put = new Put(Bytes.toBytes(MD5Encode(date_time))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"keywordLisr\"), Bytes.toBytes(keywordList)) Try(table.put(put)).getOrElse(table.close())//将数据写入HBase，若出错关闭table// table.close()//分区数据写入HBase后关闭连接 &#125;) &#125; catch &#123; case e: Exception =&gt; logger.info(e) logger.info(\"写入HBase失败\") &#125; &#125;) &#125;) ssc.start() ssc.awaitTermination() &#125; def MD5Encode(input: String): String = &#123; // 指定MD5加密算法 val md5: MessageDigest = MessageDigest.getInstance(\"MD5\") // 对输入数据进行加密,过程是先将字符串中转换成byte数组,然后进行随机哈希 val encoded: Array[Byte] = md5.digest(input.getBytes) // 将加密后的每个字节转化成十六进制，一个字节8位，相当于2个16进制，不足2位的前面补0 encoded.map(\"%02x\".format(_: Byte)).mkString &#125;&#125;es在maven中引入的依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;&#x2F;groupId&gt; &lt;artifactId&gt;elasticsearch-spark-20_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;6.7.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;kibana中的结果展示","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Streaming 进阶","slug":"Spark-Streaming-进阶","date":"2017-04-14T14:22:12.000Z","updated":"2020-05-05T16:34:55.494Z","comments":true,"path":"2017/04/14/Spark-Streaming-进阶/","link":"","permalink":"cpeixin.cn/2017/04/14/Spark-Streaming-%E8%BF%9B%E9%98%B6/","excerpt":"","text":"初始化要初始化Spark Streaming程序，必须创建StreamingContext对象，该对象是所有Spark Streaming功能的主要入口点。1234val conf: SparkConf = new SparkConf() .setAppName(\"your application name\") .setMaster(\"local[2]\")val ssc = new StreamingContext(conf, Seconds(5))maven依赖：123456&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;/dependency&gt;该appName参数是您的应用程序在集群UI上显示的名称。 master是Spark，Mesos，Kubernetes或YARN群集URL或特殊的“ local []”字符串，以本地模式运行。实际工作中，程序部署、运行在集群上，所以并不希望master在程序中进行硬编码，而是在提交程序的spark-submit –master * 命令中来指定。如果只是本地IDEA运行，则可指定 local。在初始化的代码中，我们要设置每个批处理的时间间隔，上面代码中 Seconds(5)，也就是5秒划分一个批次。我们打开源码，可以看到，StreamingContext（）第二个参数还有其他选择，最终都是将时间转换成毫秒。**DstreamDStream由一系列连续的RDD表示，这是Spark对不可变的分布式数据集的抽象。DStream中的每个RDD都包含来自特定间隔的数据。这里，我们来看一下下面的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package streamingimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object streaming_case &#123; // 设置日志级别 Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"Kafka Streaming\") .setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(10)) ssc.checkpoint(\"/Users/cpeixin/IdeaProjects/code_warehouse/spark_streaming/src/main/scala/streaming/\") val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) kafkaStream.foreachRDD((x: RDD[ConsumerRecord[String, String]]) =&gt;println(x)) ssc.start() ssc.awaitTermination() &#125; def change_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;结果：1KafkaRDD[0] at createDirectStream at streaming_case.scala:39上面的代码，在42行foreachRDD的中，我们打印DStream中的RDD，结果中我们看到，第一个批次中，只有一个RDD，这里我想说的是，在上面的这种读取数据代码中，一个 batch Interval中，DStream 只有一个RDD，当一个新的时间窗口（batchInterval)开始时，此时产生一个空的block，此后在这个窗口内接受到的数据都会累加到这个block上，当这个时间窗口结束时，停止累加，这个block对应的数据就是这个时间窗口对应的RDD包含的数据这里我们还可以深入 slideDuration：Duration来看，和后面要讲的窗口函数windiw（）中，RDD的区别。batch interval关于Spark Streaming的批处理时间设置是非常重要的，Spark Streaming在不断接收数据的同时，需要处理数据的时间，所以如果设置过段的批处理时间，会造成数据堆积，即未完成的batch数据越来越多，从而发生阻塞。另外值得注意的是，batchDuration本身也不能设置为小于500ms，这会导致Spark Streaming进行频繁地提交作业，造成额外的开销，减少整个系统的吞吐量；相反如果将batchDuration时间设置得过长，又会影响整个系统的吞吐量。如何设置一个合理的批处理时间，需要根据应用本身、集群资源情况，以及关注和监控Spark Streaming系统的运行情况来调整，重点关注Spark Web UI监控界面中的Total Delay，来进行调整。CheckPoint我们所编写的实时计算程序大多数都是24小时全天候生产环境运行的，因此必须对与应用程序逻辑无关的故障（例如，系统故障，JVM崩溃等）具有弹性。为此，Spark Streaming需要将足够的信息检查点指向容错存储系统，以便可以从故障中恢复。检查点有两种类型的数据。元数据检查点-将定义流计算的信息保存到HDFS等容错存储中。这用于从运行流应用程序的驱动程序的节点的故障中恢复。元数据包括：配置 用于创建流应用程序的配置。DStream操作 -定义流应用程序的DStream操作集。不完整的批次 -作业排队但尚未完成的批次。数据检查点 将生成的RDD保存到可靠的存储中。在一些有状态转换中，这需要跨多个批次合并数据，这是必需的。在此类转换中，生成的RDD依赖于先前批次的RDD，这导致依赖项链的长度随时间不断增加。为了避免恢复时间的这种无限制的增加（与依赖关系链成比例），有状态转换的中间RDD定期 检查点到可靠的存储（例如HDFS）以切断依赖关系链。总而言之，metadata checkpointing主要还是从drvier失败中恢复，而Data Checkpoing用于对有状态的transformation操作进行checkpointingCheckpoint和persist从根本上是不一样的：1、Cache or persist:Cache or persist保存了RDD的血统关系，假如有部分cache的数据丢失可以根据血缘关系重新生成。2、Checkpoint会将RDD数据写到hdfs这种安全的文件系统里面，并且抛弃了RDD血缘关系的记录。即使persist存储到了磁盘里面，在driver停掉之后会被删除，而checkpoint可以被下次启动使用。何时启用检查点必须为具有以下任一要求的应用程序启用检查点：有状态转换的用法 -如果在应用程序中使用updateStateByKey或reduceByKeyAndWindow（带有反函数），则必须提供检查点目录以允许定期进行RDD检查点。从运行应用程序的驱动程序故障中恢复 -元数据检查点用于恢复进度信息。注意，没有前述状态转换的简单流应用程序可以在不启用检查点的情况下运行。在这种情况下，从驱动程序故障中恢复也将是部分的（某些已接收但未处理的数据可能会丢失）。这通常是可以接受的，并且许多都以这种方式运行Spark Streaming应用程序。预计将来会改善对非Hadoop环境的支持。如何配置检查点**可以通过在容错，可靠的文件系统（例如，HDFS，S3等）中设置目录来启用检查点，将检查点信息保存到该目录中。这是通过使用完成的streamingContext.checkpoint(checkpointDirectory)。这将允许您使用前面提到的有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流应用程序以具有以下行为。程序首次启动时，它将创建一个新的StreamingContext，设置所有流，然后调用start（）。失败后重新启动程序时，它将根据检查点目录中的检查点数据重新创建StreamingContext。代码如下：123456789101112131415161718def functionToCreateContext(): StreamingContext = &#123; val ssc = new StreamingContext(...) // new context val lines = ssc.socketTextStream(...) // create DStreams ... ssc.checkpoint(checkpointDirectory) // set checkpoint directory ssc&#125;// Get StreamingContext from checkpoint data or create a new oneval context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)// Do additional setup on context that needs to be done,// irrespective of whether it is being started or restartedcontext. ...// Start the contextcontext.start()context.awaitTermination()请注意，RDD的检查点会导致保存到可靠存储的成本。这可能会导致RDD获得检查点的那些批次的处理时间增加。因此，需要仔细设置检查点的间隔。在小批量（例如1秒）时，每批检查点可能会大大降低操作吞吐量。相反，检查点太不频繁会导致沿袭和任务规模增加，这可能会产生不利影响。对于需要RDD检查点的有状态转换，默认间隔为批处理间隔的倍数，至少应为10秒。可以使用设置 dstream.checkpoint(checkpointInterval)。通常，DStream的5-10个滑动间隔的检查点间隔是一个很好的尝试设置。checkpoint时机在spark Streaming中，JobGenerator用于生成每个batch对应的jobs，它有一个定时器，定时器 的周期即初始化StreamingContext时设置batchDuration。这个周期一到，JobGenerator将调用generateJobs方法来生成并提交jobs，这之后调用doCheckpoint方法来进行checkpoint。doCheckpoint方法中，会判断 当前时间与streaming application start的时间只差是否是 checkpoint duration的倍数，只有在是的情况下才进行checkpoint。具体应用实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119package org.apache.spark.examples.streamingimport java.io.Fileimport java.nio.charset.Charsetimport com.google.common.io.Filesimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.broadcast.Broadcastimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext, Time&#125;import org.apache.spark.util.&#123;IntParam, LongAccumulator&#125;/** * Use this singleton to get or register a Broadcast variable. */object WordBlacklist &#123; @volatile private var instance: Broadcast[Seq[String]] = null def getInstance(sc: SparkContext): Broadcast[Seq[String]] = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; val wordBlacklist = Seq(\"a\", \"b\", \"c\") instance = sc.broadcast(wordBlacklist) &#125; &#125; &#125; instance &#125;&#125;/** * Use this singleton to get or register an Accumulator. */object DroppedWordsCounter &#123; @volatile private var instance: LongAccumulator = null def getInstance(sc: SparkContext): LongAccumulator = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; instance = sc.longAccumulator(\"WordsInBlacklistCounter\") &#125; &#125; &#125; instance &#125;&#125;object RecoverableNetworkWordCount &#123; def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String) : StreamingContext = &#123; // If you do not see this printed, that means the StreamingContext has been loaded // from the new checkpoint println(\"Creating new context\") val outputFile = new File(outputPath) if (outputFile.exists()) outputFile.delete() val sparkConf = new SparkConf().setAppName(\"RecoverableNetworkWordCount\") // Create the context with a 1 second batch size val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(checkpointDirectory) // Create a socket stream on target ip:port and count the // words in input stream of \\n delimited text (eg. generated by 'nc') val lines = ssc.socketTextStream(ip, port) val words = lines.flatMap(_.split(\" \")) val wordCounts = words.map((_, 1)).reduceByKey(_ + _) wordCounts.foreachRDD &#123; (rdd: RDD[(String, Int)], time: Time) =&gt; // Get or register the blacklist Broadcast val blacklist = WordBlacklist.getInstance(rdd.sparkContext) // Get or register the droppedWordsCounter Accumulator val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext) // Use blacklist to drop words and use droppedWordsCounter to count them val counts = rdd.filter &#123; case (word, count) =&gt; if (blacklist.value.contains(word)) &#123; droppedWordsCounter.add(count) false &#125; else &#123; true &#125; &#125;.collect().mkString(\"[\", \", \", \"]\") val output = s\"Counts at time $time $counts\" println(output) println(s\"Dropped $&#123;droppedWordsCounter.value&#125; word(s) totally\") println(s\"Appending to $&#123;outputFile.getAbsolutePath&#125;\") Files.append(output + \"\\n\", outputFile, Charset.defaultCharset()) &#125; ssc &#125; def main(args: Array[String]): Unit = &#123; if (args.length != 4) &#123; System.err.println(s\"Your arguments were $&#123;args.mkString(\"[\", \", \", \"]\")&#125;\") System.err.println( \"\"\" |Usage: RecoverableNetworkWordCount &lt;hostname&gt; &lt;port&gt; &lt;checkpoint-directory&gt; | &lt;output-file&gt;. &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark | Streaming would connect to receive data. &lt;checkpoint-directory&gt; directory to | HDFS-compatible file system which checkpoint data &lt;output-file&gt; file to which the | word counts will be appended | |In local mode, &lt;master&gt; should be 'local[n]' with n &gt; 1 |Both &lt;checkpoint-directory&gt; and &lt;output-file&gt; must be absolute paths \"\"\".stripMargin ) System.exit(1) &#125; val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args val ssc = StreamingContext.getOrCreate(checkpointDirectory, () =&gt; createContext(ip, port, outputPath, checkpointDirectory)) ssc.start() ssc.awaitTermination() &#125;&#125;UpdateStateByKey流处理主要有3种应用场景：无状态操作、window操作、状态操作。updateStateByKey就是典型的状态操作。下面是针对updateStateByKey举的实例，主要功能是针对流数据中的关键词进行统计，并且是根据历史状态持续统计。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package streamingimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject streaming_updatastatebykey &#123; Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"Kafka Streaming\") .setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(2)) ssc.checkpoint(\"/Users/cpeixin/IdeaProjects/code_warehouse/spark_streaming/src/main/scala/streaming/\") val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"latest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) val wordcount_dstream: DStream[(String, Int)] = kafkaStream .map((x: ConsumerRecord[String, String]) =&gt; &#123; change_data(x.value()) &#125;) .flatMap((_: String).split(\",\")) .map((x: String) =&gt; (x, 1)) val sum_dstream: DStream[(String, Int)] = wordcount_dstream.updateStateByKey((seq: Seq[Int], state: Option[Int]) =&gt; &#123; var sum: Int = state.getOrElse(0)+seq.sum Option(sum) &#125;) sum_dstream.foreachRDD((keywordFormat_rdd: RDD[(String, Int)]) =&gt; &#123; val sort_rdd: Array[(Int, String)] = keywordFormat_rdd.map((x: (String, Int)) =&gt; &#123;(x._2, x._1)&#125;).sortByKey().top(10) sort_rdd.foreach(println) println(\"====================\") &#125;) ssc.start() ssc.awaitTermination() &#125; def change_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;打印统计信息：12345678910111213141516171819202122&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;(8,剪头)(5,黑人抬棺队长称将环游世界)(5,黑人)(5,队长)(5,环游世界)(4,野餐)(4,这野餐也太实在了吧)(3,郑钧低空飞行)(3,郑钧)(3,森林)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;(8,剪头)(7,森林)(7,挪威)(7,伍佰 挪威的森林)(7,伍佰)(6,肤色)(6,状态)(6,今年夏天的肤色状态)(6,今年夏天)(5,黑人抬棺队长称将环游世界)注意：类似updateStateByKey和mapWithState等有状态转换算子，程序中必须要指定checkpoint检查点。窗口函数Spark Streaming还提供了_窗口计算_，可让您在数据的滑动窗口上应用转换。下图说明了此滑动窗口。如该图所示，每当窗口_滑动_在源DSTREAM，落入窗口内的源RDDS被组合及操作以产生RDDS的窗DSTREAM。在这种特定情况下，该操作将应用于数据的最后3个时间单位，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数。窗口长度 - _窗口_的持续时间。滑动间隔 -进行窗口操作的间隔。这两个参数必须是源DStream的批处理间隔的倍数下面给出实例代码，描述的场景是每10秒统计一次过去30秒期间，关键词出现次数的top 51234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package streamingimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.&#123;Duration, Seconds, StreamingContext&#125;object streaming_window &#123; Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"spark streaming window\") .setMaster(\"local[2]\") val sc = new StreamingContext(conf, Seconds(5)) val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"latest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](sc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) val wordcount_dstream: DStream[(String, Int)] = kafkaStream .map((x: ConsumerRecord[String, String]) =&gt; &#123; get_data(x.value()) &#125;) .flatMap((_: String).split(\",\")) .map((x: String) =&gt; (x, 1)) val window_dstream: DStream[(String, Int)] = wordcount_dstream.reduceByKeyAndWindow((x: Int,y: Int)=&gt;x+y,Seconds(30), Seconds(10)) val result: DStream[(String, Int)] = window_dstream.transform((rdd: RDD[(String, Int)]) =&gt;&#123; val top3: Array[(String, Int)] = rdd.map((x: (String, Int)) =&gt;(x._2,x._1)).sortByKey(ascending = false).map((x: (Int, String)) =&gt;(x._2,x._1)).take(5) sc.sparkContext.makeRDD(top3) &#125;) result.print() sc.start() sc.awaitTermination() &#125; def get_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;结果：1234567891011121314151617181920212223242526272829303132333435-------------------------------------------Time: 1588694330000 ms-------------------------------------------(台版,6)(黑人,6)(台版101模仿黑人抬棺,6)(训练,2)(听起来很厉害的专业术语,2)-------------------------------------------Time: 1588694340000 ms-------------------------------------------(野餐,10)(野餐还没拍好照就被牛吃了,10)(台版,6)(黑人,6)(台版101模仿黑人抬棺,6)-------------------------------------------Time: 1588694350000 ms-------------------------------------------(姐姐,10)(野餐,10)(野餐还没拍好照就被牛吃了,10)(乘风破浪的姐姐们,10)(台版,6)-------------------------------------------Time: 1588694360000 ms-------------------------------------------(姐姐,10)(野餐,10)(野餐还没拍好照就被牛吃了,10)(乘风破浪的姐姐们,10)(叶冲太难了,9)transform操作，应用在DStream上时，可以用于执行任意的RDD到RDD的转换操作；它可以用于实现，DStream API中所没有提供的操作；比如说，DStream API中，并没有提供将一个DStream中的每个batch，与一个特定的RDD进行join的操作。但是我们自己就可以使用transform操作来实现该功能。这里看 reduceByKeyAndWindow（）函数的第二个参数和第三个参数，分别代表的意义就是，窗口的长度和窗口滑动间隔。这里还需要知道一点，Dstream中的RDD也可以调用persist()方法保存在内存当中，但是基于window和state的操作，reduceByWindow,reduceByKeyAndWindow,updateStateByKey等它们已经在源码中默认persist了**","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Streaming 讲解","slug":"Spark-Streaming-讲解","date":"2017-04-10T14:22:12.000Z","updated":"2020-05-04T14:25:36.548Z","comments":true,"path":"2017/04/10/Spark-Streaming-讲解/","link":"","permalink":"cpeixin.cn/2017/04/10/Spark-Streaming-%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"简述Spark Streaming是核心Spark API的扩展，近实时计算框架。特点可伸缩，高吞吐量，容错流处理。而我们之前讲的Spark SQL是负责处理离线数据。既然是计算框架，那么实战的过程中还是三个步骤，读取数据源、计算数据、数据存储：数据源：可以从Kafka, Flume, Kinesis, or TCP sockets等数据源读入数据计算：同样可以使用同样的map，reduce，join等算子进行数据计算，还有Spark Streaming 特有的window窗口函数。数据存储：可以将处理后的数据推送到文件系统HDFS，数据库的话比较常用的是MySQL，Mongo DB和HBase等，对于实时数据的展示和监控方面，我们比较常用的是将数据写入Elasticsearch中，并且使用Kibana或者Grafana来进行展示。Spark Streaming 提供一个对于流数据的抽象 DStream。DStream 可以由来自 Apache Kafka、Flume 或者 HDFS 的流数据生成，也可以由别的 DStream 经过各种转换操作得来，底层 DStream 也是由很多个序列化的 RDD 构成，按时间片（比如一秒）切分成的每个数据单位都是一个 RDD。然后，Spark 核心引擎将对 DStream 的 Transformation 操作变为针对 Spark 中对 RDD 的 Transformation 操作，将 RDD 经过操作变成中间结果保存在内存中。之前的 DataFrame 和 DataSet 也是同样基于 RDD，所以说 RDD 是 Spark 最基本的数据抽象。就像 Java 里的基本数据类型（Primitive Type）一样，所有的数据都可以用基本数据类型描述。也正是因为这样，无论是 DataFrame，还是 DStream，都具有 RDD 的不可变性、分区性和容错性等特质。所以，Spark 是一个高度统一的平台，所有的高级 API 都有相同的性质，它们之间可以很容易地相互转化。Spark 的野心就是用这一套工具统一所有数据处理的场景。由于 Spark Streaming 将底层的细节封装起来了，所以对于开发者来说，只需要操作 DStream 就行。接下来，让我们一起学习 DStream 的结构以及它支持的转换操作对比一般在讲述Spark Streaming的时候，其他博主都会列一个表格，将Storm，Spark Streaming，Flink三个实时计算框架进行对比，这里我就不进行三者之间的比较了，直接根据我这几年使用的经验来分享一下直接的结果，Storm是一个早期的流式计算框架，可以做到毫秒级别的计算，但是编程语言使用的是Java，代码量很多，编程复杂度会高一些，并且目前很少有公司使用Storm，我实习期后，在第一家公司的第一个任务就是改写现有的Storm任务，迁移到Spark Streaming。Spark Streaming的流式计算，准确的来讲，可以说是近实时或者微批计算，所以在一般情况下，对于时间要求不是太严格的情况下，Spark Streaming可以满足大部分的场景了。支持Java，Scala， Python三种语言。但是Spark Streaming有一个缺点，就是某些需求中，我们的计算结果应该是基于event time数据产生时间，但是在Spark Streaming中，有时会因为延迟的原因，计算结果会基于process time 数据到达的时间。针对这个问题，我在后面的Structured Streaming也会讲到。Flink可以说是流式计算的后起之秀，并且就是为了流计算而诞生，它采用了基于操作符（Operator）的连续流模型，可以做到微秒级别的延迟。同样支持Java，Scala，Python三种语言，在我还认认真真写Spark代码的时候，每天都能看到铺天盖地的Flink新闻，Flink 用流处理去模拟批处理的思想，比 Spark 用批处理去模拟流处理的思想扩展性更好，所以我相信将来 Flink 会发展的越来越好，生态和社区各方面追上 Spark。比如，阿里巴巴就基于 Flink 构建了公司范围内全平台使用的数据处理平台 Blink，美团、饿了么等公司也都接受 Flink 作为数据处理解决方案。实例这里用一个实例来展示一下实际工作中，Spark Streaming的大概流程：数据的采集或者网站数据收集—&gt;Kafka—&gt;Spark Streaming—&gt;show data or save data下面这端python代码是数据采集或者收集，打到Kafka的阶段。我本想写个爬虫来做实时数据的，但是我想起来我的Elasticsearch集群中正在实时的爬取某博数据，所以就直接间隔调用ES，来打入Kafka中。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-import timefrom kafka import KafkaProducerimport jsonfrom elasticsearch import Elasticsearchproducer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8'),bootstrap_servers=['localhost:9092'])def getEsArticle(): \"\"\"动态链接\"\"\" es = Elasticsearch( ['xxx.xxx.xxx.xxx:9205'], # 在做任何操作之前，先进行嗅探 sniff_on_start=True, # 节点没有响应时，进行刷新，重新连接 sniff_on_connection_fail=True, # 每 60 秒刷新一次 sniffer_timeout=10 ) # 模糊查询 query = &#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;, \"sort\": &#123; \"datetime\": &#123; \"order\": \"desc\" # 降序 &#125; &#125;, \"size\": 10 &#125; esResult = es.search(index=\"article_warehouse*\", body=query) for hit in esResult['hits']['hits']: data = &#123;'datetime': hit[\"_source\"][\"datetime\"], 'keywordList': hit[\"_source\"][\"keywordList\"]&#125; print(data) producer.send('weibo_keyword', data)def main(): while True: getEsArticle() time.sleep(10)if __name__ == '__main__': main()打入数据的格式：12345678910111213&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:42&#39;, &#39;keywordList&#39;: &#39;特朗普,得志,领头羊,治国,爱心,美国,本领&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:56&#39;, &#39;keywordList&#39;: &#39;车型,整体,长安,手动挡,自动,便利性,新手,平顺,发动机,版本,市区,积炭,编想,小编,驻车,速手,次顶,小伙伴,车师,省心&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:53&#39;, &#39;keywordList&#39;: &#39;同桌,一个男孩,高三,人才,感觉&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:53&#39;, &#39;keywordList&#39;: &#39;玩家,段位,落地,钢枪,游戏,新人,机器人,成盒,开局,军事基地,低端,小学生,精英,高水平,惩罚,谢谢,字数,高端,优质,战场&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:52&#39;, &#39;keywordList&#39;: &#39;公话,情侣,青春校园,放学,校园,同学,印象,家人&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:52&#39;, &#39;keywordList&#39;: &#39;阿萨德,表哥,经济命脉,皇亲国戚,叙利亚,动手,国家&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:51&#39;, &#39;keywordList&#39;: &#39;集训,高带,体工队,大牌,劲旅,球员,南亚,浪费,印度,时代,国家&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:51&#39;, &#39;keywordList&#39;: &#39;图片,老师,全校,同桌,爱慕,姨妈,手册,男孩,青春,窗户,主权,我会,毕业&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:51&#39;, &#39;keywordList&#39;: &#39;段位,玩家,分会,分数,战场&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:50&#39;, &#39;keywordList&#39;: &#39;杯子,盒子,自习室,专四,保送生,作文,单词,礼物,伤心,考试,耳朵,距离,姑娘&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:50&#39;, &#39;keywordList&#39;: &#39;边路传,有球,曼联,全员,出力&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:30:07&#39;, &#39;keywordList&#39;: &#39;T恤,女生,身材,时尚,女装,印花,袖口,条纹,小编,板型,潘领,蓬蓬裙,踝款,百褶裙,碎花,拜拜,黑白相间,牛仔,牛仔裤,深色&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:30:06&#39;, &#39;keywordList&#39;: &#39;周梅森,经济&#39;&#125;下面的spark streaming代码比较简单，只是读取数据，从json格式中解析出来，随后打印，这里我没有直接写入的数据库或者es中，主要是给出一个spark streaming的程序框架。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package streamingimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;HashPartitioner, SparkConf&#125;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import org.apache.log4j.&#123;Level, Logger&#125;object streaming_weibo &#123; // 设置日志级别 Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"Kafka Streaming\") .setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(10)) ssc.checkpoint(\"/Users/cpeixin/IdeaProjects/code_warehouse/spark_streaming/src/main/scala/streaming/\") val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) kafkaStream.map((x: ConsumerRecord[String, String]) =&gt; &#123; get_data(x.value()) &#125;).foreachRDD((x: RDD[String]) =&gt; x.foreach(println)) ssc.start() ssc.awaitTermination() &#125; def get_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;12345678910111213141516手机,安卓,像素,摄像头,国产品牌,品牌,华为,小米,旗舰,英寸,屏幕,电池容量,对焦,逆光,音质,系统,后置,机身,光线,光学电场线,本子,样子,铃响,心形,卷子,猫儿,异性,男朋友,板凳,电荷,我会,桌子,小学范畴,心情,能力,东西车型,整体,长安,手动挡,自动,便利性,新手,平顺,发动机,版本,市区,积炭,编想,小编,驻车,速手,次顶,小伙伴,车师,省心高尔夫,大众,领速,高嘉,技术实力,试车,性价比,舒适性,内饰,亮点,亲戚,重庆,消费者,动力,空间,人士,原因,产品,情况同桌,一个男孩,高三,人才,感觉方向机,漆面,大叔,差点,图片公话,情侣,青春校园,放学,校园,同学,印象,家人棒棒,东忘西,太久,方言,老汉,老头,老师,时间图片,老师,全校,同桌,爱慕,姨妈,手册,男孩,青春,窗户,主权,我会,毕业刹车,啊啊啊,篮球场,转圈,脸红,张开,手臂,汉子,口气,朋友段位,玩家,分会,分数,战场法学专家谈拒绝加班被判赔1.8万,被判赔,法学,专家杯子,盒子,自习室,专四,保送生,作文,单词,礼物,伤心,考试,耳朵,距离,姑娘思域,品味周梅森,经济","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 性能优化方向","slug":"Spark-性能优化方向","date":"2017-04-05T15:48:43.000Z","updated":"2020-04-29T13:25:12.537Z","comments":true,"path":"2017/04/05/Spark-性能优化方向/","link":"","permalink":"cpeixin.cn/2017/04/05/Spark-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91/","excerpt":"","text":"目前大数据业务中，Spark开发任务是很多的，一周一个小任务，两三个月一个大项目，根据不同公司数据量的大小，那么Spark任务需要优化的程度也就有所不同，一般公司中的大数据集群计算资源都是有限的，所以如果某个Spark任务所占用的资源过大，则会影响Yarn队列中，其他任务的运行，尤其是在生产环境下，这种情况还是要避免发生的。在大数据使用、开发过程的性能优化一般可以从以下角度着手进行。SQL 语句优化使用关系数据库的时候，SQL 优化是数据库优化的重要手段，因为实现同样功能但是不同的 SQL 写法可能带来的性能差距是数量级的。我们知道在大数据分析时，由于数据量规模巨大，所以 SQL 语句写法引起的性能差距就更加巨大。典型的就是 Hive 的 MapJoin 语法，如果 join 的一张表比较小，比如只有几 MB，那么就可以用 MapJoin 进行连接，Hive 会将这张小表当作 Cache 数据全部加载到所有的 Map 任务中，在 Map 阶段完成 join 操作，无需 shuffle。数据倾斜处理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。淘宝的产品经理曾经讲过一个案例，他想把用户日志和用户表通过用户 ID 进行 join，但是日志表有几亿条 记录的用户 ID 是 null，Hive 把 null 当作一个字段值 shuffle 到同一个 Reduce，结果这个 Reduce 跑了两 天也没跑完，SQL 当然也执行不完。像这种情况的数据倾斜，因为 null 字段没有意义，所以可以在 where 条件里加一个 userID != null 过滤掉就可以了。定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子： distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。我们可以通过Spark Web UI，查看各个task的运行时间异常，或者从yarn日志中查看task的运行信息，是否存在某个task内存溢出的情况。查看导致数据倾斜的key的数据分布情况在确定了数据倾斜的情况下，下一步就是查看锁定位置的key分布，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。数据倾斜解决方案使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。过滤少数导致倾斜的key方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。提高shuffle操作的并行度方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。两阶段聚合（局部聚合+全局聚合）方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。采样倾斜key并分拆join操作方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“map join”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。使用随机前缀和扩容RDD进行join方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。方案实现思路： 首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。代码优化了解 Spark 的工作原理，了解要处理的数据的特点，了解要计算的目标，设计合理的代码处理逻辑，RDD lineage设计、算子的合理使用，shuffle调优等，使用良好的编程方法开发大数据应用，是大数据应用性能优化的重要手段，也是大数据开发工程师的重要职责。避免创建重复的RDD我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。对多次使用的RDD进行持久化Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。尽量使用可以在map端进行shuffle类算子如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以在map端进行聚合的算子这里拿reduceByKey和groupByKey进行举例reduceByKey(func, numPartitions=None)reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。groupByKey(numPartitions=None)groupByKey也是对每个key进行操作，但只生成一个sequence。如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。最后reduceByKey和groupByKey都是要经过shuffle的，groupByKey在方法shuffle之间不会合并原样进行shuffle，reduceByKey进行shuffle之前会先做合并,这样就减少了shuffle的io传送，所以效率高一点。其他算子的优化使用mapPartitions替代普通map使用reduceByKey/aggregateByKey替代groupByKey使用foreachPartitions替代foreach使用filter之后进行coalesce操作使用repartitionAndSortWithinPartitions替代repartition与sort类操作巧用广播变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： * 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 * 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 * 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）123456val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .getOrCreate() ### 配置参数优化根据公司数据特点，为部署的大数据产品以及运行的作业选择合适的配置参数，是公司大数据平台性能优化最主要的手段，也是大数据工程师或者大数据运维工程师的主要职责。比如 Yarn 的每个 Container 包含的 CPU 个数和内存数目、HDFS 数据块的大小和复制数等，每个大数据产品都有很多配置参数，这些参数会对大数据运行时的性能产生重要影响。num-executors参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。executor-memory参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。参数调优建议：每个Executor进程的内存设置4G8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/31/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。executor-cores参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。参数调优建议：Executor的CPU core数量设置为24个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/31/2左右比较合适，也是避免影响其他同学的作业运行。driver-memory参数说明：该参数用于设置Driver进程的内存。参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。spark.default.parallelism参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。参数调优建议：Spark作业的默认task数量为5001000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的23倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。spark.storage.memoryFraction参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。spark.shuffle.memoryFraction参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。123456789./bin/spark-submit \\ --master yarn-cluster \\ --num-executors 100 \\ --executor-memory 6G \\ --executor-cores 4 \\ --driver-memory 1G \\ --conf spark.default.parallelism=1000 \\ --conf spark.storage.memoryFraction=0.5 \\ --conf spark.shuffle.memoryFraction=0.3 \\在性能优化过程中，**CPU、内存、网络、磁盘**这四种主要计算资源的使用和 Spark 的计算阶段密切相关。Spark 性能优化可以分解为下面几步。性能测试，观察 Spark 性能特性和资源（CPU、Memory、Disk、Net）利用情况。分析、寻找资源瓶颈。分析系统架构、代码，发现资源利用关键所在，思考优化策略。代码、架构、基础设施调优，优化、平衡资源利用。性能测试，观察系统性能特性，是否达到优化目的，以及寻找下一个瓶颈点。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark SQL 源码解析","slug":"Spark-SQL-源码解析","date":"2017-04-01T15:48:43.000Z","updated":"2020-04-29T12:09:17.572Z","comments":true,"path":"2017/04/01/Spark-SQL-源码解析/","link":"","permalink":"cpeixin.cn/2017/04/01/Spark-SQL-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"Spark SQL是让Spark应用程序拥有高效性、高可容错性和丰富生态的“幕后英雄”。在学习Spark SQL 底层解析原理前，先看下面三张架构图。架构图中运行的流程，以及过程中涉及到的组件。两张图片互为补充的去看。运行流程首先梳理运行流程，在spark中执行SQL语句或者DataFrame的算子操作，在集群主要经过两大过程LogicalPlan（逻辑计划），理解为树型数据结构，逻辑算子树PhysicalPlan（物理计划），理解为物理算子树其次，两个主要过程中有包含几个子过程：逻辑计划（LogicalPlan）:**未解析的逻辑算子树（Unresolved LogicalPlan），仅仅是数据结构，不包含任何数据信息解析后逻辑算子树（Analyzed LogicalPlan），节点中绑定各种信息，可以看到，这个过程中有catalog参与优化后逻辑算子树（Optimized LogicalPlan），应用各种优化规则对一些低效的逻辑计划进行转换物理计划（PhysicalPlan）：**根据逻辑算子树，生成物理算子树的列表（Iterator[PhysicalPlan]）,同样逻辑算子树可能对应多个物理算子树。（上图可以看到，PhysicalPlan阶段的多个绿色五边形）从列表中按照一定策略选取最优的物理算子树（SparkPlan）对选取的物理算子树做提交前的准备工作，例如确保分区操作正确，物理算子节点重用，执行代码生成等。经过准备后的物理算子（Prepared SparkPlan）最终物理算子树生成的RDD执行action操作，即可提交执行。流程说明逻辑执行计划主要是一系列 _抽象的转换过程_，它并不涉及executors或者drivers，它仅仅是将用户的代码表达式转换成一个优化的版本。用户的代码首先会被转换成 未解决的逻辑计划(unresolved logical plan) ，之所以将之称作 未解决的 , 是因为它并一定是正确的，它所引用到的表名或者列名可能存在，也可能不存在。Spark之后会使用 calalog_，一个含有所有table和DataFrame的元数据仓库，在 _分析器(analyser) 中来解析校对所引用的表名或者列名。假如 unresolved logical plan 通过了验证，这时的计划我们称之为 已解决的逻辑计划(resolved logical plan) 。它会传递到 Catalyst Optimizer 优化器，然后被经过一些列优化生成最优逻辑计划(Optimized logical plan)AST -&gt; Unresolved Logical Plan实质：SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan首先在架构图中，最前端我们可以是在Spark中指定SQL语句，或者是Dataframe的算子操作，那么接下来，就直接进入了逻辑计划生成阶段：spark.sql() 这种方式的话会涉及到sql的解析，解析之后就会生成逻辑计划如果是直接在DataFrame的api上直接操作的话，使用的api将直接生成逻辑计划这里，我们从spark.sql()为入口，在源码里面一步一步的去探索底层逻辑是如何实现的。1spark.sql(\"select * from t_user\")sql入口⬇️123456789/** * Executes a SQL query using Spark, returning the result as a `DataFrame`. * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'. * * @since 2.0.0 */ def sql(sqlText: String): DataFrame = &#123; Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText)) &#125;⬇️12345678trait ParserInterface &#123; /** * Parse a string to a [[LogicalPlan]]. */ @throws[ParseException](\"Text cannot be parsed to a LogicalPlan\") def parsePlan(sqlText: String): LogicalPlan.......&#125;这里看到 parsePlan 方法返回为LogicalPlan，就是在这里，将一个sql语句通过ast解析成unresolved logicalPlan⬇️123456789/** Creates LogicalPlan for a given SQL string. */override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) &#123; parser =&gt; astBuilder.visitSingleStatement(parser.singleStatement()) match &#123; case plan: LogicalPlan =&gt; plan case _ =&gt; val position = Origin(None, None) throw new ParseException(Option(sqlText), \"Unsupported SQL statement\", position, position) &#125;&#125;ANTLR4是JAVA写的语言识别工具，它用来声明语言的语法。它的语法识别分为两个阶段词法分析阶段：理解为把符号分成组或者符号类解析阶段：根据词法，解析成一棵语法分析树astBuilder类主要功能是把Antlr4的解析树 转换为catalyst表达式astBuilder是SparkSqlAstBuilder的实例，在将Antlr中的匹配树转换成unresolved logical plan中，它起着桥梁作用。singleStatement()方法构建整棵语法树，然后通过 astBuilder.visitSingleStatement使用visitor模式，开始匹配SqlBase.g4中sql的入口匹配规则，（SqlBase.g4是sparksql的语法文件）递归的遍历statement，以及其后的各个节点。在匹配过程中，碰到叶子节点，就将构造Logical Plan中对应的TreeNode。⬇️12345678910111213141516171819202122232425262728293031323334353637383940protected def parse[T](command: String)(toResult: SqlBaseParser =&gt; T): T = &#123; logDebug(s\"Parsing command: $command\") val lexer = new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command))) lexer.removeErrorListeners() lexer.addErrorListener(ParseErrorListener) val tokenStream = new CommonTokenStream(lexer) val parser = new SqlBaseParser(tokenStream) parser.addParseListener(PostProcessor) parser.removeErrorListeners() parser.addErrorListener(ParseErrorListener) try &#123; try &#123; // first, try parsing with potentially faster SLL mode parser.getInterpreter.setPredictionMode(PredictionMode.SLL) toResult(parser) &#125; catch &#123; case e: ParseCancellationException =&gt; // if we fail, parse with LL mode tokenStream.seek(0) // rewind input stream parser.reset() // Try Again. parser.getInterpreter.setPredictionMode(PredictionMode.LL) toResult(parser) &#125; &#125; catch &#123; case e: ParseException if e.command.isDefined =&gt; throw e case e: ParseException =&gt; throw e.withCommand(command) case e: AnalysisException =&gt; val position = Origin(e.line, e.startPosition) throw new ParseException(Option(command), e.message, position, position) &#125; &#125;先后实例化了分词解析SqlBaseLexer和语法解析SqlBaseParser类，最后将antlr的语法解析器，然后一次尝试用不同的模式去进行解析。最终将antlr的语法解析器parser，传入parsePlan。UnresolvedLogicalPlan在经历上面过程之后，此时的未解析的逻辑树，只是一个简单逻辑，还没有绑定数据，没有任何数据信息。那么接下来发生的事情就是如何在UnresolvedLogicalPlan基础上绑定每个节点信息？Unresolved Logical Plan -&gt; Logical Plan实质：Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；流程图中，我们看到Analysis主要职责就是将通过Sql Parser未能Resolved的Logical Plan给Resolved掉。我们从下面开始：123456789/** * Executes a SQL query using Spark, returning the result as a `DataFrame`. * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'. * * @since 2.0.0 */ def sql(sqlText: String): DataFrame = &#123; Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText)) &#125;在上一阶段我们分析到sessionState.sqlParser.parsePlan(sqlText)方式是逻辑计划的开始，解析生成Unresolved Logical Plan，这时Unresolved Logical Plan作为参数，传入到ofRows()方法中。1234567891011121314151617package org.apache.spark.sqlprivate[sql] object Dataset &#123; def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = &#123; val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]]) // Eagerly bind the encoder so we verify that the encoder matches the underlying // schema. The user will get an error if this is not the case. dataset.deserializer dataset &#125; def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = &#123; val qe = sparkSession.sessionState.executePlan(logicalPlan) qe.assertAnalyzed() new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema)) &#125;&#125;这里首先创建了queryExecution类对象，QueryExecution中定义了sql执行过程中的关键步骤，是sql执行的关键类，返回一个dataframe类型的对象。QueryExecution类中的成员都是lazy的，被调用时才会执行。只有等到程序中出现action算子时，才会调用 queryExecution类中的executedPlan成员，1def executePlan(plan: LogicalPlan): QueryExecution = createQueryExecution(plan)创建QueryExecution123456789101112131415161718class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) &#123; // TODO: Move the planner an optimizer into here from SessionState. protected def planner = sparkSession.sessionState.planner def assertAnalyzed(): Unit = analyzed def assertSupported(): Unit = &#123; if (sparkSession.sessionState.conf.isUnsupportedOperationCheckEnabled) &#123; UnsupportedOperationChecker.checkForBatch(analyzed) &#125; &#125; lazy val analyzed: LogicalPlan = &#123; SparkSession.setActiveSession(sparkSession) sparkSession.sessionState.analyzer.executeAndCheck(logical) &#125; .......QueryExecution是Spark用来执行关系型查询的主要工作流。它是被设计用来为开发人员提供更方便的对查询执行中间阶段的访问。QueryExecution中最重要的是成员变量，它的成员变量几乎都是lazy variable，它的方法大部分是为了提供给这些lazy variable去调用的，调用analyzer解析器,对Unresolved LogicalPlan进行元数据绑定生成的Resolved LogicalPlan123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package org.apache.spark.sql.catalyst.analysis/** * Provides a logical query plan analyzer, which translates [[UnresolvedAttribute]]s and * [[UnresolvedRelation]]s into fully typed objects using information in a [[SessionCatalog]]. */class Analyzer( catalog: SessionCatalog, conf: SQLConf, maxIterations: Int) extends RuleExecutor[LogicalPlan] with CheckAnalysis &#123; def this(catalog: SessionCatalog, conf: SQLConf) = &#123; this(catalog, conf, conf.optimizerMaxIterations) &#125; def executeAndCheck(plan: LogicalPlan): LogicalPlan = &#123; val analyzed = execute(plan) try &#123; checkAnalysis(analyzed) EliminateBarriers(analyzed) &#125; catch &#123; case e: AnalysisException =&gt; val ae = new AnalysisException(e.message, e.line, e.startPosition, Option(analyzed)) ae.setStackTrace(e.getStackTrace) throw ae &#125; &#125; override def execute(plan: LogicalPlan): LogicalPlan = &#123; AnalysisContext.reset() try &#123; executeSameContext(plan) &#125; finally &#123; AnalysisContext.reset() &#125; &#125; private def executeSameContext(plan: LogicalPlan): LogicalPlan = super.execute(plan) def resolver: Resolver = conf.resolver protected val fixedPoint = FixedPoint(maxIterations) /** * Override to provide additional rules for the \"Resolution\" batch. */ val extendedResolutionRules: Seq[Rule[LogicalPlan]] = Nil /** * Override to provide rules to do post-hoc resolution. Note that these rules will be executed * in an individual batch. This batch is to run right after the normal resolution batch and * execute its rules in one pass. */ val postHocResolutionRules: Seq[Rule[LogicalPlan]] = Nil lazy val batches: Seq[Batch] = Seq( Batch(\"Hints\", fixedPoint, new ResolveHints.ResolveBroadcastHints(conf), ResolveHints.RemoveAllHints), Batch(\"Simple Sanity Check\", Once, LookupFunctions), Batch(\"Substitution\", fixedPoint, CTESubstitution, WindowsSubstitution, EliminateUnions, new SubstituteUnresolvedOrdinals(conf)), Batch(\"Resolution\", fixedPoint, ResolveTableValuedFunctions :: ResolveRelations :: ResolveReferences :: ResolveCreateNamedStruct :: ResolveDeserializer :: ResolveNewInstance :: ResolveUpCast :: ResolveGroupingAnalytics :: ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ResolveSubqueryColumnAliases :: ResolveWindowOrder :: ResolveWindowFrame :: ResolveNaturalAndUsingJoin :: ExtractWindowExpressions :: GlobalAggregates :: ResolveAggregateFunctions :: TimeWindowing :: ResolveInlineTables(conf) :: ResolveTimeZone(conf) :: ResolvedUuidExpressions :: TypeCoercion.typeCoercionRules(conf) ++ extendedResolutionRules : _*), Batch(\"Post-Hoc Resolution\", Once, postHocResolutionRules: _*), Batch(\"View\", Once, AliasViewChild(conf)), Batch(\"Nondeterministic\", Once, PullOutNondeterministic), Batch(\"UDF\", Once, HandleNullInputsForUDF), Batch(\"FixNullability\", Once, FixNullability), Batch(\"Subquery\", Once, UpdateOuterReferences), Batch(\"Cleanup\", fixedPoint, CleanupAliases) )其中val analyzed: LogicalPlan= analyzer.execute(logical)，logical就是sqlparser解析出来的unresolved logical plan，analyzed就是analyzed logical plan。那么exectue究竟是这么样的过程呢？super.execute(plan)方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124package org.apache.spark.sql.catalyst.rulesobject RuleExecutor &#123; protected val queryExecutionMeter = QueryExecutionMetering() /** Dump statistics about time spent running specific rules. */ def dumpTimeSpent(): String = &#123; queryExecutionMeter.dumpTimeSpent() &#125; /** Resets statistics about time spent running specific rules */ def resetMetrics(): Unit = &#123; queryExecutionMeter.resetMetrics() &#125;&#125;abstract class RuleExecutor[TreeType &lt;: TreeNode[_]] extends Logging &#123; /** * An execution strategy for rules that indicates the maximum number of executions. If the * execution reaches fix point (i.e. converge) before maxIterations, it will stop. */ abstract class Strategy &#123; def maxIterations: Int &#125; /** A strategy that only runs once. */ case object Once extends Strategy &#123; val maxIterations = 1 &#125; /** A strategy that runs until fix point or maxIterations times, whichever comes first. */ case class FixedPoint(maxIterations: Int) extends Strategy /** A batch of rules. */ protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*) /** Defines a sequence of rule batches, to be overridden by the implementation. */ protected def batches: Seq[Batch] /** * Defines a check function that checks for structural integrity of the plan after the execution * of each rule. For example, we can check whether a plan is still resolved after each rule in * `Optimizer`, so we can catch rules that return invalid plans. The check function returns * `false` if the given plan doesn't pass the structural integrity check. */ protected def isPlanIntegral(plan: TreeType): Boolean = true /** * Executes the batches of rules defined by the subclass. The batches are executed serially * using the defined execution strategy. Within each batch, rules are also executed serially. */ def execute(plan: TreeType): TreeType = &#123; var curPlan = plan val queryExecutionMetrics = RuleExecutor.queryExecutionMeter batches.foreach &#123; batch =&gt; val batchStartPlan = curPlan var iteration = 1 var lastPlan = curPlan var continue = true // Run until fix point (or the max number of iterations as specified in the strategy. while (continue) &#123; curPlan = batch.rules.foldLeft(curPlan) &#123; case (plan, rule) =&gt; val startTime = System.nanoTime() val result = rule(plan) val runTime = System.nanoTime() - startTime if (!result.fastEquals(plan)) &#123; queryExecutionMetrics.incNumEffectiveExecution(rule.ruleName) queryExecutionMetrics.incTimeEffectiveExecutionBy(rule.ruleName, runTime) logTrace( s\"\"\" |=== Applying Rule $&#123;rule.ruleName&#125; === |$&#123;sideBySide(plan.treeString, result.treeString).mkString(\"\\n\")&#125; \"\"\".stripMargin) &#125; queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime) queryExecutionMetrics.incNumExecution(rule.ruleName) // Run the structural integrity checker against the plan after each rule. if (!isPlanIntegral(result)) &#123; val message = s\"After applying rule $&#123;rule.ruleName&#125; in batch $&#123;batch.name&#125;, \" + \"the structural integrity of the plan is broken.\" throw new TreeNodeException(result, message, null) &#125; result &#125; iteration += 1 if (iteration &gt; batch.strategy.maxIterations) &#123; // Only log if this is a rule that is supposed to run more than once. if (iteration != 2) &#123; val message = s\"Max iterations ($&#123;iteration - 1&#125;) reached for batch $&#123;batch.name&#125;\" if (Utils.isTesting) &#123; throw new TreeNodeException(curPlan, message, null) &#125; else &#123; logWarning(message) &#125; &#125; continue = false &#125; if (curPlan.fastEquals(lastPlan)) &#123; logTrace( s\"Fixed point reached for batch $&#123;batch.name&#125; after $&#123;iteration - 1&#125; iterations.\") continue = false &#125; lastPlan = curPlan &#125; if (!batchStartPlan.fastEquals(curPlan)) &#123; logDebug( s\"\"\" |=== Result of Batch $&#123;batch.name&#125; === |$&#123;sideBySide(batchStartPlan.treeString, curPlan.treeString).mkString(\"\\n\")&#125; \"\"\".stripMargin) &#125; else &#123; logTrace(s\"Batch $&#123;batch.name&#125; has no effect.\") &#125; &#125; curPlan &#125;&#125;此函数实现了针对analyzer类中定义的每一个batch（类别），按照batch中定义的fix point(策略)和rule（规则）对Unresolved的逻辑计划进行解析。这里提到的batch，可以在上面的class Analyzer中看到，lazy val batches: Seq[Batch]，这里定义了一组转换规则。Analyzer中使用的Rules，具体的Rule实现是通过RuleExecutor完成 ,定义了batches，由多个batch构成，每个batch又有不同的rule构成，如Resolution由ResolveReferences 、ResolveRelations、ResolveSortReferences 、NewRelationInstances等构成；每个rule又有自己相对应的处理函数，可以具体参看Analyzer中的ResolveReferences 、ResolveRelations、ResolveSortReferences 、NewRelationInstances函数；同时要注意的是，不同的rule应用次数是不同的：如CaseInsensitiveAttributeReferences这个batch中rule只应用了一次（Once），而Resolution这个batch中的rule应用了多次（fixedPoint = FixedPoint(100)，也就是说最多应用100次，除非前后迭代结果一致退出）。1234567891011121314/** * 输入为旧的plan，输出为新的plan，仅此而已。 * 所以真正的逻辑在各个继承实现的rule里，analyze的过程也就是执行各个rule的过程 */abstract class Rule[TreeType &lt;: TreeNode[_]] extends Logging &#123; /** Name for this rule, automatically inferred based on class name. */ val ruleName: String = &#123; val className = getClass.getName if (className endsWith \"$\") className.dropRight(1) else className &#125; def apply(plan: TreeType): TreeType&#125;rule(plan)，这里的参数plan是应用rule.apply转化里面的TreeNode分析后，每张表对应的字段集，字段类型，数据存储位置都已确定。Project 与 Filter 操作的字段类型以及在表中的位置也已确定。有了这些信息，已经可以直接将该 LogicalPlan 转换为 Physical Plan 进行执行。但是由于不同用户提交的 SQL 质量不同，直接执行会造成不同用户提交的语义相同的不同 SQL 执行效率差距甚远。换句话说，如果要保证较高的执行效率，用户需要做大量的 SQL 优化，使用体验大大降低。为了尽可能保证无论用户是否熟悉 SQL 优化，提交的 SQL 质量如何， Spark SQL 都能以较高效率执行，还需在执行前进行 LogicalPlan 优化。Logical Plan -&gt; Optimized Logical Plan作用：Optimizer再通过各种基于规则的优化策略对Logical Plan进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解Optimizer的主要职责是将Analyzer给Resolved的Logical Plan根据不同的优化策略Batch，来对语法树进行优化，优化逻辑计划节点(Logical Plan)以及表达式(Expression)，也是转换成物理执行计划的前置。它的工作原理和analyzer一致，也是通过其下的batch里面的Rule[LogicalPlan]来进行处理的。123456789101112131415161718192021222324252627282930313233343536373839404142package org.apache.spark.sql.execution/** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) &#123; // TODO: Move the planner an optimizer into here from SessionState. protected def planner = sparkSession.sessionState.planner def assertAnalyzed(): Unit = analyzed def assertSupported(): Unit = &#123; if (sparkSession.sessionState.conf.isUnsupportedOperationCheckEnabled) &#123; UnsupportedOperationChecker.checkForBatch(analyzed) &#125; &#125; lazy val analyzed: LogicalPlan = &#123; SparkSession.setActiveSession(sparkSession) sparkSession.sessionState.analyzer.executeAndCheck(logical) &#125;// 如果缓存中有查询结果，则直接替换为缓存的结果，逻辑不复杂，这里不再展开讲了。 lazy val withCachedData: LogicalPlan = &#123; assertAnalyzed() assertSupported() sparkSession.sharedState.cacheManager.useCachedData(analyzed) &#125;// 对Logical Plan 优化 lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData) lazy val sparkPlan: SparkPlan = &#123; SparkSession.setActiveSession(sparkSession) // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(optimizedPlan)).next() &#125;.......看到Optimizer也是继承自RuleExecutor，和Analyzer一个套路，也是遍历tree，并对每个节点应用rule。Optimizer里的batches包含了3类优化策略：1、Combine Limits 合并Limits 2、ConstantFolding 常量合并 3、Filter Pushdown 过滤器下推,每个Batch里定义的优化伴随对象都定义在Optimizer里了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package org.apache.spark.sql.catalyst.optimizer/** * Abstract class all optimizers should inherit of, contains the standard batches (extending * Optimizers can override this. */abstract class Optimizer(sessionCatalog: SessionCatalog) extends RuleExecutor[LogicalPlan] &#123; // Check for structural integrity of the plan in test mode. Currently we only check if a plan is // still resolved after the execution of each rule. override protected def isPlanIntegral(plan: LogicalPlan): Boolean = &#123; !Utils.isTesting || plan.resolved &#125; protected def fixedPoint = FixedPoint(SQLConf.get.optimizerMaxIterations) def batches: Seq[Batch] = &#123; val operatorOptimizationRuleSet = Seq( // Operator push down PushProjectionThroughUnion, ReorderJoin, EliminateOuterJoin, PushPredicateThroughJoin, PushDownPredicate, LimitPushDown, ColumnPruning, InferFiltersFromConstraints, // Operator combine CollapseRepartition, CollapseProject, CollapseWindow, CombineFilters, CombineLimits, CombineUnions, // Constant folding and strength reduction NullPropagation, ConstantPropagation, FoldablePropagation, OptimizeIn, ConstantFolding, ReorderAssociativeOperator, LikeSimplification, BooleanSimplification, SimplifyConditionals, RemoveDispensableExpressions, SimplifyBinaryComparison, PruneFilters, EliminateSorts, SimplifyCasts, SimplifyCaseConversionExpressions, RewriteCorrelatedScalarSubquery, EliminateSerialization, RemoveRedundantAliases, RemoveRedundantProject, SimplifyCreateStructOps, SimplifyCreateArrayOps, SimplifyCreateMapOps, CombineConcats) ++ extendedOperatorOptimizationRules val operatorOptimizationBatch: Seq[Batch] = &#123; val rulesWithoutInferFiltersFromConstraints = operatorOptimizationRuleSet.filterNot(_ == InferFiltersFromConstraints) Batch(\"Operator Optimization before Inferring Filters\", fixedPoint, rulesWithoutInferFiltersFromConstraints: _*) :: Batch(\"Infer Filters\", Once, InferFiltersFromConstraints) :: Batch(\"Operator Optimization after Inferring Filters\", fixedPoint, rulesWithoutInferFiltersFromConstraints: _*) :: Nil &#125; (Batch(\"Eliminate Distinct\", Once, EliminateDistinct) :: // Technically some of the rules in Finish Analysis are not optimizer rules and belong more // in the analyzer, because they are needed for correctness (e.g. ComputeCurrentTime). // However, because we also use the analyzer to canonicalized queries (for view definition), // we do not eliminate subqueries or compute current time in the analyzer. Batch(\"Finish Analysis\", Once, EliminateSubqueryAliases, EliminateView, ReplaceExpressions, ComputeCurrentTime, GetCurrentDatabase(sessionCatalog), RewriteDistinctAggregates, ReplaceDeduplicateWithAggregate) :: ////////////////////////////////////////////////////////////////////////////////////////// // Optimizer rules start here ////////////////////////////////////////////////////////////////////////////////////////// // - Do the first call of CombineUnions before starting the major Optimizer rules, // since it can reduce the number of iteration and the other rules could add/move // extra operators between two adjacent Union operators. // - Call CombineUnions again in Batch(\"Operator Optimizations\"), // since the other rules might make two separate Unions operators adjacent. Batch(\"Union\", Once, CombineUnions) :: Batch(\"Pullup Correlated Expressions\", Once, PullupCorrelatedPredicates) :: Batch(\"Subquery\", Once, OptimizeSubqueries) :: Batch(\"Replace Operators\", fixedPoint, ReplaceIntersectWithSemiJoin, ReplaceExceptWithFilter, ReplaceExceptWithAntiJoin, ReplaceDistinctWithAggregate) :: Batch(\"Aggregate\", fixedPoint, RemoveLiteralFromGroupExpressions, RemoveRepetitionFromGroupExpressions) :: Nil ++ operatorOptimizationBatch) :+ Batch(\"Join Reorder\", Once, CostBasedJoinReorder) :+ Batch(\"Decimal Optimizations\", fixedPoint, DecimalAggregates) :+ Batch(\"Object Expressions Optimization\", fixedPoint, EliminateMapObjects, CombineTypedFilters) :+ Batch(\"LocalRelation\", fixedPoint, ConvertToLocalRelation, PropagateEmptyRelation) :+ // The following batch should be executed after batch \"Join Reorder\" and \"LocalRelation\". Batch(\"Check Cartesian Products\", Once, CheckCartesianProducts) :+ Batch(\"RewriteSubquery\", Once, RewritePredicateSubquery, ColumnPruning, CollapseProject, RemoveRedundantProject) &#125;Optimizer的优化策略不仅有对plan进行transform的，也有对expression进行transform的，究其原理就是遍历树，然后应用优化的Rule，但是注意一点，对Logical Plantransfrom的是先序遍历(pre-order)，而对Expression transfrom的时候是后序遍历(post-order)：batch的执行和analyzer一样是通过RuleExecutor的execute方法依次遍历，这里不再解析Catalyst以上的三个过程都是属于Catalyst阶段，SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；此时，Optimizer再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan。Physical Plan从Optimizer LogicalPlan传入SparkSQL物理计划，并提交，整个PhysicalPlan经历了三个阶段转换为Iterator[PhysicalPlan]SparkPlanPrepared SparkPlanSparkPlanner把Optimizer LogicalPlan转换为PhysicalPlan列表。SparkPlanner主要是通过物理计划策略（Strategy）作用于Optimizer LogicalPlan上，从而生成SparkPlan列表即Iterator[PhysicalPlan]，此时的一个Optimizer LogicalPlan会对应多个SparkPlan。在Iterator[PhysicalPlan]中选取一个最佳的SparkPlanSparkPlan通过 prepareForExecution方法调用若干规则（ Rule ）进行转换为Prepared SparkPlan为能在各个节点上正确执行。Spark SQL中有多于65种以上的SparkPlan实现，涉及到数据源RDD的创建和各种数据处理等。12345678910111213141516171819202122232425262728293031323334353637package org.apache.spark.sql.executionlazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData) lazy val sparkPlan: SparkPlan = &#123; SparkSession.setActiveSession(sparkSession) // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(optimizedPlan)).next() &#125; // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan) /** Internal version of the RDD. Avoids copies and has no schema */ lazy val toRdd: RDD[InternalRow] = executedPlan.execute() /** * Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal * row format conversions as needed. */ protected def prepareForExecution(plan: SparkPlan): SparkPlan = &#123; preparations.foldLeft(plan) &#123; case (sp, rule) =&gt; rule.apply(sp) &#125; &#125; /** A sequence of rules that will be applied in order to the physical plan before execution. */ protected def preparations: Seq[Rule[SparkPlan]] = Seq( python.ExtractPythonUDFs, PlanSubqueries(sparkSession), EnsureRequirements(sparkSession.sessionState.conf), CollapseCodegenStages(sparkSession.sessionState.conf), ReuseExchange(sparkSession.sessionState.conf), ReuseSubquery(sparkSession.sessionState.conf))......一条SQL经历了Parser，Analyzed，Optimizer之后再转换为最终能在Spark框架中实际执行的SparkPlan的物理算子树。物理算子树中，叶子类型SparkPlan是创建一个RDD开始，而往上遍历Tree过程中每个非叶子节点做一次Transformation，通过execute函数转换成新的RDD，最终会执行Action算子把结果返回给用户。SparkPlan除了给RDD做Transformation的操作之外，还做分区，排序；同时除了执行execute方法之外，还会执行executeBroadcase方法，将数据直接广播到集群上。SparkPlan分为三大块：1，每个SparkPlan都会记住其元数据(Metadata)与指标(Metric)的信息。2，RDD做Transformation操作时候还同时做了分区和排序3，最后SparkPlan作为物理计划提交到Spark集群中执行，其中以execute为主，executeBroadcast为辅。至此，将用户程序中的SQL/Dataset/DataFrame经过一系列操作，最终转化为Spark系统中执行的RDD。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark SQL 入门","slug":"Spark-SQL-入门","date":"2017-03-25T15:48:43.000Z","updated":"2020-04-22T04:19:49.614Z","comments":true,"path":"2017/03/25/Spark-SQL-入门/","link":"","permalink":"cpeixin.cn/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/","excerpt":"","text":"程序起点在Spark2.0之前， Spark程序必须做的第一件事是创建一个SparkContext对象，该对象告诉Spark如何访问集群。要创建一个SparkContext您首先需要构建一个SparkConf对象，其中包含有关您的应用程序的信息。每个JVM只能激活一个SparkContext。12345val sparkConf: SparkConf = new SparkConf() .setAppName(\"transformation_func\") .setMaster(\"local\")val sc = new SparkContext(sparkConf)在Spark2.0之后， SparkSession类是Spark中所有功能的入口点。为了引入dataframe和dataset的API，要创建一个基本的SparkSession，只需使用SparkSession.builder()。SparkConf、SparkContext和SQLContext都已经被封装在SparkSession当中，不需要显示的创建。并且提供了对Hive功能的内置支持，下图是SparkSession的源码定义：SparkSession创建**12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession .builder() .appName(\"Spark SQL basic example\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._创建DataFrame使用SparkSession，应用程序可以从现有的RDD，Hive表的或Spark数据源创建DataFrame 。基于RDD转化DataFrame：1234567891011121314151617181920212223package readimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object sparksql_rdd &#123; case class Person(user_name: String, sex: String) def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"function_case\") .master(\"local\") .getOrCreate() val rdd_ss: RDD[(String, String)] = spark.sparkContext.makeRDD(List((\"brent\",\"male\"),(\"haylee\",\"female\"),(\"vicky\",\"male\"))) import spark.implicits._ val df2: DataFrame = rdd_ss.map((x: (String, String)) =&gt;&#123;Person(x._1,x._2)&#125;).toDF() &#125;&#125;基于JSON文件的内容创建一个DataFrame：1234567891011121314151617package spark_sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object sparksql_1 &#123; def main(args: Array[String]): Unit =&#123; val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .getOrCreate() val df_json: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/user_data.json\") df_json.show(5) &#125;&#125;基于Hive表内容创建一个DataFrame：12345678910111213141516171819package spark_sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object sparksql_hive &#123; def main(args: Array[String]): Unit = &#123; val warehouseLocation=\"hdfs://localhost:8020/user/hive/warehouse\" val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .config(\"spark.sql.warehouse.dir\",warehouseLocation) .enableHiveSupport() .getOrCreate() val df_databases: DataFrame = spark.sql(\"show databases\") df_databases.show() &#125;&#125;注意： Spark读取Hive是需要两三个步骤的如果你是在集群上运行，需要注意，要将hive-site.xml复制一份到spark目录下的conf文件夹中，如果你是在本地连接集群中的Hive，那么请将hive-site.xml复制一份到你IDEA中，resources目录下。如果你是在集群上运行，需要注意，要将mysql-connector-java-8.0.19.jar复制一份到spark目录下的jars文件夹中，如果你是在本地连接集群中的Hive，那么请在pom文件中不要忘记引入mysql-connector-java在本地运行，可能会遇到/tmp/hive的权限问题，请用chmod修改/tmp权限为777**基于HBase内容创建一个DataFrame：**1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package spark_sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.execution.datasources.hbase.HBaseTableCatalogimport scala.collection.immutableobject sparksql_hbase &#123; def main(args: Array[String]): Unit = &#123; case class Record(col0: Int, col1: Int, col2: Boolean) val spark: SparkSession = SparkSession .builder() .appName(\"Spark HBase Example\") .master(\"local[4]\") .getOrCreate() def catalog: String = // 这里，我们在读取数据的过程中，无论什么类型的数据，type字段统一指定成 string 即可。否则读取报错 s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"t_user\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"user_name\", \"type\":\"string\"&#125;, |\"col2\":&#123;\"cf\":\"cf1\", \"col\":\"customer_id\", \"type\":\"string\"&#125;, |\"col3\":&#123;\"cf\":\"cf1\", \"col\":\"age\", \"type\":\"string\"&#125;, |\"col4\":&#123;\"cf\":\"cf1\", \"col\":\"birthday\", \"type\":\"string\"&#125;, |\"col5\":&#123;\"cf\":\"cf1\", \"col\":\"deposit_amount\", \"type\":\"string\"&#125;, |\"col6\":&#123;\"cf\":\"cf1\", \"col\":\"last_login_time\", \"type\":\"string\"&#125;, |\"col7\":&#123;\"cf\":\"cf1\", \"col\":\"flag\", \"type\":\"string\"&#125; |&#125; |&#125;\"\"\".stripMargin // read val df: DataFrame = spark .read .option(HBaseTableCatalog.tableCatalog, catalog) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .load() df.show() &#125;&#125;注意：如果我们对于读取和写入HBase的场景很频繁的话，就需要考虑性能的问题，内置的读取数据源是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；TableInputFormat 中不支持 BulkGet；不能享受到 Spark SQL 内置的 catalyst 引擎的优化。基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。但是对于使用SHC，目前还是有些麻烦的，网上的maven依赖可能是因为版本的原因，程序引入找不到org.apache.spark.sql.execution.datasources.hbase.HBaseTableCatalog类，这里推荐自己下载源码，进行编译成jar文件或者编译后上传到自己的maven库中进行使用下载源码 https://github.com/hortonworks-spark/shc，选择相应低于或者等于spark，hbase的版本本地中打开，点击程序根目录下的pom文件，注释掉distributionManagement，直接点击install，将jar包生成到你本地的maven库中，当然你也可以上传到你远程的私有Maven 库中。pom文件中，引入下面依赖，就可以使用了（注意 version 版本号）12345&lt;dependency&gt; &lt;groupId&gt;com.hortonworks&lt;/groupId&gt; &lt;artifactId&gt;shc-core&lt;/artifactId&gt; &lt;version&gt;1.1.2-2.2-s_2.11&lt;/version&gt;&lt;/dependency&gt;DataFrame操作上篇文章中，我们讲到DataFrame每一列并不存储类型信息，所以在编译时并不能发现类型错误，所以在这里我们也可以叫做** 无类型的数据集操作。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270package functionimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;object sparksql_function &#123; def main(args: Array[String]): Unit =&#123; val spark: SparkSession = SparkSession .builder() .appName(\"function_case\") .master(\"local\") .config(\"spark.sql.crossJoin.enabled\", \"true\") .getOrCreate() // 样例数据 /** * &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\": 22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; */ val df: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/user_data.json\") val rdd_row: RDD[Row] = spark.sparkContext .makeRDD(List((\"brent\", \"male\"), (\"haylee\", \"female\"), (\"vicky\", \"male\"))) .map((x: (String, String)) =&gt; Row(x._1, x._2)) // The schema is encoded in a string val schemaString = \"user_name sex\" // Generate the schema based on the string of schema val fields: Array[StructField] = schemaString.split(\" \") .map((fieldName: String) =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) val df2: DataFrame = spark.createDataFrame(rdd_row, schema) show_get_data(spark, df) map_data(spark, df) filter_data(spark, df) sort_data(spark, df) groupBy_data(spark, df) join_data(spark, df, df2) intersect_data(spark, df, df2) withColumn_rename_dataframe(spark, df) &#125; def show_get_data(spark: SparkSession, df: DataFrame): Unit = &#123; df.printSchema() // root // |-- age: long (nullable = true) // |-- birthday: string (nullable = true) // |-- customer_id: long (nullable = true) // |-- deposit_amount: double (nullable = true) // |-- last_login_time: string (nullable = true) // |-- user_name: string (nullable = true) df.show(5) //默认打印前20条结果 // +---+----------+-----------+--------------+-------------------+---------+ // |age| birthday|customer_id|deposit_amount| last_login_time|user_name| // +---+----------+-----------+--------------+-------------------+---------+ // | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent| // | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee| // | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky| // +---+----------+-----------+--------------+-------------------+---------+ // Select only the \"name\" column // 这个表达式不能进行计算操作 df.select(\"user_name\", \"age\").show() // +---------+ // |user_name| // +---------+ // | brent| // | haylee| // | vicky| // +---------+ // Select everybody, but increment the age by 1 // This import is needed to use the $-notation import spark.implicits._ df.select($\"user_name\", $\"age\" + 1 as \"new_age\").show() // +---------+-------+ // |user_name|new_age| // +---------+-------+ // | brent| 23| // | haylee| 24| // | vicky| 31| // +---------+-------+ import org.apache.spark.sql.functions._ df.select(col(\"customer_id\"), col(\"deposit_amount\")).show() df.limit(5).show() df.describe() &#125; def map_data(spark: SparkSession, df: DataFrame): Unit = &#123; import spark.implicits._ // 注意 这里是Row类型 df.map((x: Row) =&gt; &#123;\"name: \"+x.getAs[String](\"user_name\")&#125;).show() &#125; def filter_data(spark: SparkSession, df: DataFrame): Unit = &#123; import spark.implicits._ // 取等于时必须用=== df.filter($\"user_name\" === \"brent\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// +---+----------+-----------+--------------+-------------------+---------+ df.filter($\"age\" &gt; 25).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ df.filter(\"deposit_amount = 3000.0\").show() df.filter($\"deposit_amount\" &gt; 200 and $\"age\" &lt; 25).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// +---+----------+-----------+--------------+-------------------+---------+ df.filter(\"substring(user_name,0,1) = 'h'\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// +---+----------+-----------+--------------+-------------------+---------+// 在源码中可以看到，where算子，底层是filter实现的。 import org.apache.spark.sql.functions._ df.where(col(\"age\") &gt; 23).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ df.where(\"age&gt; 23\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ &#125; def sort_data(spark: SparkSession, df: DataFrame): Unit = &#123; import spark.implicits._ df.sort($\"age\".desc).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// +---+----------+-----------+--------------+-------------------+---------+ df.sort($\"age\".asc).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ // 只能对数字类型和日期类型生效 df.orderBy($\"age\") df.orderBy(- df(\"age\")) df.orderBy(df(\"age\").desc) &#125; def groupBy_data(spark: SparkSession, df: DataFrame): Unit = &#123; df.groupBy(\"age\").count().show()// +---+-----+// |age|count|// +---+-----+// | 22| 1|// | 30| 1|// | 23| 1|// +---+-----+ // 只能作用于数值字段 df.groupBy(\"user_name\").max(\"deposit_amount\").show() df.groupBy(\"user_name\").min(\"deposit_amount\").show() df.groupBy(\"user_name\").mean(\"deposit_amount\").as(\"mean_deposit_amount\").show() df.groupBy(\"user_name\").sum(\"deposit_amount\").toDF(\"user_name\", \"sum_deposit_amount\").show()// +---------+------------------+// |user_name|sum_deposit_amount|// +---------+------------------+// | vicky| 200.4|// | haylee| 4000.56|// | brent| 3000.0|// +---------+------------------+ import org.apache.spark.sql.functions._ df.groupBy(\"user_name\", \"age\") .agg(min(\"deposit_amount\").as(\"min_deposit_amount\")) .show()// +---------+---+------------------+// |user_name|age|min_deposit_amount|// +---------+---+------------------+// | vicky| 30| 200.4|// | haylee| 23| 4000.56|// | brent| 22| 3000.0|// +---------+---+------------------+ //单独使用 agg df.agg(\"age\" -&gt; \"max\").show() &#125; def distinct_data(spark: SparkSession, df: DataFrame): Unit = &#123; // distinct 底层实现实则为 dropDuplicates（） df.distinct() df.dropDuplicates() &#125; def join_data(spark: SparkSession, df: DataFrame, df2: DataFrame): Unit = &#123; //笛卡尔积, spark2中默认不开启笛卡尔积，需添加\"spark.sql.crossJoin.enabled\", \"true\"配置 df.join(df2).show() df.join(df2, \"user_name\").show() df.join(df2, Seq(\"user_name\"), \"left\").show()// +---------+---+----------+-----------+--------------+-------------------+------+// |user_name|age| birthday|customer_id|deposit_amount| last_login_time| sex|// +---------+---+----------+-----------+--------------+-------------------+------+// | vicky| 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| male|// | haylee| 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00|female|// | brent| 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| male|// +---------+---+----------+-----------+--------------+-------------------+------+ &#125; def intersect_data(spark: SparkSession, df: DataFrame, df2: DataFrame): Unit = &#123; // 获取两个DataFrame中共有的记录 df.intersect(df2).show(false) &#125; def withColumn_rename_dataframe(spark: SparkSession, df: DataFrame): Unit = &#123; // 字段重命名 df.withColumnRenamed(\"deposit_amount\",\"withdraw_amount\").show() // 添加新列 import spark.implicits._ df.withColumn(\"next_year_age\", $\"age\"+1).show() &#125;&#125;以编程方式运行SQL查询12345678910111213141516171819202122232425262728293031323334package sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object spark_use_sql &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .getOrCreate() // 样例数据 /** * &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\": 22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; * &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; * &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; */ val df: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/user_data.json\") df.createTempView(\"t_user\") spark.sql(\"select * from t_user\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ import org.apache.spark.sql.functions._ spark.sql(\"select * from t_user\").groupBy(\"user_name\").agg(\"deposit_amount\"-&gt;\"sum\").show() &#125; &#125;DataSet创建12345678910111213141516171819202122232425262728293031323334353637383940import org.apache.spark.sql.&#123;Dataset, SparkSession&#125;object dataset &#123; case class Person(name: String, age: Long) def main(args: Array[String]): Unit =&#123; val spark: SparkSession = SparkSession .builder() .appName(\"dataset_case\") .master(\"local\") .getOrCreate() import spark.implicits._ // $example on:create_ds$ // Encoders are created for case classes val caseClassDS: Dataset[Person] = Seq(Person(\"Andy\", 32)).toDS() caseClassDS.show() // +----+---+ // |name|age| // +----+---+ // |Andy| 32| // +----+---+ // Encoders for most common types are automatically provided by importing spark.implicits._ val primitiveDS: Dataset[Int] = Seq(1, 2, 3).toDS() primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4) // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name val path = \"examples/src/main/resources/people.json\" val peopleDS: Dataset[Person] = spark.read.json(path).as[Person] peopleDS.show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ // $example off:create_ds$ &#125;&#125;数据存储文件123456789101112131415161718192021222324252627282930313233343536373839404142// $example on:generic_load_save_functions$ val usersDF = spark.read.load(\"examples/src/main/resources/users.parquet\") usersDF.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\") // $example off:generic_load_save_functions$ // $example on:manual_load_options$ val peopleDF = spark.read.format(\"json\").load(\"examples/src/main/resources/people.json\") peopleDF.select(\"name\", \"age\").write.format(\"parquet\").save(\"namesAndAges.parquet\") // $example off:manual_load_options$ // $example on:manual_load_options_csv$ val peopleDFCsv = spark.read.format(\"csv\") .option(\"sep\", \";\") .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .load(\"examples/src/main/resources/people.csv\") // $example off:manual_load_options_csv$ // $example on:manual_save_options_orc$ usersDF.write.format(\"orc\") .option(\"orc.bloom.filter.columns\", \"favorite_color\") .option(\"orc.dictionary.key.threshold\", \"1.0\") .option(\"orc.column.encoding.direct\", \"name\") .save(\"users_with_options.orc\") // $example off:manual_save_options_orc$ // $example on:direct_sql$ val sqlDF = spark.sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\") // $example off:direct_sql$ // $example on:write_sorting_and_bucketing$ peopleDF.write.bucketBy(42, \"name\").sortBy(\"age\").saveAsTable(\"people_bucketed\") // $example off:write_sorting_and_bucketing$ // $example on:write_partitioning$ usersDF.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\") // $example off:write_partitioning$ // $example on:write_partition_and_bucket$ usersDF .write .partitionBy(\"favorite_color\") .bucketBy(42, \"name\") .saveAsTable(\"users_partitioned_bucketed\") // $example off:write_partition_and_bucket$ spark.sql(\"DROP TABLE IF EXISTS people_bucketed\") spark.sql(\"DROP TABLE IF EXISTS users_partitioned_bucketed\")jdbc123456789101112131415161718192021222324252627282930313233343536373839404142434445package writeimport java.util.Propertiesimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object write_data &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"write_case\") .master(\"local\") .getOrCreate() // read val jdbcDF: DataFrame = spark.read .format(\"jdbc\") .option(\"url\", \"jdbc:postgresql:dbserver\") .option(\"dbtable\", \"schema.tablename\") .option(\"user\", \"username\") .option(\"password\", \"password\") .load() // write // Saving data to a JDBC source jdbcDF.write .format(\"jdbc\") .option(\"url\", \"jdbc:postgresql:dbserver\") .option(\"dbtable\", \"schema.tablename\") .option(\"user\", \"username\") .option(\"password\", \"password\") .mode(\"append\") .save() // or val properties=new Properties() properties.setProperty(\"user\",\"root\") properties.setProperty(\"password\",\"secret_password\") jdbcDF.write .mode(\"append\") .jdbc(\"jdbc:mysql://your_ip:3306/my_test?useUnicode=true&amp;characterEncoding=UTF-8\",\"t_result\",properties) &#125;&#125;hive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import java.io.Fileimport org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;case class Record(key: Int, value: String)// warehouseLocation points to the default location for managed databases and tablesval warehouseLocation = new File(\"spark-warehouse\").getAbsolutePathval spark = SparkSession .builder() .appName(\"Spark Hive Example\") .config(\"spark.sql.warehouse.dir\", warehouseLocation) .enableHiveSupport() .getOrCreate()import spark.implicits._import spark.sqlsql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")// Queries are expressed in HiveQLsql(\"SELECT * FROM src\").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.sql(\"SELECT COUNT(*) FROM src\").show()// +--------+// |count(1)|// +--------+// | 500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.val sqlDF = sql(\"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key\")// The items in DataFrames are of type Row, which allows you to access each column by ordinal.val stringsDS = sqlDF.map &#123; case Row(key: Int, value: String) =&gt; s\"Key: $key, Value: $value\"&#125;stringsDS.show()// +--------------------+// | value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a SparkSession.val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s\"val_$i\")))recordsDF.createOrReplaceTempView(\"records\")// Queries can then join DataFrame data with data stored in Hive.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// | 2| val_2| 2| val_2|// | 4| val_4| 4| val_4|// | 5| val_5| 5| val_5|// ...// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax// `USING hive`sql(\"CREATE TABLE hive_records(key int, value string) STORED AS PARQUET\")// Save DataFrame to the Hive managed tableval df = spark.table(\"src\")df.write.mode(SaveMode.Overwrite).saveAsTable(\"hive_records\")// After insertion, the Hive managed table has data nowsql(\"SELECT * FROM hive_records\").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Prepare a Parquet data directoryval dataDir = \"/tmp/parquet_data\"spark.range(10).write.parquet(dataDir)// Create a Hive external Parquet tablesql(s\"CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION '$dataDir'\")// The Hive external table should already have datasql(\"SELECT * FROM hive_bigints\").show()// +---+// | id|// +---+// | 0|// | 1|// | 2|// ... Order may vary, as spark processes the partitions in parallel.// Turn on flag for Hive Dynamic Partitioningspark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")// Create a Hive partitioned table using DataFrame APIdf.write.partitionBy(\"key\").format(\"hive\").saveAsTable(\"hive_part_tbl\")// Partitioned column `key` will be moved to the end of the schema.sql(\"SELECT * FROM hive_part_tbl\").show()// +-------+---+// | value|key|// +-------+---+// |val_238|238|// | val_86| 86|// |val_311|311|// ...spark.stop()hbase12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061object Application &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master(\"local\") .appName(\"normal\") .getOrCreate() spark.sparkContext.setLogLevel(\"warn\") val data = (0 to 255).map &#123; i =&gt; HBaseRecord(i, \"extra\")&#125; val df:DataFrame = spark.createDataFrame(data) df.write .mode(SaveMode.Overwrite) .options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog)) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .save() &#125; def catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"rec\", \"name\":\"user_rec\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"&#125;, |\"col1\":&#123;\"cf\":\"t\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"t\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"t\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"t\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"t\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"t\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"t\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"t\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125; |&#125;\"\"\".stripMargin&#125;case class HBaseRecord( col0: String, col1: Boolean, col2: Double, col3: Float, col4: Int, col5: Long, col6: Short, col7: String, col8: Byte)object HBaseRecord&#123; def apply(i: Int, t: String): HBaseRecord = &#123; val s = s\"\"\"row$&#123;\"%03d\".format(i)&#125;\"\"\" HBaseRecord(s, i % 2 == 0, i.toDouble, i.toFloat, i, i.toLong, i.toShort, s\"String$i: $t\", i.toByte) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark SQL 讲解","slug":"Spark-SQL-讲解","date":"2017-03-17T13:15:45.000Z","updated":"2020-04-22T04:19:51.260Z","comments":true,"path":"2017/03/17/Spark-SQL-讲解/","link":"","permalink":"cpeixin.cn/2017/03/17/Spark-SQL-%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"Spark SQL是Apache Spark的用于处理结构化数据的模块。下图是Spark官网对Spark SQL的描述Spark SQL特点存在即合理，在数据分析查询领域中，虽然已经存在了底层运行MapReduce的Hive，但是人们越来越不满足于查询的速度，所以Spark SQL就应运而生。在我平常的工作中，Spark SQL的出场率真的超高～～集成123将SQL查询与Spark程序无缝混合。Spark SQL使您可以使用SQL或熟悉的DataFrame API在Spark程序中查询结构化数据。可在Java，Scala，Python和R中使用。统一数据访问123以相同的方式连接到任何数据源。DataFrame和SQL提供了一种访问各种数据源的通用方法，包括Hive，Avro，Parquet，ORC，JSON和JDBC。您甚至可以跨这些源联接数据。Hive整合12在现有仓库上运行SQL或HiveQL查询。Spark SQL支持HiveQL语法以及Hive SerDes和UDF，从而使您可以访问现有的Hive仓库。标准连接12通过JDBC或ODBC连接。服务器模式为商业智能工具提供了行业标准的JDBC和ODBC连接。Spark SQL架构Spark SQL 本质上是一个库。它运行在 Spark 的核心执行引擎之上。如上图所示，它提供类似于 SQL 的操作接口，允许数据仓库应用程序直接获取数据，允许使用者通过命令行操作来交互地查询数据，还提供两个编程抽象API：DataFrame API 和 DataSet API。Java、Python 和 Scala 的应用程序可以通过这两个 API 来读取和写入 RDD。此外，正如我们在RDD介绍的，应用程序还可以直接操作 RDD。使用 Spark SQL 会让开发者觉得好像是在操作一个关系型数据库一样，而不是在操作 RDD。这是它优于原生的 RDD API 的地方。与基本的 Spark RDD API 不同，Spark SQL 提供的接口为 Spark 提供了关于数据结构和正在执行的计算的更多信息。在内部，Spark SQL 使用这些额外的信息来执行额外的优化。虽然 Spark SQL 支持多种交互方式，但是在计算结果时均使用相同的执行引擎。这种统一意味着开发人员可以轻松地在不同的 API 之间来回切换，基于这些 API 提供了表达给定转换的最自然的方式。接下来让我们进一步了解 DataSet 和 DataFrame。DataSetDataSet，顾名思义，就是数据集的意思，它是 Spark 1.6 新引入的接口。同弹性分布式数据集类似，DataSet 也是不可变分布式的数据单元，它既有与 RDD 类似的各种转换和动作函数定义，而且还享受 Spark SQL 优化过的执行引擎，使得数据搜索效率更高。DataSet 支持的转换和动作也和 RDD 类似，比如 map、filter、select、count、show 及把数据写入文件系统中。同样地，DataSet 上的转换操作也不会被立刻执行，只是先生成新的 DataSet，只有当遇到动作操作，才会把之前的转换操作一并执行，生成结果。所以，DataSet 的内部结构包含了逻辑计划，即生成该数据集所需要的运算。当动作操作执行时，Spark SQL 的查询优化器会优化这个逻辑计划，并生成一个可以分布式执行的、包含分区信息的物理计划。那么，DataSet 和 RDD 的区别是什么呢？通过之前的叙述，我们知道 DataSet API 是 Spark SQL 的一个组件。那么，你应该能很容易地联想到，DataSet 也具有关系型数据库中表的特性。是的，DataSet 所描述的数据都被组织到有名字的列中，就像关系型数据库中的表一样。如上图所示，左侧的 RDD 虽然以 People 为类型参数，但 Spark 框架本身不了解 People 类的内部结构。所有的操作都以 People 为单位执行。而右侧的 DataSet 却提供了详细的结构信息与每列的数据类型。这让 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。也就是说，DataSet 提供数据表的 schema 信息。这样的结构使得 DataSet API 的执行效率更高。试想，如果我们要查询 People 的年龄信息，Spark SQL 执行的时候可以依靠查询优化器仅仅把需要的那一列取出来，其他列的信息根本就不需要去读取了。所以，有了这些信息以后在编译的时候能够做更多的优化。其次，由于 DataSet 存储了每列的数据类型。所以，在程序编译时可以执行类型检测。DataFrameDataFrame 可以被看作是一种特殊的 DataSet。它也是关系型数据库中表一样的结构化存储机制，也是分布式不可变的数据结构。但是，它的每一列并不存储类型信息，所以在编译时并不能发现类型错误。DataFrame 每一行的类型固定为 Row，他可以被当作 DataSet[Row]来处理，我们必须要通过解析才能获取各列的值。所以，对于 DataSet 我们可以用类似 people.name 来访问一个人的名字，而对于 DataFrame 我们一定要用类似 people.get As [String] (“name”) 来访问。RDD、DataFrame、DataSet 对比学习 Spark 到现在，我们已经接触了三种基本的数据结构：RDD、DataFrame 和 DataSet。接下来你的表格中，你可以看到它们的异同点，在实际工作中，经常会遇到 RDD-&gt;DataFrame-&gt;DataSet之间的相互转换，思考一下怎样在实际工程中选择。发展历史从发展历史上来看，RDD API 在第一代 Spark 中就存在，是整个 Spark 框架的基石。接下来，为了方便熟悉关系型数据库和 SQL 的开发人员使用，在 RDD 的基础上，Spark 创建了 DataFrame API。依靠它，我们可以方便地对数据的列进行操作。DataSet 最早被加入 Spark SQL 是在 Spark 1.6，它在 DataFrame 的基础上添加了对数据的每一列的类型的限制。在 Spark 2.0 中，DataFrame 和 DataSet 被统一。DataFrame 作为 DataSet[Row]存在。在弱类型的语言，如 Python 中，DataFrame API 依然存在，但是在 Java 中，DataFrame API 已经不复存在了。不变性与分区由于 DataSet 和 DataFrame 都是基于 RDD 的，所以它们都拥有 RDD 的基本特性，在此不做赘述。而且我们可以通过简单的 API 在 DataFrame 或 Dataset 与 RDD 之间进行无缝切换。性能DataFrame 和 DataSet 的性能要比 RDD 更好。Spark 程序运行时，Spark SQL 中的查询优化器会对语句进行分析，并生成优化过的 RDD 在底层执行。举个例子，如果我们想先对一堆数据进行 GroupBy 再进行 Filter 操作，这无疑是低效的，因为我们并不需要对所有数据都 GroupBy。如果用 RDD API 实现这一语句，在执行时它只会机械地按顺序执行。而如果用 DataFrame/DataSet API，Spark SQL 的 Catalyst 优化器会将 Filter 操作和 GroupBy 操作调换顺序，从而提高执行效率。下图反映了这一优化过程。详细的解析优化过程，我会单独用一篇文章来写的。错误检测RDD 和 DataSet 都是类型安全的，而 DataFrame 并不是类型安全的。这是因为它不存储每一列的信息如名字和类型。使用 DataFrame API 时，我们可以选择一个并不存在的列，这个错误只有在代码被执行时才会抛出。如果使用 DataSet API，在编译时就会检测到这个错误。小结DataFrame 和 DataSet 是 Spark SQL 提供的基于 RDD 的结构化数据抽象。它既有 RDD 不可变、分区、存储依赖关系等特性，又拥有类似于关系型数据库的结构化信息。所以，基于 DataFrame 和 DataSet API 开发出的程序会被自动优化，使得开发人员不需要操作底层的 RDD API 来进行手动优化，大大提升开发效率。但是 RDD API 对于非结构化的数据处理有独特的优势，比如文本流数据，而且更方便我们做底层的操作。所以在开发中，我们还是需要根据实际情况来选择使用哪种 API。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Core 数据存储操作","slug":"Spark-Core-数据存储操作","date":"2017-03-15T14:41:50.000Z","updated":"2020-04-14T17:27:43.590Z","comments":true,"path":"2017/03/15/Spark-Core-数据存储操作/","link":"","permalink":"cpeixin.cn/2017/03/15/Spark-Core-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%93%8D%E4%BD%9C/","excerpt":"","text":"上文讲解了RDD的概念和转换算子、行动算子的操作内容，对于算子的操作，属于Spark针对数据进行计算的过程。回到现实场景，我们使用Spark来做任务的过程，其实就是三个阶段：读取数据源对源数据进行计算，业务逻辑的编写数据存储，数据落地在日常的工作中，Spark的任务可以分为两类，离线计算和实时计算。针对实时计算，几乎95%的情况下，我们是使用kafka作为消息队列与Spark进行数据对接。针对离线计算，数据源就多种多样了，可以是产品业务线的业务数据库，可以是大数据平台的Hive，HBase等，也可以是CEO办公室给出的报表数据。总结一下上面所讲述的，就是数据源多种多样，选择合适的组件和API将数据读进来就好。并且，在现实业务场景中，都是选择上层组件Spark SQL 和 Spark Streaming来读取数据，计算数据，存储数据。所以在这里就不复杂的数据操作了，只是针对在简单的业务场景，不需要Spark SQL 和 Spark Streaming组件，使用Spark Core处理简单的读取、计算和存储任务示例。数据落地至HDFS文件系统1234567891011121314151617181920212223242526272829package rddimport com.google.gson.&#123;JsonObject, JsonParser&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object process_json &#123; def main(args: Array[String]): Unit=&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"process_json\") .setMaster(\"local\") val sc = new SparkContext(sparkConf) \"\"\" data_demo: &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\":22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; \"\"\".stripMargin var json_rdd: RDD[String] = sc.textFile(\"hdfs://localhost:8020/data/user_data.json\") val result_rdd: RDD[JsonObject] = json_rdd.map((x: String) =&gt; &#123; val json = new JsonParser() val json_item = json.parse(x).asInstanceOf[JsonObject] json_item.addProperty(\"flag\", 1) json_item &#125;) result_rdd.saveAsTextFile(\"hdfs://localhost:8020/data/result/user_data\") &#125;&#125;数据落地至MySQL**1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package rddimport java.sql.&#123;Connection, DriverManager, PreparedStatement&#125;import com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object process_json &#123; def main(args: Array[String]): Unit=&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"process_json\") \"\"\" data_demo: &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\":22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; \"\"\".stripMargin val sc = new SparkContext(sparkConf) // 读取数据源 var json_rdd: RDD[String] = sc.textFile(\"hdfs://localhost:8020/data/user_data.json\") // 数据计算 val result_rdd: RDD[JSONObject] = json_rdd.map((x: String) =&gt; &#123; val json_item: JSONObject = JSON.parseObject(x) json_item.put(\"flag\", 1) json_item &#125;) val driverClassName = \"com.mysql.jdbc.Driver\" val url = \"jdbc:mysql://localhost:3306/test?characterEncoding=utf8&amp;useSSL=false\" val user = \"root\" val password = \"cpx726175\" result_rdd.foreachPartition((partition: Iterator[JSONObject]) =&gt; &#123; Class.forName(driverClassName) val connection: Connection = DriverManager.getConnection(url, user, password) val sql = \"insert into t_user(user_name, customer_id, age, birthday, deposit_amount, last_login_time,flag) values(?,?,?,?,?,?,?)\" val statement: PreparedStatement = connection.prepareStatement(sql) try &#123; partition.foreach &#123; json_data: JSONObject =&gt; &#123; statement.setString(1, json_data.getString(\"user_name\")) statement.setInt(2, json_data.getInteger(\"customer_id\")) statement.setInt(3, json_data.getInteger(\"age\")) statement.setString(4, json_data.getString(\"birthday\")) statement.setFloat(5, json_data.getFloat(\"deposit_amount\")) statement.setString(6, json_data.getString(\"last_login_time\")) statement.setInt(7, json_data.getInteger(\"flag\")) statement.executeUpdate() &#125; &#125; &#125;catch &#123; case e: Exception =&gt; println(e.printStackTrace()) &#125; finally &#123; if(statement!=null) statement.close() if(connection!=null) connection.close() &#125; connection.close() &#125; ) sc.stop() &#125;&#125;注：在使用mysql建表时，float字段，double字段需要指定小数点位数。否则将会按照四舍五入整数显示123456789CREATE TABLE &#96;t_user&#96; ( &#96;user_name&#96; varchar(50) DEFAULT NULL, &#96;customer_id&#96; int(50) DEFAULT NULL, &#96;age&#96; int(20) DEFAULT NULL, &#96;birthday&#96; varchar(50) DEFAULT NULL, &#96;deposit_amount&#96; float(20,3) DEFAULT NULL, &#96;last_login_time&#96; varchar(100) DEFAULT NULL, &#96;flag&#96; int(10) DEFAULT NULL) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4 COLLATE&#x3D;utf8mb4_0900_ai_ci落地数据如下：**数据落地至HBase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package rddimport java.langimport java.sql.&#123;Connection, DriverManager, PreparedStatement&#125;import org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableOutputFormat&#125;import org.apache.hadoop.hbase.client.Resultimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.mapreduce.Jobimport com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.mapred.JobConfimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object process_json &#123; def main(args: Array[String]): Unit=&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"process_json\") \"\"\" data_demo: &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\":22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; \"\"\".stripMargin val sc = new SparkContext(sparkConf) var json_rdd: RDD[String] = sc.textFile(\"hdfs://localhost:8020/data/user_data.json\") val result_rdd: RDD[JSONObject] = json_rdd.map((x: String) =&gt; &#123; val json_item: JSONObject = JSON.parseObject(x) json_item.put(\"flag\", 1) json_item &#125;) var resultConf: Configuration = HBaseConfiguration.create() //设置zooKeeper集群地址，也可以通过将hbase-site.xml导入classpath，但是建议在程序里这样设置 resultConf.set(\"hbase.zookeeper.quorum\", \"localhost\") //设置zookeeper连接端口，默认2181 resultConf.set(\"hbase.zookeeper.property.clientPort\", \"2181\") //注意这里是output resultConf.set(TableOutputFormat.OUTPUT_TABLE, \"t_user\") var job: Job = Job.getInstance(resultConf) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[org.apache.hadoop.hbase.client.Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) val hbaseOut: RDD[(ImmutableBytesWritable, Put)] = result_rdd.map((json_data: JSONObject) =&gt; &#123; val put = new Put(Bytes.toBytes(json_data.getInteger(\"customer_id\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"user_name\"), Bytes.toBytes(json_data.getString(\"user_name\"))) //直接写入整型会以十六进制存储 put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"customer_id\"), Bytes.toBytes(json_data.get(\"customer_id\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"age\"), Bytes.toBytes(json_data.get(\"age\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"birthday\"), Bytes.toBytes(json_data.getString(\"birthday\"))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"deposit_amount\"), Bytes.toBytes(json_data.get(\"deposit_amount\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"last_login_time\"), Bytes.toBytes(json_data.getString(\"last_login_time\"))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"flag\"), Bytes.toBytes(json_data.get(\"flag\").toString)) (new ImmutableBytesWritable, put) &#125;) hbaseOut.saveAsNewAPIHadoopDataset(job.getConfiguration) sc.stop() &#125;&#125;hbase建表语句：1create 't_user', 'cf1'落地数据如下：以上就是针对RDD，进行数据写入的操作。过程并不复杂，但是需要注意的是，在写入过程中，要对应好数据类型。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark - RDD讲解","slug":"Spark-RDD讲解","date":"2017-03-11T22:41:50.000Z","updated":"2020-05-18T06:36:24.984Z","comments":true,"path":"2017/03/12/Spark-RDD讲解/","link":"","permalink":"cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"What is RDD在讲RDD之前，先和大家说一下在Spark中，我们分析数据的过程，主要会碰到RDD，DataFrame，DataSet概念，这三种都是我们计算过程中的数据单元。在Spark 2.0之前，主要的数据操作都是操作RDD，而在Spark 2.0以后，官方开始呼吁大家迁移到基于DataSet，即使你是从Spark 2.0以后开始接触的spark，但是我也很负责人的告诉你，RDD你必须要懂～～定义弹性分布式数据集（Resilient Distributed Datasets : RDD），表示已被分区、不可变的，并能够被并行操作，可同错的数据集合。针对上面的定义，还说描述的很抽象，接下来根据每个RDD的属性，进行逐点说明分区分区这个概念在分布式计算中我们经常会看到，例如MapReduce中，数据在Map端写入到环形缓冲区，数据进行分区，reduce端读取相应的分区文件，还有在Kafka中，topic中的分区概念。在RDD中，本质上是一个只读的分区记录集合。也就是我们要处理的源数据的抽象。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同节点上，从而可以在集群中的不同节点上进行并行计算。这也是它可以被并行处理的前提。反向来思考的话，如果RDD不可分区，只是一个单独不可拆分的数据块，那么集群中的节点怎么对这个源数据进行分布式并行计算呢？逻辑上，我们可以认为 RDD 是一个大的数组。数组中的每个元素可以代表一个分区（Partition）。在物理存储中，每个分区指向一个存放在堆内内存和堆外内存或者磁盘中的数据块（Block），而这些数据块是独立的，它们可以被存放在系统中的不同节点。所以，RDD 只是抽象意义的数据集合，分区内部并不会存储具体的数据，仅保存了元数据信息。下图很好地展示了 RDD 的分区逻辑结构：RDD 中的每个分区存有它在该 RDD 中的 index。通过 RDD 的 ID 和分区的 index 可以唯一确定对应数据块的编号，从而通过底层存储层的接口中提取到数据进行处理。在集群中，各个节点上的数据块会尽可能地存放在内存中，只有当内存没有空间时才会存入硬盘。这样可以最大化地减少硬盘读写的开销。虽然 RDD 内部存储的数据是只读的，但是，我们可以去修改（例如通过 repartition 转换操作）并行计算单元的划分结构，也就是分区的数量。不可变性不可变性代表每一个 RDD 都是只读的，它所包含的分区信息不可以被改变。既然已有的 RDD 不可以被改变，我们只可以对现有的 RDD 进行转换（Transformation）操作，得到新的 RDD 作为中间计算的结果。上图也就是刚刚提到的针对RDD的Transformation操作中，包括的map算子，flatMap算子，filter算子，这些算子我们接下来的文章在一一讲解，这里我想给大家看的是，在举例的这三个算子实现方法中，我们可以看到都new MapPartitionsRDD（），也就是说，在对一个已有的RDD进行转换操作的过程中，并不是对这个RDD进行直接的修改，变换，而是读取父RDD，创建了一个新的RDD进行转换并且最后返回。那么这样会带来什么好处呢？显然，对于代表中间结果的 RDD，我们需要记录它是通过哪个 RDD 进行哪些转换操作得来，即依赖关系，而不用立刻去具体存储计算出的数据本身。这样做有助于提升 Spark 的计算效率，并且使错误恢复更加容易。试想，在一个有 N 步的计算模型中，如果记载第 N 步输出 RDD 的节点发生故障，数据丢失，我们可以从第 N-1 步的 RDD 出发，再次计算，而无需重复整个 N 步计算过程。这样的容错特性也是 RDD 为什么是一个“弹性”的数据集的原因之一。后边我们会提到 RDD 如何存储这样的依赖关系。并行操作由于单个 RDD 的分区特性，使得它天然支持并行操作，即不同节点上的数据可以被分别处理，然后产生一个新的 RDD。容错性为了保证RDD 中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。 相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的 特定数据转换（Transformation）操作（filter, map, join etc.)行为。当这个RDD的部分分区数据丢失时 ，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制 了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。另外，在RDD计算中，也通过checkpoint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是 logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通 过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算 生成丢失的分区数据。RDD 五大特性A list of partitionsRDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。A function for computing each split一个函数计算每一个分片，RDD的每个partition上面都会有function，也就是函数应用A list of dependencies on other RDDsRDD会记录它的依赖 ，依赖还具体分为宽依赖和窄依赖，RDD在计算的过程中，不断的转换，在内存中，不落地磁盘，如果某一环节出错，可以根据依赖来找回上一状态的RDD，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。Optionally,a Partitioner for Key-value RDDs可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面Optionally, a list of preferred locations to compute each split on我们的原则是移动计算，不移动数据，默认的是，磁盘中的数据是作为RDD加载到本机的内存中，但是，Spark这里给出了一个可选项，可以选择加载到指定的机器内存中，就是可以选择将数据放在那几台性能好的节点上RDD 结构通过上述讲解，我们了解了 RDD 的基本特性。而且，我们还提到每一个 RDD 里都会包括分区信息、所依赖的父 RDD 以及通过怎样的转换操作才能由父 RDD 得来等信息。实际上 RDD 的结构远比你想象的要复杂，让我们来看一个 RDD 的简易结构示意图：SparkContext 是所有 Spark 功能的入口，它代表了与 Spark 节点的连接，可以用来创建 RDD 对象以及在节点中的广播变量等。一个线程只有一个 SparkContext。SparkConf 则是一些参数配置信息。感兴趣的同学可以去阅读官方的技术文档，一些相对不重要的概念我就不再赘述了。Partitions 前文中我已经提到过，它代表 RDD 中数据的逻辑结构，每个 Partition 会映射到某个节点内存或硬盘的一个数据块。Partitioner 决定了 RDD 的分区方式，目前有两种主流的分区方式：Hash partitioner 和 Range partitioner。Hash，顾名思义就是对数据的 Key 进行散列分区，Range 则是按照 Key 的排序进行均匀分区。此外我们还可以创建自定义的 Partitioner。依赖关系Dependencies 是 RDD 中最重要的组件之一。如前文所说，Spark 不需要将每个中间计算结果进行数据复制以防数据丢失，因为每一步产生的 RDD 里都会存储它的依赖关系，即它是通过哪个 RDD 经过哪个转换操作得到的。细心的读者会问这样一个问题，父 RDD 的分区和子 RDD 的分区之间是否是一对一的对应关系呢？Spark 支持两种依赖关系：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）。窄依赖就是父 RDD 的分区可以一一对应到子 RDD 的分区，宽依赖就是父 RDD 的每个分区可以被多个子 RDD 的分区使用。显然，窄依赖允许子 RDD 的每个分区可以被并行处理产生，而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理。宽依赖本质就是shuffle,计算代价大,经过大量shuffle生成的RDD，建议进行缓存。这样避免失败后重新计算带来的开销如上图所示，一些转换操作如 map、filter 会产生窄依赖关系，而 Join、groupBy 则会生成宽依赖关系。这很容易理解，因为 map 是将分区里的每一个元素通过计算转化为另一个元素，一个分区里的数据不会跑到两个不同的分区。而 groupBy 则要将拥有所有分区里有相同 Key 的元素放到同一个目标分区，而每一个父分区都可能包含各种 Key 的元素，所以它可能被任意一个子分区所依赖。Spark 之所以要区分宽依赖和窄依赖是出于以下两点考虑：窄依赖可以支持在同一个节点上链式执行多条命令，例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。从失败恢复的角度考虑，窄依赖的失败恢复更有效，因为它只需要重新计算丢失的父分区即可，而宽依赖牵涉到 RDD 各级的多个父分区。DAG结合上篇spark运行原理和这篇RDD的讲述，我们来讲一下关于任务运行中，Job，Stage，Task的划分spark任务运行中，会存在一个或者多个Job，action算子的触发会生成一个Job, Job会提交给DAGScheduler,分解成Stage。上图是一个job被切割成三个Stage，每个stage中有包含不用个数的partition，每个partition在计算的时候对应一个task，影响程序的并行度。job分割stage的规则是从G端向前开始分割，遇到宽依赖，就分割一个stage.。F–&gt;G 切割 stage2 和 stage3A–&gt;B stage1上图，程序的运行最小单元是Task，就拿stage2来举例：stage2有4个Task: C端有2个partition，E端有两个partition。每个partition为开始，最终到F端，作为一个Task。 其中B阶段呢，属于一个程序级别的优化操作。一般分布式程序中，为了让程序能平稳的执行，就要做一些优化操作。在以后的Spark开发中，我们在Web UI中会经常看到类似于下图的工作流程，这里展示了一个Job的划分和对应操作的细节。How use RDD上面讲了很长篇幅的RDD概念和属性，那么我们该如何开始实操RDD呢？我们的第一个RDD来自哪里？上面我说过了，其实RDD就是我们要进行处理的源数据集合。在实际的业务场景中，对于离线数据分析，大多数的场景下，源数据可以是Spark从Hive、HBase中读取，转化成RDD，再细小一点的场景，源数据可以是一个csv报表数据，读取目标CSV进行RDD的转换。那么在下面的讲解中，我们无需对接上游生产环境的数据源，我们可以在IDEA中直接进行RDD的创建，随后再进行各种转换算子和行动算子的操作演示创建RDDRDD的创建有三种方法利用内存中集合中创建利用外部文件创建由其他RDD创建新的RDD从集合中创建可以使用parallelize() 和 makeRDD()方法创建我们可以看到，这两个方法需要我们传入的参数是 Seq[T] 集合，返回的都是RDD[T]。注意，makeRDD给了两种方法，这里先记为第一种makeRDD和第二种makeRDD这里，先把源码po出来可以清晰的看到，第一种makeRDD的源码实现中，实际上是调用了parallelize（），并且都可以指定分区数量，而且在注释中清晰的写明了，第一种makeRDD和parallelize是identical（完全相同的）。再来看第二种makeRDD，第二种实现可以为数据提供位置信息，并且不能指定RDD的分区数量，除此之外的实现和parallelize函数也是一致的创建示例：123456789val make_rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))val make_rdd_1: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\"))val make_rdd_2: RDD[Int] = sc.makeRDD(List(1,2,3,4),2)val make_rdd_3: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\"),4)println(make_rdd.partitions.size)println(make_rdd_1.partitions.size)println(make_rdd_2.partitions.size)println(make_rdd_3.partitions.size)结果：12341124**外部文件创建****1234sc.textFile(\"hdfs://hadoop000:9000/xxx/data.txt\") // 读取hdfs文件sc.textFile(\"data.txt\") // 这里纯粹的本地文件是不推荐的， 因为这个文件访问是针对每一个Worker都要是能访问的 换言之,如果是本地文件,则必须保证每一个Worker的本地都有一份这个文件RDD 算子操作RDD 的数据操作分为两种：转换（Transformation）和动作（Action）。顾名思义，转换是用来把一个 RDD 转换成另一个 RDD，而动作则是通过计算返回一个结果。下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）Transformation（转换）Meaning（含义）map(func)返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中的元素应用一个函数 func 来生成。filter(func)返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中应用一个函数 func 且返回值为 true 的元素来生成。flatMap(func)与 map 类似，但是每一个输入的 item 可以被映射成 0 个或多个输出的 items（所以 func 应该返回一个 Seq 而不是一个单独的 item）。mapPartitions(func)与 map 类似，但是单独的运行在在每个 RDD 的 partition（分区，block）上，所以在一个类型为 T 的 RDD 上运行时 func 必须是 Iterator=&gt; Iterator 类型。mapPartitionsWithIndex(func)与 mapPartitions 类似，但是也需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 func_，所以在一个类型为 T 的 RDD 上运行时 _func 必须是 (Int, Iterator) =&gt; Iterator 类型。sample(withReplacement, fraction, seed)样本数据，设置是否放回（withReplacement），采样的百分比（_fraction_）、使用指定的随机数生成器的种子（seed）。union(otherDataset)返回一个新的 dataset，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的并集。intersection(otherDataset)返回一个新的 RDD，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的交集。distinct([_numTasks_]))返回一个新的 dataset，它包含了 source dataset（源数据集）中去重的元素。groupByKey([_numTasks_])在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable) .Note: 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 reduceByKey 或 aggregateByKey 来计算性能会更好.Note: 默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 numTasks 参数来设置不同的任务数。reduceByKey(func, [_numTasks_])在 (K, V) pairs 的 dataset 上调用时，返回 dataset of (K, V) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的函数 func 来进行聚合的，它必须是 type (V,V) =&gt; V 的类型。像 groupByKey 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。aggregateByKey(zeroValue)(seqOp, combOp, [_numTasks_])在 (K, V) pairs 的 dataset 上调用时，返回 (K, U) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的 combine 函数以及一个 neutral “0” 值来进行聚合的。允许聚合值的类型与输入值的类型不一样，同时避免不必要的配置。像 groupByKey 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。sortByKey([_ascending_], [_numTasks_])在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset，由 boolean 类型的 ascending 参数来指定。join(otherDataset, [_numTasks_])在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 leftOuterJoin, rightOuterJoin 和 fullOuterJoin 来实现。cogroup(otherDataset, [_numTasks_])在一个 (K, V) 和的 dataset 上调用时，返回一个 (K, (Iterable, Iterable)) tuples 的 dataset。这个操作也调用了 groupWith。cartesian(otherDataset)在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) pairs 类型的 dataset（所有元素的 pairs，即笛卡尔积）。pipe(command, [envVars])通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回。coalesce(numPartitions)Decrease（降低）RDD 中 partitions（分区）的数量为 numPartitions。对于执行过滤后一个大的 dataset 操作是更有效的。repartition(numPartitions)Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀。该操作总是通过网络来 shuffles 所有的数据。repartitionAndSortWithinPartitions(partitioner)根据给定的 partitioner（分区器）对 RDD 进行重新分区，并在每个结果分区中，按照 key 值对记录排序。这比每一个分区中先调用 repartition 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行。下表列出了一些 Spark 常用的 actions 操作。详细请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）。Action（动作）Meaning（含义）reduce(func)使用函数 func 聚合 dataset 中的元素，这个函数 func 输入为两个元素，返回为一个元素。这个函数应该是可交换（commutative）和关联（associative）的，这样才能保证它可以被并行地正确计算。collect()在 driver 程序中，以一个 array 数组的形式返回 dataset 的所有元素。这在过滤器（filter）或其他操作（other operation）之后返回足够小（sufficiently small）的数据子集通常是有用的。count()返回 dataset 中元素的个数。first()返回 dataset 中的第一个元素（类似于 take(1)。take(n)将数据集中的前 n 个元素作为一个 array 数组返回。takeSample(withReplacement, num, [_seed_])对一个 dataset 进行随机抽样，返回一个包含 num 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。takeOrdered(n, [ordering])返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 n 个元素。saveAsTextFile(path)将 dataset 中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录。saveAsSequenceFile(path)(Java and Scala)将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int，Double，String 等等)。saveAsObjectFile(path)(Java and Scala)使用 Java 序列化（serialization）以简单的格式（simple format）编写数据集的元素，然后使用 SparkContext.objectFile() 进行加载。countByKey()仅适用于（K,V）类型的 RDD。返回具有每个 key 的计数的（K , Int）pairs 的 hashmap。foreach(func)对 dataset 中每个元素运行函数 _func_。这通常用于副作用（side effects），例如更新一个 Accumulator（累加器）或与外部存储系统（external storage systems）进行交互。Note：修改除 foreach()之外的累加器以外的变量（variables）可能会导致未定义的行为（undefined behavior）。详细介绍请阅读 Understanding closures（理解闭包） 部分。该 Spark RDD API 还暴露了一些 actions（操作）的异步版本，例如针对 foreach 的 foreachAsync，它们会立即返回一个FutureAction 到调用者，而不是在完成 action 时阻塞。这可以用于管理或等待 action 的异步执行。部分Transformation算子操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package rddimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object transformation_func &#123; def main(args: Array[String]): Unit =&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"transformation_func\") .setMaster(\"local\") val sc = new SparkContext(sparkConf) var original_rdd: RDD[(String, Int)] = sc.parallelize(Array((\"a\", 1), (\"b\", 1), (\"a\", 2),(\"c\",4),(\"c\",4)),2) var map_rdd: RDD[(String, Int)] = original_rdd.map(x =&gt;(x._1,x._2+1)) println(\"map操作：对original_rdd每个数据的第二个元素+1\") map_rdd.foreach(println) println(\"filter操作：过滤掉original_rdd中，第一个元素不为a的数据\") var filter_rdd: RDD[(String, Int)] = original_rdd.filter(x =&gt; x._1 == \"a\") filter_rdd.foreach(println) println(\"flatmap操作：对original_rdd做映射扁平化操作\") val flatmap_rdd: RDD[Char] = original_rdd.flatMap(x=&gt; x._1 + x._2) flatmap_rdd.foreach(println) println(\"mapPartitions操作：对original_rdd每个分区做相应操作\") // 效率要好于map 减少了发送到执行器的交互次数，但是可能会出现内存溢出 val mapPartitions_rdd: RDD[(String, Int)] = original_rdd.mapPartitions(x=&gt;&#123;x.map(item=&gt;(item._1,item._2+1))&#125;) mapPartitions_rdd.foreach(println) println(\"sample操作：提取样本\") val sample_rdd: RDD[(String, Int)] = original_rdd.sample(true, 0.25) sample_rdd.foreach(println) println(\"distinct操作：去重\") val distinct_rdd: RDD[(String, Int)] = original_rdd.distinct() distinct_rdd.foreach(println) println(\"groupbykey操作：分组聚合\") val groupByKey_rdd: RDD[(String, Iterable[Int])] = original_rdd.groupByKey() groupByKey_rdd.foreach(println) println(\"reduceByKey操作：聚合\") val reduceByKey_rdd: RDD[(String, Int)] = original_rdd.reduceByKey(_+_) reduceByKey_rdd.foreach(println) println(\"sortByKey操作：排序\") val sortByKey_rdd: RDD[(String, Int)] = original_rdd.sortByKey() sortByKey_rdd.foreach(println) &#125;&#125;RDD 的持久化（缓存）每当我们对 RDD 调用一个新的 action 操作时，整个 RDD 都会从头开始运算。因此，如果某个 RDD 会被反复重用的话，每次都从头计算非常低效，我们应该对多次使用的 RDD 进行一个持久化操作。Spark 的 persist() 和 cache() 方法支持将 RDD 的数据缓存至内存或硬盘中，这样当下次对同一 RDD 进行 Action 操作时，可以直接读取 RDD 的结果，大幅提高了 Spark 的计算效率。12345678rdd = sc.parallelize([1, 2, 3, 4, 5])rdd1 = rdd.map(lambda x: x+5)rdd2 = rdd1.filter(lambda x: x % 2 == 0)rdd2.persist()count = rdd2.count() // 3first = rdd2.first() // 6rdd2.unpersist()在文中的代码例子中你可以看到，我们对 RDD2 进行了多个不同的 action 操作。由于在第四行我把 RDD2 的结果缓存在内存中，所以 Spark 无需从一开始的 rdd 开始算起了（持久化处理过的 RDD 只有第一次有 action 操作时才会从源头计算，之后就把结果存储下来，所以在这个例子中，count 需要从源头开始计算，而 first 不需要）。在缓存 RDD 的时候，它所有的依赖关系也会被一并存下来。所以持久化的 RDD 有自动的容错机制。如果 RDD 的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算。持久化可以选择不同的存储级别。正如我们讲 RDD 的结构时提到的一样，有 MEMORY_ONLY，MEMORY_AND_DISK，DISK_ONLY 等。cache() 方法会默认取 MEMORY_ONLY 这一级别。RDD CheckpointCheckpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，但是在正常生产环境下放在 HDFS 上，这就天然的借助HDFS 高可靠的特征来完成最大化的可靠的持久化数据的方式。在进行 RDD 的 Checkpoint 的时候，其所依赖的所有 RDD 都会清空掉；官方建议如果要进行 checkpoint 时，必需先缓存在内存中。但实际可以考虑缓存在本地磁盘上或者是第三方组件，e.g. Taychon 上。在进行 checkpoint 之前需要通过 SparkConetxt 设置 checkpoint 的文件夹作为最佳实践，一般在进行 checkpoint 方法调用前都要进行 persists 来把当前 RDD 的数据持久化到内存或者是磁盘上，这是因为 checkpoint 是 lazy 级别，必需有 Job 的执行且在Job 执行完成后才会从后往前回溯哪个 RDD 进行了Checkpoint 标记，然后对该标记了要进行 Checkpoint 的 RDD 新启动一个Job 执行具体 Checkpoint 的过程RDD ShuffleSpark 里的某些操作会触发 shuffle。shuffle 是spark 重新分配数据的一种机制，使得这些数据可以跨不同的区域进行分组。这通常涉及在 executors 和 机器之间拷贝数据，这使得 shuffle 成为一个复杂的、代价高的操作。[Background](http://spark.apachecn.org/#/docs/4?id=background%ef%bc%88%e5%b9%95%e5%90%8e%ef%bc%89)为了明白 reduceByKey 操作的过程，我们以 reduceByKey 为例。reduceBykey 操作产生一个新的 RDD，其中 key 所有相同的的值组合成为一个 tuple - key 以及与 key 相关联的所有值在 reduce 函数上的执行结果。面临的挑战是，一个 key 的所有值不一定都在一个同一个 paritition 分区里，甚至是不一定在同一台机器里，但是它们必须共同被计算。在 spark 里，特定的操作需要数据不跨分区分布。在计算期间，一个任务在一个分区上执行，为了所有数据都在单个 reduceByKey 的 reduce 任务上运行，我们需要执行一个 all-to-all 操作。它必须从所有分区读取所有的 key 和 key对应的所有的值，并且跨分区聚集去计算每个 key 的结果 - 这个过程就叫做 shuffle。尽管每个分区新 shuffle 的数据集将是确定的，分区本身的顺序也是这样，但是这些数据的顺序是不确定的。如果希望 shuffle 后的数据是有序的，可以使用:mapPartitions 对每个 partition 分区进行排序，例如，.sortedrepartitionAndSortWithinPartitions 在分区的同时对分区进行高效的排序.sortBy 对 RDD 进行全局的排序触发的 shuffle 操作包括 repartition 操作，如 repartition 和 coalesce，‘ByKey 操作（除了 count 之外）像 groupByKey 和 reduceByKey，和 join 操作，像 cogroup 和 join.性能影响该 **Shuffle 是一个代价比较高的操作，它涉及磁盘 I/O、数据序列化、网络 I/O。为了准备 shuffle 操作的数据，Spark 启动了一系列的任务，map 任务组织数据，reduce 完成数据的聚合。这些术语来自 MapReduce，跟 Spark 的 map 操作和 reduce 操作没有关系。在内部，一个 map 任务的所有结果数据会保存在内存，直到内存不能全部存储为止。然后，这些数据将基于目标分区进行排序并写入一个单独的文件中。在 reduce 时，任务将读取相关的已排序的数据块。某些 shuffle 操作会大量消耗堆内存空间，因为 shuffle 操作在数据转换前后，需要在使用内存中的数据结构对数据进行组织。需要特别说明的是，reduceByKey 和 aggregateByKey 在 map 时会创建这些数据结构，&#39;ByKey 操作在 reduce 时创建这些数据结构。当内存满的时候，Spark 会把溢出的数据存到磁盘上，这将导致额外的磁盘 I/O 开销和垃圾回收开销的增加。shuffle 操作还会在磁盘上生成大量的中间文件。在 Spark 1.3 中，这些文件将会保留至对应的 RDD 不在使用并被垃圾回收为止。这么做的好处是，如果在 Spark 重新计算 RDD 的血统关系（lineage）时，shuffle 操作产生的这些中间文件不需要重新创建。如果 Spark 应用长期保持对 RDD 的引用，或者垃圾回收不频繁，这将导致垃圾回收的周期比较长。这意味着，长期运行 Spark 任务可能会消耗大量的磁盘空间。临时数据存储路径可以通过 SparkContext 中设置参数 spark.local.dir 进行配置。shuffle 操作的行为可以通过调节多个参数进行设置。详细的说明请看 Spark 配置指南 中的 “Shuffle 行为” 部分。Why use RDD首先，它的数据可以尽可能地存在内存中，从而大大提高的数据处理的效率；其次它是分区存储，所以天然支持并行处理；而且它还存储了每一步骤计算结果之间的依赖关系，从而大大提升了数据容错性和错误恢复的正确率，使 Spark 更加可靠。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 运行原理","slug":"Spark-运行原理","date":"2017-03-10T13:41:50.000Z","updated":"2020-04-14T17:27:41.606Z","comments":true,"path":"2017/03/10/Spark-运行原理/","link":"","permalink":"cpeixin.cn/2017/03/10/Spark-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/","excerpt":"","text":"spark的运行原理对于spark的学习尤为重要，如果不了解其运行原理，也就不会从根本上将spark的程序写好。这是一篇spark理论的文章，如果这篇文章并不能让你理解，可以先从实战入手，等你能简单的用spark写一个word count，需要提交任务在本地或者扔到服务器上的时候，再回来看看这篇文章，也许理解的程度就会更高一些。运行架构运行架构12345释义：• Cluster Manager：控制整个集群，监控worker。在standalone模式中即为Master主节点，在YARN模式中为ResourceManager• Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。• Driver： 运行Application 的main()函数• Executor：执行器，是为某个Application运行在worker node上的一个进程,Executor中有线程池任务运行流程构建Spark Application的运行环境，启动SparkContextSparkContext向资源管理器（Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackendExecutor向SparkContext申请TaskSparkContext将应用程序分发给ExecutorSparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行Task在Executor上运行，运行完释放所有资源运行特点**每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行Task。这种Application隔离机制是有优势的，无论是从调度角度看（每个Driver调度他自己的任务），还是从运行角度看（来自不同Application的Task运行在不同JVM中），当然这样意味着Spark Application不能跨应用程序共享数据，除非将数据写入外部存储系统Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了提交SparkContext的Client应该靠近Worker节点（运行Executor的节点），最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换Task采用了数据本地性和推测执行的优化机制释义：Application: Appliction都是指用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码Driver: Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用SparkContext代表DriverExecutor: 某个Application运行在worker节点上的一个进程， 该进程负责运行某些Task， 并且负责将数据存到内存或磁盘上，每个Application都有各自独立的一批Executor， 在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象， 负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task， 这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型Standalone : spark原生的资源管理，由Master负责资源的分配Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架Hadoop Yarn: 主要是指Yarn中的ResourceManagerWorker: 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NodeManager节点Task: 被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责Job: 包含多个Task组成的并行计算，往往由Spark Action触发生成， 一个Application中往往会产生多个JobStage: 每个Job会被拆分成多组Task， 作为一个TaskSet， 其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方DAGScheduler: 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图)，并提交Stage给TASkScheduler。 其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法TASKSedulter: 将TaskSET提交给worker运行，每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet，当Executor向Driver发生心跳时，TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行标签，重试失败的Task。下图展示了DAGScheduler， TaskScheduler的作用Job=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency1234在不同运行模式中任务调度器具体为： a. Spark on Standalone模式为TaskScheduler b. YARN-Client模式为YarnClientClusterScheduler c. YARN-Cluster模式为YarnClusterScheduler将这些术语串起来的运行层次图如下：![512251993-5b043bbda8a04_articlex.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586437820952-5268cd17-13cb-41d9-ba92-5bb1208b34b7.png#align=left&display=inline&height=449&name=512251993-5b043bbda8a04_articlex.png&originHeight=449&originWidth=559&size=113090&status=done&style=none&width=559) ### ### 运行模式Spark的运行模式多种多样，灵活多变，部署在单机上时，既可以用本地模式运行，也可以用伪分布模式运行，而当以分布式集群的方式部署时，也有众多的运行模式可供选择，这取决于集群的实际情况，底层的资源调度即可以依赖外部资源调度框架，也可以使用Spark内建的Standalone模式。对于外部资源调度框架的支持，目前的实现包括相对稳定的Mesos模式，以及hadoop YARN模式本地模式：常用于本地开发测试，本地还分别 local 和 local clusterstandalone: 独立集群运行模式**Standalone模式使用Spark自带的资源调度框架，采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA。框架结构图如下:该模式主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的Yarn模式运行：Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-ClusterYarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://xxxx:4040访问，而YARN通过http:// xxxx:8088访问YARN-client的工作流程步骤为：YARN-cluster的工作流程步骤Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己Spark Client 和 Spark Cluster的区别理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 初识","slug":"Spark-初识","date":"2017-03-09T11:27:07.000Z","updated":"2020-04-10T03:22:37.781Z","comments":true,"path":"2017/03/09/Spark-初识/","link":"","permalink":"cpeixin.cn/2017/03/09/Spark-%E5%88%9D%E8%AF%86/","excerpt":"","text":"这篇分享从介绍Spark开始，算是入门前的入门，不涉及原理，不涉及技术，但是却很重要，重要的是理解Spark是什么？用来做什么？为什么要选择Spark?这将对以后工程师们在选择和使用Spark的时候，更加能知道，应该怎样的去用Spark，在什么样的场景下去使用什么样的Spark组件，来发挥它的价值。What - SparkSpark是一个通用数据处理引擎。适用于各种环境 ，这里介绍一下，主要应用于两种最常见的场景离线场景：可以是时间为维度，几年的数据集，或者是业务为维度，某个领域的大数据集，这种数据可以我们一般叫做离线数据，或者冷数据。实时场景：网站埋点，实时从前端页面传输过来的数据，或者业务系统，物理硬件实时传输过来的数据，硬件信号或者图像数据，我们需要实时的去计算处理并且返回结果。应用程序开发人员和数据科学家将Spark纳入其应用程序，以便快速查询，分析和转换数据。所以与Spark关联最频繁的任务包括跨大型数据集的交互式查询，处理来自传感器或金融系统的流数据以及机器学习任务。Spark于2009年开始在加利福尼亚大学伯克利分校的AMPLab项目中生活。更具体地说，它是由于需要证明Meso的概念而诞生的，这也是在AMPLab中创建的。（科普：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核。）从一开始，Spark就被优化成在内存中运行。它比Hadoop的MapReduce等替代方法更快速地处理数据，这往往会在每个处理阶段之间向计算机硬盘驱动器写入数据。Spark的支持者声称，Spark在内存中的运行速度比Hadoop MapReduce快100倍，而且在处理基于Hadoop MapReduce本身的磁盘数据时速度也快了10倍。这种比较并不完全公平，这不仅仅是因为对于Spark的典型用例，原始速度往往比批处理更重要，在这种情况下类似MapReduce的解决方案仍然非常出色。基于Hadoop基于YARN的体系结构为Spark和其他应用程序共享通用集群和数据集提供了基础，同时确保一致的服务和响应级别。下图是构成Spark的生态系统，强大的生态系统：其中 Spark Core是Spark的核心API，并且支持Scala,Python,Java编程语言，R,SQL分析语言,在以Spark Core为基础之上，有着Spark SQL，Spark Streaming，Spark Mlib,Spark Graphx四个亲儿子。这四个组件，我将在之后的文章详细的介绍，并且会从应用和代码示例来讲解。Spark do WhatSpark能够一次处理数PB的数据，分布在数千个协作的物理或虚拟服务器集群中。（科普：什么是分布式计算？所谓分布式计算是一门计算机科学，它研究如何把一个需要非常巨大的计算能力才能解决的问题分成许多小的部分，然后把这些部分分配给许多计算机进行处理，最后把这些计算结果综合起来得到最终的结果。分布式网络存储技术是将数据分散的存储于多台独立的机器设备上。分布式网络存储系统采用 可扩展的系统结构，利用多台存储服务器分担存储负荷，利用位置服务器定位存储信息，不但解决了传统集中式存储系统中单存储服务器的瓶颈问题，还提高了系统的可靠性、可用性和扩展性。）它有一套广泛的开发者库和API，并且支持Java，Python，R和Scala等语言; (现在在写spark应用程序时，最长使用的是Scala语言，因为spark的源码就是用scala来编写的，再其次就是python语言，python的第三方库很多，节省了程序员很多的时间要去自己实现某些功能，这两个语言的语法都很简洁，上手简单，支持函数式编程)它的灵活性使其非常适合于各种用例。Spark通常与Hadoop的数据存储模块HDFS一起使用，但它也可以与HBase，Cassandra，MapR-DB，MongoDB和Amazon S3 等其他流行的数据存储子系统集成，并且可以和Kafka，Flume等数据传输队列和数据采集工具一起搭配使用。Who Use SparkSpark是为数据科学设计的，其抽象使数据科学变得更加简单。数据科学家通常使用机器学习 - 一套可以从数据中学习的技术和算法。这些算法通常是迭代的，Spark将数据集缓存在内存中的能力大大加快了迭代数据处理速度，使得Spark成为实现这种算法的理想处理引擎。Spark是为大数据工程师设计的，在强大的计算能力和优秀的架构设计面前，可以让数据工程师在不管是离线情景下还是实时的业务需求下，都可以放心的选择使用Spark,一次读取，并行化处理，对数据集支持容错，操作灵活性，第三方社区的积极支持，虽然Spark还面对着缺点，但我相信Spark的明天会更好。Why Use Spark在技术不断高速更迭的程序圈，一个新工具的出现与流行，必然是因为它满足了很大一部分人长期未被满足的需求，或是解决了一个长期让很多人难受的痛点。这里就不能不提MapReduce了，既然已经有了看似很成熟的 Hadoop 和 MapReduce，为什么我们还需要 Spark？MapReduce 被硅谷一线公司淘汰的两大主要原因：高昂的维护成本、时间性能“达不到”用户的期待。除此之外，MapReduce 模型的抽象层次低，大量的底层逻辑都需要开发者手工完成。只提供 Map 和 Reduce 两个操作。在 Hadoop 中，每一个 Job 的计算结果都会存储在 HDFS 文件存储系统中，所以每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟。由于这一原因，MapReduce 对于迭代算法的处理性能很差，而且很耗资源。因为迭代的每一步都要对 HDFS 进行读写，所以每一步都需要差不多的等待时间。第四，只支持批数据处理，欠缺对流数据处理的支持。因此，在 Hadoop 推出后，有很多人想办法对 Hadoop 进行优化，其中发展到现在最成熟的就是 Spark。选择Spark有很多原因，但三个关键：简单性：Spark的功能可以通过一组丰富的API来访问，所有这些都是专门为大规模数据快速轻松地交互而设计的。这些API都有详细的文档和结构，使数据科学家和应用程序开发人员能够快速地将Spark工作。速度：Spark是为速度而设计的，可以在内存和磁盘上运行。来自Databricks的团队使用Spark在2014年Daytona Grey Sort 100TB Benchmark挑战赛中与加利福尼亚大学圣地亚哥分校的一队队员并列第一名。挑战包括处理静态数据集; Databricks团队能够在23分钟内处理存储在固态硬盘上的100TB的数据，而之前的获胜者通过使用Hadoop和不同的集群配置需要72分钟的时间。在支持存储在内存中的数据的交互式查询时，Spark可以执行得更好。在这种情况下，有人声称Spark可以比Hadoop的MapReduce快100倍。关于速度来好好讲解一下，Spark以速度为出名，所以要把Spark为什么这么快来聊一聊由于 Spark 可以把迭代过程中每一步的计算结果都缓存在内存中，所以非常适用于各类迭代算法。Spark 第一次启动时需要把数据载入到内存，之后的迭代可以直接在内存里利用中间结果做不落地的运算。所以，后期的迭代速度快到可以忽略不计。在当今机器学习和人工智能大热的环境下，Spark 无疑是更好的数据处理引擎。下图是在 Spark 和 Hadoop 上运行逻辑回归算法的运行时间对比。在任务（task）级别上，Spark 的并行机制是多线程模型，而 MapReduce 是多进程模型。多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间。而 Spark 同一节点上的任务以多线程的方式运行在一个 JVM 进程中，可以带来更快的启动速度、更高的 CPU 利用率，以及更好的内存共享。从前文中你可以看出，Spark 作为新的分布式数据处理引擎，对 MapReduce 进行了很多改进，使得性能大大提升，并且更加适用于新时代的数据处理场景。支持：Spark支持一系列编程语言，包括Java，Python，R和Scala。尽管通常与HDFS密切相关，但Spark还包括对Hadoop生态系统及其以后的许多领先存储解决方案的紧密集成的本地支持。此外，Apache Spark社区是大型的，活跃的和国际性的。包括Databricks，IBM以及所有主要Hadoop供应商在内的不断增长的商业提供商为Spark解决方案提供全面的支持。Spark Use Case随着 Apache Spark的发展势头继续增长，几乎所有一站式大数据平台都早已集成Spark,国外最为著名的CDH,HDP，国内的TDH等，所有行业用于实际应用。正在使用Spark来改善他们的业务，通过检测模式和提供可操作的洞察力来推动组织变革，并开始改变生活的某些方面。下面提供了一些从保险到互联网公司如何使用Spark的例子：保险行业： 通过使用Spark的机器学习功能来处理和分析所有索赔，优化索赔报销流程。医疗保健： 使用Spark Core，Streaming和SQL构建病人护理系统。零售业 ： 使用Spark分析销售点数据和优惠券使用情况。互联网 ： 使用Spark的ML功能来识别虚假的配置文件，并增强他们向客户展示的产品匹配。银行业 ： 使用机器学习模型来预测某些金融产品的零售银行客户的资料。政府 ： 分析地理，时间和财政支出。科学研究 ： 通过时间，深度，地理分析地震事件来预测未来的事件。投资银行 ： 分析日内股价以预测未来的价格走势。地理空间分析： 按时间和地理分析Uber旅行，以预测未来的需求和定价。Twitter情绪分析： 分析大量的推文，以确定特定组织和产品的积极，消极或中立的情绪。航空公司 ： 建立预测航空旅行延误的模型。设备 ： 预测建筑物超过临界温度的可能性。上面所举的应用实例是想让大家更直接的去理解，Spark到底在实际的生产环境中能带来什么样的作用和发挥什么样的价值，对以后的学习，能更好的指导方向！最后来纠正一个不正确的观点，貌似很多技术论坛和网站上都有一些标题党在说“Spark是Hadoop的代替者”，“Hadoop被Spark终结”等类似标题的文章，内行的人一看就是脑残一样的标题😄，Spark 并不是一个完全替代 Hadoop 的全新工具。因为 Hadoop 还包含了很多组件：数据存储层：分布式文件存储系统 HDFS，分布式数据库存储的 HBase；数据处理层：进行数据处理的 MapReduce，负责集群和资源管理的 YARN；数据访问层：Hive、Pig、Mahout……从狭义上来看，Spark 只是 MapReduce 的替代方案，大部分应用场景中，它还要依赖于 HDFS 和 HBase 来存储数据，依赖于 YARN 来管理集群和资源。当然，Spark 并不是一定要依附于 Hadoop 才能生存，它还可以运行在 Apache Mesos、Kubernetes、standalone 等其他云平台上。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"【转载】数据结构和算法的思维框架","slug":"【转载】数据结构和算法的思维框架","date":"2017-02-20T03:34:45.000Z","updated":"2020-05-20T03:39:02.516Z","comments":true,"path":"2017/02/20/【转载】数据结构和算法的思维框架/","link":"","permalink":"cpeixin.cn/2017/02/20/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%9D%E7%BB%B4%E6%A1%86%E6%9E%B6/","excerpt":"","text":"这是好久之前的一篇文章「学习数据结构和算法的框架思维」的修订版。之前那篇文章收到广泛好评，没看过也没关系，这篇文章会涵盖之前的所有内容，并且会举很多代码的实例，教你如何使用框架思维。首先，这里讲的都是普通的数据结构，咱不是搞算法竞赛的，野路子出生，我只会解决常规的问题。另外，以下是我个人的经验的总结，没有哪本算法书会写这些东西，所以请读者试着理解我的角度，别纠结于细节问题，因为这篇文章就是希望对数据结构和算法建立一个框架性的认识。从整体到细节，自顶向下，从抽象到具体的框架思维是通用的，不只是学习数据结构和算法，学习其他任何知识都是高效的。一、数据结构的存储方式数据结构的存储方式只有两种：数组（顺序存储）和链表（链式存储）。这句话怎么理解，不是还有散列表、栈、队列、堆、树、图等等各种数据结构吗？我们分析问题，一定要有递归的思想，自顶向下，从抽象到具体。你上来就列出这么多，那些都属于「上层建筑」，而数组和链表才是「结构基础」。因为那些多样化的数据结构，究其源头，都是在链表或者数组上的特殊操作，API 不同而已。比如说「队列」、「栈」这两种数据结构既可以使用链表也可以使用数组实现。用数组实现，就要处理扩容缩容的问题；用链表实现，没有这个问题，但需要更多的内存空间存储节点指针。「图」的两种表示方法，邻接表就是链表，邻接矩阵就是二维数组。邻接矩阵判断连通性迅速，并可以进行矩阵运算解决一些问题，但是如果图比较稀疏的话很耗费空间。邻接表比较节省空间，但是很多操作的效率上肯定比不过邻接矩阵。「散列表」就是通过散列函数把键映射到一个大数组里。而且对于解决散列冲突的方法，拉链法需要链表特性，操作简单，但需要额外的空间存储指针；线性探查法就需要数组特性，以便连续寻址，不需要指针的存储空间，但操作稍微复杂些。「树」，用数组实现就是「堆」，因为「堆」是一个完全二叉树，用数组存储不需要节点指针，操作也比较简单；用链表实现就是很常见的那种「树」，因为不一定是完全二叉树，所以不适合用数组存储。为此，在这种链表「树」结构之上，又衍生出各种巧妙的设计，比如二叉搜索树、AVL 树、红黑树、区间树、B 树等等，以应对不同的问题。了解 Redis 数据库的朋友可能也知道，Redis 提供列表、字符串、集合等等几种常用数据结构，但是对于每种数据结构，底层的存储方式都至少有两种，以便于根据存储数据的实际情况使用合适的存储方式。综上，数据结构种类很多，甚至你也可以发明自己的数据结构，但是底层存储无非数组或者链表，二者的优缺点如下：数组由于是紧凑连续存储,可以随机访问，通过索引快速找到对应元素，而且相对节约存储空间。但正因为连续存储，内存空间必须一次性分配够，所以说数组如果要扩容，需要重新分配一块更大的空间，再把数据全部复制过去，时间复杂度 O(N)；而且你如果想在数组中间进行插入和删除，每次必须搬移后面的所有数据以保持连续，时间复杂度 O(N)。链表**因为元素不连续，而是靠指针指向下一个元素的位置，所以不存在数组的扩容问题；如果知道某一元素的前驱和后驱，操作指针即可删除该元素或者插入新元素，时间复杂度 O(1)。但是正因为存储空间不连续，你无法根据一个索引算出对应元素的地址，所以不能随机访问；而且由于每个元素必须存储指向前后元素位置的指针，会消耗相对更多的储存空间。二、数据结构的基本操作对于任何数据结构，其基本操作无非遍历 + 访问，再具体一点就是：增删查改。数据结构种类很多，但它们存在的目的都是在不同的应用场景，尽可能高效地增删查改**。话说这不就是数据结构的使命么？如何遍历 + 访问？我们仍然从最高层来看，各种数据结构的遍历 + 访问无非两种形式：线性的和非线性的。线性就是 for/while 迭代为代表，非线性就是递归为代表。再具体一步，无非以下几种框架：数组遍历框架，典型的线性迭代结构：12345void traverse(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; // 迭代访问 arr[i] &#125;&#125;链表遍历框架，兼具迭代和递归结构：1234567891011121314/* 基本的单链表节点 */class ListNode &#123; int val; ListNode next;&#125;void traverse(ListNode head) &#123; for (ListNode p = head; p != null; p = p.next) &#123; // 迭代访问 p.val &#125;&#125;void traverse(ListNode head) &#123; // 递归访问 head.val traverse(head.next)&#125;二叉树遍历框架，典型的非线性递归遍历结构：123456789/* 基本的二叉树节点 */class TreeNode &#123; int val; TreeNode left, right;&#125;void traverse(TreeNode root) &#123; traverse(root.left) traverse(root.right)&#125;你看二叉树的递归遍历方式和链表的递归遍历方式，相似不？再看看二叉树结构和单链表结构，相似不？如果再多几条叉，N 叉树你会不会遍历？二叉树框架可以扩展为 N 叉树的遍历框架：123456789/* 基本的 N 叉树节点 */class TreeNode &#123; int val; TreeNode[] children;&#125;void traverse(TreeNode root) &#123; for (TreeNode child : root.children) traverse(child)&#125;N 叉树的遍历又可以扩展为图的遍历，因为图就是好几 N 叉棵树的结合体。你说图是可能出现环的？这个很好办，用个布尔数组 visited 做标记就行了，这里就不写代码了。所谓框架，就是套路。不管增删查改，这些代码都是永远无法脱离的结构，你可以把这个结构作为大纲，根据具体问题在框架上添加代码就行了，下面会具体举例**。三、算法刷题指南首先要明确的是，数据结构是工具，算法是通过合适的工具解决特定问题的方法。也就是说，学习算法之前，最起码得了解那些常用的数据结构，了解它们的特性和缺陷。那么该如何在 LeetCode 刷题呢？之前的文章算法学习之路写过一些，什么按标签刷，坚持下去云云。现在距那篇文章已经过去将近一年了，我不说那些不痛不痒的话，直接说具体的建议：先刷二叉树，先刷二叉树，先刷二叉树！这是我这刷题一年的亲身体会，下图是去年十月份的提交截图：公众号文章的阅读数据显示，大部分人对数据结构相关的算法文章不感兴趣，而是更关心动规回溯分治等等技巧。为什么要先刷二叉树呢，因为二叉树是最容易培养框架思维的，而且大部分算法技巧，本质上都是树的遍历问题。刷二叉树看到题目没思路？根据很多读者的问题，其实大家不是没思路，只是没有理解我们说的「框架」是什么。不要小看这几行破代码，几乎所有二叉树的题目都是一套这个框架就出来了**。1234567void traverse(TreeNode root) &#123; // 前序遍历 traverse(root.left) // 中序遍历 traverse(root.right) // 后序遍历&#125;比如说我随便拿几道题的解法出来，不用管具体的代码逻辑，只要看看框架在其中是如何发挥作用的就行。LeetCode 124 题，难度 Hard，让你求二叉树中最大路径和，主要代码如下：12345678int ans = INT_MIN;int oneSideMax(TreeNode* root) &#123; if (root == nullptr) return 0; int left = max(0, oneSideMax(root-&gt;left)); int right = max(0, oneSideMax(root-&gt;right)); ans = max(ans, left + right + root-&gt;val); return max(left, right) + root-&gt;val;&#125;你看，这就是个后序遍历嘛。LeetCode 105 题，难度 Medium，让你根据前序遍历和中序遍历的结果还原一棵二叉树，很经典的问题吧，主要代码如下：123456789101112TreeNode buildTree(int[] preorder, int preStart, int preEnd, int[] inorder, int inStart, int inEnd, Map&lt;Integer, Integer&gt; inMap) &#123; if(preStart &gt; preEnd || inStart &gt; inEnd) return null; TreeNode root = new TreeNode(preorder[preStart]); int inRoot = inMap.get(root.val); int numsLeft = inRoot - inStart; root.left = buildTree(preorder, preStart + 1, preStart + numsLeft, inorder, inStart, inRoot - 1, inMap); root.right = buildTree(preorder, preStart + numsLeft + 1, preEnd, inorder, inRoot + 1, inEnd, inMap); return root;&#125;不要看这个函数的参数很多，只是为了控制数组索引而已，本质上该算法也就是一个前序遍历。LeetCode 99 题，难度 Hard，恢复一棵 BST，主要代码如下：12345678910void traverse(TreeNode* node) &#123; if (!node) return; traverse(node-&gt;left); if (node-&gt;val &lt; prev-&gt;val) &#123; s = (s == NULL) ? prev : s; t = node; &#125; prev = node; traverse(node-&gt;right);&#125;这不就是个中序遍历嘛，对于一棵 BST 中序遍历意味着什么，应该不需要解释了吧。你看，Hard 难度的题目不过如此，而且还这么有规律可循，只要把框架写出来，然后往相应的位置加东西就行了，这不就是思路吗。对于一个理解二叉树的人来说，刷一道二叉树的题目花不了多长时间。那么如果你对刷题无从下手或者有畏惧心理，不妨从二叉树下手，前 10 道也许有点难受；结合框架再做 20 道，也许你就有点自己的理解了；刷完整个专题，再去做什么回溯动规分治专题，你就会发现只要涉及递归的问题，都是树的问题。再举例吧，说几道我们之前文章写过的问题。动态规划详解说过凑零钱问题，暴力解法就是遍历一棵 N 叉树：12345678910111213def coinChange(coins: List[int], amount: int): def dp(n): if n == 0: return 0 if n &lt; 0: return -1 res = float('INF') for coin in coins: subproblem = dp(n - coin) # 子问题无解，跳过 if subproblem == -1: continue res = min(res, 1 + subproblem) return res if res != float('INF') else -1 return dp(amount)这么多代码看不懂咋办？直接提取出框架，就能看出核心思路了：1234# 不过是一个 N 叉树的遍历问题而已def dp(n): for coin in coins: dp(n - coin)其实很多动态规划问题就是在遍历一棵树，你如果对树的遍历操作烂熟于心，起码知道怎么把思路转化成代码，也知道如何提取别人解法的核心思路。再看看回溯算法，前文回溯算法详解干脆直接说了，回溯算法就是个 N 叉树的前后序遍历问题，没有例外。比如 N 皇后问题吧，主要代码如下：12345678910111213141516171819void backtrack(int[] nums, LinkedList&lt;Integer&gt; track) &#123; if (track.size() == nums.length) &#123; res.add(new LinkedList(track)); return; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; if (track.contains(nums[i])) continue; track.add(nums[i]); // 进入下一层决策树 backtrack(nums, track); track.removeLast(); &#125;/* 提取出 N 叉树遍历框架 */void backtrack(int[] nums, LinkedList&lt;Integer&gt; track) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; backtrack(nums, track);&#125;N 叉树的遍历框架，找出来了把～你说，树这种结构重不重要？综上，对于畏惧算法的朋友来说，可以先刷树的相关题目，试着从框架上看问题，而不要纠结于细节问题。纠结细节问题，就比如纠结 i 到底应该加到 n 还是加到 n - 1，这个数组的大小到底应该开 n 还是 n + 1 ？从框架上看问题，就是像我们这样基于框架进行抽取和扩展，既可以在看别人解法时快速理解核心逻辑，也有助于找到我们自己写解法时的思路方向。当然，如果细节出错，你得不到正确的答案，但是只要有框架，你再错也错不到哪去，因为你的方向是对的。但是，你要是心中没有框架，那么你根本无法解题，给了你答案，你也不会发现这就是个树的遍历问题。这种思维是很重要的，动态规划详解中总结的找状态转移方程的几步流程，有时候按照流程写出解法，说实话我自己都不知道为啥是对的，反正它就是对了。。。这就是框架的力量，能够保证你在快睡着的时候，依然能写出正确的程序；就算你啥都不会，都能比别人高一个级别。四、总结几句数据结构的基本存储方式就是链式和顺序两种，基本操作就是增删查改，遍历方式无非迭代和递归。刷算法题建议从「树」分类开始刷，结合框架思维，把这几十道题刷完，对于树结构的理解应该就到位了。这时候去看回溯、动规、分治等算法专题，对思路的理解可能会更加深刻一些。转载自：labuladong / fucking-algorithm","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Scala assert()断言函数","slug":"Scala-assert-断言函数","date":"2017-02-03T16:46:14.000Z","updated":"2020-05-18T16:47:47.992Z","comments":true,"path":"2017/02/04/Scala-assert-断言函数/","link":"","permalink":"cpeixin.cn/2017/02/04/Scala-assert-%E6%96%AD%E8%A8%80%E5%87%BD%E6%95%B0/","excerpt":"","text":"Scala assert()断言函数assert源码assert的作用是现计算表达式 assertion ，如果其值为假（即为0），则抛出异常，终止程序运行。常用在单元测试中进行值或者条件的判断调试，例如在编程中，我们在手动测试时，可以不用写大量的if else以下是使用断言的几个原则：（1）使用断言捕捉不应该发生的非法情况。不要混淆非法情况与错误情况之间的区别，后者是必然存在的并且是一定要作出处理的。（2）使用断言对函数的参数进行确认。（3）在编写函数时，要进行反复的考查，并且自问：“我打算做哪些假定？”一旦确定了的假定，就要使用断言对假定进行检查。（4）一般教科书都鼓励程序员们进行防错性的程序设计，但要记住这种编程风格会隐瞒错误。当进行防错性编程时，如果“不可能发生”的事情的确发生了，则要使用断言进行报警。ASSERT ()是一个调试程序时经常使用的宏，在程序运行时它计算括号内的表达式，如果表达式为FALSE (0), 程序将报告错误，并终止执行。如果表达式不为0，则继续执行后面的语句。这个宏通常原来判断程序中是否出现了明显非法的数据，如果出现了终止程序以免导致严重后果，同时也便于查找错误。ASSERT只有在Debug版本中才有效，如果编译为Release版本则被忽略。比较好的在程序中使用assert的地方：(1)空指针检查。例如，针对一个函数的参数进行空指针检查。当出现空指针时，你的程序就会退出，并很好的给出错误信息。(2)检查函数参数的值。例如，如果一个函数只能在它的一个参数啊为正值的时候被调用，你可以在函数开始时这样写:assert (a &gt; 0);，这将帮助你检测函数的错误使用，这也给源代码阅读者很清晰的印象，那就是在这里对函数的参数值有限制。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 方法和函数","slug":"Scala-方法和函数","date":"2017-01-04T09:23:59.000Z","updated":"2020-04-10T03:22:41.218Z","comments":true,"path":"2017/01/04/Scala-方法和函数/","link":"","permalink":"cpeixin.cn/2017/01/04/Scala-%E6%96%B9%E6%B3%95%E5%92%8C%E5%87%BD%E6%95%B0/","excerpt":"","text":"Scala 方法和函数方法与函数Scala 有方法与函数，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。Scala 中的方法跟 Java 的类似，方法是组成类的一部分。Scala 中的函数则是一个完整的对象，Scala 中的函数其实就是继承了 Trait 的类的对象。Scala 中使用 val 语句可以定义函数，def 语句定义方法。方法定义方法定义由一个 def 关键字开始，紧接着是可选的参数列表，一个冒号 : 和方法的返回类型，一个等于号 = ，最后是方法的主体。定义格式如下：1234def functionName ([参数列表]) : [return type] = &#123; function body return [expr]&#125;以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。以下方法的功能是将两个传入的参数相加并求和：1234567object add&#123; def addInt( a:Int, b:Int ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;函数Scala 也是一种函数式语言，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程，定义方式上，和scala方法是一样的。传值函数&amp;传名函数**Scala的解释器在解析函数参数(function arguments)时有两种方式：先计算参数表达式的值(reduce the arguments)，再应用到函数内部；或者是将未计算的参数表达式直接应用到函数内部。前者叫做传值调用（call-by-value），后者叫做传名调用（call-by-name）123456789101112131415object Test &#123; def main(args: Array[String]) &#123; delayed(time()); &#125; def time() = &#123; println(\"获取时间，单位为纳秒\") System.nanoTime &#125; def delayed( t: =&gt; Long ) = &#123; println(\"在 delayed 方法内\") println(\"参数： \" + t) 此时计算 time()函数 t &#125;&#125;注：scala函数体和方法体中的最后一行为返回值结果：1234在 delayed 方法内获取时间，单位为纳秒参数： 241550840475831获取时间，单位为纳秒=&gt; Unit 与 () =&gt;Unit的区别简单来说, =&gt; Unit是 传名函数, 只传入了一个表达式, 在调用时才会去执行, 使用 code调用() =&gt; 是传值函数, 传入的计算后的值，示例例如：1234567def function_1(t: () =&gt; Long): Unit = &#123; xxxx &#125; def function_2(t: =&gt; Long): Unit = &#123; xxxx &#125;指定函数参数名**不管在用什么语言在进行开发的过程中，对于函数的传参，我们几乎都是按照函数定义中，参数的顺序进行传参的，但是在Scala中会灵活一些，我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下：123456789object Test &#123; def main(args: Array[String]) &#123; printInt(b=5, a=7); &#125; def printInt( a:Int, b:Int ) = &#123; println(\"Value of a : \" + a ); println(\"Value of b : \" + b ); &#125;&#125;可变参数**Scala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。Scala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：123456789101112object Test &#123; def main(args: Array[String]) &#123; printStrings(\"Runoob\", \"Scala\", \"Python\"); &#125; def printStrings( args:String* ) = &#123; var i : Int = 0; for( arg &lt;- args )&#123; println(\"Arg value[\" + i + \"] = \" + arg ); i = i + 1; &#125; &#125;&#125;**递归函数**12345678910111213object Test &#123; def main(args: Array[String]) &#123; for (i &lt;- 1 to 10) println(i + \" 的阶乘为: = \" + factorial(i) ) &#125; def factorial(n: BigInt): BigInt = &#123; if (n &lt;= 1) 1 else n * factorial(n - 1) &#125;&#125;默认参数值Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：12345678910object Test &#123; def main(args: Array[String]) &#123; println( \"返回值 : \" + addInt() ); &#125; def addInt( a:Int=5, b:Int=7 ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;高阶函数高阶函数（Higher-Order Function）就是操作其他函数的函数。Scala 中允许使用高阶函数, 高阶函数可以使用其他函数作为参数，或者使用函数作为输出结果。以下实例中，apply() 函数使用了另外一个函数 f 和 值 v 作为参数，而函数 f 又调用了参数 v：**123456789101112object Test &#123; def main(args: Array[String]) &#123; println( apply( layout, 10) ) &#125; // 函数 f 和 值 v 作为参数，而函数 f 又调用了参数 v def apply(f: Int =&gt; String, v: Int) = f(v) def layout[A](x: A) = \"[\" + x.toString() + \"]\" &#125;匿名函数**匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。使用匿名函数后，我们的代码变得更简洁了。下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:1var inc = (x:Int) =&gt; x+1上述定义的匿名函数，其实是下面这种写法的简写：123def add2 = new Function1[Int,Int]&#123; def apply(x:Int):Int = x+1; &#125;以上实例的 inc 现在可作为一个函数，使用方式如下：1var x = inc(7)-1同样我们可以在匿名函数中定义多个参数：1var mul = (x: Int, y: Int) =&gt; x*ymul 现在可作为一个函数，使用方式如下：1println(mul(3, 4))我们也可以不给匿名函数设置参数，如下所示：1var userDir = () =&gt; &#123; System.getProperty(\"user.dir\") &#125;userDir 现在可作为一个函数，使用方式如下：1println( userDir() )偏应用函数偏应用函数也是一个蛮有意思的用法，**Scala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。1234567891011121314import java.util.Dateobject Test &#123; def main(args: Array[String]) &#123; val date = new Date log(date, \"message1\" ) Thread.sleep(1000) log(date, \"message2\" ) Thread.sleep(1000) log(date, \"message3\" ) &#125; def log(date: Date, message: String) = &#123; println(date + \"----\" + message) &#125;&#125;执行以上代码，输出结果为：12345$ scalac Test.scala$ scala TestMon Dec 02 12:52:41 CST 2018----message1Mon Dec 02 12:52:41 CST 2018----message2Mon Dec 02 12:52:41 CST 2018----message3实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。以上实例修改如下：123456789101112131415import java.util.Dateobject Test &#123; def main(args: Array[String]) &#123; val date = new Date val logWithDateBound = log(date, _ : String) logWithDateBound(\"message1\" ) Thread.sleep(1000) logWithDateBound(\"message2\" ) Thread.sleep(1000) logWithDateBound(\"message3\" ) &#125; def log(date: Date, message: String) = &#123; println(date + \"----\" + message) &#125;&#125;执行以上代码，输出结果为和上面的是一样的。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 基本语法","slug":"Scala-基本语法","date":"2017-01-03T09:21:58.000Z","updated":"2020-04-10T03:22:44.338Z","comments":true,"path":"2017/01/03/Scala-基本语法/","link":"","permalink":"cpeixin.cn/2017/01/03/Scala-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/","excerpt":"","text":"数据类型Scala 与 Java有着相同的数据类型，下表列出了 Scala 支持的数据类型：上表中列出的数据类型都是对象，也就是说scala没有java中的原生类型。在scala是可以对数字等基础类型调用方法的。这里就不多说了，其中需要注意的一点，就是字符与字符串的表示，因为平常使用python编程也是比较多的，在python中字符串是用单引号表示的，所以切换回scala，总会出现使用单引号定义字符串的情况在 Scala 字符变量使用单引号 ‘ 来定义，如下：‘a’而字符串字面量使用双引号 “ 来定义，如下：“Hello,World!”变量定义声明变量实例如下：1var myVar : String = \"Foo\"声明常量实例如下：1val myVal : String = \"Foo\"以上定义了常量 myVal，它是不能修改的。如果程序尝试修改常量 myVal 的值，程序将会在编译时报错。变量类型声明变量的类型在变量名之后的 ：声明。定义变量的类型的语法格式如下：12345var VariableName : DataType = Value或val VariableName : DataType = Value在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。上图则为省略变量类型，scala根据初始值进行判断。但是在项目开发中，建议大家不要省略变量类型。访问修饰符Scala 访问修饰符基本和Java的一样，分别有：private，protected，public。如果没有指定访问修饰符，默认情况下，Scala 对象的访问级别都是 public。Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。条件语句1234567891011121314151617181920if(布尔表达式)&#123; // 如果布尔表达式为 true 则执行该语句块&#125;if(布尔表达式)&#123; // 如果布尔表达式为 true 则执行该语句块&#125;else&#123; // 如果布尔表达式为 false 则执行该语句块&#125;if(布尔表达式 1)&#123; // 如果布尔表达式 1 为 true 则执行该语句块&#125;else if(布尔表达式 2)&#123; // 如果布尔表达式 2 为 true 则执行该语句块&#125;else if(布尔表达式 3)&#123; // 如果布尔表达式 3 为 true 则执行该语句块&#125;else &#123; // 如果以上条件都为 false 执行该语句块&#125;循环语句这里就不描述 while() do while() 语句了和其他语言基本一致，这里只描述和其他语言有差异的for循环语法如下：1234567891011for( var x &lt;- Range )&#123; statement(s);&#125;以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值。for( var x &lt;- List )&#123; statement(s);&#125;以上语法中， List 变量是一个集合，for 循环会迭代所有集合的元素。yield：你可以利用yield 将 for 循环中符合条件的值作为一个变量存储。语法格式如下：12345678910111213141516object Test &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6,7,8,9,10); // for 循环 var retVal = for&#123; a &lt;- numList if a != 3; if a &lt; 8 &#125;yield a // 输出返回值 for( a &lt;- retVal)&#123; println( \"Value of a: \" + a ); &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 创建程序文件","slug":"Scala-创建程序文件","date":"2017-01-02T09:20:44.000Z","updated":"2020-04-10T03:22:51.424Z","comments":true,"path":"2017/01/02/Scala-创建程序文件/","link":"","permalink":"cpeixin.cn/2017/01/02/Scala-%E5%88%9B%E5%BB%BA%E7%A8%8B%E5%BA%8F%E6%96%87%E4%BB%B6/","excerpt":"","text":"Scala 是一门类 Java 的编程语言，它结合了面向对象编程和函数式编程。Scala 是纯面向对象的，每个值都是一个对象，对象的类型和行为由类定义，不同的类可以通过混入(mixin)的方式组合在一起。Scala 的设计目的是要和两种主流面向对象编程语言Java 和 C#实现无缝互操作，这两种主流语言都非纯面向对象。Scala 也是一门函数式变成语言，每个函数都是一个值，原生支持嵌套函数定义和高阶函数。Scala 也支持一种通用形式的模式匹配，模式匹配用来操作代数式类型，在很多函数式语言中都有实现。Scala 被设计用来和 Java 无缝互操作（另一个修改的 Scala 实现可以工作在.NET上）。Scala 类可以调用 Java 方法，创建 Java 对象，继承 Java 类和实现 Java 接口。这些都不需要额外的接口定义或者胶合代码。前言我相信大多数接触scala的工程师，都是因为接触到了大数据技术栈，而被迫去学习scala的。scala这门语言完全可以理解成因为其开源项目的火爆，而将这门语言带到了大众的视野，例如计算框架Spark，消息队列Kafka， akka，使用Scala编写Spark程序真的是太便利了，简洁的函数编程让你欲罢不能，那么从另一方面来说，如果有一天，Spark被大数据技术栈淘汰了，那么Scala对于大数据工程师还是硬需求么？从现在来看，异军突起的Flink实时计算框架也是支持Scala的，同样相对于Java和python来说，Scala都有独特迷人的地方。创建文件对于有其他语言基础的工程师来讲，新学一门语言并不难，那么这里我就先写一些简明扼要的，工程师看完就可以直接创建文件，编写hello word就可以运行的。首先，我假定你已经配置好了scala的环境，编译器IDEA也已经配置完毕了，在选择好的目录中可以创建Scala文件了，如下图，我们该怎么选择呢？简单的来说：类class里无static类型，类里的属性和方法，必须通过new出来的对象来调用，所以有main主函数也没用。而object的特点是：可以拥有属性和方法，且默认都是”static”类型，可以直接用object名直接调用属性和方法，不需要通过new出来的对象（也不支持）。object里的main函数式应用程序的入口。object和class有很多和class相同的地方，可以extends父类或Trait，但object不可以extends object，即object无法作为父类。Scala的Trait相当于Java里的Interface根据上面的红字，我们也可以知道，如果想创建文件，并且运行文件，我们需要选择Object来写main()函数main函数怎么写呢？1def main(args: Array[String]) - Scala程序从main()方法开始处理，这是每一个Scala程序的强制程序入口部分。打印 hello word12345object helloword &#123; def main(args: Array[String]): Unit = &#123; println(\"hello word\") &#125;&#125;假如现在你对scala其他语法一无所知的情况下，你懂的了怎么创建文件，怎么去定义main()函数，还有和其他语言差不太多的打印函数 println(‘’)，就完成了编程语言第一课，HelloWorld这里有几个需要记住的点scala语言，每行语句后，不用 ； 结尾，java中需要使用 ；号结尾，python和scala一样，不需要分号。但是如果你想如果一行里写多个语句那么分号是需要的。例如val s = “哈哈哈”; println(s)Scala 使用 import 关键字引用包","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Hive 基本操作","slug":"Hive-基本操作","date":"2016-10-20T14:14:13.000Z","updated":"2020-04-05T14:32:51.398Z","comments":true,"path":"2016/10/20/Hive-基本操作/","link":"","permalink":"cpeixin.cn/2016/10/20/Hive-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Hive 交互命令刚刚安装好hive后，先来看一看基本的交互12345678910111213141516(base) [cpeixin@CpeixindeMBP:] ~ $ hive -helpHive Session ID &#x3D; 626af458-fe89-406e-9a63-5967e2962486usage: hive -d,--define &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. -d A&#x3D;B or --define A&#x3D;B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property&#x3D;value&gt; Use value for given property --hivevar &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. --hivevar A&#x3D;B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console)在不进入客户端的情况下，我们可以上面这些交互命令来操作hive-e 在linux命令行窗口执行sql语句[cpeixin@CpeixindeMBP]$ hive -e “hive -e “select * from test.car””注： 记得加库名-f 执行脚本中sql语句，并将结果写到指定文件[cpeixin@CpeixindeMBP]$ hive -f /opt/sql/test.sql &gt; /opt/data/test_result.txt查看hive的执行历史[cpeixin@CpeixindeMBP:] ~ $ cat .hivehistory在进入hive客户端中，我们还可以使用命令查看本地和HDFS文件系统的文件，这个还是蛮实用的Hive 数据类型基本的交互了解后呢，我们来看一下Hive中的数据类型基本数据类型：数据类型字节范围示例TINYINT1byte-128 ~ 127100YSMALLINT2byte-32,768 ~ 32,767100SINT/INTEGER4byte-2,147,483,648 ~ 2,147,483,647100BIGINT8byte-9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807100LFLOAT4byte单精度浮点数0.2DOUBLE8byte双精度浮点数0.2DECIMAL高精度浮点数DECIMAL(9,8)BOOLEANTRUE/FALSEtrueBINARY二进制类型TIMESTAMP时间戳DATE日期2016-08-08STRINGVARCHAR长度 1～65535CHAR最大长度255关于整型：默认情况下，整型数据默认为INT，除非数字超出INT的范围，在这种情况下它被表示为 BIGINT，或者直接指定100Y，100S, 100L 才会对应转换成TINYINT、SMALLINT、BIGINT。关于浮点型：浮点数假定为 DOUBLE，Decimal 字为 DOUBLE 类型提供精确值和浮点数的更大范围。 Decimal 数据类型存储数值的精确表示，而 DOUBLE 数据类型存储非常接近数值的近似值。DECIMAL不指定精度时默认为DECIMAL(10,0)；DOUBLE 的(非常接近)近似值不足以满足要求是，需要使用Decimal 类型，例如财务应用程序，相等和不等式检查以及舍入操作。对于处理 DOUBLE 范围(大约-10308 到 10308)或非常接近零(-10-308 到 10-308)之外的数的用例，也需要它们。另外，Decimal为专门为财务相关问题设计的数据类型。关于字符型：Strings 字符串数据可以用单引号(‘)或 双引号(“)表示.Hive 在 strings 中使用 C-style 转义。Varchar 使用长度说明符(介于 1 和 65535 之间)创建 Varchar 类型，该长度说明符定义字符 string 中允许的最大字符数。如果varchar value超过了长度说明符，则会以静默方式截断 string。字符长度由字符 串中包含的字符数决定。与 string 一样，尾随空格在 varchar 中很重要，会影响比较结果。Char 类型与 Varchar 类似，但它们是固定长度意味着短于指定长度 value 的值用空格填充，但尾部空格在比较期间不影响比较。最大长度固定为 255。关于时间类型：Timestampstimestamp表示UTC时间，可以是以秒为单位的整数；带精度的浮点数，最大精确到小数点后9位，纳秒级；java.sql.Timestamp格式的字符串 YYYY-MM-DD hh:mm:ss.fffffffffDateHive中的Date只支持YYYY-MM-DD格式的日期，其余写法都是错误的，如需带上时分秒，请使用timestamp复杂数据类型：数据类型释义ARRAYARRAY类型是由一系列相同数据类型的元素组成，这些元素可以通过下标来访问。比如有一个ARRAY类型的变量fruits，它是由[‘apple’,’orange’,’mango’]组成，那么我们可以通过fruits[1]来访问元素orange，因为ARRAY类型的下标是从0开始的；MAPMAP包含key-&gt;value键值对，可以通过key来访问元素。比如”userlist”是一个map类型，其中username是key，password是value；那么我们可以通过userlist[‘username’]来得到这个用户对应的password；STRUCTSTRUCT可以包含不同数据类型的元素。这些元素可以通过”点语法”的方式来得到所需要的元素，比如user是一个STRUCT类型，那么可以通过user.address得到这个用户的地址。UNIONUNIONTYPE&lt;data_type, data_type, …&gt;Hive DDL操作关于库：创建库CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name[COMMENT database_comment] //关于数据块的描述[LOCATION hdfs_path] //指定数据库在HDFS上的存储位置[WITH DBPROPERTIES (property_name=property_value, …)]; //指定数据块属性eg: create database t1;** create database if not exists t1;** create database if not exists t1 comment ‘comment dor t1’;**create database if not exists t3 with dbproperties(‘creator’=’cpeixin’,’date’=’2016-04-05’);查看库show databases;** desc database extended t1;**show create database t1;删除库** **drop database dbname;切换库use dbname;关于表：创建表下面为官网给出的建表参数：1234567891011121314CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [&lt;font&gt;&lt;&#x2F;font&gt; [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path]内部表和外部表默认情况下，Hive创建内部表，其中文件，元数据和统计信息由内部Hive进程管理这里我们需要知道最根本的区别就是，在删除内部表的时候，数据也会被删除，而外部表不会。STORED as 存储格式是指定文件的类型，保存在hive中的文件的类型有多种，一般简单就保存为文本格式，即TEXTFILE，但是企业中一般不使用这种格式来保存数据，主要是因为文本格式占的空间比较大，不利于大数据分析。企业中一般使用ORC和PARQUET，AVRO三种文件类型来保存，具体的会在后面讲解。ROW FORMAT DELIMITED 行分隔符ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘ ‘这句的意思是以空格来分隔行数据，那么这一行中的数据只要遇到一个空格就可以划分为一个数据。这里的分隔符可以是其他字符，比如”,”,”#”,”|”等，一般只要用数据文件中可以区分每一行中的不同数据即可。列与列直接的分隔符通常是以换行符来区分，可以用如下的语句来指定：`COLLECTION ITEMS TERMINATED BY ‘\\n’， 通常列与列直接的分隔符是不需要写的。LOCATION hdfs_path **可以在创建表的时候指定该表映射到到hdfs的文件路径，默认是映射到/user/hive/warehouse目录下。PARTITIONED BY 分区为了对表进行合理的管理以及提高查询效率，Hive可以将表组织成“分区”。一个分区实际上就是表下的一个目录，一个表可以在多个维度上进行分区，分区之间的关系就是目录树的关系。通过PARTITIONED BY子句指定，分区的顺序决定了谁是父目录，谁是子目录。在这里分区又分为 静态分区和动态分区。简单的来说，静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断下面进行建表，建表的示例也会尽可量的使用各种数据类型进行解释说明：建表实例：1.创建普通表，不添加任何参数**123456789101112create table t_user_details(user_name string,age tinyint,phone_number string,birth_date string,deposit_amount float,promotion_amount double,register_date date,last_login_time timestamp,user_level int,vip_flag boolean);**12345678910111213141516171819202122232425hive (test)&gt; show create table t_user_details;OKcreatetab_stmtCREATE TABLE &#96;t_user_details&#96;( &#96;user_name&#96; string, &#96;age&#96; tinyint, &#96;phone_number&#96; string, &#96;birth_date&#96; string, &#96;deposit_amount&#96; float, &#96;promotion_amount&#96; double, &#96;register_date&#96; date, &#96;last_login_time&#96; timestamp, &#96;user_level&#96; int, &#96;vip_flag&#96; boolean)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39; STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;LOCATION &#39;hdfs:&#x2F;&#x2F;localhost:8020&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;t_user_details&#39;TBLPROPERTIES ( &#39;bucketing_version&#39;&#x3D;&#39;2&#39;, &#39;transient_lastDdlTime&#39;&#x3D;&#39;1586087852&#39;)1insert into t_user_details values (&#39;brent&#39;,27,&#39;13611111111&#39;,&#39;1993-03-14&#39;,100.91,4000.56,2016-04-05,&#39;2016-04-07 14:20:36.345&#39;,6,True),(&#39;haylee&#39;,25,&#39;13211111111&#39;,&#39;1994-03-14&#39;,130.91,40000000.56,&#39;2016-04-06&#39;,&#39;2016-04-08 14:20:36.345&#39;,6,False);观察以上内容，是针对没有输入任何建表参数的情况下，所生成的结果，在show create table xx的结果中，体现出了完整的建表默认参数。关于ROW FORMAT SERDE，用于指定序列化和反序列化的规则，默认值：LazySimpleSerDe简单来说，就是它希望对于Deserialization，反序列化，可以lazy一点。对于Serialization，序列化，可以simple一点在没有指定字段之间的分隔符时，默认是用\\001 不可见字符进行分割的，我们也可以在建表的时候使用FIELDS TERMINATED BY ‘,’ 参数来指定字段之间使用逗号分割。除此之外，在使用hive的时候，存储格式的选择非常重要，不同存储格式直接在最底层影响着你的执行效率，所以这部分在之后用单独的一篇文章来说。这里大家先知道企业里面常用的ORC，Parquet，Avro等格式就可以了2.创建分区表**分区表在显示工作中非常常用，例如针对网站数据的存储，网站每天产生数据量过大的话，我们不能始终在表末尾进行数据的追加，而是应该利用动态分区或者静态分区，按月，按天的粒度进行分区存储123456789101112131415161718192021222324CREATE TABLE &#96;t_user_detail_partition&#96;( user_name string, age tinyint, phone_number string, birth_date string, deposit_amount float, promotion_amount double, register_date date, last_login_time timestamp, user_level int, vip_flag boolean)PARTITIONED BY ( &#96;snapshot_date&#96; string)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39; WITH SERDEPROPERTIES ( &#39;field.delim&#39;&#x3D;&#39;\\u0001&#39;, &#39;line.delim&#39;&#x3D;&#39;\\n&#39;, &#39;serialization.format&#39;&#x3D;&#39;\\u0001&#39;) STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;;1insert into t_user_detail_partition values (&#39;brent&#39;,27,&#39;13611111111&#39;,&#39;1993-03-14&#39;,100.91,4000.56,2016-04-05,&#39;2016-04-07 14:20:36.345&#39;,6,True,&#39;2016-04-04&#39;),(&#39;haylee&#39;,25,&#39;13211111111&#39;,&#39;1994-03-14&#39;,130.91,40000000.56,&#39;2016-04-06&#39;,&#39;2016-04-08 14:20:36.345&#39;,6,False,&#39;2016-04-05&#39;);上面的建表语句使用 PARTITIONED BY 指定了 snapshot_date 为分区字段，当然你还可以再添加分区字段，hive支持多分区，理论上最多支持8级分区在插入数据的语句中，将分区字段的值顺序的写在表中字段值的后面，则可以按照分区进行插入数据。在hdfs中，t_user_detail_partition表则按照分区字段进行划分，将数据存储到不同的分区目录下。Hive中的分区是使用的表外字段，MySQL使用的是表内字段静态分区和动态分区在创建表时，语句是一样的。只是在赋值的时候有区别动态分区和静态分区的区别加载数据的方式：静态分区可以通过load命令，向不同的分区加载数据，加载数据时要指定分区的值；静态分区只能通过select加载数据，并且不需要指定分区的名字，而是根据伪列的值，动态的确定分区值确定分区值的方式：两者在创建表的时候命令完全一致，只是在确定分区值的时候不同，静态分区需要手动指定分区值，而动态分区会自动识别伪列的属性，动态生成分区值Hive DML操作加载文件数据到表123LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)]复制代码LOCAL 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：从本地文件系统加载文件时， filepath 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；从 HDFS 加载文件时候，filepath 为文件完整的 URL 地址：如 hdfs://namenode:port/user/hive/project/ data1filepath 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；加载文件的格式必须与建表时使用 STORED AS 指定的存储格式相同。查询结果插入到表12345INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;复制代码Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 immutable 属性的影响）;可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；从 Hive 1.1.0 开始，TABLE 关键字是可选的；从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列；可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：12345FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...;**动态插入分区****12345INSERT OVERWRITE TABLE tablename PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) select_statement FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) select_statement FROM from_statement;复制代码在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。使用SQL语句插入值123INSERT INTO TABLE tablename [PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...)] VALUES ( value [, value ...] )复制代码使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；不支持支持复杂类型 (array, map, struct, union) 的插入。更新和删除数据更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。1234-- 更新UPDATE tablename SET column &#x3D; value [, column &#x3D; value ...] [WHERE expression]--删除DELETE FROM tablename [WHERE expression]查询结果写出到文件系统1234INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] SELECT ... FROM ...复制代码OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：1234567-- 定义列分隔符为&#39;\\t&#39; insert overwrite local directory &#39;.&#x2F;test-04&#39; row format delimited FIELDS TERMINATED BY &#39;\\t&#39;COLLECTION ITEMS TERMINATED BY &#39;,&#39;MAP KEYS TERMINATED BY &#39;:&#39;select * from src;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"Hive 初识","slug":"Hive-初识","date":"2016-10-15T14:11:24.000Z","updated":"2020-04-05T14:32:53.687Z","comments":true,"path":"2016/10/15/Hive-初识/","link":"","permalink":"cpeixin.cn/2016/10/15/Hive-%E5%88%9D%E8%AF%86/","excerpt":"","text":"What - Hivehive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。Why - HiveHive最初是Facebook为了满足对海量社交网络数据的管理和机器学习的需求而产生和发展的。大数据是现在互联网的趋势，而hadoop就是大数据时代里的核心技术，但是hadoop的mapreduce操作专业性太强，所以facebook在这些基础上开发了hive框架，业务人员在不学习编程语言的情况下，只要学会基本的SQL语句，就可以对大数据平台的数据进行分析。How - Hive在hadoop集群中，安装配置好hive，就可以在命令行中直接输入hive，进入hive客户端接下来的操作就和操作数据库的SQL几乎一样。具体操作将在后面的文章里进行介绍。Hive - 优缺点优点：简单容易上手：提供了类SQL查询语言HQL可扩展：为超大数据集设计了计算/扩展能力（MR作为计算引擎，HDFS作为存储系统，Yarn作为资源调度）提供统一的元数据管理延展性：Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数容错：良好的容错性，节点出现问题SQL仍可完成执行支持用户自定义函数缺点：hive的HQL表达能力有限hive的效率比较低（后面可用spark计算框架代替Hive分析）hive调优比较困难，粒度较粗Hive - 架构","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"HDFS-三思","slug":"HDFS-三思","date":"2016-10-06T07:30:21.000Z","updated":"2020-04-04T17:25:03.805Z","comments":true,"path":"2016/10/06/HDFS-三思/","link":"","permalink":"cpeixin.cn/2016/10/06/HDFS-%E4%B8%89%E6%80%9D/","excerpt":"","text":"读写流程写流程具体过程如下：Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象；通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入；调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。**读流程**![p2.gif](https://cdn.nlark.com/yuque/0/2020/gif/1072113/1586020955556-6aa55757-4f0d-4d46-91bb-90e9963517fd.gif#align=left&display=inline&height=541&name=p2.gif&originHeight=541&originWidth=960&size=1319443&status=done&style=none&width=960)具体过程：Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；NameNode 返回存储的每个块的 DataNode 列表；Client 将连接到列表中最近的 DataNode；Client 开始从 DataNode 并行读取数据；一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。写流程中备份三，其中一个写失败了怎么办？只要成功写入的节点数量达到dfs.replication.min(默认为1)，那么就任务是写成功的。然后NameNode会通过异步的方式将block复制到其他节点，使数据副本达到dfs.replication参数配置的个数HDFS HA 启动流程①开启zookeeper服务1zkServer.sh start②开启`journalNode`守护进程（在`journal`协议指定的节点上执行）[ˈdʒɜːnl]1hadoop-daemon.sh start journalnode③开启namenode守护进程（在nn1和nn2执行）1hadoop-daemon.sh start namenode④开启datanode守护进程123hadoop-daemons.sh start datanode（在namenode节点上执行开启全部datanode）⑤开启zkfc守护进程1hadoop-daemon.sh --script $HADOOP_PREFIX&#x2F;bin&#x2F;hdfs start zkfcHDFS 存储类型HDFS支持如下4种存储类型：DISK：表示普通磁盘(机械磁盘)SSD：表示固态硬盘RAM_DISK：表示内存硬盘，参考虚拟内存盘，说白了就是内存ARCHIVE：这个并不是特指某种存储介质，而是为了满足高密度存储而定义的一种存储类型，一般对于归档的、访问不怎么频繁的数据可以以 ARCHIVE 的形式存储。以上四种的存储类型的存取的速度大小为：RAM_DISK-&gt;SSD-&gt;DISK-&gt;ARCHIVE。但是单bit存储成本由高到低那么我们在配置DataNode的存储路径的时候，我们可以分别为上面四种存储类型配置存储位置，如下图：12345&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;[RAM_DISK]file:///ram_disk,[SSD]file:///ssd1/dn,[DISK]file:///disk1/dn,[ARCHIVE]file:///archive1/dn&lt;/value&gt; &lt;description&gt;DataNode存放数据的地方&lt;/description&gt;&lt;/property&gt;上面配置的DataNode的多个存储位置由逗号隔开，每一个存储位置由存储类型和存储物理路径组成。HDFS通过该配置感知底层存储的位置和类型HDFS是否有异步访问模式？在现有HDFS的RPC调用方式上,采用的基本是blocking call的形式,也就是阻塞式的调用方式.阻塞方式的一个明显的缺点是它的请求过程是同步的,也就是说,客户端必须等待当前请求结果的返回,才能接着发送下一次请求.如果此客户端打算在一个线程中发送大量请求的话,阻塞式的RPC调用将会非常耗时.但是如果为了每一次请求调用而专门单独开一个线程的话,系统资源将会被大幅度的使用,显然这也不是一个好的解决的办法.那么有没有什么好的办法呢,在HDFS中是否存在有异步模式的RPC请求接口呢本文我们就来聊聊HDFS的异步访问模式.HDFS异步访问模式老实说,在目前Hadoop的发布版本中,确实还不存在HDFS异步访问的模式,但是这并不代表社区没有在关注这方面的问题.在许多特殊的场景下,HDFS的异步访问模式还是有它独到的用处的.社区在JIRA HDFS-9924([umbrella] Nonblocking HDFS Access)上对此功能特性进行了实现.在本文中,我们姑且取名”HDFS异步访问模式”为AsyncDistributedFileSystem,与DistributedFileSystem相对应.HDFS异步访问模式原理在HDFS异步访问模式的设计文档中,给出了新的异步的RPC调用模式,采用了Future-Get的异步调用模式,以FileSystem的rename方法客户端异步请求的控制在前面HDFS异步访问模式的过程中,有一点必须格外引起注意:客户端异步请求的控制.客户端应有异步请求数的限制,以此防止客户端利用大量的异步请求冲垮服务端.如果超过了此限制阈值,客户端的请求将会处于阻塞状态.这点必须要引起足够重视,否则客户端随随便便发起的请求将会摧毁NameNode.HDFS异步访问模式的优化点第一, 保证异步请求的有序性.在某些场景下,我们需要保证异步请求能够按照请求发起的时间,顺序执行.第二, 客户端对HDFS读写异步请求的支持.总结HDFS Async调用模式的出现将会带给用户更灵活的RPC请求方式的选择,但是可能考虑到此种方式对比之前的方式而言,改动较大,目前这些异步调用相关的方法许多是打上了@Unstable标记的.HDFS异步调用的方式同样可以很好的运用在单元测试上.鉴于此特性是还暂未发布,大家可以根据自己的需要,进行部分的合入.调整数据块大小会有什么影响？Hadoop 1.x 中， 默认的数据块大小是 64MHadoop 2.x 中， 默认的数据块大小是 128M在HDFS中，数据块不宜设置的过大，也不适宜设置的过小。主要是从 减少寻址时间和MR任务并行方面去考虑为什么HDFS中块（block）不能设置太大，也不能设置太小？如果块设置过大，一方面，从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢；另一方面，mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。2. 如果块设置过小，一方面存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；另一方面文件块过小，寻址时间增大，导致程序一直在找block的开始位置。因而，块适当设置大一些，减少寻址时间，那么传输一个由多个块组成的文件的时间主要取决于磁盘的传输速率。HDFS中块（block）的大小为什么设置为128M？HDFS中平均寻址时间大概为10ms；2. 经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态；所以最佳传输时间为10ms/0.01=1000ms=1s3. 目前磁盘的传输速率普遍为100MB/s；计算出最佳block大小：100MB/s x 1s = 100MB所以我们设定block大小为128MB。实际在工业生产中，磁盘传输速率为200MB/s时，一般设定block大小为256MB，磁盘传输速率为400MB/s时，一般设定block大小为512MB以后随着新一代磁盘驱动器传输速率的提升，块的大小将被设置得更大","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"Hadoop 2.x - HDFS","slug":"Hadoop-2-x-HDFS","date":"2016-10-06T03:33:21.000Z","updated":"2020-04-04T17:26:26.578Z","comments":true,"path":"2016/10/06/Hadoop-2-x-HDFS/","link":"","permalink":"cpeixin.cn/2016/10/06/Hadoop-2-x-HDFS/","excerpt":"","text":"在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。NameNode 高可用整体架构概述在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。HDFS NameNode 和 YARN ResourceManger 的高可用 (High Availability，HA) 方案基本类似，两者也复用了部分代码，但是由于 HDFS NameNode 对于数据存储和数据一致性的要求比 YARN ResourceManger 高得多，所以 HDFS NameNode 的高可用实现更为复杂一些，本文从内部实现的角度对 HDFS NameNode 的高可用机制进行详细的分析。HDFS NameNode 的高可用整体架构如图 1 所示图 1.HDFS NameNode 高可用整体架构从上图中，我们可以看出 NameNode 的高可用架构主要分为下面几个部分：Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。Zookeeper 集群：为主备切换控制器提供主备选举支持。共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。下面开始分别介绍 NameNode 的主备切换实现和共享存储系统的实现，在文章的最后会结合笔者的实践介绍一下在 NameNode 的高可用运维中的一些注意事项。NameNode 的主备切换实现NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现：ZKFailoverController 作为 NameNode 机器上一个独立的进程启动 (在 hdfs 启动脚本之中的进程名为 zkfc)，启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，也会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调方法。HealthMonitor 主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举。ActiveStandbyElector 主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。NameNode 实现主备切换的流程如图 2 所示，有以下几步：HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。图 2.NameNode 的主备切换流程下面分别对 HealthMonitor、ActiveStandbyElector 和 ZKFailoverController 的实现细节进行分析：HealthMonitor 实现分析ZKFailoverController 在初始化的时候会创建 HealthMonitor，HealthMonitor 在内部会启动一个线程来循环调用 NameNode 的 HAServiceProtocol RPC 接口的方法来检测 NameNode 的状态，并将状态的变化通过回调的方式来通知 ZKFailoverController。HealthMonitor 主要检测 NameNode 的两类状态，分别是 HealthMonitor.State 和 HAServiceStatus。HealthMonitor.State 是通过 HAServiceProtocol RPC 接口的 monitorHealth 方法来获取的，反映了 NameNode 节点的健康状况，主要是磁盘存储资源是否充足。HealthMonitor.State 包括下面几种状态：INITIALIZING：HealthMonitor 在初始化过程中，还没有开始进行健康状况检测；SERVICE_HEALTHY：NameNode 状态正常；SERVICE_NOT_RESPONDING：调用 NameNode 的 monitorHealth 方法调用无响应或响应超时；SERVICE_UNHEALTHY：NameNode 还在运行，但是 monitorHealth 方法返回状态不正常，磁盘存储资源不足；HEALTH_MONITOR_FAILED：HealthMonitor 自己在运行过程中发生了异常，不能继续检测 NameNode 的健康状况，会导致 ZKFailoverController 进程退出；HealthMonitor.State 在状态检测之中起主要的作用，在 HealthMonitor.State 发生变化的时候，HealthMonitor 会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。而 HAServiceStatus 则是通过 HAServiceProtocol RPC 接口的 getServiceStatus 方法来获取的，主要反映的是 NameNode 的 HA 状态，包括：INITIALIZING：NameNode 在初始化过程中；ACTIVE：当前 NameNode 为主 NameNode；STANDBY：当前 NameNode 为备 NameNode；STOPPING：当前 NameNode 已停止；HAServiceStatus 在状态检测之中只是起辅助的作用，在 HAServiceStatus 发生变化时，HealthMonitor 也会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。ActiveStandbyElector 实现分析Namenode(包括 YARN ResourceManager) 的主备选举是通过 ActiveStandbyElector 来完成的，ActiveStandbyElector 主要是利用了 Zookeeper 的写一致性和临时节点机制，具体的主备选举实现如下：创建锁节点如果 HealthMonitor 检测到对应的 NameNode 的状态正常，那么表示这个 NameNode 有资格参加 Zookeeper 的主备选举。如果目前还没有进行过主备选举的话，那么相应的 ActiveStandbyElector 就会发起一次主备选举，尝试在 Zookeeper 上创建一个路径为/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 的临时节点 (${dfs.nameservices} 为 Hadoop 的配置参数 dfs.nameservices 的值，下同)，Zookeeper 的写一致性会保证最终只会有一个 ActiveStandbyElector 创建成功，那么创建成功的 ActiveStandbyElector 对应的 NameNode 就会成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Active 状态。而创建失败的 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Standby 状态。注册 Watcher 监听不管创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点是否成功，ActiveStandbyElector 随后都会向 Zookeeper 注册一个 Watcher 来监听这个节点的状态变化事件，ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件。自动触发主备选举如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。防止脑裂Zookeeper 在工程实践的过程中经常会发生的一个现象就是 Zookeeper 客户端“假死”，所谓的“假死”是指如果 Zookeeper 客户端机器负载过高或者正在进行 JVM Full GC，那么可能会导致 Zookeeper 客户端到 Zookeeper 服务端的心跳不能正常发出，一旦这个时间持续较长，超过了配置的 Zookeeper Session Timeout 参数的话，Zookeeper 服务端就会认为客户端的 session 已经过期从而将客户端的 Session 关闭。“假死”有可能引起分布式系统常说的双主或脑裂 (brain-split) 现象。具体到本文所述的 NameNode，假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，即使随后 NameNode1 对应的 ZKFailoverController 因为负载下降或者 Full GC 结束而恢复了正常，感知到自己和 Zookeeper 的 Session 已经关闭，但是由于网络的延迟以及 CPU 线程调度的不确定性，仍然有可能会在接下来的一段时间窗口内 NameNode1 认为自己还是处于 Active 状态。这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况对于 NameNode 这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。Zookeeper 社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息。Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候 (注意由于/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 是临时节点，也会随之删除)，会一起删除节点/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb。但是如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来。后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing，具体处理见后文 ZKFailoverController 部分所述。ZKFailoverController 实现分析ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调函数，ZKFailoverController 的处理逻辑主要靠 HealthMonitor 和 ActiveStandbyElector 的回调函数来驱动。对 HealthMonitor 状态变化的处理如前所述，HealthMonitor 会检测 NameNode 的两类状态，HealthMonitor.State 在状态检测之中起主要的作用，ZKFailoverController 注册到 HealthMonitor 上的处理 HealthMonitor.State 状态变化的回调函数主要关注 SERVICE_HEALTHY、SERVICE_NOT_RESPONDING 和 SERVICE_UNHEALTHY 这 3 种状态：如果检测到状态为 SERVICE_HEALTHY，表示当前的 NameNode 有资格参加 Zookeeper 的主备选举，如果目前还没有进行过主备选举的话，ZKFailoverController 会调用 ActiveStandbyElector 的 joinElection 方法发起一次主备选举。如果检测到状态为 SERVICE_NOT_RESPONDING 或者是 SERVICE_UNHEALTHY，就表示当前的 NameNode 出现问题了，ZKFailoverController 会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举，这样其它的 NameNode 就有机会成为主 NameNode。而 HAServiceStatus 在状态检测之中仅起辅助的作用，在 HAServiceStatus 发生变化时，ZKFailoverController 注册到 HealthMonitor 上的处理 HAServiceStatus 状态变化的回调函数会判断 NameNode 返回的 HAServiceStatus 和 ZKFailoverController 所期望的是否一致，如果不一致的话，ZKFailoverController 也会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举。对 ActiveStandbyElector 主备选举状态变化的处理在 ActiveStandbyElector 的主备选举状态发生变化时，会回调 ZKFailoverController 注册的回调函数来进行相应的处理：如果 ActiveStandbyElector 选主成功，那么 ActiveStandbyElector 对应的 NameNode 成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeActive 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToActive 方法，将 NameNode 转换为 Active 状态。如果 ActiveStandbyElector 选主失败，那么 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeStandby 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，将 NameNode 转换为 Standby 状态。如果 ActiveStandbyElector 选主成功之后，发现了上一个 Active NameNode 遗留下来的/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 节点 (见“ActiveStandbyElector 实现分析”一节“防止脑裂”部分所述)，那么 ActiveStandbyElector 会首先回调 ZKFailoverController 注册的 fenceOldActive 方法，尝试对旧的 Active NameNode 进行 fencing，在进行 fencing 的时候，会执行以下的操作：首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离；只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。NameNode 的共享存储实现过去几年中 Hadoop 社区涌现过很多的 NameNode 共享存储方案，比如 shared NAS+NFS、BookKeeper、BackupNode 和 QJM(Quorum Journal Manager) 等等。目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。为了理解 QJM 的设计和实现，首先要对 NameNode 的元数据存储结构有所了解。NameNode 的元数据存储概述一个典型的 NameNode 的元数据存储目录结构如图 3 所示 (图片来源于参考文献 [4])，这里主要关注其中的 EditLog 文件和 FSImage 文件：图 3 .NameNode 的元数据存储目录结构NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog 仅仅只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment。正在写入的 EditLog Segment 处于 in-progress 状态，其文件名形如 edits_inprogress_${start_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，例如上图中的 edits_inprogress_0000000000000000020。而已经写入完成的 EditLog Segment 处于 finalized 状态，其文件名形如 edits_${start_txid}-${end_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，${end_txid} 表示这个 segment 的结束事务 id，例如上图中的 edits_0000000000000000001-0000000000000000019。NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，FSImage 文件的文件名形如 fsimage_${end_txid}，其中${end_txid} 表示这个 fsimage 文件的结束事务 id，例如上图中的 fsimage_0000000000000000020。在 NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。基于 QJM 的共享存储系统的总体架构基于 QJM 的共享存储系统主要用于保存 EditLog，并不保存 FSImage 文件。FSImage 文件还是在 NameNode 的本地磁盘上。QJM 共享存储的基本思想来自于 Paxos 算法 (参见参考文献 [3])，采用多个称为 JournalNode 的节点组成的 JournalNode 集群来存储 EditLog。每个 JournalNode 保存同样的 EditLog 副本。每次 NameNode 写 EditLog 的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向 JournalNode 集群之中的每一个 JournalNode 发送写请求，只要大多数 (majority) 的 JournalNode 节点返回成功就认为向 JournalNode 集群写入 EditLog 成功。如果有 2N+1 台 JournalNode，那么根据大多数的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。基于 QJM 的共享存储系统的内部实现架构图如图 4 所示，主要包含下面几个主要的组件：图 4 . 基于 QJM 的共享存储系统的内部实现架构图FSEditLog：这个类封装了对 EditLog 的所有操作，是 NameNode 对 EditLog 的所有操作的入口。JournalSet： 这个类封装了对本地磁盘和 JournalNode 集群上的 EditLog 的操作，内部包含了两类 JournalManager，一类为 FileJournalManager，用于实现对本地磁盘上 EditLog 的操作。一类为 QuorumJournalManager，用于实现对 JournalNode 集群上共享目录的 EditLog 的操作。FSEditLog 只会调用 JournalSet 的相关方法，而不会直接使用 FileJournalManager 和 QuorumJournalManager。FileJournalManager：封装了对本地磁盘上的 EditLog 文件的操作，不仅 NameNode 在向本地磁盘上写入 EditLog 的时候使用 FileJournalManager，JournalNode 在向本地磁盘写入 EditLog 的时候也复用了 FileJournalManager 的代码和逻辑。QuorumJournalManager：封装了对 JournalNode 集群上的 EditLog 的操作，它会根据 JournalNode 集群的 URI 创建负责与 JournalNode 集群通信的类 AsyncLoggerSet， QuorumJournalManager 通过 AsyncLoggerSet 来实现对 JournalNode 集群上的 EditLog 的写操作，对于读操作，QuorumJournalManager 则是通过 Http 接口从 JournalNode 上的 JournalNodeHttpServer 读取 EditLog 的数据。AsyncLoggerSet：内部包含了与 JournalNode 集群进行通信的 AsyncLogger 列表，每一个 AsyncLogger 对应于一个 JournalNode 节点，另外 AsyncLoggerSet 也包含了用于等待大多数 JournalNode 返回结果的工具类方法给 QuorumJournalManager 使用。AsyncLogger：具体的实现类是 IPCLoggerChannel，IPCLoggerChannel 在执行方法调用的时候，会把调用提交到一个单线程的线程池之中，由线程池线程来负责向对应的 JournalNode 的 JournalNodeRpcServer 发送 RPC 请求。JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。下面对基于 QJM 的共享存储系统的两个关键性问题同步数据和恢复数据进行详细分析。基于 QJM 的共享存储系统的数据同步机制分析Active NameNode 和 StandbyNameNode 使用 JouranlNode 集群来进行数据同步的过程如图 5 所示，Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog：图 5 . 基于 QJM 的共享存储的数据同步机制Active NameNode 提交 EditLog 到 JournalNode 集群当处于 Active 状态的 NameNode 调用 FSEditLog 类的 logSync 方法来提交 EditLog 的时候，会通过 JouranlSet 同时向本地磁盘目录和 JournalNode 集群上的共享存储目录写入 EditLog。写入 JournalNode 集群是通过并行调用每一个 JournalNode 的 QJournalProtocol RPC 接口的 journal 方法实现的，如果对大多数 JournalNode 的 journal 方法调用成功，那么就认为提交 EditLog 成功，否则 NameNode 就会认为这次提交 EditLog 失败。提交 EditLog 失败会导致 Active NameNode 关闭 JournalSet 之后退出进程，留待处于 Standby 状态的 NameNode 接管之后进行数据恢复。从上面的叙述可以看出，Active NameNode 提交 EditLog 到 JournalNode 集群的过程实际上是同步阻塞的，但是并不需要所有的 JournalNode 都调用成功，只要大多数 JournalNode 调用成功就可以了。如果无法形成大多数，那么就认为提交 EditLog 失败，NameNode 停止服务退出进程。如果对应到分布式系统的 CAP 理论的话，虽然采用了 Paxos 的“大多数”思想对 C(consistency，一致性) 和 A(availability，可用性) 进行了折衷，但还是可以认为 NameNode 选择了 C 而放弃了 A，这也符合 NameNode 对数据一致性的要求。Standby NameNode 从 JournalNode 集群同步 EditLog当 NameNode 进入 Standby 状态之后，会启动一个 EditLogTailer 线程。这个线程会定期调用 EditLogTailer 类的 doTailEdits 方法从 JournalNode 集群上同步 EditLog，然后把同步的 EditLog 回放到内存之中的文件系统镜像上 (并不会同时把 EditLog 写入到本地磁盘上)。这里需要关注的是：从 JournalNode 集群上同步的 EditLog 都是处于 finalized 状态的 EditLog Segment。“NameNode 的元数据存储概述”一节说过 EditLog Segment 实际上有两种状态，处于 in-progress 状态的 Edit Log 当前正在被写入，被认为是处于不稳定的中间态，有可能会在后续的过程之中发生修改，比如被截断。Active NameNode 在完成一个 EditLog Segment 的写入之后，就会向 JournalNode 集群发送 finalizeLogSegment RPC 请求，将完成写入的 EditLog Segment finalized，然后开始下一个新的 EditLog Segment。一旦 finalizeLogSegment 方法在大多数的 JournalNode 上调用成功，表明这个 EditLog Segment 已经在大多数的 JournalNode 上达成一致。一个 EditLog Segment 处于 finalized 状态之后，可以保证它再也不会变化。从上面描述的过程可以看出，虽然 Active NameNode 向 JournalNode 集群提交 EditLog 是同步的，但 Standby NameNode 采用的是定时从 JournalNode 集群上同步 EditLog 的方式，那么 Standby NameNode 内存中文件系统镜像有很大的可能是落后于 Active NameNode 的，所以 Standby NameNode 在转换为 Active NameNode 的时候需要把落后的 EditLog 补上来。基于 QJM 的共享存储系统的数据恢复机制分析处于 Standby 状态的 NameNode 转换为 Active 状态的时候，有可能上一个 Active NameNode 发生了异常退出，那么 JournalNode 集群中各个 JournalNode 上的 EditLog 就可能会处于不一致的状态，所以首先要做的事情就是让 JournalNode 集群中各个节点上的 EditLog 恢复为一致。另外如前所述，当前处于 Standby 状态的 NameNode 的内存中的文件系统镜像有很大的可能是落后于旧的 Active NameNode 的，所以在 JournalNode 集群中各个节点上的 EditLog 达成一致之后，接下来要做的事情就是从 JournalNode 集群上补齐落后的 EditLog。只有在这两步完成之后，当前新的 Active NameNode 才能安全地对外提供服务。补齐落后的 EditLog 的过程复用了前面描述的 Standby NameNode 从 JournalNode 集群同步 EditLog 的逻辑和代码，最终调用 EditLogTailer 类的 doTailEdits 方法来完成 EditLog 的补齐。使 JournalNode 集群上的 EditLog 达成一致的过程是一致性算法 Paxos 的典型应用场景，QJM 对这部分的处理可以看做是 Single Instance Paxos(参见参考文献 [3]) 算法的一个实现，在达成一致的过程中，Active NameNode 和 JournalNode 集群之间的交互流程如图 6 所示，具体描述如下：图 6.Active NameNode 和 JournalNode 集群的交互流程图生成一个新的 EpochEpoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制，为什么需要 fencing 已经在前面“ActiveStandbyElector 实现分析”一节的“防止脑裂”部分进行了说明。产生新 Epoch 的流程与 Zookeeper 的 ZAB(Zookeeper Atomic Broadcast) 协议在进行数据恢复之前产生新 Epoch 的过程完全类似：Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功。在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。选择需要数据恢复的 EditLog Segment 的 id需要恢复的 Edit Log 只可能是各个 JournalNode 上的最后一个 Edit Log Segment，如前所述，JournalNode 在处理完 newEpoch RPC 请求之后，会向 NameNode 返回它自己的本地磁盘上最新的一个 EditLog Segment 的起始事务 id，这个起始事务 id 实际上也作为这个 EditLog Segment 的 id。NameNode 会在所有这些 id 之中选择一个最大的 id 作为要进行数据恢复的 EditLog Segment 的 id。向 JournalNode 集群发送 prepareRecovery RPC 请求NameNode 接下来向 JournalNode 集群发送 prepareRecovery RPC 请求，请求的参数就是选出的 EditLog Segment 的 id。JournalNode 收到请求后返回本地磁盘上这个 Segment 的起始事务 id、结束事务 id 和状态 (in-progress 或 finalized)。这一步对应于 Paxos 算法的 Phase 1a 和 Phase 1b(参见参考文献 [3]) 两步。Paxos 算法的 Phase1 是 prepare 阶段，这也与方法名 prepareRecovery 相对应。并且这里以前面产生的新的 Epoch 作为 Paxos 算法中的提案编号 (proposal number)。只要大多数的 JournalNode 的 prepareRecovery RPC 调用成功返回，NameNode 就认为成功。选择进行同步的基准数据源，向 JournalNode 集群发送 acceptRecovery RPC 请求 NameNode 根据 prepareRecovery 的返回结果，选择一个 JournalNode 上的 EditLog Segment 作为同步的基准数据源。选择基准数据源的原则大致是：在 in-progress 状态和 finalized 状态的 Segment 之间优先选择 finalized 状态的 Segment。如果都是 in-progress 状态的话，那么优先选择 Epoch 比较高的 Segment(也就是优先选择更新的)，如果 Epoch 也一样，那么优先选择包含的事务数更多的 Segment。在选定了同步的基准数据源之后，NameNode 向 JournalNode 集群发送 acceptRecovery RPC 请求，将选定的基准数据源作为参数。JournalNode 接收到 acceptRecovery RPC 请求之后，从基准数据源 JournalNode 的 JournalNodeHttpServer 上下载 EditLog Segment，将本地的 EditLog Segment 替换为下载的 EditLog Segment。这一步对应于 Paxos 算法的 Phase 2a 和 Phase 2b(参见参考文献 [3]) 两步。Paxos 算法的 Phase2 是 accept 阶段，这也与方法名 acceptRecovery 相对应。只要大多数 JournalNode 的 acceptRecovery RPC 调用成功返回，NameNode 就认为成功。向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成上一步执行完成之后，NameNode 确认大多数 JournalNode 上的 EditLog Segment 已经从基准数据源进行了同步。接下来，NameNode 向 JournalNode 集群发送 finalizeLogSegment RPC 请求，JournalNode 接收到请求之后，将对应的 EditLog Segment 从 in-progress 状态转换为 finalized 状态，实际上就是将文件名从 edits_inprogress_${startTxid} 重命名为 edits_${startTxid}-${endTxid}，见“NameNode 的元数据存储概述”一节的描述。只要大多数 JournalNode 的 finalizeLogSegment RPC 调用成功返回，NameNode 就认为成功。此时可以保证 JournalNode 集群的大多数节点上的 EditLog 已经处于一致的状态，这样 NameNode 才能安全地从 JournalNode 集群上补齐落后的 EditLog 数据。需要注意的是，尽管基于 QJM 的共享存储方案看起来理论完备，设计精巧，但是仍然无法保证数据的绝对强一致，下面选取参考文献 [2] 中的一个例子来说明：假设有 3 个 JournalNode：JN1、JN2 和 JN3，Active NameNode 发送了事务 id 为 151、152 和 153 的 3 个事务到 JournalNode 集群，这 3 个事务成功地写入了 JN2，但是在还没能写入 JN1 和 JN3 之前，Active NameNode 就宕机了。同时，JN3 在整个写入的过程中延迟较大，落后于 JN1 和 JN2。最终成功写入 JN1 的事务 id 为 150，成功写入 JN2 的事务 id 为 153，而写入到 JN3 的事务 id 仅为 125，如图 7 所示 (图片来源于参考文献 [2])。按照前面描述的只有成功地写入了大多数的 JournalNode 才认为写入成功的原则，显然事务 id 为 151、152 和 153 的这 3 个事务只能算作写入失败。在进行数据恢复的过程中，会发生下面两种情况：图 7.JournalNode 集群写入的事务 id 情况如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段收到了 JN2 的回复，那么肯定会以 JN2 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 153。从恢复的结果来看，实际上可以认为前面宕机的 Active NameNode 对事务 id 为 151、152 和 153 的这 3 个事务的写入成功了。但是如果从 NameNode 自身的角度来看，这显然就发生了数据不一致的情况。如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段没有收到 JN2 的回复，那么肯定会以 JN1 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 150。在这种情况下，如果从 NameNode 自身的角度来看的话，数据就是一致的了。事实上不光本文描述的基于 QJM 的共享存储方案无法保证数据的绝对一致，大家通常认为的一致性程度非常高的 Zookeeper 也会发生类似的情况，这也从侧面说明了要实现一个数据绝对一致的分布式存储系统的确非常困难。NameNode 在进行状态转换时对共享存储的处理下面对 NameNode 在进行状态转换的过程中对共享存储的处理进行描述，使得大家对基于 QJM 的共享存储方案有一个完整的了解，同时也作为本部分的总结。NameNode 初始化启动，进入 Standby 状态在 NameNode 以 HA 模式启动的时候，NameNode 会认为自己处于 Standby 模式，在 NameNode 的构造函数中会加载 FSImage 文件和 EditLog Segment 文件来恢复自己的内存文件系统镜像。在加载 EditLog Segment 的时候，调用 FSEditLog 类的 initSharedJournalsForRead 方法来创建只包含了在 JournalNode 集群上的共享目录的 JournalSet，也就是说，这个时候只会从 JournalNode 集群之中加载 EditLog，而不会加载本地磁盘上的 EditLog。另外值得注意的是，加载的 EditLog Segment 只是处于 finalized 状态的 EditLog Segment，而处于 in-progress 状态的 Segment 需要后续在切换为 Active 状态的时候，进行一次数据恢复过程，将 in-progress 状态的 Segment 转换为 finalized 状态的 Segment 之后再进行读取。加载完 FSImage 文件和共享目录上的 EditLog Segment 文件之后，NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式。如前所述，EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog。而 StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。NameNode 从 Standby 状态切换为 Active 状态当 NameNode 从 Standby 状态切换为 Active 状态的时候，首先需要做的就是停止它在 Standby 状态的时候启动的线程和相关的服务，包括上面提到的 EditLogTailer 线程和 StandbyCheckpointer 线程，然后关闭用于读取 JournalNode 集群的共享目录上的 EditLog 的 JournalSet，接下来会调用 FSEditLog 的 initJournalSetForWrite 方法重新打开 JournalSet。不同的是，这个 JournalSet 内部同时包含了本地磁盘目录和 JournalNode 集群上的共享目录。这些工作完成之后，就开始执行“基于 QJM 的共享存储系统的数据恢复机制分析”一节所描述的流程，调用 FSEditLog 类的 recoverUnclosedStreams 方法让 JournalNode 集群中各个节点上的 EditLog 达成一致。然后调用 EditLogTailer 类的 catchupDuringFailover 方法从 JournalNode 集群上补齐落后的 EditLog。最后打开一个新的 EditLog Segment 用于新写入数据，同时启动 Active NameNode 所需要的线程和服务。NameNode 从 Active 状态切换为 Standby 状态当 NameNode 从 Active 状态切换为 Standby 状态的时候，首先需要做的就是停止它在 Active 状态的时候启动的线程和服务，然后关闭用于读取本地磁盘目录和 JournalNode 集群上的共享目录的 EditLog 的 JournalSet。接下来会调用 FSEditLog 的 initSharedJournalsForRead 方法重新打开用于读取 JournalNode 集群上的共享目录的 JournalSet。这些工作完成之后，就会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，EditLogTailer 线程会定时从 JournalNode 集群上同步 Edit Log。NameNode 高可用运维中的注意事项本节结合笔者的实践，从初始化部署和日常运维两个方面介绍一些在 NameNode 高可用运维中的注意事项。初始化部署如果在开始部署 Hadoop 集群的时候就启用 NameNode 的高可用的话，那么相对会比较容易。但是如果在采用传统的单 NameNode 的架构运行了一段时间之后，升级为 NameNode 的高可用架构的话，就要特别注意在升级的时候需要按照以下的步骤进行操作：对 Zookeeper 进行初始化，创建 Zookeeper 上的/hadoop-ha/${dfs.nameservices} 节点。创建节点是为随后通过 Zookeeper 进行主备选举做好准备，在进行主备选举的时候会在这个节点下面创建子节点 (具体可参照“ActiveStandbyElector 实现分析”一节的叙述)。这一步通过在原有的 NameNode 上执行命令 hdfs zkfc -formatZK 来完成。启动所有的 JournalNode，这通过脚本命令 hadoop-daemon.sh start journalnode 来完成。对 JouranlNode 集群的共享存储目录进行格式化，并且将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件 (具体可参照“NameNode 的元数据存储概述”一节的叙述) 之后的 EditLog 拷贝到 JournalNode 集群上的共享目录之中，这通过在原有的 NameNode 上执行命令 hdfs namenode -initializeSharedEdits 来完成。启动原有的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。对新增的 NameNode 节点进行初始化，将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件拷贝到这个新增的 NameNode 的本地磁盘上，同时需要验证 JournalNode 集群的共享存储目录上已经具有了这个 FSImage 文件之后的 EditLog(已经在第 3 步完成了)。这一步通过在新增的 NameNode 上执行命令 hdfs namenode -bootstrapStandby 来完成。启动新增的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。在这两个 NameNode 上启动 zkfc(ZKFailoverController) 进程，谁通过 Zookeeper 选主成功，谁就是主 NameNode，另一个为备 NameNode。这通过脚本命令 hadoop-daemon.sh start zkfc 完成。日常维护笔者在日常的维护之中主要遇到过下面两种问题：Zookeeper 过于敏感：Hadoop 的配置项中 Zookeeper 的 session timeout 的配置参数 ha.zookeeper.session-timeout.ms 的默认值为 5000，也就是 5s，这个值比较小，会导致 Zookeeper 比较敏感，可以把这个值尽量设置得大一些，避免因为网络抖动等原因引起 NameNode 进行无谓的主备切换。单台 JouranlNode 故障时会导致主备无法切换：在理论上，如果有 3 台或者更多的 JournalNode，那么挂掉一台 JouranlNode 应该仍然可以进行正常的主备切换。但是笔者在某次 NameNode 重启的时候，正好赶上一台 JournalNode 挂掉宕机了，这个时候虽然某一台 NameNode 通过 Zookeeper 选主成功，但是这台被选为主的 NameNode 无法成功地从 Standby 状态切换为 Active 状态。事后追查原因发现，被选为主的 NameNode 卡在退出 Standby 状态的最后一步，这个时候它需要等待到 JournalNode 的请求全部完成之后才能退出。但是由于有一台 JouranlNode 宕机，到这台 JournalNode 的请求都积压在一起并且在不断地进行重试，同时在 Hadoop 的配置项中重试次数的默认值非常大，所以就会导致被选为主的 NameNode 无法及时退出 Standby 状态。这个问题主要是 Hadoop 内部的 RPC 通信框架的设计缺陷引起的，Hadoop HA 的源代码 IPCLoggerChannel 类中有关于这个问题的 TODO，但是截止到社区发布的 2.7.1 版本这个问题仍然存在。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"算法 - 排序总结","slug":"算法-排序总结","date":"2016-10-05T07:56:14.000Z","updated":"2020-05-22T08:00:46.964Z","comments":true,"path":"2016/10/05/算法-排序总结/","link":"","permalink":"cpeixin.cn/2016/10/05/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E6%80%BB%E7%BB%93/","excerpt":"","text":"如何选择合适的排序算法？如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法？我们先回顾一下前面讲过的几种排序算法。我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。如果对小规模数据进行排序，可以选择时间复杂度是 O(n2) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。时间复杂度是 O(nlogn) 的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。不知道你有没有发现，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是 O(n2)，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是 O(nlogn)，从这点上看起来很诱人，那为什么它还是没能得到“宠信”呢？还记得我们上一节讲的归并排序的空间复杂度吗？归并排序并不是原地排序算法，空间复杂度是 O(n)。所以，粗略点、夸张点讲，如果要排序 100MB 的数据，除了数据本身占用的内存之外，排序算法还要额外再占用 100MB 的内存空间，空间耗费就翻倍了。前面我们讲到，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序在最坏情况下的时间复杂度是 O(n2)，如何来解决这个“复杂度恶化”的问题呢？如何优化快速排序？我们先来看下，为什么最坏情况下快速排序的时间复杂度是 O(n2) 呢？我们前面讲过，如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 O(n2)。实际上，这种 O(n2) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理。那什么样的分区点是好的分区点呢？或者说如何来选择分区点呢？最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是 O(n2)。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。我这里介绍两个比较常用、比较简单的分区算法，你可以直观地感受一下。1. 三数取中法我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。2. 随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n2) 的情况，出现的可能性不大。快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。Python中的排序python的 sort 内部实现机制为：Timesort最坏时间复杂度为：O（n log n）空间复杂度为：O（n）Timsort是结合了归并排序（merge sort）和插入排序（insertion sort）而得出的排序算法，它在现实中有很好的效率。Tim Peters在2002年设计了该算法并在Python中使用（TimSort 是 Python 中 list.sort 的默认实现）。该算法找到数据中已经排好序的块-分区，每一个分区叫一个run，然后按规则合并这些run。Pyhton自从2.3版以来一直采用Timsort算法排序，现在Java SE7和Android也采用Timsort算法对数组排序。Timsort是稳定的算法，当待排序的数组中已经有排序好的数，它的时间复杂度会小于n logn。与其他合并排序一样，Timesrot是稳定的排序算法，最坏时间复杂度是O（n log n）。在最坏情况下，Timsort算法需要的临时空间是n/2，在最好情况下，它只需要一个很小的临时存储空间Java 语言排序函数堆排序实现C 语言排序函数快速排序实现","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"Hadoop 2.x - Yarn","slug":"Hadoop-2-x-Yarn","date":"2016-10-04T05:30:26.000Z","updated":"2020-04-04T17:25:59.731Z","comments":true,"path":"2016/10/04/Hadoop-2-x-Yarn/","link":"","permalink":"cpeixin.cn/2016/10/04/Hadoop-2-x-Yarn/","excerpt":"","text":"What - YarnYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。Why - Yarn随着互联网的高速发展，新的计算框架不断出现，如内存计算框架、流式计算框架、迭代计算资源框架、这几种框架通常都会被用到考虑到资源的利用率运维和数据共享等因素，企业通常希望将所有的计算框架部署到一个 公共集群中，让他们共享集群的计算资源，并对资源进行同意使用，同时又能采用简单的资源隔离方案，这样便催生了轻量弹性计算平台需求Yarn的设计是一个弹性计算平台，不仅仅支持Mapreduce计算框架而是朝着对多种计算框架进行统一管理方向发展。优点:资源利利用率高，按照框架角度进行资源划分，往往存在应用程序数据和计算资源需求的不均衡性，使得某段时间内计算资源紧张，而另外一种计算方式的资源空闲，共享集群模式则通过框架共享全部的计算资源，使得集群中的资源更加充分合理的利用。运维成本低，如果使用：”一个框架一个集群“的模式，运维人员需要独立管理多个框架，进而增加运维的难度，共享模式通常只需要少数管理员可以完成多个框架的管理。数据共享，随着数据量的增加，跨集群之间的数据不仅增加了硬件成本，而且耗费时间，共享集群模式可以共享框架和硬件资源，大大降低了数据移动带来的成本。How - Yarnyarn是内置在Hadoop平台中的，所以在已经搭建好的Hadoop集群中就可以直接使用。对于其他计算框架，在配置文档配置后即可使用，例如spark的任务提交方式在生产环境中，一定是要用yarn方式来提交，例如：spark-submit –master yarn –class com.xx.xx.classname spark_job_name.jarYarn - 组成YARN总体上仍然是master/slave结构，在整个资源管理框架中，resourcemanager为master，nodemanager是slave。Resourcemanager负责对各个nademanger上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。YARN的基本组成结构，YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。ResourceManager是Master上一个独立运行的进程，负责集群统一的资源管理、调度、分配等等；NodeManager是Slave上一个独立运行的进程，负责上报节点的状态；App Master和Container是运行在Slave上的组件，Container是yarn中分配资源的一个单位，包涵内存、CPU等等资源，yarn以Container为单位分配资源。Client向ResourceManager提交的每一个应用程序都必须有一个Application Master，它经过ResourceManager分配资源后，运行于某一个Slave节点的Container中，具体做事情的Task，同样也运行与某一个Slave节点的Container中。RM，NM，AM乃至普通的Container之间的通信，都是用RPC机制。YARN的架构设计使其越来越像是一个云操作系统，数据处理操作系统。Yarn - 架构ResourcemanagerRM是一个全局的资源管理器，集群只有一个，负责整个系统的资源管理和分配，包括处理客户端请求、启动/监控APP master、监控nodemanager、资源的分配与调度。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）。调度器（Scheduler）调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。应用程序管理器（Applications Manager，ASM）应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。ApplicationMaster（AM）** **管理YARN内运行的应用程序的每个实例。功能：数据切分为应用程序申请资源并进一步分配给内部任务。任务监控与容错负责协调来自resourcemanager的资源，并通过nodemanager监视容易的执行和资源使用情况。NodeManager（NM）Nodemanager整个集群有多个，负责每个节点上的资源和使用。功能：单个节点上的资源管理和任务。处理来自于resourcemanager的命令。处理来自域app master的命令。Nodemanager管理着抽象容器，这些抽象容器代表着一些特定程序使用针对每个节点的资源。Nodemanager定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态（cpu和内存等资源）ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。需要注意的是，Container不同于MRv1中的slot，它是一个动态资源划分单位，是根据应用程序的需求动态生成的。目前为止，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。功能：对task环境的抽象描述一系列信息任务运行资源的集合（cpu、内存、io等）任务运行环境Yarn - 资源管理资源调度和隔离是yarn作为一个资源管理系统，最重要且最基础的两个功能。资源调度由resourcemanager完成，而资源隔离由各个nodemanager实现。Resourcemanager将某个nodemanager上资源分配给任务（这就是所谓的“资源调度”）后，nodemanager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础和保证，这就是所谓的资源隔离。当谈及到资源时，我们通常指内存、cpu、io三种资源。Hadoop yarn目前为止仅支持cpu和内存两种资源管理和调度。内存资源多少决定任务的生死，如果内存不够，任务可能运行失败；相比之下，cpu资源则不同，它只会决定任务的快慢，不会对任务的生死产生影响。 #### Yarn - 队列调度策略Yarn的队列调度策略主要分三种：FIFO、Capacity调度、Fair调度。FIFO调度策略：为先进去的任务分配资源，后入的任务等待前面任务完成才能获得资源。(大任务可能导致后续任务饿死)Capacity调度策略：将集群资源分为一条条队列，每个队列包含一定百分比资源。每个队列中任务采取FIFO的调度策略。在某些队列资源宽裕的情况下，允许跨队列申请资源，同时允许抢占机制。当其他队列任务使用了当前队列任务资源时，当前队列任务在等待一定时间后，允许抢占该队列资源(将改队列内不属于其他队列任务的Container杀死)。Fair调度策略：n个任务情况下，每个任务占据1/n份额的资源。在某任务结束后，该任务资源会被其余资源瓜分，每个任务占据1/(n-1)份资源。与Capacity一样也将集群分为队列且允许抢占机制。不同的是队列内部的资源调度同时允许FIFO和Fair调度。Yarn - 内存管理yarn允许用户配置每个节点上可用的物理内存资源，注意，这里是“可用的”，因为一个节点上内存会被若干个服务共享，比如一部分给了yarn，一部分给了hdfs，一部分给了hbase等，yarn配置的只是自己可用的，配置参数如下：yarn.nodemanager.resource.memory-mb表示该节点上yarn可以使用的物理内存总量，默认是8192m，注意，如果你的节点内存资源不够8g，则需要调减这个值，yarn不会智能的探测节点物理内存总量。yarn.nodemanager.vmem-pmem-ratio任务使用1m物理内存最多可以使用虚拟内存量，默认是2.1yarn.nodemanager.pmem-check-enabled是否启用一个线程检查每个任务证使用的物理内存量，如果任务超出了分配值，则直接将其kill，默认是true。yarn.nodemanager.vmem-check-enabled是否启用一个线程检查每个任务证使用的虚拟内存量，如果任务超出了分配值，则直接将其kill，默认是true。yarn.scheduler.minimum-allocation-mb单个任务可以使用最小物理内存量，默认1024m，如果一个任务申请物理内存量少于该值，则该对应值改为这个数。yarn.scheduler.maximum-allocation-mb单个任务可以申请的最多的内存量，默认8192mYarn - cpu管理目前cpu被划分为虚拟cpu，这里的虚拟cpu是yarn自己引入的概念，初衷是考虑到不同节点cpu性能可能不同，每个cpu具有计算能力也是不一样的，比如，某个物理cpu计算能力可能是另外一个物理cpu的2倍，这时候，你可以通过为第一个物理cpu多配置几个虚拟cpu弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟cpu个数。在yarn中，cpu相关配置参数如下：yarn.nodemanager.resource.cpu-vcores表示该节点上yarn可使用的虚拟cpu个数，默认是8个，注意，目前推荐将该值为与物理cpu核数相同。如果你的节点cpu合数不够8个，则需要调减小这个值，而yarn不会智能的探测节点物理cpu总数。yarn.scheduler.minimum-allocation-vcores单个任务可申请最小cpu个数，默认1，如果一个任务申请的cpu个数少于该数，则该对应值被修改为这个数yarn.scheduler.maximum-allocation-vcores单个任务可以申请最多虚拟cpu个数，默认是32.","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"cpeixin.cn/tags/yarn/"}]},{"title":"Hadoop 2.x - MapReduce","slug":"Hadoop-2-x-MapReduce","date":"2016-10-02T06:30:21.000Z","updated":"2020-04-04T17:26:12.269Z","comments":true,"path":"2016/10/02/Hadoop-2-x-MapReduce/","link":"","permalink":"cpeixin.cn/2016/10/02/Hadoop-2-x-MapReduce/","excerpt":"","text":"MapReduce 工作原理What - MapReduceHadoop主要解决了两个问题，海量数据的存储和海量数据的计算。MapReduce就是Hadoop大数据框架下的分布式计算框架，是基于Hadoop数据分析中的核心计算框架。同时，Mapreduce是一种编程模型，是一种编程方法，抽象理论Why - MapReduce用简洁的方式，就能实现 TB，PB级别数据在百台，千台，万台服务器上的并行运算，程序人员并不需要关心如何处理并行计算、如何分发数据、如何处理错误，这些问题都已经由mapreduce框架来处理。How - MapReduce我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。MapReduce 优缺点优点Mapreduce易于编程它简单的实现一些接口，就可以完成一个分布式程序，这个程序可以分布到大量的廉价的pc机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特性使的Mapreduce编程变得非常流行。良好的扩展性项目当你的计算资源得不到满足的时候，你可以通过简单的通过增加机器来扩展它的计算能力高容错性Mapreduce的设计初衷就是使程序能够部署在廉价的pc机器上，这就要求它具有很高的容错性。比如一个机器挂了，它可以把上面的计算任务转移到另一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由hadoop内部完成的。适合PB级以上海量数据的离线处理**缺点不擅长实时计算Mapreduce无法做到像Mysql那样做到毫秒或者秒级的返回结果不擅长流式计算流式计算的输入数据是动态的，而Mapreduce的输入数据集是静态的，不能流态变化。这是Mapreduce自身的设计特点决定了数据源必须是静态的。不擅长DAG(有向图)计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个应用程序的输出，在这种情况下，Mapreduce并不是不能做，而是使用后每个Mapreduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常低下。MapReduce 工作流程MapReduce整体流程图input file的切分成小文件，默认情况下是按照hdfs block块大小一致为128M对文件进行切分。Map的数量：input file文件按照128Msplit后，有多少个数据块，就对应着有多少个Map。Reduce的数量： 与Map阶段定义的Partition数量一致。MapReduce shuffle阶段：下图先从整体来看，other maps、 other reduces的指向，可以看出，一共有4个Map 和 3个 reduces。那么现在再细分来看，下图表示的是一个 map task 和 reduce task 以及 map 和 reduce之间的shuffle执行流程map 阶段开始后，会经过用户自定义逻辑对数据进行处理。完成map端处理完的数据，会被写入到环形缓冲区（buffer in memory）。这里的环形缓冲区需要说明一下，环形缓冲区的底层实现为环形队列， 默认大小为100MB数据在写入环形缓冲区后，首先进行分区（partition）, 随后对每个partition内的数据进行sort操作，这里的排序方式是按照字典排序，采用快速排序算法进行排序。当map阶段数据不断的向环形缓冲区写的过程中，环形缓冲区有一个阈值，默认为80%，当数据量达到80%后，缓冲区的数据会溢写到磁盘上（merge on disk）溢写到磁盘上后，可以想像成磁盘上存放了很多内部已经排序好，并且带有分区信息的小文件，这时候要对属于同一分区的小文件进行merge，并同时进行排序，这里的排序算法选择的是归并排序此时，每个Map端，都已经准备好了每个partition的文件。上图中，绿色箭头的指向，各个Map端，带有partition信息的文件，fetch到对应的reduce端，可以看到，红色虚线箭头 other reduces对应的磁盘文件则指向其他的两个reduce端。fetch到同一reduce端的partition数据，则属于同一分区数据，这时还要进行一次merge合并排序操作，排序算法选择的是归并排序，来合并成大文件最后，reduce端的shuffle阶段完成，合并好的大文件由reduce函数进行处理，最后到output输出结果。超超超超超超超超详细的工作流程图","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"cpeixin.cn/tags/mapreduce/"}]},{"title":"Hadoop 1.x - HDFS","slug":"Hadoop-1-x-HDFS","date":"2016-10-01T07:36:21.000Z","updated":"2020-04-04T17:26:36.675Z","comments":true,"path":"2016/10/01/Hadoop-1-x-HDFS/","link":"","permalink":"cpeixin.cn/2016/10/01/Hadoop-1-x-HDFS/","excerpt":"","text":"介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。假设与目标硬件故障硬件故障是正常现象，而非例外。HDFS实例可能包含数百或数千个服务器计算机，每个服务器计算机都存储文件系统数据的一部分。存在大量组件并且每个组件的故障概率都很低的事实意味着HDFS的某些组件始终无法运行。因此，检测故障并快速，自动地从故障中恢复是HDFS的核心目标。流数据访问在HDFS上运行的应用程序需要对其数据集进行流式访问。它们不是通常在通用文件系统上运行的通用应用程序。HDFS设计用于批处理，而不是用户交互使用。重点在于数据访问的高吞吐量，而不是数据访问的低延迟。POSIX提出了许多针对HDFS的应用程序不需要的硬性要求。在一些关键领域中，POSIX语义已经被交易以提高数据吞吐率。大数据集在HDFS上运行的应用程序具有大量数据集。HDFS中的典型文件大小为GB到TB。因此，HDFS已调整为支持大文件。它应提供较高的聚合数据带宽，并可以扩展到单个群集中的数百个节点。它应该在单个实例中支持数千万个文件。简单一致性模型HDFS应用程序需要文件一次写入多次读取访问模型。一旦创建，写入和关闭文件，除了追加和截断外，无需更改。支持将内容追加到文件末尾，但不能在任意点更新。该假设简化了数据一致性问题并实现了高吞吐量数据访问。MapReduce应用程序或Web爬网程序应用程序非常适合此模型。移动计算比移动数据便宜如果应用程序所请求的计算在其所操作的数据附近执行，则效率会更高。当数据集的大小巨大时，尤其如此。这样可以最大程度地减少网络拥塞，并提高系统的整体吞吐量。假设通常是将计算迁移到更靠近数据的位置，而不是将数据移动到应用程序正在运行的位置。HDFS为应用程序提供了接口，使它们自己更靠近数据所在的位置。跨异构硬件和软件平台的可移植性HDFS的设计目的是可以轻松地从一个平台移植到另一个平台。这有助于将HDFS广泛用作大量应用程序的首选平台。HDFS 设计原理2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。架构的稳定性心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。HDFS 的特点高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。附：图解HDFS存储原理说明：以下图片引用自博客：翻译经典 HDFS 原理讲解漫画HDFS写数据原理HDFS读数据原理HDFS故障类型和其检测方法检测故障并快速，自动地从故障中恢复是HDFS的核心目标。第二部分：读写故障的处理第三部分：DataNode 故障处理副本布局策略：HDFS shell-ls查看目录hdfs dfs -ls /-mkdir创建目录hdfs dfs -mkdir -p /aaa/bbb/cc/dd-rm删除文件或文件夹hdfs dfs -rm -r /aaa/bbb/cc/dd-rmdir删除空目录hdfs dfs -rmdir /aaa/bbb/cc/dd-puthdfs dfs -put /opt/jdk-8u181-linux-x64.tar.gz /opt/-gethdfs dfs -get /aaa/jdk.tar.gz-cp从hdfs的一个路径拷贝hdfs的另一个路径hdfs dfs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2-count统计一个指定目录下的文件节点数量hdfs dfs -count /-df统计文件系统的可用空间信息hdfs dfs -df -h /-setrep设置hdfs中文件的副本数量hdfs dfs -setrep 3 /aaa/jdk.tar.gzdifference between hdfs dfs and hadoop fshadoop fs是一种更“通用”的命令，它使您可以与包括Hadoop在内的多个文件系统进行交互，而hdfs dfs该命令专用于HDFS。请注意，如果使用的文件系统是HDFS ，则hdfs dfs and hadoop fs命令成为同义词。实际上，如果您发出命令，它将告诉您该命令已被弃用，您应该改用hdfs dfs但是，当您调用这些命令时，实际文件将在hadoop安装目录的bin目录中执行。如果运行hdfs dfs命令，它将调用hadoop安装目录中bin目录中的hdfs文件，后跟第一个参数dfs 告诉hdfs命令我们要使用分布式文件系统（hdfs）而不是本地文件系统文件系统，这是默认选项。第二个命令hadoop fs。它将在hadoop安装目录的bin目录中调用hadoop文件。它将通过发送fs作为第一个参数来跟进该命令/文件。它会告诉“ HADOOP”，我们想要做的任何操作都应该在该特定群集（即HDFS）上由HADOOP安装管理的文件系统上完成。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"算法 - 基数排序","slug":"算法-基数排序","date":"2016-09-26T05:56:15.000Z","updated":"2020-05-22T06:12:46.151Z","comments":true,"path":"2016/09/26/算法-基数排序/","link":"","permalink":"cpeixin.cn/2016/09/26/%E7%AE%97%E6%B3%95-%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/","excerpt":"","text":"基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。我们再来看这样一个排序问题。假设我们有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，你有什么比较快速的排序方法呢？我们之前讲的快排，时间复杂度可以做到 O(nlogn)，还有更高效的排序算法吗？桶排序、计数排序能派上用场吗？手机号码有 11 位，范围太大，显然不适合用这两种排序算法。针对这个排序问题，有没有时间复杂度是 O(n) 的算法呢？现在我就来介绍一种新的排序算法，基数排序。刚刚这个问题里有这样的规律：假设要比较两个手机号码 a，b 的大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了。借助稳定排序算法，这里有一个巧妙的实现思路，先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。手机号码稍微有点长，画图比较不容易看清楚，我用字符串排序的例子，画了一张基数排序的过程分解图，你可以看下。注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 O(n)。实际上，有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的 20 万个英文单词，最短的只有 1 个字母，最长的我特意去查了下，有 45 个字母，中文翻译是尘肺病。对于这种不等长的数据，基数排序还适用吗？实际上，我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII 值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。我来总结一下，基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。基数排序 vs 计数排序 vs 桶排序基数排序有两种方法：这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异：基数排序：根据键值的每位数字来分配桶；计数排序：每个桶只存储单一键值；桶排序：每个桶存储一定范围的数值；我们学习了 3 种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 递归","slug":"算法-递归","date":"2016-09-15T13:01:56.000Z","updated":"2020-05-21T15:13:08.403Z","comments":true,"path":"2016/09/15/算法-递归/","link":"","permalink":"cpeixin.cn/2016/09/15/%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92/","excerpt":"","text":"递归对我来说真的是晦涩难懂啊，看似简单，但是当动手写代码想逻辑的时候，无从下手的感觉。一本书上说，爱递归的人爱的要死，不爱递归的人恨的要死，还有一种不爱递归但是随后又爱的要死。leigh caldwell 在 stack overflow上说，如果使用循环，程序的效率可能更高，但是使用递归，程序更容易理解。存在即合理，接下来我们看一下递归推荐注册返佣金的这个功能我想你应该不陌生吧？现在很多 App 都有这个功能。这个功能中，用户 A 推荐用户 B 来注册，用户 B 又推荐了用户 C 来注册。我们可以说，用户 C 的“最终推荐人”为用户 A，用户 B 的“最终推荐人”也为用户 A，而用户 A 没有“最终推荐人”。一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中 actor_id 表示用户 id，referrer_id 表示推荐人 id。基于这个背景，我的问题是，给定一个用户 ID，如何查找这个用户的“最终推荐人”？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！如何理解“递归”？从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。不过，别看我说了这么多，递归本身可是一点儿都不“高冷”，咱们生活中就有很多用到递归的例子。周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？别忘了你是程序员，这个可难不倒你，递归就开始排上用场了。于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：1f(n)=f(n-1)+1 其中，f(1)=1f(n) 表示你想知道自己在哪一排，f(n-1) 表示前面一排所在的排数，f(1)=1 表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松地将它改为递归代码，如下：1234int f(int n) &#123; if (n == 1) return 1; return f(n-1) + 1;&#125;如果上面找座位的例子你没有明白递归是怎么一回事，那么请读一读下面的问题：如何给一堆数字排序？ 答：分成两半，先排左半边再排右半边，最后合并就行了，至于怎么排左边和右边，请重新阅读这句话。孙悟空身上有多少根毛？ 答：一根毛加剩下的毛。你今年几岁？ 答：去年的岁数加一岁,1999 年我出生。递归需要满足的三个条件刚刚这个例子是非常典型的递归，那究竟什么样的问题可以用递归来解决呢？我总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。1. 一个问题的解可以分解为几个子问题的解何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。3. 存在递归终止条件把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是 f(1)=1，这就是递归的终止条件。如何编写递归代码？刚刚铺垫了这么多，现在我们来看，如何来写递归代码？我个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。你先记住这个理论。我举一个例子，带你一步一步实现一个递归代码，帮你理解。假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：12f(n) = f(n-1)+f(n-2)有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以 f(1)=1。这个递归终止条件足够吗？我们可以用 n=2，n=3 这样比较小的数试验一下。n=2 时，f(2)=f(1)+f(0)。如果递归终止条件只有一个 f(1)=1，那 f(2) 就无法求解了。所以除了 f(1)=1 这一个递归终止条件外，还要有 f(0)=1，表示走 0 个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。所以，我们可以把 f(2)=2 作为一种终止条件，表示走 2 个台阶，有两种走法，一步走完或者分两步来走。所以，递归终止条件就是 f(1)=1，f(2)=2。这个时候，你可以再拿 n=3，n=4 来验证一下，这个终止条件是否足够并且正确。我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：123f(1) = 1;f(2) = 2;f(n) = f(n-1)+f(n-2)有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：12345int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2);&#125;我总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。计算机擅长做重复的事情，所以递归正和它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。递归代码要警惕堆栈溢出在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果会非常严重。为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？我在“栈”那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。比如前面的讲到的电影院的例子，如果我们将系统栈或者 JVM 堆栈大小设置为 1KB，在求解 f(19999) 时便会出现如下堆栈报错：12Exception in thread \"main\" java.lang.StackOverflowError递归代码要警惕重复计算除此之外，使用递归时还会出现重复计算的问题。刚才我讲的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的：从图中，我们可以直观地看到，想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。怎么将递归代码改写为非递归代码？我们刚说了，递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。那我们是否可以把递归代码改写为非递归代码呢笼统地讲，是的。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。但是这种思路实际上是将递归改为了“手动”递归，本质并没有变，而且也并没有解决前面讲到的某些问题，徒增了实现的复杂度。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"递归","slug":"递归","permalink":"cpeixin.cn/tags/%E9%80%92%E5%BD%92/"}]},{"title":"算法 - 计数排序","slug":"算法-计数排序","date":"2016-09-15T05:26:20.000Z","updated":"2020-05-22T06:12:37.856Z","comments":true,"path":"2016/09/15/算法-计数排序/","link":"","permalink":"cpeixin.cn/2016/09/15/%E7%AE%97%E6%B3%95-%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/","excerpt":"","text":"我个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。我们都经历过高考，高考查分数系统你还记得吗？我们查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次呢？考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？想弄明白这个问题，我们就要来看计数排序算法的实现方法。我还拿考生那个例子来解释。为了方便说明，我对数据规模做了简化。假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8]中，它们分别是：2，5，3，0，2，3，0，3。考生的成绩从 0 到 5 分，我们使用大小为 6 的数组 C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到 C[6]的值。从图中可以看出，分数为 3 分的考生有 3 个，小于 3 分的考生有 4 个，所以，成绩为 3 分的考生在排序之后的有序数组 R[8]中，会保存下标 4，5，6 的位置。那我们如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？这个处理方法非常巧妙，很不容易想到。思路是这样的：我们对 C[6]数组顺序求和，C[6]存储的数据就变成了下面这样子。C[k]里存储小于等于分数 k 的考生个数。有了前面的数据准备之后，现在我就要讲计数排序中最复杂、最难理解的一部分了，请集中精力跟着我的思路！我们从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3]要减 1，变成 6。以此类推，当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 R 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数组 A 后，数组 R 内的数据就是按照分数从小到大有序排列的了。这种利用另外一个数组来计数的实现方式是不是很巧妙呢？这也是为什么这种排序算法叫计数排序的原因。不过，你千万不要死记硬背上面的排序过程，重要的是理解和会用。我总结一下，计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。比如，还是拿考生这个例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。**当输入的元素是 n 个 0 到 k 之间的整数时，它的运行时间是 Θ(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。例如：计数排序是用来排序0到100之间的数字的最好的算法，但是它不适合按字母顺序排序人名。但是，计数排序可以用在基数排序中的算法来排序数据范围很大的数组。通俗地理解，例如有 10 个年龄不同的人，统计出有 8 个人的年龄比 A 小，那 A 的年龄就排在第 9 位,用这个方法可以得到其他每个人的位置,也就排好了序。当然，年龄有重复时需要特殊处理（保证稳定性），这就是为什么最后要反向填充目标数组，以及将每个数字的统计减去 1 的原因。算法的步骤如下：（1）找出待排序的数组中最大和最小的元素（2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项（3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）（4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1动图代码**123456789101112131415def countingSort(arr, maxValue): bucketLen = maxValue+1 bucket = [0]*bucketLen sortedIndex =0 arrLen = len(arr) for i in range(arrLen): if not bucket[arr[i]]: bucket[arr[i]]=0 bucket[arr[i]]+=1 for j in range(bucketLen): while bucket[j]&gt;0: arr[sortedIndex] = j sortedIndex+=1 bucket[j]-=1 return arr","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 桶排序","slug":"算法-桶排序","date":"2016-09-12T05:01:27.000Z","updated":"2020-05-22T06:12:41.814Z","comments":true,"path":"2016/09/12/算法-桶排序/","link":"","permalink":"cpeixin.cn/2016/09/12/%E7%AE%97%E6%B3%95-%E6%A1%B6%E6%8E%92%E5%BA%8F/","excerpt":"","text":"前面分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。接下来我会讲三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天学习重点的是掌握这些排序算法的适用场景。按照惯例，我先给你出一道思考题：如何根据年龄给 100 万用户排序？ 你可能会说，我用上一节课讲的归并、快排就可以搞定啊！是的，它们也可以完成功能，但是时间复杂度最低也是 O(nlogn)。有没有更快的排序方法呢？让我们一起进入今天的内容！桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。（有木有点像归并排序）桶排序的时间复杂度为什么是 O(n) 呢？我们一块儿来分析一下。如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(nlog(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？答案当然是否定的。为了让你轻松理解桶排序的核心思想，我刚才做了很多假设。实际上，桶排序对要排序数据的要求是非常苛刻的。*首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。桶排序比较适合用在外部排序**中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？现在我来讲一下，如何借助桶排序的处理思想来解决这个问题。我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02…99）。理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。不过，你可能也发现了，订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元…901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：在额外空间充足的情况下，尽量增大桶的数量使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。1. 什么时候最快当输入的数据可以均匀的分配到每一个桶中。2. 什么时候最慢当输入的数据被分配到了同一个桶中。元素分布在桶中然后，元素在每个桶中排序：代码排序一个数组[5,3,6,1,2,7,5,10]， 值都在1-10之间，建立10个桶：[0 0 0 0 0 0 0 0 0 0] 桶，十个空桶[1 2 3 4 5 6 7 8 9 10] 桶代表的值遍历数组，第一个数字5，第五个桶加11[0 0 0 0 1 0 0 0 0 0]第二个数字3，第三个桶加11[0 0 1 0 1 0 0 0 0 0]遍历后1[1 1 1 0 2 1 1 0 0 1]输出1[1 2 3 5 5 6 7 10]代码：**1234567891011121314151617181920def bucket_sort(lst): \"\"\"创建桶\"\"\" buckets = [0] * ((max(lst) - min(lst))+1) for i in range(len(lst)): \"\"\"对应下标添加标识位\"\"\" buckets[lst[i]-min(lst)] += 1 res=[] for i in range(len(buckets)): if buckets[i] != 0: res += [i+min(lst)]*buckets[i] print(res)def main(): array = [5,3,6,2,7,5,10,11,20] bucket_sort(array)if __name__ == '__main__': main()","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 快速排序","slug":"算法-快速排序","date":"2016-09-10T15:05:47.000Z","updated":"2020-05-21T15:07:21.682Z","comments":true,"path":"2016/09/10/算法-快速排序/","link":"","permalink":"cpeixin.cn/2016/09/10/%E7%AE%97%E6%B3%95-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。快速排序的名字起的是简单粗暴，因为一听到这个名字你就知道它存在的意义，就是快，而且效率高！它是处理大数据最快的排序算法之一了。虽然 Worst Case 的时间复杂度达到了 O(n²)，但是人家就是优秀，在大多数情况下都比平均时间复杂度为 O(n logn) 的排序算法表现要更好，可是这是为什么呢，我也不知道。好在我的强迫症又犯了，查了 N 多资料终于在《算法艺术与信息学竞赛》上找到了满意的答案：快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。根据分治、递归的处理思想，我们可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。如果我们用递推公式来将上面的过程写出来的话，就是这样：123456递推公式：quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)终止条件：p &gt;= r我将递推公式转化成递归代码。跟归并排序一样，我还是用伪代码来实现，你可以翻译成你熟悉的任何语言。12345678910111213// 快速排序，A是数组，n表示数组的大小quick_sort(A, n) &#123; quick_sort_c(A, 0, n-1)&#125;// 快速排序递归函数，p,r为下标quick_sort_c(A, p, r) &#123; if p &gt;= r then return q = partition(A, p, r) // 获取分区点 quick_sort_c(A, p, q-1) quick_sort_c(A, q+1, r)&#125;归并排序中有一个 merge() 合并函数，我们这里有一个 partition() 分区函数。partition() 分区函数实际上我们前面已经讲过了，就是随机选择一个元素作为 pivot（一般情况下，可以选择 p 到 r 区间的最后一个元素），然后对 A[p…r]分区，函数返回 pivot 的下标。如果我们不考虑空间消耗的话，partition() 分区函数可以写得非常简单。我们申请两个临时数组 X 和 Y，遍历 A[p…r]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。但是，如果按照这种思路实现的话，partition() 函数就需要很多额外的内存空间，所以快排就不是原地排序算法了。如果我们希望快排是原地排序算法，那它的空间复杂度得是 O(1)，那 partition() 分区函数就不能占用太多额外的内存空间，我们就需要在 A[p…r]的原地完成分区操作。原地分区函数的实现思路非常巧妙，我写成了伪代码，我们一起来看一下。1234567891011partition(A, p, r) &#123; pivot := A[r] i := p for j := p to r-1 do &#123; if A[j] &lt; pivot &#123; swap A[i] with A[j] i := i+1 &#125; &#125; swap A[i] with A[r] return i这里的处理有点类似选择排序。我们通过游标 i 把 A[p…r-1]分成两部分。A[p…i-1]的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i…r-1]是“未处理区间”。我们每次都从未处理的区间 A[i…r-1]中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i]的位置。数组的插入操作还记得吗？在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也讲了一种处理技巧，就是交换，在 O(1) 的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将 A[i]与 A[j]交换，就可以在 O(1) 时间复杂度内将 A[j]放到下标为 i 的位置。文字不如图直观，所以我画了一张图来展示分区的整个过程。因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。到此，快速排序的原理你应该也掌握了。现在，我再来看另外一个问题：快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢？可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。算法步骤从数列中挑出一个元素，称为 “基准”（pivot）;重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；动图代码1234567891011121314151617181920212223def quickSort(arr, left=None, right=None): left = 0 if not isinstance(left,(int, float)) else left right = len(arr)-1 if not isinstance(right,(int, float)) else right if left &lt; right: partitionIndex = partition(arr, left, right) quickSort(arr, left, partitionIndex-1) quickSort(arr, partitionIndex+1, right) return arrdef partition(arr, left, right): pivot = left index = pivot+1 i = index while i &lt;= right: if arr[i] &lt; arr[pivot]: swap(arr, i, index) index+=1 i+=1 swap(arr,pivot,index-1) return index-1def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i]快速排序的性能分析现在，我们来分析一下快速排序的性能。我在讲解快排的实现原理的时候，已经分析了稳定性和空间复杂度。快排是一种原地、不稳定的排序算法。现在，我们集中精力来看快排的时间复杂度。快排也是用递归来实现的。对于递归代码的时间复杂度，我前面总结的公式，这里也还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn)。123T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = 2*T(n/2) + n； n&gt;1但是，公式成立的前提是每次分区操作，我们选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的。我举一个比较极端的例子。如果数组中的数据原来已经是有序的了，比如 1，3，5，6，8。如果我们每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n2)。我们刚刚讲了两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。它们分别对应快排的最好情况时间复杂度和最坏情况时间复杂度。那快排的平均情况时间复杂度是多少呢？我们假设每次分区操作都将区间分成大小为 9:1 的两个小区间。我们继续套用递归时间复杂度的递推公式，就会变成这样：1234T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = T(n/10) + T(9*n/10) + n； n&gt;1这个公式的递推求解的过程非常复杂，虽然可以求解，但我不推荐用这种方法。实际上，递归的时间复杂度的求解方法除了递推公式之外，还有递归树，在树那一节我再讲，这里暂时不说。我这里直接给你结论：T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 二分查找_重复数据情况","slug":"算法-二分查找-重复数据情况","date":"2016-09-02T16:37:00.000Z","updated":"2020-05-15T14:58:27.795Z","comments":true,"path":"2016/09/03/算法-二分查找-重复数据情况/","link":"","permalink":"cpeixin.cn/2016/09/03/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE-%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%E6%83%85%E5%86%B5/","excerpt":"","text":"上一篇讲解了二分查找的原理，并且介绍了最简单的一种二分查找的代码实现。今天我们来讲几种二分查找的变形问题。不知道你有没有听过这样一个说法：“十个二分九个错”。二分查找虽然原理极其简单，但是想要写出没有 Bug 的二分查找并不容易。唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第 3 卷《排序和查找》中说到：“尽管第一个二分查找算法于 1946 年出现，然而第一个完全正确的二分查找算法实现直到 1962 年才出现。”你可能会说，我们上一节学的二分查找的代码实现并不难写啊。那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。二分查找的变形问题很多，我只选择几个典型的来讲解，其他的你可以借助我今天讲的思路自己来分析变体一：查找第一个值等于给定值的元素上一节中的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据1234567891011121314151617def binarySearch_first(array, array_size, search_value): \"\"\"查找第一个值等于给定值的元素\"\"\" low = 0 high = array_size - 1 while low &lt;= high: mid = low + ((high - low) &gt;&gt; 1) if array[mid] == search_value: while array[mid-1] == search_value: if mid - 1 == 0: return 0 mid -= 1 return mid elif array[mid] &lt; search_value: low = mid + 1 else: high = mid - 1这里还有一种更好的写法：123456789101112131415def binarySearch_first_better(array, array_size, search_value): \"\"\"查找第一个值等于给定值的元素\"\"\" low = 0 high = array_size - 1 while low &lt;= high: mid = low + ((high - low) &gt;&gt; 1) if array[mid] &lt; search_value: low = mid + 1 elif array[mid] &gt; search_value: high = mid - 1 else: if mid == 0 or array[mid - 1] != search_value: return mid high = mid - 1变体二：查找最后一个值等于给定值的元素前面的问题是查找第一个值等于给定值的元素，我现在把问题稍微改一下，查找最后一个值等于给定值的元素，又该如何做呢？123456789101112131415def binarySearch_last(array, array_size, search_value): \"\"\"查找最后一个值等于给定值的元素\"\"\" low = 0 high = array_size - 1 while low &lt;= high: mid = low + ((high - low) &gt;&gt; 1) if array[mid] &lt; search_value: low = mid + 1 elif array[mid] &gt; search_value: high = mid - 1 else: if mid == array_size-1 or array[mid + 1] != search_value: return mid low = mid + 1变体三：查找第一个大于等于给定值的元素现在我们再来看另外一类变形问题。在有序数组中，查找第一个大于等于给定值的元素。比如，数组中存储的这样一个序列：3，4，6，7，10。如果查找第一个大于等于 5 的元素，那就是 6。123456789101112131415public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt;= value) &#123; if ((mid == 0) || (a[mid - 1] &lt; value)) return mid; else high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; return -1;&#125;变体四：查找最后一个小于等于给定值的元素123456789101112131415public int bsearch7(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) &#123; high = mid - 1; &#125; else &#123; if ((mid == n - 1) || (a[mid + 1] &gt; value)) return mid; else low = mid + 1; &#125; &#125; return -1;&#125;实际上，很多人都觉得变形的二分查找很难写，主要原因是太追求第一种那样完美、简洁的写法。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"二分查找","slug":"二分查找","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"}]},{"title":"算法 - 归并排序","slug":"算法-归并排序","date":"2016-09-01T14:16:42.000Z","updated":"2020-05-22T06:12:39.910Z","comments":true,"path":"2016/09/01/算法-归并排序/","link":"","permalink":"cpeixin.cn/2016/09/01/%E7%AE%97%E6%B3%95-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/","excerpt":"","text":"归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法：自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第 2 种方法）；自下而上的迭代；在《数据结构与算法 JavaScript 描述》中，作者给出了自下而上的迭代方法。但是对于递归法，作者却认为：However, it is not possible to do so in JavaScript, as the recursion goes too deep for the language to handle.然而，在 JavaScript 中这种方式不太可行，因为这个算法的递归深度对它来讲太深了。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是 O(nlogn) 的时间复杂度。代价是需要额外的内存空间。归并排序（Merge Sort）的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。从我刚才的描述，你有没有感觉到，分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。前面我通过举例让你对归并有了一个感性的认识，又告诉你，归并排序用的是分治思想，可以用递归来实现。我们现在就来看看如何用递归代码来实现归并排序。写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。123456递推公式：merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))终止条件：p &gt;= r 不用再继续分解我来解释一下这个递推公式。merge_sort(p…r) 表示，给下标从 p 到 r 之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q) 和 merge_sort(q+1…r)，其中下标 q 等于 p 和 r 的中间位置，也就是 (p+r)/2。当下标从 p 到 q 和从 q+1 到 r 这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起merge，这样下标从 p 到 r 之间的数据就也排好序了。有了递推公式，转化成代码就简单多了。为了阅读方便，我这里只给出伪代码，你可以翻译成你熟悉的编程语言。算法步骤申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；设定两个指针，最初位置分别为两个已经排序序列的起始位置；比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；重复步骤 3 直到某一指针达到序列尾；将另一序列剩下的所有元素直接复制到合并序列尾。动图根据直方图的颜色来对应相应划分的组更具体的分解步骤可以看到这种结构很像一棵完全二叉树，本文的归并排序我们采用递归去实现（也可采用迭代的方式去实现）。分阶段可以理解为就是递归拆分子序列的过程，递归深度为logn。合并相邻有序子序列再来看看治**阶段，我们需要将两个已经有序的子序列合并成一个有序序列，比如上图中的最后一次合并，要将[4,5,7,8]和[1,2,3,6]两个已经有序的子序列，合并为最终序列[1,2,3,4,5,6,7,8]，来看下实现步骤。代码123456789101112131415161718192021def mergeSort(arr): import math if(len(arr)&lt;2): return arr middle = math.floor(len(arr)/2) left, right = arr[0:middle], arr[middle:] return merge(mergeSort(left), mergeSort(right))def merge(left,right): result = [] while left and right: if left[0] &lt;= right[0]: result.append(left.pop(0)) else: result.append(right.pop(0)); //针对一方为空的情况 while left: result.append(left.pop(0)) while right: result.append(right.pop(0)); return result归并排序的性能分析这样跟着我一步一步分析，归并排序是不是没那么难啦？我们来看归并排序的三个问题。第一，归并排序是稳定的排序算法吗？结合我前面画的那张图和归并排序的伪代码，你应该能发现，归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。在合并的过程中，如果 A[p…q]和 A[q+1…r]之间有值相同的元素，那我们可以像伪代码中那样，先把 A[p…q]中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。第二，归并排序的时间复杂度是多少？归并排序涉及递归，时间复杂度的分析稍微有点复杂。我们正好借此机会来学习一下，如何分析递归代码的时间复杂度。在递归那一节我们讲过，递归的适用场景是，一个问题 a 可以分解为多个子问题 b、c，那求解问题 a 就可以分解为求解问题 b、c。问题 b、c 解决之后，我们再把 b、c 的结果合并成 a 的结果。如果我们定义求解问题 a 的时间是 T(a)，求解问题 b、c 的时间分别是 T(b) 和 T( c)，那我们就可以得到这样的递推关系式：12T(a) = T(b) + T(c) + K其中 K 等于将两个子问题 b、c 的结果合并成问题 a 的结果所消耗的时间。从刚刚的分析，我们可以得到一个重要的结论：不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。套用这个公式，我们来分析一下归并排序的时间复杂度。我们假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。我们知道，merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是：123T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = 2*T(n/2) + n； n&gt;1通过这个公式，如何来求解 T(n) 呢？还不够直观？那我们再进一步分解一下计算过程。12345678T(n) = 2*T(n/2) + n = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n ...... = 2^k * T(n/2^k) + k * n ......通过这样一步一步分解推导，我们可以得到 T(n) = 2^kT(n/2^k)+kn。当 T(n/2^k)=T(1) 时，也就是 n/2^k=1，我们得到 k=log2n 。我们将 k 值代入上面的公式，得到 T(n)=Cn+nlog2n 。如果我们用大 O 标记法来表示的话，T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。第三，归并排序的空间复杂度是多少？归并排序的时间复杂度任何情况下都是 O(nlogn)，看起来非常优秀。（待会儿你会发现，即便是快速排序，最坏情况下，时间复杂度也是 O(n2)。）但是，归并排序并没有像快排那样，应用广泛，这是为什么呢？因为它有一个致命的“弱点”，那就是归并排序不是原地排序算法。这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。这一点你应该很容易理解。那我现在问你，归并排序的空间复杂度到底是多少呢？是 O(n)，还是 O(nlogn)，应该如何分析呢？如果我们继续按照分析递归时间复杂度的方法，通过递推公式来求解，那整个归并过程需要的空间复杂度就是 O(nlogn)。不过，类似分析时间复杂度那样来分析空间复杂度，这个思路对吗？实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 二分查找","slug":"算法-二分查找","date":"2016-08-31T16:37:00.000Z","updated":"2020-05-13T16:40:10.208Z","comments":true,"path":"2016/09/01/算法-二分查找/","link":"","permalink":"cpeixin.cn/2016/09/01/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","excerpt":"","text":"针对有序数据集合的查找算法：二分查找（Binary Search）算法，也叫折半查找算法。二分查找的思想非常简单，很多非计算机专业的同学很容易就能理解，但是看似越简单的东西往往越难掌握好，想要灵活应用就更加困难。假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB，你会怎么做呢？无处不在的二分思想二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。比如说，我们现在来做一个猜字游戏。我随机写一个 0 到 99 之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？假设我写的数字是 23，你可以按照下面的步骤来试一试。（如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。）7 次就猜出来了，是不是很快？这个例子用的就是二分思想，按照这个思想，即便我让你猜的是 0 到 999 的数字，最多也只要 10 次就能猜中。不信的话，你可以试一试。这是一个生活中的例子，我们现在回到实际的开发场景中。假设有 1000 条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于 19 元的订单。如果存在，则返回订单数据，如果不存在则返回 null。最简单的办法当然是从第一个订单开始，一个一个遍历这 1000 个订单，直到找到金额等于 19 元的订单为止。但这样查找会比较慢，最坏情况下，可能要遍历完这 1000 条记录才能找到。那用二分查找能不能更快速地解决呢？为了方便讲解，我们假设只有 10 个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。还是利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，我画了一张查找过程的图。其中，low 和 high 表示待查找区间的下标，mid 表示待查找区间的中间元素下标。看懂这两个例子，你现在对二分的思想应该掌握得妥妥的了。我这里稍微总结升华一下，二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。O(logn) 惊人的查找速度二分查找是一种非常高效的查找算法，高效到什么程度呢？我们来分析一下它的时间复杂度。我们假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。可以看出来，这是一个等比数列。其中 n/2k=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过 n/2k=1，我们可以求得 k=log2n，所以时间复杂度就是 O(logn)。二分查找是我们目前为止遇到的第一个时间复杂度为 O(logn) 的算法。后面章节我们还会讲堆、二叉树的操作等等，它们的时间复杂度也是 O(logn)。我这里就再深入地讲讲 O(logn) 这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1) 的算法还要高效。为什么这么说呢？因为 logn 是一个非常“恐怖”的数量级，即便 n 非常非常大，对应的 logn 也很小。比如 n 等于 2 的 32 次方，这个数很大了吧？大约是 42 亿。也就是说，如果我们在 42 亿个数据中用二分查找一个数据，最多需要比较 32 次。我们前面讲过，用大 O 标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1) 有可能表示的是一个非常大的常量值，比如 O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高。反过来，对数对应的就是指数。有一个非常著名的“阿基米德与国王下棋的故事”，你可以自行搜索一下，感受一下指数的“恐怖”。这也是为什么我们说，指数时间复杂度的算法在大规模数据面前是无效的。二分查找的递归与非递归实现实际上，简单的二分查找并不难写，注意我这里的“简单”二字。下一节，我们会讲到二分查找的变体问题，那才是真正烧脑的。今天，我们来看如何来写最简单的二分查找。最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。我用 python 代码实现了一个最简单的二分查找算法。12345678910111213141516171819202122232425def binarySearch(array, array_size, search_value): low = 0 high = array_size - 1 while low &lt;= high: mid = low+((high-low)&gt;&gt;1) if array[mid] == search_value: return mid elif mid &lt; search_value: low = mid+1 else: high = mid-1def main(): array = [] for i in range(100): array.append(i) mid_num = binarySearch(array,len(array),57) print(mid_num)if __name__ == '__main__': main()实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)&gt;&gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。二分查找应用场景的局限性前面我们分析过，二分查找的时间复杂度是 O(logn)，查找数据的效率非常高。不过，并不是什么情况下都可以用二分查找，它的应用场景是有很大局限性的。那什么情况下适合用二分查找，什么情况下不适合呢？首先，二分查找依赖的是顺序表结构，简单点说就是数组。那二分查找能否依赖其他数据结构呢？比如链表。答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元素。我们在数组和链表那两节讲过，数组按照下标随机访问数据的时间复杂度是 O(1)，而链表随机访问的时间复杂度是 O(n)。所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。其次，二分查找针对的是有序数据。二分查找对这一点的要求比较苛刻，数据必须是有序的。如果数据没有序，我们需要先排序。前面章节里我们讲到，排序的时间复杂度最低是 O(nlogn)。所以，如果我们针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。但是，如果我们的数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都是很高的。所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。那针对动态数据集合，我们可以选择二叉树。再次，数据量太小不适合二分查找。如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了。比如我们在一个大小为 10 的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。不过，这里有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过 300 的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。最后，数据量太大也不适合二分查找。二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间。注意这里的“连续”二字，也就是说，即便有 2GB 的内存空间剩余，但是如果这剩余的 2GB 内存空间都是零散的，没有连续的 1GB 大小的内存空间，那照样无法申请一个 1GB 大小的数组。而我们的二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。解答开篇二分查找的理论知识你应该已经掌握了。我们来看下开篇的思考题：如何在 1000 万个整数中快速查找某个整数？这个问题并不难。我们的内存限制是 100MB，每个数据大小是 8 字节，最简单的办法就是将数据存储在数组中，内存占用差不多是 80MB，符合内存的限制。借助今天讲的内容，我们可以先对这 1000 万数据从小到大排序，然后再利用二分查找算法，就可以快速地查找想要的数据了。看起来这个问题并不难，很轻松就能解决。实际上，它暗藏了“玄机”。如果你对数据结构和算法有一定了解，知道散列表、二叉树这些支持快速查找的动态数据结构。你可能会觉得，用散列表和二叉树也可以解决这个问题。实际上是不行的。虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但是，我们后面会讲，不管是散列表还是二叉树，都会需要比较多的额外的内存空间。如果用散列表或者二叉树来存储这 1000 万的数据，用 100MB 的内存肯定是存不下的。而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题。总结二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"二分查找","slug":"二分查找","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"}]},{"title":"算法 - 选择排序","slug":"算法-选择排序","date":"2016-08-22T15:22:43.000Z","updated":"2020-05-21T09:13:39.426Z","comments":true,"path":"2016/08/22/算法-选择排序/","link":"","permalink":"cpeixin.cn/2016/08/22/%E7%AE%97%E6%B3%95-%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/","excerpt":"","text":"选择排序是一种简单直观的排序算法，无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。动态示例图算法步骤首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。重复第二步，直到所有元素均排序完毕。代码12345678def selection_sort(array): for i in range(0, len(array)-1): min_index = i for j in range(i+1, len(array)): if array[j] &lt; array[min_index]: min_index = j if i != min_index: array[i], array[min_index] = array[min_index], array[i]照例，也有三个问题需要你思考首先，选择排序空间复杂度为 O(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)。你可以自己来分析看看。那选择排序是稳定的排序算法吗？这个问题我着重来说一下。答案是否定的，选择排序是一种不稳定的排序算法。从我前面画的那张图中，你可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。比如 5，8，5，2，9 这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素 2，与第一个 5 交换位置，那第一个 5 和中间的 5 顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法-插入排序","slug":"算法-插入排序","date":"2016-08-21T15:22:43.000Z","updated":"2020-05-21T09:12:58.909Z","comments":true,"path":"2016/08/21/算法-插入排序/","link":"","permalink":"cpeixin.cn/2016/08/21/%E7%AE%97%E6%B3%95-%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/","excerpt":"","text":"插入排序的代码实现虽然没有冒泡排序和选择排序那么简单粗暴，但它的原理应该是最容易理解的了，因为只要打过扑克牌的人都应该能够秒懂。插入排序是一种最简单直观的排序算法，它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序和冒泡排序一样，也有一种优化算法，叫做拆半插入。我们先来看一个问题。一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。那插入排序具体是如何借助上面的思想来实现排序的呢？首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。为什么说移动次数就等于逆序度呢？我拿刚才的例子画了一个图表，你一看就明白了。满有序度是 n*(n-1)/2=15，初始序列的有序度是 5，所以逆序度是 10。插入排序中，数据移动的个数总和也等于 10=3+3+4。下面给出插入排序的动图代码如下：123456789def insert_sort_1(arr): for i in range(len(arr)): preIndex = i-1 current = arr[i] while preIndex &gt;= 0 and arr[preIndex] &gt; current: arr[preIndex+1] = arr[preIndex] preIndex-=1 arr[preIndex+1] = current return arr这里有一点需要注意，如上面的代码，current是我们每次进行比较的值，每次与前面的preIndex值比较，如果比preIndex值小，则preIndex值后移，current值这时候不要与preIndex的值进行交换，这样就写成了冒泡排序😄。现在，我们来看点稍微复杂的东西。我这里还是有三个问题要问你。第一，插入排序是原地排序算法吗？从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。第二，插入排序是稳定的排序算法吗？在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。第三，插入排序的时间复杂度是多少？如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n)。注意，这里是从尾到头遍历已经有序的数据。如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n2)。还记得我们在数组中插入一个数据的平均时间复杂度是多少吗？没错，是 O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n2)。冒泡排序和插入排序的时间复杂度都是 O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？我们前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。但是，从前面的代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个。我们把执行一个赋值语句的时间粗略地计为单位时间（unit_time），然后分别用冒泡排序和插入排序对同一个逆序度是 K 的数组进行排序。用冒泡排序，需要 K 次交换操作，每次需要 3 个赋值语句，所以交换操作总耗时就是 3*K 单位时间。而插入排序中数据移动操作只需要 K 个单位时间。这个只是我们非常理论的分析，对两种排序性能对比测试程序，随机生成 10000 个数组，每个数组中包含 200 个数据，然后在我的机器上分别用冒泡和插入排序算法来排序，冒泡排序算法大约 700ms 才能执行完成，而插入排序只需要 100ms 左右就能搞定！所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n2)，但是如果我们希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，我们只是讲了最基础的一种。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法-冒泡排序","slug":"算法-冒泡排序","date":"2016-08-20T15:22:43.000Z","updated":"2020-05-22T06:12:49.940Z","comments":true,"path":"2016/08/20/算法-冒泡排序/","link":"","permalink":"cpeixin.cn/2016/08/20/%E7%AE%97%E6%B3%95-%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法，可能就是排序。大部分编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序。排序非常重要，所以我会花多一点时间来详细讲一讲经典的排序算法。排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。更为精简的，在《算法图解》这本书中，排序模块只拿选择排序作为典型进行了讲解我按照时间复杂度把它们分成了三类，带着问题去学习，是最有效的学习方法。所以按照惯例，我还是先给你出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是 O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？你可以先思考一两分钟，带着这个问题，我们开始今天的内容！如何分析一个“排序算法”？学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：1. 最好情况、最坏情况、平均情况时间复杂度我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。2. 时间复杂度的系数、常数 、低阶我们知道，时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。3. 比较次数和交换（或移动）次数基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。排序算法的内存消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。插入排序、冒泡排序、选择排序都是原地排序算法。**排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。我通过一个例子来解释一下。比如我们有一组数据 2，9，3，4，8，3，按照大小排序之后就是 2，3，3，4，8，9。这组数据里有两个 3。经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。你可能要问了，两个 3 哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序。比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。冒泡排序（Bubble Sort）我们从冒泡排序开始，冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。下面就不讲具体的排序过程了，一张冒泡排序动图，带你看下冒泡排序的整个过程。冒泡排序算法的原理比较容易理解，具体的代码我贴到下面，你可以结合着代码来看我前面讲的原理。12345678910111213141516171819202122232425262728293031323334353637import time\"\"\"第一个(外层)for循环作用：控制排序的轮数第二个(内层)for循环作用：控制每一轮里的每一个比较步骤\"\"\"def bubble_sort(arr): for i in range(1, len(arr)): for j in range(0, len(arr)-i): if arr[j] &gt; arr[j+1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] # 可以配合打印i,j来更好的理解 # print(i,j) # time.sleep(2) return arrdef bubble_sort_v2(arr): \"\"\"改进版，避免右侧最大值重复比较\"\"\" for i in range(len(arr) - 1, 0, -1): # 反向遍历 for j in range(0, i): # 由于最右侧的值已经有序，不再比较，每次都减少遍历次数 if arr[j] &gt; arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] return arrdef main(): array = [3,4,1,2,5,6] result_array = bubble_sort(array) print(result_array)if __name__ == '__main__': main()现在，结合刚才我分析排序算法的三个方面，我有三个问题要问你。第一，冒泡排序是原地排序算法吗？冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。第二，冒泡排序是稳定的排序算法吗？在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。第三，冒泡排序的时间复杂度是多少？最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n2)。最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。对于包含 n 个数据的数组，这 n 个数据就有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：12有序元素对：a[i] &lt;= a[j], 如果i &lt; j。同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是 n(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作*满有序度**。逆序度的定义正好跟有序度相反（默认从小到大为有序），我想你应该已经想到了。关于逆序度，我就不举例子讲了。你可以对照我讲的有序度的例子自己看下。12逆序元素对：a[i] &gt; a[j], 如果i &lt; j。关于这三个概念，我们还可以得到一个公式：逆序度 = 满有序度 - 有序度。我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是 4，5，6，3，2，1 ，其中，有序元素对有 (4，5) (4，6)(5，6)，所以有序度是 3。n=6，所以排序完成之后终态的满有序度为 n(n-1)/2=15。冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n(n-1)/2–初始有序度。此例中就是 15–3=12，要进行 12 次交换操作。对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n(n-1)/2 次交换。最好情况下，初始状态的有序度是 n(n-1)/2，就不需要进行交换。我们可以取个中间值 n(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。换句话说，平均情况下，需要 n(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以平均情况下的时间复杂度就是 O(n2)。这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"数据结构-链表反转","slug":"数据结构-链表反转","date":"2016-08-18T14:16:51.000Z","updated":"2020-04-04T17:34:24.764Z","comments":true,"path":"2016/08/18/数据结构-链表反转/","link":"","permalink":"cpeixin.cn/2016/08/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/","excerpt":"","text":"题目：输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL这道题目可以用迭代，递归两种方法来实现迭代假设存在链表 1 → 2 → 3 → 4 → 5 → Ø，我们想要把它改成 Ø ← 1 ← 2 ← 3 ← 4 ← 5。在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！根据题目总结一下核心的操作逻辑：从头节点 1 开始，1 指向 2，更改指向顺序，需要将 2 的指针指向 1 (此时，虽然 2 的指针方向已经指向 1，但是 1 的指针方向没有改变，依然指向 2，则是 2 → 1 → 2 → 1….. ), 同时也要存储 2 的下一个节点 3，以便接下来迭代操作 2 和 3 节点之间的指针反转，每两个相邻节点进行相同的操作。迭代最后，尾节点 5 无后续节点，则跳出迭代。在迭代完相邻节点的指针反转后，还需要做的两步原头节点指向 None，变成尾节点将原尾节点设置成新的头节点操作完成～～！！12345678910111213141516171819202122232425262728293031323334def reversed_self(self): \"\"\"翻转链表自身.\"\"\" if self.head is None or self.head.next_node is None: return pre = self.head node = self.head.next_node while node is not None: pre,node = self.__reversed_with_two_node(pre,node) \"\"\"循环到最后一位 node is None退出\"\"\" \"\"\"将原链表的头节点指向为下一节点 改为 指向为None\"\"\" self.head.next_node = None \"\"\"将头节点设置为原链表的尾节点,链表反转则成功。链表元素位置不变，但是元素之间的指向改变\"\"\" self.head = predef __reversed_with_two_node(self, pre, node): \"\"\"翻转相邻两个节点. 参数: pre:前一个节点 node:当前节点 返回: *******(pre,node):下一个相邻节点的元组 \"\"\" tmp = node.next_node node.next_node = pre pre = node node = tmp return pre, node调试图：123while node is not None: pre,node = self.__reversed_with_two_node(pre,node)核心步骤迭代完，链表中的节点指向已经反转上图。此时，节点之间指针反转完毕，虽然指针反转了，但是头节点还是 1 ， 因为 1 指向仍然为 2 还没有处理。并且 5 还不是头节点，对应的还没有执行12self.head.next_node = Noneself.head = pre操作完下面两步后，上图中，1 的下一个节点指针变成None， 头节点赋值为 512self.head.next_node = Noneself.head = pre","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-如何写好链表代码","slug":"数据结构-如何写好链表代码","date":"2016-08-17T14:15:49.000Z","updated":"2020-04-04T11:59:24.090Z","comments":true,"path":"2016/08/17/数据结构-如何写好链表代码/","link":"","permalink":"cpeixin.cn/2016/08/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E5%86%99%E5%A5%BD%E9%93%BE%E8%A1%A8%E4%BB%A3%E7%A0%81/","excerpt":"","text":"如何写好链表代码？理解指针或引用的含义什么是指针？指针是一个变量将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。警惕指针丢失和内存泄漏在插入和删除结点时，要注意先持有后面的结点再操作，否者一旦后面结点的前继指针被断开，就无法再访问，导致内存泄漏。插入结点时，一定要注意操作的顺序删除链表结点时，也一定要记得手动释放内存空间利用哨兵简化难度链表的插入、删除操作，需要对插入第一个结点和删除最后一个节点做特殊处理。利用哨兵对象可以不用边界判断，链表的哨兵对象是只存指针不存数据的头结点。重点留意边界条件处理操作链表时要考虑如果链表为空时，代码是否能正常工作？如果链表只包含一个结点时，代码是否能正常工作？如果链表只包含两个结点时，代码是否能正常工作？代码逻辑在处理头结点和尾结点的时候，是否能正常工作？学习数据结构和算法主要是掌握一系列思想，能在其它的编码中也养成考虑边界的习惯。举例画图，辅助思考对于比较复杂的操作，可以用纸笔画一画，释放脑容量来做逻辑处理（时间换空间思想），也便于完成后的检查。多写多练，没有捷径孰能生巧，不管是什么算法，只有经过反复的练习，才能信手拈来。哨兵对象思想，在 iOS AutoreleasePool 中有用到，在 AutoreleasePoolPush 时添加一个哨兵对象，Pop 时将到哨兵对象之间的所有 Autorelease 对象发送 release 消息。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-如何实现LRU缓存淘汰算法","slug":"数据结构-如何实现LRU缓存淘汰算法","date":"2016-08-16T07:30:21.000Z","updated":"2020-04-04T11:59:17.945Z","comments":true,"path":"2016/08/16/数据结构-如何实现LRU缓存淘汰算法/","link":"","permalink":"cpeixin.cn/2016/08/16/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","excerpt":"","text":"今天我们来聊聊“链表（Linked list）”这个数据结构。学习链表有什么用呢？为了回答这个问题，我们先来讨论一个经典的链表应用场景，那就是 LRU 缓存淘汰算法。缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）最少使用策略 LFU（Least Frequently Used）最近最少使用策略 LRU（Least Recently Used）这些策略你不用死记，我打个比方你很容易就明白了。假如说，你买了很多本技术书，但有一天你发现，这些书太多了，太占书房空间了，你要做个大扫除，扔掉一些书籍。那这个时候，你会选择扔掉哪些书呢？对应一下，你的选择标准是不是和上面的三种策略神似呢？好了，回到正题，我们今天的开篇问题就是：如何用链表来实现 LRU 缓存淘汰策略呢？ 带着这个问题，我们开始今天的内容吧！五花八门的链表结构相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常将会放到一块儿来比较。所以我们先来看，这两者有什么区别。我们先从底层的存储结构上来看一看。为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表双向链表循环链表我们首先来看最简单、最常用的单链表。我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针 next。从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。与数组一样，链表也支持数据的查找、插入和删除操作。我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。但是，有利就有弊。链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第 k 位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要 O(n) 的时间复杂度。好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，循环链表和双向链表。循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。单链表和循环链表是不是都不难？接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适合解决哪种问题呢？从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是 O(1) 了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。我们先来看删除操作。在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：删除结点中“值等于某个给定值”的结点；删除给定指针指向的结点。对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉 Java 语言，你肯定用过 LinkedHashMap 这个容器。如果你深入研究 LinkedHashMap 的实现原理，就会发现其中就用到了双向链表这种数据结构。实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。**当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。所以我总结一下，对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不用我多讲，你应该知道双向循环链表长什么样子了吧？你可以自己试着在纸上画一画。链表 VS 数组性能大比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。你可能会说，我们 Java 中的 ArrayList 容器，也可以支持动态扩容啊？我们上一节课讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。我举一个稍微极端的例子。如果我们用 ArrayList 存储了了 1GB 大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList 会申请一个 1.5GB 大小的存储空间，并且把原来那 1GB 的数据拷贝到新申请的空间上。听起来是不是就很耗时？除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。解答开篇好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。如果此数据没有在缓存链表中，又可以分为两种情况：如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。这样我们就用链表实现了一个 LRU 缓存，是不是很简单？现在我们来看下 m 缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？我把这个问题留给你思考。LRU最后回到前题，来实现LRU缓存机制设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。O(1) 时间复杂度内完成这两种操作。设计lru算法的思路如何表示最近访问的数据和最早访问的数据如何查找是否缓存了数据有缓存，如何处理数据没有缓存，如何处理缓存未满缓存已满实现LRU数据结构选型有序字典题目要求实现 LRU 缓存机制，需要在 O(1)O(1) 时间内完成如下操作：获取键 / 检查键是否存在设置键删除最先插入的键前两个操作可以用标准的哈希表在 O(1)O(1) 时间内完成。有一种叫做有序字典的数据结构，综合了哈希表和链表，在 Python 中为 OrderedDict，在 Java 中为 LinkedHashMap。哈希表 + 双向链表这个问题可以用哈希表，辅以双向链表记录键值对的信息。所以可以在 O(1) 时间内完成 put 和 get 操作，同时也支持 O(1) 删除第一个添加的节点。使用双向链表的一个好处是不需要额外信息删除一个节点，同时可以在常数时间内从头部或尾部插入删除节点。一个需要注意的是，在双向链表实现中，这里使用一个伪头部和伪尾部标记界限，这样在更新的时候就不需要检查是否是 null 节点。实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125class DbListNode(object): def __init__(self, x, y): \"\"\" 节点为哈希表+双向链表 :param x: :param y: \"\"\" self.key = x self.value = y self.next = None self.prev = Noneclass LRUCache(object): def __init__(self, capacity): \"\"\" 初始化一个空双向链表 :type capacity: int \"\"\" self.cap = capacity self.catche = &#123;&#125; self.top = DbListNode(None, -1) self.tail = DbListNode(None, -1) self.top.next = self.tail self.tail.prev = self.top def get(self, key): \"\"\" :type key: int :rtype: int \"\"\" \"\"\"判断节点是否存在\"\"\" if key in self.catche.keys(): cur = self.catche[key] \"\"\"首先跳出原来的位置\"\"\" cur.prev.next = cur.next cur.next.prev = cur.prev \"\"\"top,tail为哨兵节点\"\"\" top_node = self.top.next cur.next = top_node top_node.prev = cur self.top.next = cur cur.prev = self.top return cur.value return -1 def put(self, key, value): \"\"\" :type key: int :type value: int :rtype: None \"\"\" if key in self.catche.keys(): \"\"\"如果插入节点存在，则将插入节点调换为位，插入到哨兵节点后的头节点。此时不存在增删节点，所以不用判断链表长度\"\"\" cur = self.catche[key] \"\"\"首先跳出原来的位置\"\"\" cur.prev.next = cur.next cur.next.prev = cur.prev \"\"\"top,tail为哨兵节点\"\"\" top_node = self.top.next cur.next = top_node top_node.prev = cur self.top.next = cur cur.prev = self.top else: # 增加新结点至首部 cur = DbListNode(key, value) self.catche[key] = cur # 最近用过的置于链表首部 top_node = self.top.next self.top.next = cur cur.prev = self.top cur.next = top_node top_node.prev = cur \"\"\"判断长度删除尾节点\"\"\" if len(self.catche.keys()) &gt; self.cap: self.catche.pop(self.tail.prev.key) # 去掉原尾结点 self.tail.prev.prev.next = self.tail self.tail.prev = self.tail.prev.prev def __repr__(self): vals = [] p = self.top.next while p.next: vals.append(str(p.value)) p = p.next return '-&gt;'.join(vals)if __name__ == '__main__': cache = LRUCache(2) cache.put(1, 1) cache.put(2, 2) print(cache) cache.get(1) # 返回 1 print(cache) cache.put(3, 3) # 该操作会使得密钥 2 作废 print(cache) cache.get(2) # 返回 -1 (未找到) print(cache) cache.put(4, 4) # 该操作会使得密钥 1 作废 print(cache) cache.get(1) # 返回 -1 (未找到) cache.get(3) # 返回 3 print(cache) cache.get(4) # 返回 4 print(cache)内容小结今天我们讲了一种跟数组“相反”的数据结构，链表。它跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。课后思考如何判断一个字符串是否是回文字符串的问题，我想你应该听过，我们今天的题目就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？欢迎留言和我分享，我会第一时间给你反馈。Q&amp;A关于CPU缓存机制CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块(这个大小我不太确定。。)并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LRU淘汰算法","slug":"LRU淘汰算法","permalink":"cpeixin.cn/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"}]},{"title":"数据结构-栈","slug":"数据结构-栈","date":"2016-08-14T09:27:21.000Z","updated":"2020-04-04T17:30:57.853Z","comments":true,"path":"2016/08/14/数据结构-栈/","link":"","permalink":"cpeixin.cn/2016/08/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88/","excerpt":"","text":"浏览器的前进、后退功能，我想你肯定很熟悉吧？当你依次访问完一串页面 a-b-c 之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面 b 和 a。当你后退到页面 a，点击前进按钮，就可以重新查看页面 b 和 c。但是，如果你后退到页面 b 后，点击了新的页面 d，那就无法再通过前进、后退功能查看页面 c 了。假设你是 Chrome 浏览器的开发工程师，你会如何实现这个功能呢？这就要用到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。如何理解“栈”？关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为我觉得，相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表不就好了吗？为什么还要用这个“操作受限”的“栈”呢？事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。如何实现一个“栈”？从刚才栈的定义里，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。我这里实现一个基于数组的顺序栈。基于链表实现的链式栈的代码，基于数组12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class stack: def __init__(self, size): \"\"\" 栈结构 :param size: 栈大小 \"\"\" self.count = 0 self.size = size self.array = [] def push(self, value): \"\"\" 入栈 入栈判满 :param value: \"\"\" if self.count == self.size: return False self.array.append(value) self.count += 1 return True def pop(self): \"\"\" 出栈 出栈判空 :return: \"\"\" if self.count == 0: return False data = self.array[self.count - 1] self.count -= 1 return dataif __name__ == '__main__': st = stack(4) st.push(1) st.push(2) st.push(3) st.push(4) st.pop() st.pop() st.pop() data = st.pop() print(data) st.pop()基于链表12345678910111213141516171819202122232425262728293031323334353637383940414243from typing import Optionalclass Node: def __init__(self, data: int, next=None): self.data = data self.next = nextclass linkstack: def __init__(self): self.top: Node = None def push(self, value: int): new_node = Node(value) new_node.next = self.top self.top = new_node def pop(self)-&gt;Optional[int]: if self.top: data = self.top.data self.top = self.top.next return data def __repr__(self) -&gt; str: current = self._top nums = [] while current: nums.append(current._data) current = current._next return \" \".join(f\"&#123;num&#125;]\" for num in nums)if __name__ == \"__main__\": stack = linkstack() for i in range(9): stack.push(i) print(stack) for _ in range(3): stack.pop() print(stack)了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？不管是顺序栈还是链式栈，我们存储数据只需要一个大小为 n 的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。注意，这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。因为，这 n 个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。空间复杂度分析是不是很简单？时间复杂度也不难。不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 O(1)。支持动态扩容的顺序栈刚才那个基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储 next 指针，内存消耗相对较多。那我们如何基于数组实现一个可以支持动态扩容的栈呢？你还记得，我们在数组那一节，是如何来实现一个支持动态扩容的数组的吗？当数组空间不够时，我们就重新申请一块更大的内存，将原来数组中数据统统拷贝过去。这样就实现了一个支持动态扩容的数组。所以，如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。我画了一张图，你可以对照着理解一下。实际上，支持动态扩容的顺序栈，我们平时开发中并不常用到。我讲这一块的目的，主要还是希望带你练习一下前面讲的复杂度分析方法。所以这一小节的重点是复杂度分析。你不用死记硬背入栈、出栈的时间复杂度，你需要掌握的是分析方法。能够自己分析才算是真正掌握了。现在我就带你分析一下支持动态扩容的顺序栈的入栈、出栈操作的时间复杂度。对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。也就是说，对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。那平均情况下的时间复杂度又是多少呢？还记得我们在复杂度分析那一节中讲的摊还分析法吗？这个入栈操作的平均情况下的时间复杂度可以用摊还分析法来分析。我们也正好借此来实战一下摊还分析法。为了分析的方便，我们需要事先做一些假设和定义：栈空间不够时，我们重新申请一个是原来大小两倍的数组；为了简化分析，假设只有入栈操作没有出栈操作；定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。为了让你更加直观地理解这个过程，我画了一张图。你应该可以看出来，这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。通过这个例子的实战分析，也印证了前面讲到的，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。栈在函数调用中的应用前面我讲的都比较偏理论，我们现在来看下，栈在软件工程中的实际应用。栈作为一个比较基础的数据结构，应用场景还是蛮多的。其中，比较经典的一个应用场景就是函数调用栈。我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。为了让你更好地理解，我们一块来看下这段代码的执行过程。123456789101112131415int main() &#123; int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(\"%d\", res); reuturn 0;&#125;int add(int x, int y) &#123; int sum = 0; sum = x + y; return sum;&#125;从代码中我们可以看出，main() 函数调用了 add() 函数，获取计算结果，并且与临时变量 a 相加，最后打印 res 的值。为了让你清晰地看到这个过程对应的函数栈里出栈、入栈的操作，我画了一张图。图中显示的是，在执行到 add() 函数时，函数调用栈的情况。栈在表达式求值中的应用我们再来看栈的另一个常见的应用场景，编译器如何利用栈来实现表达式求值。为了方便解释，我将算术表达式简化为只包含加减乘除四则运算，比如：34+139+44-12/3。对于这个四则运算，我们人脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事儿。如果换作你，让你来实现这样一个表达式求值的功能，你会怎么做呢？实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。我将 3+58-6 这个表达式的计算过程画成了一张图，你可以结合图来理解我刚讲的计算过程。这样用两个栈来解决的思路是不是非常巧妙？你有没有想到呢？栈在括号匹配中的应用除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[] ()[{}]}或[{()}([])]等都为合法格式，而{[}()]或[({)]为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。解答开篇好了，我想现在你已经完全理解了栈的概念。我们再回来看看开篇的思考题，如何实现浏览器的前进、后退功能？其实，用两个栈就可以非常完美地解决这个问题。我们使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。比如你顺序查看了 a，b，c 三个页面，我们就依次把 a，b，c 压入栈，这个时候，两个栈的数据就是当你通过浏览器的后退按钮，从页面 c 后退到页面 a 之后，我们就依次把 c 和 b 从栈 X 中弹出，并且依次放入到栈 Y。这个时候，两个栈的数据就是这个样子：这个时候你又想看页面 b，于是你又点击前进按钮回到 b 页面，我们就把 b 再从栈 Y 中出栈，放入栈 X 中。此时两个栈的数据是这个样子：这个时候，你通过页面 b 又跳转到新的页面 d 了，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。此时两个栈的数据这个样子：内容小结我们来回顾一下今天讲的内容。栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。除此之外，我们还讲了一种支持动态扩容的顺序栈，你需要重点掌握它的均摊时间复杂度分析方法。课后思考为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。我们都知道，JVM 内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"栈","slug":"栈","permalink":"cpeixin.cn/tags/%E6%A0%88/"}]},{"title":"数据结构-队列","slug":"数据结构-队列","date":"2016-08-13T10:30:21.000Z","updated":"2020-04-04T17:32:07.465Z","comments":true,"path":"2016/08/13/数据结构-队列/","link":"","permalink":"cpeixin.cn/2016/08/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97/","excerpt":"","text":"我们知道，CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学的内容，队列（queue）。如何理解“队列”队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的 “队列”。我们知道，栈只支持两个基本操作： 入栈 push()和出栈 pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。所以，队列跟栈一样，也是一种操作受限的线性表数据结构。队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。顺序队列和链式队列我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在队头删除元素，那究竟该如何实现一个队列呢？跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。顺序队列数组实现**我们先来思考一下用数组实现队列，对于栈来说，我们只需要一个栈顶指针就可以了。但是队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾。你可以结合下面这幅图来理解。当 a、b、c、d 依次入队之后，队列中的 head 指针指向下标为 0 的位置，tail 指针指向下标为 4 的位置。当我们调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置。你肯定已经发现了，在固定长度的数组中，随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。再向后加，就会产生数组越界的错误。可实际上，我们的队列在下标为0和1的地方还是空闲的。此时又不应该扩充数组，我们把这种现象叫做“假溢出”。否则会造成数组越界而遭致程序出错。这个问题该如何解决呢？你是否还记得，在数组那一节，我们也遇到过类似的问题，就是数组的删除操作会导致数组中的数据不连续。你还记得我们当时是怎么解决的吗？对，用数据搬移！但是，每次进行出队操作都相当于删除数组下标为 0 的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的 O(1) 变为 O(n)。能不能优化一下呢？实际上，我们在出队时可以不用搬移数据。如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作。借助这个思想，出队函数 dequeue() 保持不变，我们稍加改造一下入队函数 enqueue() 的实现，就可以轻松解决刚才的问题了。下面是具体的代码：我们先来看下基于数组，需要进行数据迁移的实现方法。12345678910111213141516171819202122232425262728293031from typing import Optionalclass ArrayQueue: def __init__(self, capacity: int): self._items = [] self._capacity = capacity self._head = 0 self._tail = 0 def enqueue(self, item: str) -&gt; bool: if self._tail == self._capacity: if self._head == 0: return False else: for i in range(0, self._tail - self._head): self._items[i] = self._items[i + self._head] self._tail = self._tail - self._head self._head = 0 self._items.insert(self._tail, item) self._tail += 1 return True def dequeue(self) -&gt; Optional[str]: if self._head != self._tail: item = self._items[self._head] self._head += 1 return item else: return None从代码中我们看到，当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。链式队列链表实现接下来，我们再来看下基于链表的队列实现方法。基于链表的实现，我们同样需要两个指针：head 指针和 tail 指针。它们分别指向链表的第一个结点和最后一个结点。如图所示，入队时，tail-&gt;next= new_node, tail = tail-&gt;next；出队时，head = head-&gt;next。123456789101112131415161718192021222324252627282930313233343536373839404142class Node: def __init__(self, data, next_node=None): self.data = data self.next = next_nodeclass LinkedQueue: def __init__(self): self.head = None self.tail = None def enqueue(self, data): new_node = Node(data) if self.tail: self.tail.next = new_node else: self.head = new_node self.tail = new_node def dequeue(self): if self.head is None: return False self.head = self.head.next \"\"\"移除头节点后，如果头节点为空，则尾节点也\"\"\" if not self.head: self.tail = Noneif __name__ == '__main__': linked_queue = LinkedQueue() linked_queue.enqueue(1) linked_queue.enqueue(2) linked_queue.enqueue(3) linked_queue.dequeue() linked_queue.dequeue() linked_queue.dequeue() print(linked_queue.tail)循环队列循环队列动态效果如下图：我们刚才用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？我们来看看循环队列的解决思路。循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。现在我们把首尾相连，扳成了一个环, 可以想像像钟表一样，enqueue和dequeue节点时，tail, head指针都是逆时针运动。我们可以看到，图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子：通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有 bug 的循环队列的实现代码，我个人觉得，最关键的是，确定好队空和队满的判定条件。在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。那针对循环队列，如何判断队空和队满呢？队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。我画了一张队列满的图，你可以看一下，试着总结一下规律。就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。多画几张队满的图，你就会发现，当队满时，(tail+1)%n=head。你有没有发现，当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。12345678910111213141516171819202122232425262728293031323334class circular_queue(): def __init__(self, size): self.array = [] self.head = 0 self.tail = 0 self.size = size def enqueue(self, value): \"\"\"入队判满\"\"\" if (self.tail + 1) % self.size == self.head: return False self.array.append(value) \"\"\"入队，尾节点下标变化\"\"\" self.tail = (self.tail + 1) % self.size return True def dequeue(self): \"\"\"出队判空\"\"\" if self.head != self.tail: item = self.array[self.head] \"\"\"出队，头节点下标变化\"\"\" self.head = (self.head + 1) % self.size return itemif __name__ == '__main__': q = circular_queue(5) for i in range(5): q.enqueue(i) q.dequeue() q.dequeue() q.enqueue(1) print(q)队列满的表达式这里讲一下，这个表达式是怎么来的。在一般情况下，我们可以看出来，当队列满时，tail+1=head。但是，有个特殊情况，就是tail=n-1，而head=0时，这时候，tail+1=n，而head=0，所以用(tail+1)%n == n%n == 0。而且，tail+1最大的情况就是 n ，不会大于 n，这样，tail+1 除了最大情况，不然怎么余 n 都是 tail+1 本身，也就是 head。这样，表达式就出现了。阻塞队列和并发队列前面讲的内容理论比较多，看起来很难跟实际的项目开发扯上关系。确实，队列这种数据结构很基础，平时的业务开发不大可能从零实现一个队列，甚至都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。你应该已经发现了，上述的定义就是一个“生产者 - 消费者模型”！是的，我们可以使用阻塞队列，轻松实现一个“生产者 - 消费者模型”！这种基于阻塞队列实现的“生产者 - 消费者模型”，可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。而且不仅如此，基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。前面我们讲了阻塞队列，在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲 Disruptor 的时候，我会再详细讲并发队列的应用。队列的知识就讲完了，我们现在回过来看下开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢？基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。除了前面讲到队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。内容小结今天我们讲了一种跟栈很相似的数据结构，队列。关于队列，你能掌握下面的内容，这节就没问题了。队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想解决数据搬移的问题，我们就需要像环一样的循环队列。循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。除此之外，我们还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全队列应用在现实生活中Queue的应用也很广泛，最广泛的就是排队了，”先来后到” First come first service ，以及Queue这个单词就有排队的意思。还有，比如我们的播放器上的播放列表，我们的数据流对象，异步的数据传输结构(文件IO，管道通讯，套接字等)还有一些解决对共享资源的冲突访问，比如打印机的打印队列等。消息队列等。交通状况模拟，呼叫中心用户等待的时间的模拟等等。最后，关于队列，入队要判满，出队要判空**","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"队列","slug":"队列","permalink":"cpeixin.cn/tags/%E9%98%9F%E5%88%97/"}]},{"title":"数据结构与算法-线性表","slug":"数据结构与算法-线性表","date":"2016-08-12T02:39:21.000Z","updated":"2020-04-04T17:29:23.790Z","comments":true,"path":"2016/08/12/数据结构与算法-线性表/","link":"","permalink":"cpeixin.cn/2016/08/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E7%BA%BF%E6%80%A7%E8%A1%A8/","excerpt":"","text":"线性表及其逻辑结构线性表是最简单也是最常用的一种数据结构。英文字母表（A、B、…、Z）是一个线性表，表中每个英文字母是一个数据元素；成绩单是一个线性表，表中每一行是一个数据元素，每个数据元素又由学号、姓名、成绩等数据项组成。线性表的定义线性表是具有相同特性的数据元素的一个有限序列。线性表一般表示为：1 L = (a1, a2, …, ai,ai+1 ,…, an)线性表中元素在位置上是有序的，这种位置上有序性就是一种线性关系，用二元组表示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101L = (D, R) D = &#123;ai| 1≤i≤n, n≥0&#125; R = &#123;r&#125; r = &#123;&lt;ai, ai+1&gt; | 1≤i≤n-1&#125;``` ### 线性表的抽象数据类型描述 将线性表数据结构抽象成为一种数据类型，这个数据类型中包含数据元素、元素之间的关系、操作元素的基本算法。对于基本数据类型（int、float、boolean等等）java已经帮我们实现了用于操作他们的基本运算，我们需要基于这些基本运算，为我们封装的自定义数据类型提供操作它们的算法。比如数组就是一种被抽象出来的线性表数据类型，数组自带很多基本方法用于操作数据元素。 Java中的List我们经常会使用到，但是很少关注其内部实现，List是一个接口，里面定义了一些抽象的方法，其目的就是对线性表的抽象，其中的方法就是线性表的一些常用基本运算。 而对于线性表的不同**存储结构**其实现方式就有所不同了，比如**ArrayList**是对线性表顺序存储结构的实现，**LinkedList**是线性表链式存储结构的实现等。存储结构没有确定我们就不知道数据怎么存储，但是对于线性表这种逻辑结构中数据的基本操作我们可以预知，无非就是获取长度、获取指定位置的数据、插入数据、删除数据等等操作，可以参考List。 对于本系列文章，只是对数据结构和一些常用算法学习，接下来的代码将选择性实现并分析算法。对线性表的抽象数据类型描述如下：```javapublic interface IList&lt;T&gt; &#123; /** * 判断线性表是否为空 * @return */ boolean isEmpty(); /** * 获取长度 * @return */ int length(); /** * 将结点添加到指定序列的位置 * @param index * @param data * @return */ boolean add(int index, T data); /** * 将指定的元素追加到列表的末尾 * @param data * @return */ boolean add(T data); /** * 根据index移除元素 * @param index * @return */ T remove(int index); /** * 移除值为data的第一个结点 * @param data * @return */ boolean remove(T data); /** * 移除所有值为data的结点 * @param data * @return */ boolean removeAll(T data); /** * 清空表 */ void clear(); /** * 设置指定序列元素的值 * @param index * @param data * @return */ T set(int index, T data); /** * 是否包含值为data的结点 * @param data * @return */ boolean contains(T data); /** * 根据值查询索引 * @param data * @return */ int indexOf(T data); /** * 根据data值查询最后一次出现在表中的索引 * @param data * @return */ int lastIndexOf(T data); /** * 获取指定序列的元素 * @param index * @return */ T get(int index); /** * 输出格式 * @return */ String toString();&#125;线性表的顺序存储结构顺序表把线性表中的所有元素按照其逻辑顺序依次存储在计算机存储器中指定存储位置开始的一块连续的存储空间中。在Java中创建一个数组对象就是分配了一块可供用户使用的连续的存储空间，该存储空间的起始位置就是由数组名表示的地址常量。线性表的顺序存储结构是利用数组来实现的。在Java中，我们通常利用下面的方式来使用数组：123int[] array = new int[]&#123;1,2,3&#125;; //创建一个数组Array.getInt(array, 0); //获取数组中序列为0的元素Array.set(array, 0, 1); //设置序列为0的元素值为1Array这种方式创建的数组是固定长度的，其容量无法修改，当array被创建出来的时候，系统只为其分配3个存储空间，所以我们无法对其进行添加和删除操作。Array这个类里面提供了很多方法用于操作数组，这些方法都是静态的，所以Array是一个用于操作数组的工具类，这个类提供的方法只有两种：get和set，所以只能获取和设置数组中的元素，然后对于这两种操作，我们通常使用array[i]、array[i] = 0的简化方式，所以Array这个类用的比较少。另外一种数组ArrayList，其内部维护了一个数组，所以本质上也是数组，其操作都是对数组的操作，与上述数组不同的是，ArrayList是一种可变长度的数组。既然数组创建时就已经分配了存储空间，为什么ArrayList是长度可变的呢？长度可变意味着可以从数组中添加、删除元素，向ArrayList中添加数据时，实际上是创建了一个新的数组，将原数组中元素一个个复制到新数组后，将新元素添加进来。如果ArrayList仅仅做了这么简单的操作，那他就不应该出现了。ArrayList中的数组长度是大于等于其元素个数的，当执行add()操作时首先会检查数组长度是否够用，只有当数组长度不够用时才会创建新的数组，由于创建新数组意味着老数据的搬迁，所以这个机制也算是利用空间换取时间上的效率。但是如果添加操作并不是尾部添加，而是头部或者中间位置插入，也避免不了元素位置移动。顺序表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229public class LinearArray&lt;T&gt; implements IList&lt;T&gt;&#123; private Object[] datas; /** * 通过给定的数组 建立顺序表 * @param objs * @return */ public static &lt;T&gt; LinearArray&lt;T&gt; createArray(T[] objs)&#123; LinearArray&lt;T&gt; array = new LinearArray(); array.datas = new Object[objs.length]; for(int i = 0; i&lt;objs.length; i++) array.datas[i] = objs[i]; return array; &#125; private LinearArray()&#123; &#125; @Override public boolean isEmpty() &#123; return datas.length == 0; &#125; @Override public int length() &#123; return datas.length; &#125; /** * 获取指定位置的元素 * 分析：时间复杂度O(1) * 从顺序表中检索值是简单高效的，因为顺序表内部采用数组作为容器，数组可直接通过索引值访问元素 */ @Override public T get(int index) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); return (T) datas[index]; &#125; /** * 为指定索引的结点设置值 * 分析：时间复杂度O(1) */ @Override public T set(int index, T data) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; datas[index] = data; return oldValue; &#125; /** * 判断是否包含某值只需要判断该值有没有出现过 * 分析：时间复杂度O(n) */ @Override public boolean contains(T data) &#123; return indexOf(data) &gt;= 0; &#125; /** * 获取某值第一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int indexOf(T data) &#123; if (data == null) &#123; for (int i = 0; i &lt; datas.length; i++) if (datas[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; datas.length; i++) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 获取某值最后一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int lastIndexOf(T data) &#123; if (data == null) &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (datas[i]==null) return i; &#125; else &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 指定位置插入元素 * 分析：时间复杂度O(n) * 在数组中插入元素时，需要创建一个比原数组容量大1的新数组， * 将原数组中(0,index-1)位置的元素拷贝到新数组，指定新数组index位置元素值为新值， * 继续将原数组(index, length-1)的元素拷贝到新数组 * @param index * @param data * @return */ @Override public boolean add(int index, T data) &#123; if (index &gt; datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); destination[index] = data; System.arraycopy(datas, index, destination, index + 1, datas.length - index); datas = destination; return true; &#125; /** * 在顺序表末尾处插入元素 * 分析：时间复杂度O(n) * 同上面一样，也需要创建新数组 * @param data * @return */ @Override public boolean add(T data) &#123; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, datas.length); destination[datas.length] = data; datas = destination; return true; &#125; /** * 有序表添加元素 * @param data * @return */ public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true; &#125; /** * 移除指定索引的元素 * 分析：时间复杂度O(n) * 此处由于数组元素数量-1，所以需要创建新数组。 * ArrayList由于是动态数组（list.size()≠data.length），所以只需要将删除的元素之后的前移一位 * @param index * @return */ @Override public T remove(int index) &#123; if (index &gt;= datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; fastRemove(index); return oldValue; &#125; /** * 删除指定值的第一个元素 * @param data * @return */ @Override public boolean remove(T data) &#123; if (data == null) &#123; for (int index = 0; index &lt; datas.length; index++) if (datas[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; datas.length; index++) if (data.equals(datas[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /** * 移除指定序列的元素 * @param index */ private void fastRemove(int index) &#123; Object destination[] = new Object[datas.length - 1]; System.arraycopy(datas, 0, destination, 0, index); System.arraycopy(datas, index+1, destination, index, datas.length - index-1); datas = destination; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; @Override public void clear() &#123; datas = new Object[]&#123;&#125;; &#125; @Override public String toString() &#123; if(isEmpty()) return \"\"; String str = \"[\"; for(int i = 0; i&lt;datas.length; i++)&#123; str += (datas[i]+\", \"); &#125; str = str.substring(0, str.lastIndexOf(\", \")); return str+\"]\"; &#125;&#125;算法分析：插入元素：删除元素：**线性表的链式存储结构顺序表必须占用一整块事先分配大小固定的存储空间，这样不便于存储空间的管理。为此提出了可以实现存储空间动态管理的链式存储方式–链表。链表在链式存储中，每个存储结点不仅包含元素本身的信息（数据域），还包含元素之间逻辑关系的信息，即一个结点中包含有直接后继结点的地址信息，这称为指针域。这样可以通过一个结点的指针域方便的找到后继结点的位置。由于顺序表中每个元素至多只有一个直接前驱元素和一个直接后继元素。当采用链式存储时，一种最简单也最常用的方法是：在每个结点中除包含数据域外，只设置一个指针域用以指向其直接后继结点，这种构成的链接表称为线性单向链接表，简称单链表。另一种方法是，在每个结点中除包含数值域外，设置两个指针域，分别用以指向直接前驱结点和直接后继结点，这样构成的链接表称为线性双向链接表，简称双链表。单链表当访问一个结点后，只能接着访问它的直接后继结点，而无法访问他的直接前驱结点。双链表则既可以依次向后访问每个结点，也可以依次向前访问每个结点。单链表结点元素类型定义：1234public class LNode &#123; protected LNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;双链表结点元素类型定义：12345public class DNode &#123; protected DNode prior; //指针域，指向直接前驱结点 protected DNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;在顺序表中，逻辑上相邻的元素，对应的存储位置也相邻，所以进行插入或删除操作时，通常需要平均移动半个表的元素，这是相当费时的操作。在链表中，每个结点存储位置可以任意安排，不必要求相邻，插入或删除操作只需要修改相关结点的指针域即可，方便省时。对于单链表，如果要在结点p之前插入一个新结点，由于通过p并不能找到其前驱结点，我们需要从链表表头遍历至p的前驱结点然后进行插入操作，这样时间复杂度就是O(n)，而顺序表插入删除结点时间复杂度也是O(n)，那为什么说链表插入删除操作更加高效呢？因为单链表插入删除操作所消耗的时间主要在于查找前驱结点，这个查找工作的时间复杂度为O(n)，而真正超如删除时间为O(1)还有顺序表需要移动结点，移动结点通常比单纯的查找更加费时，链表不需要连续的空间，不需要扩容创建新表，所以同样时间复杂度O(n)，链表更适合插入和删除操作。对于遍历查找前驱结点的问题，在双链表中就能很好的解决，双链表在已知某结点的插入和删除操作时间复杂度是O(1)。由于链表的每个结点带有指针域，从存储密度来讲，这是不经济的。所谓存储密度是指结点数据本身所占存储量和整改结点结构所占存储量之比。3.2 单链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534public class LinkList&lt;T&gt; implements IList&lt;T&gt;&#123; public LNode&lt;T&gt; head; //单链表开始结点 /** * 1.1 创建单链表（头插法：倒序） * 解：遍历数组，创建新结点，新结点的指针域指向头结点，让新结点作为头结点 * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListF(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; for (T obj : array) &#123; LNode&lt;T&gt; node = new LNode(); node.data = obj; node.next = llist.head; llist.head = node; &#125; &#125; return llist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 解： * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListR(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; llist.head = new LNode(); llist.head.data = array[0]; LNode&lt;T&gt; temp = llist.head; for(int i = 1; i &lt; array.length; i++)&#123; LNode node = new LNode(); node.data = array[i]; temp.next = node; temp = node; &#125; &#125; return llist; &#125; /** * 判断单链表是否为空表 * 时间复杂度O(1) * @return */ @Override public boolean isEmpty() &#123; return head==null; &#125; /** * 4 获取单链表长度 * 时间复杂度O(n) * @return */ @Override public int length() &#123; if(head==null) return 0; int l = 1; LNode node = head; while(node.next!=null) &#123; l++; node = node.next; &#125; return l; &#125; @Override public void clear() &#123; head = null; &#125; @Override public T set(int index, T data) &#123; return null; &#125; @Override public boolean contains(T data) &#123; return false; &#125; @Override public T get(int index) &#123; return getNode(index).data; &#125; /** * 6.1 获取指定索引的结点 * 时间复杂度O(n) * @param index * @return */ public LNode&lt;T&gt; getNode(int index)&#123; LNode node = head; int j = 0; while(j &lt; index &amp;&amp; node!=null)&#123; j++; node = node.next; &#125; return node; &#125; /** * 6.2 获取指定数据值结点的索引 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @return */ @Override public int indexOf(T data) &#123; if(head==null) return -1; //没有此结点 LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) return j; j++; node = node.next; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; if(head==null) return -1; int index = -1; LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) &#123; index = j; &#125; j++; node = node.next; &#125; return index; &#125; /** * 6.3 单链表中的倒数第k个结点（k &gt; 0） * 解：先找到顺数第k个结点，然后使用前后指针移动到结尾即可 * 时间复杂度O(n) 空间复杂度O(1) * @param k * @return */ public LNode&lt;T&gt; getReNode(int k)&#123; if(head==null) return null; int len = length(); if(k &gt; len) return null; LNode target = head; LNode next = head; for(int i=0;i &lt; k;i++) next = next.next; while(next!=null)&#123; target = target.next; next = next.next; &#125; return target; &#125; /** * 6.4 查找单链表的中间结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode getMiddleNode()&#123; if(head == null|| head.next == null) return head; LNode target = head; LNode temp = head; while(temp != null &amp;&amp; temp.next != null)&#123; target = target.next; temp = temp.next.next; &#125; return target; &#125; /** * 2.1 将单链表合并为一个单链表 * 解：遍历第一个表，用其尾结点指向第二个表头结点 * 时间复杂度O(n) * @return */ public static LNode mergeList(LNode head1, LNode head2)&#123; if(head1==null) return head2; if(head2==null) return head1; LNode loop = head1; while(loop.next!=null) //找到list1尾结点 loop = loop.next; loop.next = head2; //将list1尾结点指向list2头结点 return head1; &#125; /** * 2.1 通过递归，合并两个有序的单链表head1和head2 * * 解：两个指针分别指向两个头结点，比较两个结点大小， * 小的结点指向下一次比较结果（两者中较小），最终返回第一次递归的最小结点 * @param head1 * @param head2 * @return */ public static LNode mergeSortedListRec(LNode head1, LNode head2)&#123; if(head1==null)return head2; if(head2==null)return head1; if (((int)head1.data)&gt;((int)head2.data)) &#123; head2.next = mergeSortedListRec(head2.next, head1); return head2; &#125; else &#123; head1.next = mergeSortedListRec(head1.next, head2); return head1; &#125; &#125; /** * 3.1 循环的方式将单链表反转 * 时间复杂度O(n) 空间复杂度O(1) */ public void reverseListByLoop() &#123; if (head == null || head.next == null) return; LNode pre = null; LNode nex = null; while (head != null) &#123; nex = head.next; head.next = pre; pre = head; head = nex; &#125; head = pre; &#125; /** * 3.2 递归的方式将单链表反转,返回反转后的链表头结点 * 时间复杂度O(n) 空间复杂度O(n) */ public LNode reverseListByRec(LNode head) &#123; if(head==null||head.next==null) return head; LNode reHead = reverseListByRec(head.next); head.next.next = head; head.next = null; return reHead; &#125; /** * 5.1 获取单链表字符串表示 * 时间复杂度O(n) */ @Override public String toString() &#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; return buffer.toString(); &#125; public static String display(LNode head)&#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(\" -&gt; \"+node.data); node = node.next; &#125; return buffer.toString(); &#125; /** * 5.2 用栈的方式获取单链表从尾到头倒叙字符串表示 * 解：由于栈具有先进后出的特性，现将表中的元素放入栈中，然后取出就倒序了 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public String displayReverseStack()&#123; if(head == null) return \"\"; Stack &lt;LNode&gt; stack = new Stack &lt; &gt;(); //堆栈 先进先出 LNode head = this.head; while(head!=null)&#123; stack.push(head); head=head.next; &#125; StringBuffer buffer = new StringBuffer(); while(!stack.isEmpty())&#123; //pop()移除堆栈顶部的对象，并将该对象作为该函数的值返回。 buffer.append(\" -&gt; \"+stack.pop().data); &#125; return buffer.toString(); &#125; /** * 5.3 用递归的方式获取单链表从尾到头倒叙字符串表示 * @return */ public void displayReverseRec(StringBuffer buffer, LNode head)&#123; if(head==null) return; displayReverseRec(buffer, head.next); buffer.append(\" -&gt; \"); buffer.append(head.data); &#125; /** * 7.1 插入结点 * 解：先找到第i-1个结点，让创建的新结点的指针域指向第i-1结点指针域指向的结点， * 然后将i-1结点的指针域指向新结点 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @param index */ @Override public boolean add(int index, T data) &#123; if(index==0)&#123; //插入为头结点 LNode temp = new LNode(); temp.next = head; return true; &#125; int j = 0; LNode node = head; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return false; LNode temp = new LNode(); //创建新结点 temp.data = data; temp.next = node.next; //新结点插入到Index-1结点之后 node.next = temp; return true; &#125; @Override public boolean add(T data) &#123; LNode node = head; while(node!=null &amp;&amp; node.next!=null) //找到尾结点 node = node.next; LNode temp = new LNode(); //创建新结点 temp.data = data; node.next = temp; return false; &#125; @Override public T remove(int index) &#123; LNode&lt;T&gt; node = deleteNode(index); return node==null?null:node.data; &#125; /** * 7.2 删除结点 * 解：让被删除的结点前一个结点的指针域指向后一个结点指针域 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode deleteNode(int index)&#123; LNode node = head; if(index==0)&#123; //删除头结点 if(node==null) return null; head = node.next; return node; &#125; //非头结点 int j = 0; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return null; LNode delete = node.next; if(delete==null) return null; //不存在第index个结点 node.next = delete.next; return delete; &#125; @Override public boolean remove(T data) &#123; return false; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; /** * 7.3 给出一单链表头指针head和一节点指针delete，要求O(1)时间复杂度删除节点delete * 解：将delete节点value值与它下个节点的值互换的方法， * 但是如果delete是最后一个节点，需要特殊处理，但是总得复杂度还是O(1) * @return */ public static void deleteNode(LNode head, LNode delete)&#123; if(delete==null) return; //首先处理delete节点为最后一个节点的情况 if(delete.next==null)&#123; if(head==delete) //只有一个结点 head = null; else&#123; //删除尾结点 LNode temp = head; while(temp.next!=delete) temp = temp.next; temp.next=null; &#125; &#125; else&#123; delete.data = delete.next.data; delete.next = delete.next.next; &#125; return; &#125; /** * 8.1 判断一个单链表中是否有环 * 解：使用快慢指针方法，如果存在环，两个指针必定指向同一结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public static boolean hasCycle(LNode head)&#123; LNode p1 = head; LNode p2 = head; while(p1!=null &amp;&amp; p2!=null)&#123; p1 = p1.next; //一次跳一步 p2 = p2.next.next; //一次跳两步 if(p2 == p1) return true; &#125; return false; &#125; /** * 8.2、已知一个单链表中存在环，求进入环中的第一个节点 * 利用hashmap，不要用ArrayList，因为判断ArrayList是否包含某个元素的效率不高 * @param head * @return */ public static LNode getFirstNodeInCycleHashMap(LNode head)&#123; LNode target = null; HashMap&lt;LNode,Boolean &gt; map=new HashMap&lt; &gt;(); while(head != null)&#123; if(map.containsKey(head)) &#123; target = head; break; &#125; else &#123; map.put(head, true); head = head.next; &#125; &#125; return target; &#125; /** * 8.3、已知一个单链表中存在环，求进入环中的第一个节点,不用hashmap * 用快慢指针，与判断一个单链表中是否有环一样，找到快慢指针第一次相交的节点， * 此时这个节点距离环开始节点的长度和链表头距离环开始的节点的长度相等 * 参考 https://www.cnblogs.com/fankongkong/p/7007869.html * @param head * @return */ public static LNode getFirstNodeInCycle(LNode head)&#123; LNode fast = head; LNode slow = head; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; if(slow == fast) break; &#125; if(fast == null||fast.next == null) return null;//判断是否包含环 //相遇节点距离环开始节点的长度和链表投距离环开始的节点的长度相等 slow=head; while(slow!=fast)&#123; slow=slow.next; fast=fast.next; &#125;//同步走 return slow; &#125; /** * 9、判断两个单链表是否相交,如果相交返回第一个节点，否则返回null * ①、暴力遍历两个表，是否有相同的结点(时间复杂度O(n²)) * ②、第一个表的尾结点指向第二个表头结点，然后判断第二个表是否存在环，但不容易找出交点（时间复杂度O(n)） * ③、两个链表相交，必然会经过同一个结点，这个结点的后继结点也是相同的（链表结点只有一个指针域，后继结点只能有一个）， * 所以他们的尾结点必然相同。两个链表相交，只能是前面的结点不同，所以，砍掉较长链表的差值后同步遍历，判断结点是否相同，相同的就是交点了。 * 时间复杂度（时间复杂度O(n)） * @param list1 * @param list2 * @return 交点 */ public static LNode isIntersect(LinkList list1, LinkList list2)&#123; LNode head1 = list1.head; LNode head2 = list2.head; if(head1==null || head2==null)return null; int len1 = list1.length(); int len2 = list2.length(); //砍掉较长链表的差值 if(len1 &gt;= len2)&#123; for(int i=0;i &lt; len1-len2;i++)&#123; head1=head1.next; &#125; &#125;else&#123; for(int i=0;i &lt; len2-len1;i++)&#123; head2=head2.next; &#125; &#125; //同步遍历 while(head1 != null&amp;&amp;head2 != null)&#123; if(head1 == head2) return head1; head1=head1.next; head2=head2.next; &#125; return null; &#125;&#125;算法分析判断一个单链表中是否有环我们可以通过HashMap判断，遍历结点，将结点值放入HashMap，如果某一刻发现当前结点在map中已经存在，则存在环，并且此结点正是环的入口，此算法见8.2方法。但是有一种问法是不通过任何其他数据结构怎么判断单链表是否存在环。这样我们可利用的就只有单链表本身，一种解法是通过快慢指针，遍历链表，一个指针跳一步（慢指针步长为1），另一个指针跳两步（快指针步长为2），如果存在环，这两个指针必将在某一刻指向同一结点，假设此时慢指针跳了n步，则快指针跳的步数为n/2步：判断两个单链表是否相交由于单链表的特性（只有一个指针域），如果两个表相交，那必定是Y形相交，不会是X形相交，如图所示。两个单链表后面的结点相同，不同的部分只有前面，砍掉较长的链表的前面部分，然后两个链表同步遍历，必将指向同一个结点，这个结点就是交点：双链表双链表中每个结点有两个指针域，一个指向其直接后继结点，一个指向其直接前驱结点。建立双链表也有两种方法，头插法和尾插法，这与创建单链表过程相似。在双链表中，有些算法如求长度、取元素值、查找元素等算法与单链表中相应算法是相同的。但是在单链表中，进行结点插入和删除时涉及前后结点的一个指针域的变化，而双链表中结点的插入和删除操作涉及前后结点的两个指针域的变化。java中LinkedList正是对双链表的实现，算法可参考此类。双链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327public class DLinkList&lt;T&gt; implements IList&lt;T&gt;&#123; transient DNode&lt;T&gt; first; //双链表开始结点 transient DNode&lt;T&gt; last; //双链表末端结点 private int size; //结点数 /** * 创建单链表（头插法：倒序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListF(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; dlist.size = array.length; for (T obj : array) &#123; DNode&lt;T&gt; node = new DNode(); node.data = obj; node.next = dlist.first; if(dlist.first!=null) dlist.first.prior = node; //相比单链表多了此步 else dlist.last = node; dlist.first = node; &#125; &#125; return dlist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListR(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; dlist.size = array.length; dlist.first = new DNode&lt;T&gt;(); dlist.first.data = array[0]; dlist.last = dlist.first; for(int i = 1; i &lt; array.length; i++)&#123; DNode&lt;T&gt; node = new DNode(); node.data = array[i]; dlist.last.next = node; node.prior = dlist.last; //相比单链表多了此步 dlist.last = node; &#125; &#125; return dlist; &#125; @Override public boolean isEmpty() &#123; return size==0; &#125; @Override public int length() &#123; return size; &#125; /**2 添加结点*/ @Override public boolean add(int index, T data) &#123; if(index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; newNode = new DNode(); newNode.data = data; if (index == size) &#123; //在末尾添加结点不需要遍历 final DNode&lt;T&gt; l = last; if (l == null) //空表 first = newNode; else &#123; l.next = newNode; newNode.prior = l; &#125; last = newNode; size++; &#125; else &#123; //其他位置添加结点需要遍历找到index位置的结点 DNode&lt;T&gt; indexNode = getNode(index); DNode&lt;T&gt; pred = indexNode.prior; newNode.prior = pred; newNode.next = indexNode; indexNode.prior = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; &#125; return false; &#125; @Override public boolean add(T data) &#123; return add(size, data); &#125; /**3 删除结点*/ @Override public T remove(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return unlink(getNode(index)); &#125; @Override public boolean remove(T data) &#123; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); return true; &#125; &#125; &#125; return false; &#125; @Override public boolean removeAll(T data) &#123; boolean result = false; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); result = true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); result = true; &#125; &#125; &#125; return result; &#125; /** * 将指定的结点解除链接 * @param x * @return */ private T unlink(DNode&lt;T&gt; x) &#123; // assert x != null; final T element = x.data; final DNode&lt;T&gt; next = x.next; final DNode&lt;T&gt; prev = x.prior; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prior = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prior = prev; x.next = null; &#125; x.data = null; size--; return element; &#125; /** * 清空 */ @Override public void clear() &#123; for (DNode&lt;T&gt; x = first; x != null; ) &#123; DNode&lt;T&gt; next = x.next; x.data = null; x.next = null; x.prior = null; x = next; &#125; first = last = null; size = 0; &#125; /** * 设置结点值 * @param index * @param data * @return */ @Override public T set(int index, T data) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; x = getNode(index); T oldVal = x.data; x.data = data; return oldVal; &#125; /** * 判断是否存在结点值 * @param data * @return */ @Override public boolean contains(T data) &#123; return indexOf(data) != -1; &#125; /** * 检索结点值 * @param data * @return */ @Override public int indexOf(T data) &#123; int index = 0; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) return index; index++; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) return index; index++; &#125; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; int index = size; if (data == null) &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (x.data == null) return index; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (data.equals(x.data)) return index; &#125; &#125; return -1; &#125; @Override public T get(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return getNode(index).data; &#125; /** * 获取指定索引的结点 * 解：由于双链表能双向检索，判断index离开始结点近还是终端结点近，从近的一段开始遍历 * 时间复杂度O(n) * @param index * @return */ private DNode&lt;T&gt; getNode(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123; DNode&lt;T&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; DNode&lt;T&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prior; return x; &#125; &#125; /** * 倒序 * 遍历每个结点，让node.next = node.prior; node.prior = (node.next此值需要体现保存); */ public void reverse()&#123; last = first; //反转后终端结点=开始结点 DNode now = first; DNode next; while(now!=null)&#123; next = now.next; //保存当前结点的后继结点 now.next = now.prior; now.prior = next; first = now; now = next; &#125; &#125; @Override public String toString() &#123; if(size == 0) return \"\"; DNode node = first; StringBuffer buffer = new StringBuffer(); buffer.append(\" \"); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; buffer.append(\"next\\npre\"); node = last; int start = buffer.length(); LogUtil.i(getClass().getSimpleName(), \"buffer长度：\"+buffer.length()); while(node != null)&#123; buffer.insert(start ,\" &lt;- \"+node.data); node = node.prior; &#125; return buffer.toString(); &#125;&#125;算法分析双链表与单链表不同之处在于，双链表能从两端依次访问各个结点。单链表相对于顺序表优点是插入、删除数据更方便，但是访问结点需要遍历，时间复杂度为O(n)；双链表就是在单链表基础上做了一个优化，使得访问结点更加便捷（从两端），这样从近的一端出发时间复杂度变为O(n/2)，虽然不是指数阶的区别，但也算是优化。双链表在插入、删除结点时逻辑比单链表稍麻烦：循环链表循环链表是另一种形式的链式存储结构，它的特点是表中尾结点的指针域不再是空，而是指向头结点，整个链表形成一个环。由此，从表中任意一结点出发均可找到链表中其他结点。如图所示为带头结点的循环单链表和循环双链表：有序表所谓有序表，是指所有结点元素值以递增或递减方式排列的线性表，并规定有序表中不存在元素值相同的结点。有序表可以采用顺序表和链表进行存储，若以顺序表存储有序表，其算法除了add(T data)以外，其他均与前面说的顺序表对应的运算相同。有序表的add(T data)操作不是插入到末尾，而需要遍历比较大小后插入相应位置。123456789101112131415public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true;&#125;最后 想强调的是 由于顺序表结构的底层实现借助的就是数组，因此对于初学者来说，可以把顺序表完全等价为数组，但实则不是这样。数据结构是研究数据存储方式的一门学科，它囊括的都是各种存储结构，而数组只是各种编程语言中的基本数据类型，并不属于数据结构的范畴。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"线性表","slug":"线性表","permalink":"cpeixin.cn/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/"}]},{"title":"数据结构-数组","slug":"数据结构-数组","date":"2016-08-11T07:30:21.000Z","updated":"2020-04-04T17:33:56.188Z","comments":true,"path":"2016/08/11/数据结构-数组/","link":"","permalink":"cpeixin.cn/2016/08/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%95%B0%E7%BB%84/","excerpt":"","text":"提到数组，我想你肯定不陌生，甚至还会自信地说，它很简单啊。是的，在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。在大部分编程语言中，数组都是从 0 开始编号的，但你是否下意识地想过，为什么数组要从 0 开始编号，而不是从 1 开始呢？ 从 1 开始不是更符合人类的思维习惯吗？Tips: 在C，Java中都有明显的数组实现，但是在Python中有list、tuple、set、dict, 并没有显示的数组关键字，“数组”实际上确实存在于python中。当人们谈论数组时，Python至少有三件事要谈论：在Python list有许多在其他语言，如C或Java中 数组的行为。但是，它不需要预先分配内存，分配数组长度，它带有许多便捷的方法。此外，它的基础数据结构通常是一个数组。请记住，语言只是语法-它可以通过不同的方式实现。CPython中Python列表的基础数据结构是C数组，在Jython中是ArrayList（来源：Python列表的基础数据结构是什么？）列表对象被实现为数组。它们针对快速的固定长度操作进行了优化，并且会为pop（0）和insert（0，v）操作产生O（n）内存移动成本，这些操作会同时更改基础数据表示的大小和位置。所述的Python阵列模块。可以在这里找到：8.6。array-有效的数字数组。阵列模块用于某些相当特殊的情况，当您有大量的一种类型的数据时，它们可以提供更好的内存性能（不一定是速度）。我在Chris Riederer的回答“ Python中的列表和数组之间有什么区别？”的答案中写了更多内容。最后，还有NumPy 数组。这是一个非常快速，非常有用的数据结构，可让您快速进行许多数值计算。在此处了解有关NumPy的更多信息：NumPy-Numpy这个定义里有几个关键词，理解了这几个关键词，我想你就能彻底掌握数组的概念了。下面就从我的角度分别给你“点拨”一下。第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10]来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：12a[i]_address = base_address + i * data_type_size其中 data_type_size 表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节。这个公式非常简单，我就不多做解释了。这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。Tips: 针对上面提到的 数组存储的必须是相同的数据类型，在这里我自己的理解是，对于数组内的元素，进行访问的过程中，实际上是根据每个元素的内存地址来进行访问的。元素的内存地址不是随意分配的，而是通过一个公式计算而来的，每个元素的内存地址合成一段连续的内存地址区间段。如上图，不同类型的元素所占字节不同，如果数组中存储不同数据类型的数据，那么就不能统一的按照 内存地址计算公式来计算。低效的“插入”和“删除”前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。现在我们就来详细说一下，究竟为什么会导致低效？又有哪些改进方法呢？我们先来看插入操作。假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？你可以自己先试着分析一下。如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。为了更好地理解，我们举一个例子。假设数组 a[10]中存储了如下 5 个元素：a，b，c，d，e。我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2]赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到，我会在排序那一节具体来讲，这里就说到这儿。我们再来看删除操作。跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？我们继续来看例子。数组 a[10]中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。如果你了解 JVM，你会发现，这不就是 JVM 标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。容器能否完全替代数组？这个问题，我在刚学编程语言之处，是根本没有想过的，在我眼里，程序语言中集合就是数据结构中的数组，没有任何差别，实际情况中，针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL 中的 vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？这里我拿 Java 语言来举例。如果你是 Java 工程师，几乎天天都在用 ArrayList，对它应该非常熟悉。那它与数组相比，到底有哪些优势呢？我个人觉得，ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。如果使用 ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList 已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。比如我们要从数据库中取出 10000 条数据放入 ArrayList。我们看下面这几行代码，你会发现，相比之下，事先指定数据大小可以省掉很多次内存申请和数据搬移操作。作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适些，我总结了几点自己的经验。Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 Object[][] array；而用容器的话则需要这样定义：ArrayListarray。我总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。解答开篇现在我们来思考开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：a[k]_address = base_address + k * type_size但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：a[k]_address = base_address + (k-1)*type_size对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。不过我认为，上面解释得再多其实都算不上压倒性的证明，说数组起始编号非 0 开始不可。所以我觉得最主要的原因可能是历史原因。C 语言设计者用 0 开始计数数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。实际上，很多语言中数组也并不是从 0 开始计数的，比如 Matlab。甚至还有一些语言支持负数下标，比如 Python。内容小结我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数组","slug":"数组","permalink":"cpeixin.cn/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"数据结构与算法-复盘","slug":"数据结构与算法-复盘","date":"2016-08-10T07:30:21.000Z","updated":"2020-04-04T17:30:49.884Z","comments":true,"path":"2016/08/10/数据结构与算法-复盘/","link":"","permalink":"cpeixin.cn/2016/08/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E5%A4%8D%E7%9B%98/","excerpt":"","text":"对于软件工程专业的同学来说，数据结构与算法这门课程是必修课程，课程时间应该是大学二年级，在学习完了一门编程语言后，进行展开学习的。我是在学习完C语言后，进行数据结构与算法这门课程的。个人感觉这门课程 veryyyyyyyyy 枯燥的。老师在课堂上也没有做相关知识点的扩展，再加上那时候我们对于知识的渴望度也没有那么高 哈哈哈哈，单纯的就是为了考试而学习，也没有利用网络资源来对这门课程进行进一步的拓展。等到工作之后，才慢慢发现，这门课程就是在大学时候埋的雷啊 XD但是毕竟还是学过的，时间还不晚，知识学到了，就是自己的，定期对自己复盘，重视自己，查缺补漏，就是最棒的！学前三问：WHAT ？ HOW ？ WHY？在之后写博客的时候，我也会遵循着 WWH 法则作为提纲。简单明了，直入主题。WHAT - 数据结构 算法数据结构简单直白的理解 ： 数据结构就是指一组数据的存储结构。百度百科：数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率。数据结构往往同高效的检索算法和索引技术有关算法简单直白的理解 ： 算法就是操作数据的一组方法。百度百科：算法（Algorithm）是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制。也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。HOW - 数据结构 算法有的人喜欢看书稳扎稳打有的人习惯看视频生动一点的获取知识也有人倾向于直接进入主题，找教程，找博文攻破单个知识点我这里只是说一下我的方法首先就是这篇博文，先搞清楚WWH然后呢，对数据结构和算法整个知识体系列一个大纲或者思维导图接下来就进入学习的过程了，我个人是比较喜欢看视 频和读好的博客的，每一个点学完后啊，先找习题，完了打开IDEA ，写一些Demo去理解，然后呢，带着学过的算法和数据结构啊，进入实战场，看看之前自己写过的项目，可不可以重新装修一下。最后一点，找一些成熟项目，读源码，读源码，读源码！！！WHY - 数据结构 算法这一栏位的内容，我找了很久，也想了很久怎样的去写，后来在搜寻的时候，发现很多博主和学习论坛都引用了知乎上涛吴的观点（涛吴，知乎上最受欢迎程序员前十名👍🏻），写的确实好，好东西就要分享出来😂如果说 Java 是自动档轿车，C 就是手动档吉普。数据结构呢？是变速箱的工作原理。你完全可以不知道变速箱怎样工作，就把自动档的车子从 A 开到 B，而且未必就比懂得的人慢。写程序这件事，和开车一样，经验可以起到很大作用，但如果你不知道底层是怎么工作的，就永远只能开车，既不会修车，也不能造车。如果你对这两件事都不感兴趣也就罢了，数据结构懂得用就好。但若你此生在编程领域还有点更高的追求，数据结构是绕不开的课题。Java 替你做了太多事情，那么多动不动还支持范型的容器类，加上垃圾收集，会让你觉得编程很容易。但你有没有想过，那些容器类是怎么来的，以及它存在的意义是什么？最粗浅的，比如 ArrayList 这个类，你想过它的存在是多么大的福利吗——一个可以随机访问、自动增加容量的数组，这种东西 C 是没有的，要自己实现。但是，具体怎么实现呢？如果你对这种问题感兴趣，那数据结构是一定要看的。甚至，面向对象编程范式本身，就是个数据结构问题：怎么才能把数据和操作数据的方法封装到一起，来造出 class / prototype 这种东西？此外，很重要的一点是，数据结构也是通向各种实用算法的基石，所以学习数据结构都是提升内力的事情。原文链接学 Java 有必要看数据结构的书吗？如果是，那么哪本书比较好？ - 涛吴的回答 - 知乎学数据结构和算法可以应用在哪些场景？在大学的时候，学习数据结构与算法，学的时候云里雾里的，搞不清楚这些数据结构和算法应该用在哪里？能干些什么？还没有一个长长的SQL带来的成就感强烈。以为这门课程只是应试教育中的必须进行的一环，现在想起来真是单纯的孩子啊 哈哈😄下面我引用InfoQ上的一篇文章，看完之后，有种醍醐灌顶的感觉，我觉得我明天应该会去翻遍所有的源码去看，有点迫不及待了……实际项目中的常见算法其次，以下场景中使用了丰富的数据结构来实现，可以择一来深入研读Linux 内核SQL引擎网络拓扑结构集群节点各种协议学数据结构和算法的精髓是什么？想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。时间复杂度和空间复杂度这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招怎样去选择合适的数据结构首先，选择数据结构的时候，第一点就要知道，数据怎么去组织，和数据规模是有关的。不一样规模的数据，难度也就不一样。解决问题的效率，是与数据组织方式相关的。首先上来我先Po出一张图，来应对第一步大类结构的选择查询操作更多的程序中，你应该用顺序表修改操作更多的程序中，你要使用链表单向链 双向链表 循环链表。栈，涉及后入先出的问题，例如函数递归就是个栈模型、Android的屏幕跳转就用到栈，很多类似的东西，你就会第一时间想到：我会用这东西来去写算法实现这个功能。队列，先入先出要排队的问题，你就要用到队列，例如多个网络下载任务，我该怎么去调度它们去获得网络资源呢？再例如操作系统的进程（or线程）调度，我该怎么去分配资源（像 CPU）给多个任务呢？肯定不能全部一起拥有的，资源只有一个，那就要排队！那么怎么排队呢？用普通的队列？但是对于那些优先级高的线程怎么办？这时，你就会想到了优先队列，优 先队列怎么实现？用堆，然后你就有疑问了，堆是啥玩意？怎样评测算法的好坏时间复杂度空间复杂度怎样将数据结构和算法应用到实际之中？写一些程序，尤其是比较底层的程序。数据结构如红黑树,后缀树, 算法如快速傅里叶变换,网络流等… 平时工作本来就很难碰上这些东西(如果确实从事尖端研究除外)，个人经常碰到的也无非就是些搜索, 优化剪枝…留下一个问题去给大家思考：如果让你实现qq那种分组的好友列表，支持各种qq里边的好友操作，你会怎么做？Sooooooooooo 学习数据结构与算法是提升自身内力，内力提升了，学习其他技术和框架，就会简单容易一些🐨","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[]},{"title":"小谈多线程","slug":"小谈多线程","date":"2016-08-09T07:30:21.000Z","updated":"2020-04-04T11:10:51.174Z","comments":true,"path":"2016/08/09/小谈多线程/","link":"","permalink":"cpeixin.cn/2016/08/09/%E5%B0%8F%E8%B0%88%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"多线程简介Java 给多线程编程提供了内置的支持。 一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。多线程是多任务的一种特别的形式，但多线程使用了更小的资源开销。这里定义和线程相关的另一个术语 - 进程：一个进程包括由操作系统分配的内存空间，包含一个或多个线程。一个线程不能独立的存在，它必须是进程的一部分。一个进程一直运行，直到所有的非守护线程都结束运行后才能结束。多线程能满足程序员编写高效率的程序来达到充分利用 CPU 的目的。多线程状态VM启动时会有一个由主方法Main所定义的主线程，在主线程中可以通过Thread创建其它线程。Thread对象的方法run()称为线程体。通过调用Thread类的start()方法来启动一个线程。通俗的说就是在run()方法中定义要做什么事情，start()方法用来发出命令可以开始做了，但这不代表JVM会立即运行 run()方法中的内容，而只是让他具备运行的资格，具体什么时侯开始真正运行run()方法，需要看JVM的调度。在Java当中，线程通常都有五种状态，新建、就绪、运行、阻塞和死亡。新建状态（New）：新创建了一个线程对象。就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：（一）、等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。（二）、同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。（三）、其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。在这里我要说明一下start()和run()之间的关系，因为在多线程编程的过程中并没有去调用run()方法，但是run()又是主要的执行体，首先Po上来start()，run()的源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Causes this thread to begin execution; the Java Virtual Machine * calls the &lt;code&gt;run&lt;/code&gt; method of this thread. * &lt;p&gt; * The result is that two threads are running concurrently: the * current thread (which returns from the call to the * &lt;code&gt;start&lt;/code&gt; method) and the other thread (which executes its * &lt;code&gt;run&lt;/code&gt; method). * &lt;p&gt; * It is never legal to start a thread more than once. * In particular, a thread may not be restarted once it has completed * execution. * * @exception IllegalThreadStateException if the thread was already * started. * @see #run() * @see #stop() */ public synchronized void start() &#123; /** * This method is not invoked for the main method thread or \"system\" * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * A zero status value corresponds to state \"NEW\". */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group's list of threads * and the group's unstarted count can be decremented. */ group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; &#125; private native void start0(); /** * If this thread was constructed using a separate * &lt;code&gt;Runnable&lt;/code&gt; run object, then that * &lt;code&gt;Runnable&lt;/code&gt; object's &lt;code&gt;run&lt;/code&gt; method is called; * otherwise, this method does nothing and returns. * &lt;p&gt; * Subclasses of &lt;code&gt;Thread&lt;/code&gt; should override this method. * * @see #start() * @see #stop() * @see #Thread(ThreadGroup, Runnable, String) */ @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125;根据Java API ： Causes this thread to begin execution; the Java Virtual Machine calls the run method of this thread.start()方法会使得该线程开始执行；java虚拟机会自动去调用该线程的run()方法。因此，t.start()会导致run()方法被调用，run()方法中的内容称为线程体，它就是这个线程需要执行的工作。在start方法里调用了一次start0方法，这个方法是一个只声明未定义的方法，并且使用了native关键字进行定义native指的是调用本机的原生系统函数。所以，调用start方法，会告诉JVM去分配本机系统的资源，才能实现多线程。而如果使用run()来启动线程，就不是异步执行了，而是同步执行，不会达到使用线程的意义。用start()来启动线程，实现了真正意义上的启动线程，此时会出现异步执行的效果，即在线程的创建和启动中所述的随机性。多线程创建Java 提供了三种创建线程的方法实现 Runnable 接口我们经常使用的构造方法，threadOb为创建的实例对象Thread(Runnable threadOb,String threadName);12345678910111213141516171819202122232425262728293031323334353637383940414243class RunnableDemo implements Runnable &#123; private Thread t; private String threadName; RunnableDemo( String name) &#123; threadName = name; System.out.println(\"Creating \" + threadName ); &#125; public void run() &#123; System.out.println(\"Running \" + threadName ); try &#123; for(int i = 4; i &gt; 0; i--) &#123; System.out.println(\"Thread: \" + threadName + \", \" + i); // 让线程睡眠一会 Thread.sleep(50); &#125; &#125;catch (InterruptedException e) &#123; System.out.println(\"Thread \" + threadName + \" interrupted.\"); &#125; System.out.println(\"Thread \" + threadName + \" exiting.\"); &#125; public void start () &#123; System.out.println(\"Starting \" + threadName ); if (t == null) &#123; t = new Thread (this, threadName); t.start (); &#125; &#125;&#125; public class TestThread &#123; public static void main(String args[]) &#123; RunnableDemo R1 = new RunnableDemo( \"Thread-1\"); R1.start(); RunnableDemo R2 = new RunnableDemo( \"Thread-2\"); R2.start(); &#125; &#125;继承 Thread 类本身创建一个线程的第二种方法是创建一个新的类，该类继承 Thread 类，然后创建一个该类的实例。继承类必须重写 run() 方法，该方法是新线程的入口点。它也必须调用 start() 方法才能执行。该方法尽管被列为一种多线程实现方式，但是本质上也是实现了 Runnable 接口的一个实例。123456789101112131415161718192021222324252627282930313233343536373839404142class ThreadDemo extends Thread &#123; private Thread t; private String threadName; ThreadDemo( String name) &#123; threadName = name; System.out.println(\"Creating \" + threadName ); &#125; public void run() &#123; System.out.println(\"Running \" + threadName ); try &#123; for(int i = 4; i &gt; 0; i--) &#123; System.out.println(\"Thread: \" + threadName + \", \" + i); // 让线程睡眠一会 Thread.sleep(50); &#125; &#125;catch (InterruptedException e) &#123; System.out.println(\"Thread \" + threadName + \" interrupted.\"); &#125; System.out.println(\"Thread \" + threadName + \" exiting.\"); &#125; public void start () &#123; System.out.println(\"Starting \" + threadName ); if (t == null) &#123; t = new Thread (this, threadName); t.start (); &#125; &#125;&#125; public class TestThread &#123; public static void main(String args[]) &#123; ThreadDemo T1 = new ThreadDemo( \"Thread-1\"); T1.start(); ThreadDemo T2 = new ThreadDemo( \"Thread-2\"); T2.start(); &#125; &#125;Callable和Future创建线程创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值。123456789101112131415161718192021222324252627282930313233343536public class CallableThreadTest implements Callable&lt;Integer&gt; &#123; public static void main(String[] args) &#123; CallableThreadTest ctt = new CallableThreadTest(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt); for(int i = 0;i &lt; 100;i++) &#123; System.out.println(Thread.currentThread().getName()+\" 的循环变量i的值\"+i); if(i==20) &#123; new Thread(ft,\"有返回值的线程\").start(); &#125; &#125; try &#123; System.out.println(\"子线程的返回值：\"+ft.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Integer call() throws Exception &#123; int i = 0; for(;i&lt;100;i++) &#123; System.out.println(Thread.currentThread().getName()+\" \"+i); &#125; return i; &#125; &#125;创建线程方法之间的区别实现Runnable和实现Callable接口的方式基本相同，不过是后者执行call()方法有返回值，后者线程执行体run()方法无返回值，因此可以把这两种方式归为一种这种方式与继承Thread类的方法之间的差别如下：1、线程只是实现Runnable或实现Callable接口，还可以继承其他类。2、这种方式下，多个线程可以共享一个target对象，非常适合多线程处理同一份资源的情形。3、但是编程稍微复杂，如果需要访问当前线程，必须调用Thread.currentThread()方法。4、继承Thread类的线程类不能再继承其他父类（Java单继承决定）。注：一般推荐采用实现接口的方式来创建多线程示例Account类123456789101112131415161718192021222324252627public class Acount &#123; private String UserName; private float Money; public Acount(String userName, float money) &#123; super(); UserName = userName; Money = money; &#125; public String getUserName() &#123; return UserName; &#125; public void setUserName(String userName) &#123; UserName = userName; &#125; public float getMoney() &#123; return Money; &#125; public void setMoney(float money) &#123; Money = money; &#125;&#125;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class BankOperator implements Runnable &#123; @Override public void run() &#123; //锁定对象 synchronized (a) &#123; withdraw(5); deposit(50); System.out.println(\"brent账户余额： \"+a.getMoney()); &#125; &#125; static Acount a; public void deposit(float money) &#123; a.setMoney(a.getMoney() + money); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \":\" + \"存款后 \" + a.getMoney()); &#125; public void withdraw(float money) &#123; a.setMoney(a.getMoney() - money); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \":\" + \"取款后 \" + a.getMoney()); &#125; BankOperator(Acount a) &#123; super(); this.a = a; &#125; public static void main(String args[]) &#123; Acount a = new Acount(\"brent\", 100); BankOperator bko = new BankOperator(a); Thread t1 = new Thread(bko, \"customer1\"); Thread t2 = new Thread(bko, \"customer2\"); Thread t3 = new Thread(bko, \"customer3\"); Thread t4 = new Thread(bko, \"customer4\"); t1.start(); t2.start(); t3.start(); t4.start(); &#125;&#125;","categories":[],"tags":[]},{"title":"进程和线程基本概念","slug":"进程和线程基本概念","date":"2016-08-07T07:30:21.000Z","updated":"2020-04-04T17:55:34.701Z","comments":true,"path":"2016/08/07/进程和线程基本概念/","link":"","permalink":"cpeixin.cn/2016/08/07/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"前言进程（process）和线程（thread）是操作系统的基本概念，也是平常编程的过程中，我们经常遇到和听到的名词由于上大学的时候，老师在讲解有关线程、进程课程时，我应该在玩《神庙逃亡》，所以这一方面一直不是很扎实就在最近，我读到了国外的一篇文章和阮一峰博士的文章，这两篇文章利用‘车间’和‘工人’的关系来类比，浅显易懂，分享给大家概念计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。进程假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工背后的含义就是，单个CPU一次只能运行一个任务进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。线程一个车间里，可以有很多工人。他们协同完成一个任务。线程就好比车间里的工人。背后的含义就是，一个进程可以包括多个线程线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。共享内存车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。背后的含义就是，这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存互斥锁可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。“互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域信号量还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。优先级如果卫生间目前已锁定并且有许多人正在等待使用它怎么办？显然，所有的人都坐在外面，等待浴室里的任何人出去。真正的问题是，“当门解锁时会发生什么？谁下次去？“你会认为允许等待时间最长的人接下来是“公平的”。或者，让最老的人走向下一步可能是“公平的”。或者最高。或者最重要的。有很多方法可以确定什么是“公平的”。我们通过两个因素来解决这个问题：优先级和等待时间。假设两个人同时出现在（锁定的）浴室门口。其中一个人有一个紧迫的截止日期（他们已经迟到了会议），而另一个则没有。让紧迫的截止日期的人下一步是不是有意义？嗯，当然会。唯一的问题是你如何决定谁更“重要”。 这可以通过分配优先级来完成（让我们使用像Neutrino这样的数字 - 一个是最低的可用优先级，255是此版本中最高的优先级）。房屋内有紧迫期限的人将获得更高的优先权，而那些没有最后期限的人将被赋予较低的优先权。线程也一样。线程从其父线程继承其调度算法，但可以调用 pthread_setschedparam（） 来更改其调度策略和优先级（如果它有权执行此操作）。如果有多个线程在等待，并且互斥锁被解锁，我们会将互斥锁提供给具有最高优先级的等待线程。但是，假设两个人都有同样的优先权。那你现在怎么办？那么，在这种情况下，允许等待时间最长的人下一步是“公平的”。这不仅是“公平的”，而且也是Neutrino内核的作用。在一堆线程等待的情况下，我们主要通过优先级，其次 是等待的长度。互斥锁肯定不是我们遇到的唯一同步对象。我们来看看其他一些。操作系统的设计，因此可以归结为三点：（1）以多进程形式，允许多个任务同时运行；（2）以多线程形式，允许单个任务分成不同的部分运行；（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。进程和线程简单而基本靠谱的定义如下：进程：程序的一次执行线程：CPU的基本调度单位","categories":[],"tags":[]}],"categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"},{"name":"开发工具","slug":"开发工具","permalink":"cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"},{"name":"架构","slug":"架构","permalink":"cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"},{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"},{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"工具","slug":"工具","permalink":"cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"},{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"DataBase","slug":"DataBase","permalink":"cpeixin.cn/categories/DataBase/"},{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"},{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"},{"name":"IDEA","slug":"IDEA","permalink":"cpeixin.cn/tags/IDEA/"},{"name":"GPT-2","slug":"GPT-2","permalink":"cpeixin.cn/tags/GPT-2/"},{"name":"kali","slug":"kali","permalink":"cpeixin.cn/tags/kali/"},{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"},{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"},{"name":"爬虫","slug":"爬虫","permalink":"cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"shadowsock","slug":"shadowsock","permalink":"cpeixin.cn/tags/shadowsock/"},{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"时间序列","slug":"时间序列","permalink":"cpeixin.cn/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"cpeixin.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"cpeixin.cn/tags/Random-Forest/"},{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"},{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"},{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"},{"name":"K-Means","slug":"K-Means","permalink":"cpeixin.cn/tags/K-Means/"},{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"},{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"},{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"},{"name":"sklearn","slug":"sklearn","permalink":"cpeixin.cn/tags/sklearn/"},{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"数据清洗","slug":"数据清洗","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"},{"name":"数据采集","slug":"数据采集","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"},{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"},{"name":"词向量","slug":"词向量","permalink":"cpeixin.cn/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"},{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"},{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"},{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"},{"name":"OLAP","slug":"OLAP","permalink":"cpeixin.cn/tags/OLAP/"},{"name":"mysql","slug":"mysql","permalink":"cpeixin.cn/tags/mysql/"},{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"},{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"},{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"},{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"},{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"},{"name":"yarn","slug":"yarn","permalink":"cpeixin.cn/tags/yarn/"},{"name":"mapreduce","slug":"mapreduce","permalink":"cpeixin.cn/tags/mapreduce/"},{"name":"递归","slug":"递归","permalink":"cpeixin.cn/tags/%E9%80%92%E5%BD%92/"},{"name":"二分查找","slug":"二分查找","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"},{"name":"LRU淘汰算法","slug":"LRU淘汰算法","permalink":"cpeixin.cn/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"},{"name":"栈","slug":"栈","permalink":"cpeixin.cn/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"cpeixin.cn/tags/%E9%98%9F%E5%88%97/"},{"name":"线性表","slug":"线性表","permalink":"cpeixin.cn/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/"},{"name":"数组","slug":"数组","permalink":"cpeixin.cn/tags/%E6%95%B0%E7%BB%84/"}]}