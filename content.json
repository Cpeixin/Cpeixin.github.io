{"meta":{"title":"布兰特 | 不忘初心","subtitle":"人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华","description":"","author":"Brent","url":"cpeixin.cn","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-04-04T08:29:08.762Z","updated":"2020-04-03T14:56:33.596Z","comments":false,"path":"/404.html","permalink":"cpeixin.cn/404.html","excerpt":"","text":""},{"title":"关于","date":"2020-04-04T14:37:07.111Z","updated":"2020-04-04T14:37:07.111Z","comments":false,"path":"about/index.html","permalink":"cpeixin.cn/about/index.html","excerpt":"","text":""},{"title":"书单","date":"2020-04-04T08:29:08.739Z","updated":"2020-04-03T14:56:33.596Z","comments":false,"path":"books/index.html","permalink":"cpeixin.cn/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-04-04T08:29:08.717Z","updated":"2020-04-03T14:56:33.597Z","comments":true,"path":"links/index.html","permalink":"cpeixin.cn/links/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-04-04T08:29:08.728Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"categories/index.html","permalink":"cpeixin.cn/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-04-04T08:29:08.705Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"repository/index.html","permalink":"cpeixin.cn/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-04-04T08:29:08.693Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"tags/index.html","permalink":"cpeixin.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"def neverGrowUp()","slug":"def-neverGrowUp","date":"2020-04-05T16:00:00.000Z","updated":"2020-04-05T15:10:24.541Z","comments":true,"path":"2020/04/06/def-neverGrowUp/","link":"","permalink":"cpeixin.cn/2020/04/06/def-neverGrowUp/","excerpt":"","text":"123456789def neverGrowUp() while true: 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心","categories":[],"tags":[]},{"title":"抗疫英雄","slug":"抗疫英雄","date":"2020-04-04T14:45:15.000Z","updated":"2020-04-05T14:46:33.308Z","comments":true,"path":"2020/04/04/抗疫英雄/","link":"","permalink":"cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/","excerpt":"","text":"致敬缅怀每一位抗疫英雄","categories":[],"tags":[]},{"title":"python Flask & Ajax 数据传输","slug":"python-Flask-Ajax-数据传输","date":"2020-03-11T14:43:01.000Z","updated":"2020-04-04T17:13:00.080Z","comments":true,"path":"2020/03/11/python-Flask-Ajax-数据传输/","link":"","permalink":"cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/","excerpt":"","text":"帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂忙活三个小时，终于把前端和后端打通了～～前端demo：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;script src=\"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\"&gt;&lt;/script&gt;&lt;body&gt;&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;&lt;form action=\"&#123;&#123; url_for('send_message') &#125;&#125;\" method=\"post\"&gt; &lt;textarea name =\"domain\" rows=\"30\" cols=\"100\" placeholder=\"请输入需要查询的域名,如cq5999.com\"&gt;&lt;/textarea&gt; &lt;!--&lt;input id=\"submit\" type=\"submit\" value=\"发送\"&gt;--&gt; &lt;button type=\"submit\" id=\"btn-bq\" data-toggle=\"modal\" data-target=\"#myModal\"&gt;查询&lt;/button&gt;&lt;/form&gt;&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;&lt;div&gt; &lt;label for=\"send_content\"&gt;向后台发送消息：&lt;/label&gt; &lt;input id=\"send_content\" type=\"text\" name=\"send_content\"&gt; &lt;input id=\"send\" type=\"button\" value=\"发送\"&gt;&lt;/div&gt;&lt;div&gt; &lt;label for=\"recv_content\"&gt;从后台接收消息：&lt;/label&gt; &lt;input id=\"recv_content\" type=\"text\" name=\"recv_content\"&gt;&lt;/div&gt;&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;&lt;!-- 发送 --&gt;&lt;script type=\"text/javascript\"&gt; $(\"#send\").click(function () &#123; var message = $(\"#send_content\").val() alert(message) $.ajax(&#123; url:\"/send_message\", type:\"POST\", data:&#123; message:message &#125;, dataType: 'json', success:function (data) &#123; &#125; &#125;) &#125;)&lt;/script&gt;&lt;!-- 接收 --&gt;&lt;script type=\"text/javascript\"&gt; $(\"#send\").click(function () &#123; $.getJSON(\"/change_to_json\",function (data) &#123; $(\"#recv_content\").val(data.message) //将后端数据显示在前端 console.log(\"传到前端的数据的类型：\" + typeof (data.message)) $(\"#send_content\").val(\"\")//发送的输入框清空 &#125;) &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;后端demo:1234567891011121314151617181920212223242526272829303132333435from flask import Flask, render_template, request, jsonifyapp = Flask(__name__)@app.route('/')def index(): return render_template(\"index_v6.html\")@app.route('/send_message', methods=['POST'])def send_message(): global message_get message_get = \"\" message_get = request.form[\"domain\"].split('\\n') # message_get = request.form['message'] #input提交 print(\"收到前端发过来的信息：%s\" % message_get) print(\"收到数据的类型为：\" + str(type(message_get))) return \"收到消息\"@app.route('/change_to_json', methods=['GET'])def change_to_json(): global message_get message_json = &#123; \"message\": message_get &#125; return jsonify(message_json)if __name__ == '__main__': app.run(host='0.0.0.0', port=80,debug=True)","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"}]},{"title":"Python Flask接口设计-示例","slug":"Python-Flask接口设计-示例","date":"2020-03-10T15:08:35.000Z","updated":"2020-04-04T17:12:52.356Z","comments":true,"path":"2020/03/10/Python-Flask接口设计-示例/","link":"","permalink":"cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"Get 请求开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。**方法 1:****123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask import requestfrom flask import redirectfrom flask import jsonifyimport jsonapp = Flask(__name__)@app.route(\"/test_1.0\", methods=[\"GET\"])def check(): # 默认返回内容 return_dict = &#123;'return_code': '200', 'return_info': '处理成功', 'result': False&#125; # 判断入参是否为空 if request.args is None: return_dict['return_code'] = '5004' return_dict['return_info'] = '请求参数为空' return json.dumps(return_dict, ensure_ascii=False) # 获取传入的params参数 get_data = request.args.to_dict() name = get_data.get('name') age = get_data.get('age') # 对参数进行操作 return_dict['result'] = tt(name, age) return json.dumps(return_dict, ensure_ascii=False)# 功能函数def tt(name, age): result_str = \"%s今年%s岁\" % (name, age) return result_strif __name__ == '__main__': app.run(host='0.0.0.0', port=80)此种方式对应的request请求方式：拼接请求链接, 直接请求：http://0.0.0.0/test_1.0?name=ccc&amp;age=18request 请求中带有参数，如下图方法 2:123@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])def banWordSingleStart(word): return getWordStatus(word)此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字请求链接：http://0.0.0.0/api/banWordSingle/输入词Post 请求123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask import requestfrom flask import redirectfrom flask import jsonifyimport jsonapp = Flask(__name__)@app.route(\"/test_1.0\", methods=[\"POST\"])def check(): # 默认返回内容 return_dict = &#123;'return_code': '200', 'return_info': '处理成功', 'result': False&#125; # 判断入参是否为空 if request.args is None: return_dict['return_code'] = '5004' return_dict['return_info'] = '请求参数为空' return json.dumps(return_dict, ensure_ascii=False) # 获取传入的params参数 get_data = request.args.to_dict() name = get_data.get('name') age = get_data.get('age') # 对参数进行操作 return_dict['result'] = tt(name, age) return json.dumps(return_dict, ensure_ascii=False)# 功能函数def tt(name, age): result_str = \"%s今年%s岁\" % (name, age) return result_strif __name__ == '__main__': app.run(host='0.0.0.0', port=8080)请求方式：","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"}]},{"title":"IDEA install TabNine","slug":"IDEA-install-TabNine","date":"2020-01-22T02:26:15.000Z","updated":"2020-04-04T11:06:48.223Z","comments":true,"path":"2020/01/22/IDEA-install-TabNine/","link":"","permalink":"cpeixin.cn/2020/01/22/IDEA-install-TabNine/","excerpt":"","text":"TabNine是我目前遇到过最好的智能补全工具TabNine基于GPT-2的插件安装IDEA编译器，找到pluginsWindows pycharm：File&gt;settings&gt;plugins;Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins查找 TabNine, 点击 install, 随后 restart重启后：Help&gt;Edit Custom Properties…&gt;Create;在跳出来的idea.properties中输入（注：英文字符） TabNine::config随即会自动弹出TabNine激活页面；激活点击Activation Key下面的here；输入你的邮箱号；复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）等待自动安装，观察页面（最下面有log可以看当前进度）；激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；确认激活完成，重启pycharm即可；","categories":[{"name":"开发工具","slug":"开发工具","permalink":"cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"cpeixin.cn/tags/IDEA/"}]},{"title":"GPT-2 Chinese 自动生成文章 - 环境准备","slug":"GPT-2-Chinese-自动生成文章-环境准备","date":"2020-01-01T14:28:43.000Z","updated":"2020-04-13T09:28:23.224Z","comments":true,"path":"2020/01/01/GPT-2-Chinese-自动生成文章-环境准备/","link":"","permalink":"cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"Google ColabColaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。优点 免费！ 免费！免费！**谷歌云盘当登录账号进入谷歌云盘时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：引入Colab设置GPU环境打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”挂载和切换工作目录1234567from google.colab import drivedrive.mount('/content/drive')import os# os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录os.chdir('/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese') ## 共享云盘的目录其中： My Drive 代表你的google网盘根目录code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录在Colab中运行任务下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。.ipynb文件内容","categories":[{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"}],"tags":[{"name":"GPT-2","slug":"GPT-2","permalink":"cpeixin.cn/tags/GPT-2/"}]},{"title":"架构思想","slug":"架构思想","date":"2019-12-20T02:26:15.000Z","updated":"2020-04-04T11:23:45.206Z","comments":true,"path":"2019/12/20/架构思想/","link":"","permalink":"cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/","excerpt":"","text":"关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。架构思想CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度满足业务发展需求是最高准则业务建模，抽象和枚举是两种方式，需要平衡，不能走极端模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难业务架构、系统架构、数据模型面对一块新业务，如何系统架构？业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。如何模块化基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。提升系统的稳定性流控双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。容灾最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。稳定性第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。容量规划做好容量规划、系统间强弱依赖关系梳理。如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。分布式架构分布式系统分布式缓存分布式数据API 和乐高积木有什么相似之处？相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。你可以发现很多 API 和乐高积木的相似之处：标准化：通用、标准化的组件，作为基本的构建块（building blocks）；可用性：强调可用性，附有文档或使用说明；可定制：为不同功能使用不同的API；创造性：能够组合不同的 API 来创造混搭的结果；乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。","categories":[{"name":"架构","slug":"架构","permalink":"cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[]},{"title":"kali中文设置","slug":"kali中文设置","date":"2019-12-01T02:26:15.000Z","updated":"2020-04-04T11:06:21.313Z","comments":true,"path":"2019/12/01/kali中文设置/","link":"","permalink":"cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"更新源https://blog.csdn.net/qq_38333291/article/details/89764967设置编码和中文字体安装http://www.linuxdiyf.com/linux/20701.html","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"kali","slug":"kali","permalink":"cpeixin.cn/tags/kali/"}]},{"title":"分布式下的数据hash分布","slug":"分布式下的数据hash分布","date":"2019-11-19T15:05:08.000Z","updated":"2020-04-04T11:24:04.737Z","comments":true,"path":"2019/11/19/分布式下的数据hash分布/","link":"","permalink":"cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"我的服务器被黑了（二）","slug":"我的服务器被黑了（二）","date":"2019-09-09T02:26:15.000Z","updated":"2020-04-04T12:00:14.168Z","comments":true,"path":"2019/09/09/我的服务器被黑了（二）/","link":"","permalink":"cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令1journalctl -xe映入眼帘的是满屏的扫描和ssh尝试登陆1234567891011121314151617181920212223242526272829303132333435363738394041424344Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Failed password for invalid user admin from 117.132.175.25 port 42972 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Received disconnect from 117.132.175.25 port 42972:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Disconnected from 117.132.175.25 port 42972 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Failed password for invalid user ansible from 149.56.96.78 port 44980 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Received disconnect from 149.56.96.78 port 44980:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Disconnected from 149.56.96.78 port 44980 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Failed password for root from 218.92.0.163 port 45157 ssh2Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: error: maximum authentication attempts exceeded for root from 218.92.0.163 port 45157 ssh2 [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: error: maximum authentication attempts exceeded for root from 49.88.112.54 port 24184 ssh2 [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:54 4Z-J16-A47 sshd[314]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:54 4Z-J16-A47 sshd[314]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"lines 1105-1127/1127 (END)Sep 09 11:02:49 4Z-J16-A47 sshd[65522]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Failed password for invalid user admin from 117.132.175.25 port 42972 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Received disconnect from 117.132.175.25 port 42972:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Disconnected from 117.132.175.25 port 42972 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Failed password for invalid user ansible from 149.56.96.78 port 44980 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Received disconnect from 149.56.96.78 port 44980:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Disconnected from 149.56.96.78 port 44980 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Failed password for root from 218.92.0.163 port 45157 ssh2Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: error: maximum authentication attempts exceeded for root from 218.92.0.163 port 45157 ssh2 [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: error: maximum authentication attempts exceeded for root from 49.88.112.54 port 24184 ssh2 [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM service(sshd) ignoring max retries; 6 &gt; 3看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。12[root@*** ~]# useradd one #创建用户[root@*** ~]# passwd one #设置密码输入新用户密码首先确保文件 /etc/sudoers 中1234567%wheel ALL=(ALL) ALL``` 没有被注释```linuxusermod -g wheel onerocket设置只有指定用户组才能使用su命令切换到root用户在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。1234567#! /bin/bash# Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限sed -i '/pam_wheel.so use_uid/c\\auth required pam_wheel.so use_uid ' /etc/pam.d/sun=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`if [ $n -eq 0 ];thenecho SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defsfi打开SSHD的配置文件1vim /etc/ssh/sshd_config查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。修改sshd默认端口虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。打开sshd配置文件1vi /etc/ssh/sshd_config找到#Port 22 删掉注释服务器端口最大可以开到65536同时再添加一个Port 61024 （随意设置）Port 22Port 61024重启sshd服务123service sshd restart #centos6系列systemctl restart sshd #centos7系列firewall-cmd --add-port=61024/tcp测试，使用新用户，新端口进行登录如果登陆成功后，再将Port22注释掉，重启sshd服务。到这里，关于远程登录的防护工作，就做好了。最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"}]},{"title":"我的服务器被黑了","slug":"我的服务器被黑了","date":"2019-08-24T02:26:15.000Z","updated":"2020-04-04T12:00:09.871Z","comments":true,"path":"2019/08/24/我的服务器被黑了/","link":"","permalink":"cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/","excerpt":"","text":"服务器自述我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢为什么黑我的服务器这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器如何发现被黑回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作1.关闭了Linux防火墙2.没有设置redis访问密码3.没有更改redis默认端口4.开放了任意IP可以远程连接以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。1lsof -i tcp:6379使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想直到我运行：1top发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了做了哪些急救工作这时，感觉自己开始投入了一场对抗战1.首先查找植入程序的位置。在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。2.删除清理可疑文件杀死进程删除了正在运行的程序文件还有安装包3.查看所有用户的定时任务1cat /etc/passwd |cut -f 1 -d:crontab -uXXX -l4.开启防火墙仅开放会使用到的端口5.修改redis默认端口redis.conf中的port6.添加redis授权密码redis.conf中的requirepass7.修改绑定远程绑定ipredis.conf中的bind最后重启redis服务！从中学到了什么明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"}]},{"title":"python爬虫 - 动态爬取","slug":"python爬虫之动态爬取","date":"2019-06-12T15:26:15.000Z","updated":"2020-04-04T17:11:17.062Z","comments":true,"path":"2019/06/12/python爬虫之动态爬取/","link":"","permalink":"cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/","excerpt":"","text":"我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：Headers：请求和响应的详细信息Preview：响应体格式化之后的显示Response：响应体原始内容Cookies：CookiesTiming：时间开销通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：上面的截图展示了这次请求的请求方式、请求地址等信息。上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。我们回过头重新观察一下返回的json，格式化之后的层级关系如下：很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：1234import jsonjson_obj = json.loads(\"&#123;'key': 'value'&#125;\") # 字符串到对象json_str = json.dumps(json_obj) # 对象到字符串json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"shadowsock-vps搭建VPN","slug":"shadowsock-vps搭建VPN","date":"2019-04-19T15:26:15.000Z","updated":"2020-04-04T17:11:07.011Z","comments":true,"path":"2019/04/19/shadowsock-vps搭建VPN/","link":"","permalink":"cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/","excerpt":"","text":"前言还有10天左右就要回国了，由于职业的需要，对Google的依赖的越来越大的，那么回国后怎么才能‘科学上网’呢？之前在国内的时候，有使用过Lantern，稳定性和速度都还是不错了，可惜后来被和谐了。所以今天准备尝试搭建VPN，自己独立使用，一边搭建一边将过程记录下来。名词解释VPS: VPS（Virtual Private Server 虚拟专用服务器）技术，将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器 [1] 技术，和虚拟化技术 [2] 。在容器或虚拟机中，每个VPS都可分配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。VPS为使用者提供了管理配置的自由，可用于企业虚拟化，也可以用于IDC资源租用。VPN: VPN的学名叫虚拟专用网，洋文叫“Virtual Private Network”。维基百科的介绍在“这里”。本来这玩意儿主要是用于商业公司，为了让那些不在公司里的员工（比如出差在外的）能够方便地访问公司的内部网络。为了防止黑客冒充公司的员工，从外部访问公司的内部网络，VPN 软件都会提供强大的加密功能。而这个加密功能，也就让它顺便成为翻墙的利器。科学上网原理VPN浏览外网的原理使用 VPN 通常需要先安装客户端软件。当你运行 VPN 客户端，它会尝试联到 VPN 服务器（这点跟加密代理类似）。一旦和 VPN 服务器建立连接，VPN 客户端就会在你的系统中建立了一个虚拟局域网。而且，你的系统中也会多出一个虚拟网卡（在 Windows 下，可以用 ipconfig /all 命令，看到这多出来的网卡）。这样一来，你的系统中就有不止一块网卡。这就引出一个问题：那些访问网络的程序，它的数据流应该通过哪个网卡进出？为了解决此问题，VPN 客户端通常会修改你系统的路由表，让那些数据流，优先从虚拟的网卡进出。由于虚拟的网卡是通往 VPN 服务器的，当数据流到达 VPN 服务器之后，VPN 服务器再帮你把数据流转向到真正的目的地。前面说了，VPN 为了保证安全，都采用强加密的方式传输数据。这样一来，GFW 就无法分析你的网络数据流，进行敏感词过滤。所以，使用墙外的VPN服务器，无形中就能达到翻墙的效果。方案选择VPN是一个大类，其中有很多实现的方法，防火长城现在将 VPN 屏蔽的已经所剩无几，后来大家看到了SSH，使用SSH的sock5很稳定，但是特征也十分明显，防火长城可以对其直接进行定向干扰。而除了VPN，对于翻墙大家仍然有很多方法，比如Shadowsocks 、Lantern、VPNGate 等等，而实际上无论哪种方式，他们本身都需要一台服务器作为中间人进行消息传递。而VPS虚拟专用服务器就十分适合担当这个角色，并且由于VPS平时就作为商品在各类云服务器平台上售卖，自行购买并搭建相当方便，唯一需要的就是人们对于服务器的操作技术。而这次选择的方案是：VPS+Shadowsocks**Shadowsocks特点：省电，在电量查看里几乎看不到它的身影；支持开机自启动，且断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴；可使用自己的服务器，安全和速度的保证；支持区分国内外流量，传统VPN在翻出墙外后访问国内站点会变慢；可对应用设置单独代理，5.0之后的系统无需root。Shadowsocks 目前不容易被封杀主要是因为：建立在socks5协议之上，socks5是运用很广泛的协议，所以没办法直接封杀socks5协议使用socks5协议建立连接，而没有使用VPN中的服务端身份验证和密钥协商过程。而是在服务端和客户端直接写死密钥和加密算法。所以防火墙很难找到明显的特征，因为这就是个普通的socks5协议。Shadowsock搭建也比较简单，所以很多人自己架设VPS搭建，个人使用流量也很小，没法通过流量监控方式封杀。自定义加密方式和密钥。因为加密主要主要是防止被检测，所以要选择安全系数高的加密方式。之前RC4会很容易被破解，而导致被封杀。所以现在推荐使用AES加密。而在客户端和服务端自定义密钥，泄露的风险相对较小。所以如果是自己搭建的Shadosocks被封的概率很小，但是如果是第三方的Shadeowsocks，密码是server定的，你的数据很可能遭受到中间人攻击。开工购买vps首先我们需要购买一台境外的服务器，接着我们在这台云服务器里面安装代理服务，那么以后我们上网的时候就可以通过它来中转，轻松畅快的畅游全网了。购买VPS,我选择了vultr，大家用过都说好，购买的过程也很方便。第一步：选择离中国较近国家的服务器。第二步：选择服务器配置和系统这里，系统选择的是CentOS 7,配置的话，如果只是自己浏览网页的话，选择最低配置就好。其他的选项可以略过。第三步：支付和部署支付可以选择支付宝支付，非常方便。购买成功后，点击Server中的“+”号，来部署你刚刚选择的服务器。第四步：登陆服务器查看服务器详情 Server Details,根据提供的服务器信息，登陆服务器。我是使用Mac本身终端ssh到服务器上的，因为Mac上多数的SSH客户端要么收费，要么不好用，要么安装过程非常繁琐。1ssh -p 22 root@ip搭建shadowsocks服务器连接到你的 vultr 服务器之后，接下来就可以使用几个命令让你快速搭建一个属于自己的 ss 服务器：1yum install wget接着执行安装shadowsocks：1wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh获取 shadowsocks.sh 读取权限：1chmod +x shadowsocks.sh设置你的 ss 密码和端口号：1./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log接下来后就可以设置密码和端口号了密码和端口号可以使用默认的，也可以直接重新输入新的。选择加密方式设置完密码和端口号之后，我们选择加密方式，这里选择 7 ，使用aes-256-cfb的加密模式接着按任意键进行安装。安装ss完成后会给你显示你需要连接 vpn 的信息：搞定，将这些信息保存起来，那么这时候你就可以使用它们来科学上网啦。使用BBR加速上网安装 BBR1wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh获取读写权限1chmod +x bbr.sh启动BBR安装1./bbr.sh接着按任意键，开始安装，坐等一会。安装完成一会之后它会提示我们是否重新启动vps，我们输入 y 确定重启服务器。重新启动之后，输入：1lsmod | grep bbr如果看到 tcp_bbr 就说明 BBR 已经启动了。客户端进行连接windows使用Shadowsockswindows点击下载：Shadowsocks windows客户端打开 Shadowsocks 客户端，输入ip地址，密码，端口，和加密方式。接着点击确定，右下角会有个小飞机按钮，右键–&gt;启动代理。Android使用ShadowsocksAndroid点击下载：Shadowsocks Android客户端打开apk安装，接着打开APP，输入ip地址，密码，端口，和加密方式。即可科学上网。iPhone使用ShadowsocksiPhone要下载的app需要在appstore下载，但是需要用美区账号才能下载，而且这个APP需要钱。在这里提供一种解决方案，就是可以再搭建一个IPsec/L2TP VPN,专门给你的iPhone使用。Mac配置用的是Mac电脑，所以点击相关链接。东西都挂在github上，下载对应的zip文件，下载完成后安装并运行起来。点击图标，进入 服务器设置主要有四个地方要填，服务器的地址，端口号，加密方法，密码。服务器地址即为之前 Main controls选项中的IP地址。端口号、加密方法、密码必须与之前 Shadowsocks Server 中的信息一一匹配，否则会连接失败。设置完成后点击确定，然后服务器选择这个配置，默认选中PAC自动模式，确保Shadowsocks状态为On，这时候打开谷歌试试~接着就可以上外网了 😂","categories":[{"name":"工具","slug":"工具","permalink":"cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"shadowsock","slug":"shadowsock","permalink":"cpeixin.cn/tags/shadowsock/"}]},{"title":"数据分析 - PageRank 实战","slug":"数据分析 - PageRank-实战","date":"2019-03-12T16:17:42.000Z","updated":"2020-04-06T16:19:44.167Z","comments":true,"path":"2019/03/13/数据分析 - PageRank-实战/","link":"","permalink":"cpeixin.cn/2019/03/13/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%AE%9E%E6%88%98/","excerpt":"","text":"上文讲了 PageRank 算法经常被用到网络关系的分析中，比如在社交网络中计算个人的影响力，计算论文的影响力或者网站的影响力等。今天我们就来做一个关于 PageRank 算法的实战，在这之前，你需要思考三个问题：如何使用工具完成 PageRank 算法，包括使用工具创建网络图，设置节点、边、权重等，并通过创建好的网络图计算节点的 PR 值；对于一个实际的项目，比如希拉里的 9306 封邮件（工具包中邮件的数量），如何使用 PageRank 算法挖掘出有影响力的节点，并且绘制网络图；如何对创建好的网络图进行可视化，如果网络中的节点数较多，如何筛选重要的节点进行可视化，从而得到精简的网络关系图。如何使用工具实现 PageRank 算法PageRank 算法工具在 sklearn 中并不存在，我们需要找到新的工具包。实际上有一个关于图论和网络建模的工具叫 NetworkX，它是用 Python 语言开发的工具，内置了常用的图与网络分析算法，可以方便我们进行网络数据分析。上节课，我举了一个网页权重的例子，假设一共有 4 个网页 A、B、C、D，它们之间的链接信息如图所示：针对这个例子，我们看下用 NetworkX 如何计算 A、B、C、D 四个网页的 PR 值，具体代码如下：123456789import networkx as nx# 创建有向图G = nx.DiGraph() # 有向图之间边的关系edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"), (\"B\", \"A\"), (\"B\", \"D\"), (\"C\", \"A\"), (\"D\", \"B\"), (\"D\", \"C\")]for edge in edges: G.add_edge(edge[0], edge[1])pagerank_list = nx.pagerank(G, alpha=1)print(\"pagerank值是：\", pagerank_list)结果：1pagerank值是： &#123;&#39;A&#39;: 0.33333396911621094, &#39;B&#39;: 0.22222201029459634, &#39;C&#39;: 0.22222201029459634, &#39;D&#39;: 0.22222201029459634&#125;我们通过 NetworkX 创建了一个有向图之后，设置了节点之间的边，然后使用 PageRank 函数就可以求得节点的 PR 值，结果和上节课中我们人工模拟的结果一致。好了，运行完这个例子之后，我们来看下 NetworkX 工具都有哪些常用的操作。1. 关于图的创建图可以分为无向图和有向图，在 NetworkX 中分别采用不同的函数进行创建。无向图指的是不用节点之间的边的方向，使用 nx.Graph() 进行创建；有向图指的是节点之间的边是有方向的，使用 nx.DiGraph() 来创建。在上面这个例子中，存在 A→D 的边，但不存在 D→A 的边。2. 关于节点的增加、删除和查询如果想在网络中增加节点，可以使用 G.add_node(‘A’) 添加一个节点，也可以使用 G.add_nodes_from([‘B’,‘C’,‘D’,‘E’]) 添加节点集合。如果想要删除节点，可以使用 G.remove_node(node) 删除一个指定的节点，也可以使用 G.remove_nodes_from([‘B’,‘C’,‘D’,‘E’]) 删除集合中的节点。那么该如何查询节点呢？如果你想要得到图中所有的节点，就可以使用 G.nodes()，也可以用 G.number_of_nodes() 得到图中节点的个数。3. 关于边的增加、删除、查询增加边与添加节点的方式相同，使用 G.add_edge(“A”, “B”) 添加指定的“从 A 到 B”的边，也可以使用 add_edges_from 函数从边集合中添加。我们也可以做一个加权图，也就是说边是带有权重的，使用 add_weighted_edges_from 函数从带有权重的边的集合中添加。在这个函数的参数中接收的是 1 个或多个三元组[u,v,w]作为参数，u、v、w 分别代表起点、终点和权重。另外，我们可以使用 remove_edge 函数和 remove_edges_from 函数删除指定边和从边集合中删除。另外可以使用 edges() 函数访问图中所有的边，使用 number_of_edges() 函数得到图中边的个数。以上是关于图的基本操作，如果我们创建了一个图，并且对节点和边进行了设置，就可以找到其中有影响力的节点，原理就是通过 PageRank 算法，使用 nx.pagerank(G) 这个函数，函数中的参数 G 代表创建好的图。如何用 PageRank 揭秘希拉里邮件中的人物关系了解了 NetworkX 工具的基础使用之后，我们来看一个实际的案例：希拉里邮件人物关系分析。希拉里邮件事件相信你也有耳闻，对这个数据的背景我们就不做介绍了。你可以从 GitHub 上下载这个数据集：https://github.com/cystanford/PageRank。整个数据集由三个文件组成：Aliases.csv，Emails.csv 和 Persons.csv，其中 Emails 文件记录了所有公开邮件的内容，发送者和接收者的信息。Persons 这个文件统计了邮件中所有人物的姓名及对应的 ID。因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用 Aliases 文件来查询别名和人物的对应关系。整个数据集包括了 9306 封邮件和 513 个人名，数据集还是比较大的。不过这一次我们不需要对邮件的内容进行分析，只需要通过邮件中的发送者和接收者（对应 Emails.csv 文件中的 MetadataFrom 和 MetadataTo 字段）来绘制整个关系网络。因为涉及到的人物很多，因此我们需要通过 PageRank 算法计算每个人物在邮件关系网络中的权重，最后筛选出来最有价值的人物来进行关系网络图的绘制。了解了数据集和项目背景之后，我们来设计到执行的流程步骤：首先我们需要加载数据源；在准备阶段：我们需要对数据进行探索，在数据清洗过程中，因为邮件中存在别名的情况，因此我们需要统一人物名称。另外邮件的正文并不在我们考虑的范围内，只统计邮件中的发送者和接收者，因此我们筛选 MetadataFrom 和 MetadataTo 这两个字段作为特征。同时，发送者和接收者可能存在多次邮件往来，需要设置权重来统计两人邮件往来的次数。次数越多代表这个边（从发送者到接收者的边）的权重越高；在挖掘阶段：我们主要是对已经设置好的网络图进行 PR 值的计算，但邮件中的人物有 500 多人，有些人的权重可能不高，我们需要筛选 PR 值高的人物，绘制出他们之间的往来关系。在可视化的过程中，我们可以通过节点的 PR 值来绘制节点的大小，PR 值越大，节点的绘制尺寸越大。设置好流程之后，实现的代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# -*- coding: utf-8 -*-# 用 PageRank 挖掘希拉里邮件中的重要任务关系import pandas as pdimport networkx as nximport numpy as npfrom collections import defaultdictimport matplotlib.pyplot as plt# 数据加载emails = pd.read_csv(\"./input/Emails.csv\")# 读取别名文件file = pd.read_csv(\"./input/Aliases.csv\")aliases = &#123;&#125;for index, row in file.iterrows(): aliases[row['Alias']] = row['PersonId']# 读取人名文件file = pd.read_csv(\"./input/Persons.csv\")persons = &#123;&#125;for index, row in file.iterrows(): persons[row['Id']] = row['Name']# 针对别名进行转换 def unify_name(name): # 姓名统一小写 name = str(name).lower() # 去掉, 和 @后面的内容 name = name.replace(\",\",\"\").split(\"@\")[0] # 别名转换 if name in aliases.keys(): return persons[aliases[name]] return name# 画网络图def show_graph(graph, layout='spring_layout'): # 使用 Spring Layout 布局，类似中心放射状 if layout == 'circular_layout': positions=nx.circular_layout(graph) else: positions=nx.spring_layout(graph) # 设置网络图中的节点大小，大小与 pagerank 值相关，因为 pagerank 值很小所以需要 *20000 nodesize = [x['pagerank']*20000 for v,x in graph.nodes(data=True)] # 设置网络图中的边长度 edgesize = [np.sqrt(e[2]['weight']) for e in graph.edges(data=True)] # 绘制节点 nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=0.4) # 绘制边 nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=0.2) # 绘制节点的 label nx.draw_networkx_labels(graph, positions, font_size=10) # 输出希拉里邮件中的所有人物关系图 plt.show()# 将寄件人和收件人的姓名进行规范化emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)emails.MetadataTo = emails.MetadataTo.apply(unify_name)# 设置遍的权重等于发邮件的次数edges_weights_temp = defaultdict(list)for row in zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText): temp = (row[0], row[1]) if temp not in edges_weights_temp: edges_weights_temp[temp] = 1 else: edges_weights_temp[temp] = edges_weights_temp[temp] + 1# 转化格式 (from, to), weight =&gt; from, to, weightedges_weights = [(key[0], key[1], val) for key, val in edges_weights_temp.items()]# 创建一个有向图graph = nx.DiGraph()# 设置有向图中的路径及权重 (from, to, weight)graph.add_weighted_edges_from(edges_weights)# 计算每个节点（人）的 PR 值，并作为节点的 pagerank 属性pagerank = nx.pagerank(graph)# 将 pagerank 数值作为节点的属性nx.set_node_attributes(graph, name = 'pagerank', values=pagerank)# 画网络图show_graph(graph)# 将完整的图谱进行精简# 设置 PR 值的阈值，筛选大于阈值的重要核心节点pagerank_threshold = 0.005# 复制一份计算好的网络图small_graph = graph.copy()# 剪掉 PR 值小于 pagerank_threshold 的节点for n, p_rank in graph.nodes(data=True): if p_rank['pagerank'] &lt; pagerank_threshold: small_graph.remove_node(n)# 画网络图,采用circular_layout布局让筛选出来的点组成一个圆show_graph(small_graph, 'circular_layout')结果如下：针对代码中的几个模块我做个简单的说明：1. 函数定义人物的名称需要统一，因此我设置了 unify_name 函数，同时设置了 show_graph 函数将网络图可视化。NetworkX 提供了多种可视化布局，这里我使用 spring_layout 布局，也就是呈中心放射状。除了 spring_layout 外，NetworkX 还有另外三种可视化布局，circular_layout（在一个圆环上均匀分布节点），random_layout（随机分布节点 ），shell_layout（节点都在同心圆上）。2. 计算边权重邮件的发送者和接收者的邮件往来可能不止一次，我们需要用两者之间邮件往来的次数计算这两者之间边的权重，所以我用 edges_weights_temp 数组存储权重。而上面介绍过在 NetworkX 中添加权重边（即使用 add_weighted_edges_from 函数）的时候，接受的是 u、v、w 的三元数组，因此我们还需要对格式进行转换，具体转换方式见代码。3.PR 值计算及筛选我使用 nx.pagerank(graph) 计算了节点的 PR 值。由于节点数量很多，我们设置了 PR 值阈值，即 pagerank_threshold=0.005，然后遍历节点，删除小于 PR 值阈值的节点，形成新的图 small_graph，最后对 small_graph 进行可视化（对应运行结果的第二张图）。总结我们通过矩阵乘法求得网页的权重，这我们使用 NetworkX 可以得到相同的结果。另外我带你用 PageRank 算法做了一次实战，我们将一个复杂的网络图，通过 PR 值的计算、筛选，最终得到了一张精简的网络图。在这个过程中我们学习了 NetworkX 工具的使用，包括创建图、节点、边及 PR 值的计算。实际上掌握了 PageRank 的理论之后，在实战中往往就是一行代码的事。但项目与理论不同，项目中涉及到的数据量比较大，你会花 80% 的时间（或 80% 的代码量）在预处理过程中，比如今天的项目中，我们对别名进行了统一，对边的权重进行计算，同时还需要把计算好的结果以可视化的方式呈现。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"}]},{"title":"PageRank 原理","slug":"数据分析 - PageRank-原理","date":"2019-03-10T14:37:25.000Z","updated":"2020-04-06T16:19:48.787Z","comments":true,"path":"2019/03/10/数据分析 - PageRank-原理/","link":"","permalink":"cpeixin.cn/2019/03/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%8E%9F%E7%90%86/","excerpt":"","text":"互联网发展到现在，搜索引擎已经非常好用，基本上输入关键词，都能找到匹配的内容，质量还不错。但在 1998 年之前，搜索引擎的体验并不好。早期的搜索引擎，会遇到下面的两类问题：返回结果质量不高：搜索结果不考虑网页的质量，而是通过时间顺序进行检索；容易被人钻空子：搜索引擎是基于检索词进行检索的，页面中检索词出现的频次越高，匹配度越高，这样就会出现网页作弊的情况。有些网页为了增加搜索引擎的排名，故意增加某个检索词的频率。基于这些缺陷，当时 Google 的创始人拉里·佩奇提出了 PageRank 算法，目的就是要找到优质的网页，这样 Google 的排序结果不仅能找到用户想要的内容，而且还会从众多网页中筛选出权重高的呈现给用户。Google 的两位创始人都是斯坦福大学的博士生，他们提出的 PageRank 算法受到了论文影响力因子的评价启发。当一篇论文被引用的次数越多，证明这篇论文的影响力越大。正是这个想法解决了当时网页检索质量不高的问题。PageRank 的简化模型我们先来看下 PageRank 是如何计算的。我假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：这里有两个概念你需要了解一下。出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：u 为待评估的页面，Bu 为页面 u 的入链集合。针对入链集合中的任意页面 v，它能给 u 带来的影响力是其自身的影响力 PR(v) 除以 v 页面的出链数量，即页面 v 把影响力 PR(v) 平均分配给了它的出链，这样统计所有能给 u 带来链接的页面 v，得到的总和就是网页 u 的影响力，即为 PR(u)。所以你能看到，出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。在这个例子中，你能看到 A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：转移矩阵解释：第一列是A的出链的概率A-&gt;A: 0 A-&gt;B: 1/3 A-&gt;C: 1/3 A-&gt;D: 1/3第二列是B的的出链的概率B-&gt;A: 1/2 B-&gt;B: 0 B-&gt;C:0 B-&gt;D: 1/2第三列是C的出链概率C-&gt;A:1 C-&gt;B:0 C-&gt;C:0 C-&gt;D: 0第四列是D的出链概率D-&gt;A: 0 D-&gt;B:1/2 D-&gt;C:1/2 D-&gt;D: 0我们假设 A、B、C、D 四个页面的初始影响力都是相同的，即：当进行第一次转移之后，各页面的影响力 w1 变为：然后我们再用转移矩阵乘以 w1 得到 w2 结果，直到第 n 次迭代后 wn 影响力不再发生变化，可以收敛到 (0.3333，0.2222，0.2222，0.2222），也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。你能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。至此，我们模拟了一个简化的 PageRank 的计算过程，实际情况会比这个复杂，可能会面临两个问题：1. 等级泄露（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。2. 等级沉没（Rank Sink）：如果一个网页只有出链，没有入链（如下图所示），计算的过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。针对等级泄露和等级沉没的情况，我们需要灵活处理。比如针对等级泄露的情况，我们可以把没有出链的节点，先从图中去掉，等计算完所有节点的 PR 值之后，再加上该节点进行计算。不过这种方法会导致新的等级泄露的节点的产生，所以工作量还是很大的。有没有一种方法，可以同时解决等级泄露和等级沉没这两个问题呢？PageRank 的随机浏览模型为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。其中 N 为网页总数，这样我们又可以重新迭代网页的权重计算了，因为加入了阻尼因子 d，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理（这里不进行讲解）也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。PageRank 在社交影响力评估中的应用网页之间会形成一个网络，是我们的互联网，论文之间也存在着相互引用的关系，可以说我们所处的环境就是各种网络的集合。只要是有网络的地方，就存在出链和入链，就会有 PR 权重的计算，也就可以运用我们今天讲的 PageRank 算法。我们可以把 PageRank 算法延展到社交网络领域中。比如在微博上，如果我们想要计算某个人的影响力，该怎么做呢？一个人的微博粉丝数并不一定等于他的实际影响力。如果按照 PageRank 算法，还需要看这些粉丝的质量如何。如果有很多明星或者大 V 关注，那么这个人的影响力一定很高。如果粉丝是通过购买僵尸粉得来的，那么即使粉丝数再多，影响力也不高。同样，在工作场景中，比如说脉脉这个社交软件，它计算的就是个人在职场的影响力。如果你的工作关系是李开复、江南春这样的名人，那么你的职场影响力一定会很高。反之，如果你是个学生，在职场上被链入的关系比较少的话，职场影响力就会比较低。同样，如果你想要看一个公司的经营能力，也可以看这家公司都和哪些公司有合作。如果它合作的都是世界 500 强企业，那么这个公司在行业内一定是领导者，如果这个公司的客户都是小客户，即使数量比较多，业内影响力也不一定大。除非像淘宝一样，有海量的中小客户，最后大客户也会找上门来寻求合作。所以权重高的节点，往往会有一些权重同样很高的节点在进行合作。PageRank 给我们带来的启发PageRank 可以说是 Google 搜索引擎重要的技术之一，在 1998 年帮助 Google 获得了搜索引擎的领先优势，现在 PageRank 已经比原来复杂很多，但它的思想依然能带给我们很多启发。比如，如果你想要自己的媒体影响力有所提高，就尽量要混在大 V 圈中；如果想找到高职位的工作，就尽量结识公司高层，或者认识更多的猎头，因为猎头和很多高职位的人员都有链接关系。同样，PageRank 也可以帮我们识别链接农场。链接农场指的是网页为了链接而链接，填充了一些没有用的内容。这些页面相互链接或者指向了某一个网页，从而想要得到更高的权重。总结今天我给你讲了 PageRank 的算法原理，对简化的 PageRank 模型进行了模拟。针对简化模型中存在的等级泄露和等级沉没这两个问题，PageRank 的随机浏览模型引入了阻尼因子 d 来解决。同样，PageRank 有很广的应用领域，在许多网络结构中都有应用，比如计算一个人的微博影响力等。它也告诉我们，在社交网络中，链接的质量非常重要。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"}]},{"title":"数据分析-关联规则原理","slug":"数据分析-关联规则原理","date":"2019-03-02T14:19:09.000Z","updated":"2020-04-05T14:30:54.004Z","comments":true,"path":"2019/03/02/数据分析-关联规则原理/","link":"","permalink":"cpeixin.cn/2019/03/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%8E%9F%E7%90%86/","excerpt":"","text":"关联规则这个概念，最早是由 Agrawal 等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于关联规则的 Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。关联规则挖掘可以让我们从数据集中发现项与项（item 与 item）之间的关系，它在我们的生活中有很多应用场景，“购物篮分析”就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。在今天的内容中，希望你能带着问题，和我一起来搞懂以下几个知识点：搞懂关联规则中的几个重要概念：支持度、置信度、提升度；Apriori 算法的工作原理；在实际工作中，我们该如何进行关联规则挖掘。关联规则概念搞懂关联规则中的几个概念我举一个超市购物的例子，下面是几名客户购买的商品列表：什么是支持度呢？支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。在这个例子中，我们能看到“牛奶”出现了 4 次，那么这 5 笔订单中“牛奶”的支持度就是 4/5=0.8。同样“牛奶 + 面包”出现了 3 次，那么这 5 笔订单中“牛奶 + 面包”的支持度就是 3/5=0.6。什么是置信度呢？它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度 (牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2 次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。什么是提升度呢？我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升的”程度。还是看上面的例子，如果我们单纯看置信度 (可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品 A 对商品 B 的提升度：提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。所以提升度有三种可能：提升度 (A→B)&gt;1：代表有提升；提升度 (A→B)=1：代表有没有提升，也没有下降；提升度 (A→B)&lt;1：代表有下降。提升度 (牛奶→啤酒)Apriori 的工作原理明白了关联规则中支持度、置信度和提升度这几个重要概念，我们来看下 Apriori 算法是如何工作的。首先我们把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：Apriori 算法其实就是查找频繁项集 (frequent itemset) **的过程，所以首先我们需要定义什么是频繁项集。频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集**，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。我们再来看下这个例子，假设我随机指定最小支持度是 50%，也就是 0.5。我们来看下 Apriori 算法是如何运算的。首先，我们先计算单个商品的支持度，也就是得到 K=1 项的支持度：因为最小支持度是 0.5，所以你能看到商品 4、6 是不符合最小支持度的，不属于频繁项集，于是经过筛选商品的频繁项集就变成：在这个基础上，我们将商品两两组合， 根据订单编号图，得到 k=2 项的支持度：我们再筛掉小于最小值支持度的商品组合，可以得到：我们再将商品进行 K=3 项的商品组合，可以得到：商品项集支持度1，2，33/51，2，51/51，3，52/52，3，52/5再筛掉小于最小值支持度的商品组合，可以得到：通过上面这个过程，我们可以得到 K=3 项的频繁项集{1,2,3}，也就是{牛奶、面包、尿布}的组合。到这里，你已经和我模拟了一遍整个 Apriori 算法的流程，下面我来给你总结下 Apriori 算法的递归流程：K=1，计算 K 项集的支持度；筛选掉小于最小支持度的项集；如果项集为空，则对应 K-1 项集的结果为最终结果。否则 K=K+1，重复 1-3 步。Apriori 的改进算法：FP-Growth 算法我们刚完成了 Apriori 算法的模拟，你能看到 Apriori 在计算的过程中有以下几个缺点：可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；每次计算都需要重新扫描数据集，来计算每个项集的支持度。所以 Apriori 算法会浪费很多计算空间和计算时间，为此人们提出了 FP-Growth 算法，它的特点是：创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；整个生成过程只遍历数据集 2 次，大大减少了计算量。所以在实际工作中，我们常用 FP-Growth 来做频繁项集的挖掘，下面我给你简述下 FP-Growth 的原理。1. 创建项头表（item header table）创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。2. 构造 FP 树FP 树的根节点记为 NULL 节点。整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。3. 通过 FP 树挖掘频繁项集到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。具体的操作会用到一个概念，叫“条件模式基”，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。我以“啤酒”的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：你能看出来，相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以“啤酒”为节点的 FP 子树，也就是说，在频繁项集中一定要含有“啤酒”这个项。你可以再看下原始的数据，其中订单 1{牛奶、面包、尿布}和订单 5{牛奶、面包、尿布、可乐}并不存在“啤酒”这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以“啤酒”为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。条件模式基不包括“啤酒”节点，而且祖先节点如果小于最小支持度就会被剪枝，所以“啤酒”的条件模式基为空。同理，我们可以求得“面包”的条件模式基为：所以可以求得面包的频繁项集为{尿布，面包}，{尿布，牛奶，面包}。同样，我们还可以求得牛奶，尿布的频繁项集，这里就不再计算展示。总结今天我给你讲了 Apriori 算法，它是在“购物篮分析”中常用的关联规则挖掘算法，在 Apriori 算法中你最主要是需要明白支持度、置信度、提升度这几个概念，以及 Apriori 迭代计算频繁项集的工作流程。Apriori 算法在实际工作中需要对数据集扫描多次，会消耗大量的计算时间，所以在 2000 年 FP-Growth 算法被提出来，它只需要扫描两次数据集即可以完成关联规则的挖掘。FP-Growth 算法最主要的贡献就是提出了 FP 树和项头表，通过 FP 树减少了频繁项集的存储以及计算时间。当然 Apriori 的改进算法除了 FP-Growth 算法以外，还有 CBA 算法、GSP 算法，这里就不进行介绍。你能发现一种新理论的提出，往往是先从最原始的概念出发，提出一种新的方法。原始概念最接近人们模拟的过程，但往往会存在空间和时间复杂度过高的情况。所以后面其他人会对这个方法做改进型的创新，重点是在空间和时间复杂度上进行降维，比如采用新型的数据结构。你能看出树在存储和检索中是一个非常好用的数据结构。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"}]},{"title":"数据分析-关联规则 实战","slug":"数据分析-关联规则-实战","date":"2019-03-01T14:19:27.000Z","updated":"2020-04-05T14:30:52.019Z","comments":true,"path":"2019/03/01/数据分析-关联规则-实战/","link":"","permalink":"cpeixin.cn/2019/03/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-%E5%AE%9E%E6%88%98/","excerpt":"","text":"昨天讲解了关联规则挖掘的原理。关联规则挖掘在生活中有很多使用场景，不仅是商品的捆绑销售，甚至在挑选演员决策上，你也能通过关联规则挖掘看出来某个导演选择演员的倾向。今天我来带你用 Apriori 算法做一个项目实战。你需要掌握的是以下几点：熟悉上节课讲到的几个重要概念：支持度、置信度和提升度；熟悉与掌握 Apriori 工具包的使用；在实际问题中，灵活运用。包括数据集的准备等。如何使用 AprioriApriori 虽然是十大算法之一，不过在 sklearn 工具包中并没有它，也没有 FP-Growth 算法。这里教你个方法，来选择 Python 中可以使用的工具包，你可以通过https://pypi.org/ 搜索工具包。这个网站提供的工具包都是 Python 语言的，你能找到 8 个 Python 语言的 Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即 efficient-apriori。后面我会讲到为什么推荐这个工具包。首先你需要通过 pip install efficient-apriori 安装这个工具包。然后看下如何使用它，核心的代码就是这一行：1itemsets, rules = apriori(data, min_support, min_confidence)其中 data 是我们要提供的数据集，它是一个 list 数组类型。min_support 参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。min_confidence 是最小置信度，数值也代表百分比，比如 1 代表 100%。一般来说最小支持度常见的取值有0.5，0.1, 0.05。最小置信度常见的取值有1.0, 0.9, 0.8。可以通过尝试一些取值，然后观察关联结果的方式来调整最小值尺度和最小置信度的取值。关于支持度、置信度和提升度，我们再来简单回忆下。支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的概率越大。置信度是一个条件概念，就是在 A 发生的情况下，B 发生的概率是多少。提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升了多少”。接下来我们用这个工具包，跑一下上节课中讲到的超市购物的例子。下面是客户购买的商品列表：具体实现的代码如下：123456789101112from efficient_apriori import apriori# 设置数据集data = [('牛奶','面包','尿布'), ('可乐','面包', '尿布', '啤酒'), ('牛奶','尿布', '啤酒', '鸡蛋'), ('面包', '牛奶', '尿布', '啤酒'), ('面包', '牛奶', '尿布', '可乐')]# 挖掘频繁项集和频繁规则itemsets, rules = apriori(data, min_support=0.5, min_confidence=1)print(itemsets)print(rules)结果：1234&#123;1: &#123;('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 4&#125;, 2: &#123;('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3&#125;, 3: &#123;('尿布', '牛奶', '面包'): 3&#125;&#125;[&#123;啤酒&#125; -&gt; &#123;尿布&#125;, &#123;牛奶&#125; -&gt; &#123;尿布&#125;, &#123;面包&#125; -&gt; &#123;尿布&#125;, &#123;牛奶, 面包&#125; -&gt; &#123;尿布&#125;]你能从代码中看出来，data 是个 List 数组类型，其中每个值都可以是一个集合。实际上你也可以把 data 数组中的每个值设置为 List 数组类型，比如：123456data = [['牛奶','面包','尿布'], ['可乐','面包', '尿布', '啤酒'], ['牛奶','尿布', '啤酒', '鸡蛋'], ['面包', '牛奶', '尿布', '啤酒'], ['面包', '牛奶', '尿布', '可乐']]两者的运行结果是一样的，efficient-apriori 工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。而其他的 Apriori 算法可能会因为考虑了先后顺序，出现计算频繁项集结果不对的情况。所以这里采用的是 efficient-apriori 这个工具包。**挖掘-导演是如何选择演员在实际工作中，数据集是需要自己来准备的，比如今天我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的 Python 爬虫进行数据采集。不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。先来梳理下采集的工作流程。首先我们先在https://movie.douban.com搜索框中输入导演姓名，比如“宁浩”。页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：页面默认是 15 条数据反馈，第一页会返回 16 条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用“/”分割。有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在 csv 文件中。完整的抓取代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# -*- coding: utf-8 -*-# 下载某个导演的电影数据集from efficient_apriori import apriorifrom lxml import etreeimport timefrom selenium import webdriverimport csvdriver = webdriver.Chrome()# 设置想要下载的导演 数据集director = u'宁浩'# 写CSV文件file_name = './' + director + '.csv'base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&amp;cat=1002&amp;start='out = open(file_name,'w', newline='', encoding='utf-8-sig')csv_write = csv.writer(out, dialect='excel')flags=[]# 下载指定页面的数据def download(request_url): driver.get(request_url) time.sleep(1) html = driver.find_element_by_xpath(\"//*\").get_attribute(\"outerHTML\") html = etree.HTML(html) # 设置电影名称，导演演员 的XPATH movie_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']\") name_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']\") # 获取返回的数据个数 num = len(movie_lists) if num &gt; 15: #第一页会有16条数据 # 默认第一个不是，所以需要去掉 movie_lists = movie_lists[1:] name_lists = name_lists[1:] for (movie, name_list) in zip(movie_lists, name_lists): # 会存在数据为空的情况 if name_list.text is None: continue # 显示下演员名称 print(name_list.text) names = name_list.text.split('/') # 判断导演是否为指定的director if names[0].strip() == director and movie.text not in flags: # 将第一个字段设置为电影名称 names[0] = movie.text flags.append(movie.text) csv_write.writerow(names) print('OK') # 代表这页数据下载成功 print(num) if num &gt;= 14: #有可能一页会有14个电影 # 继续下一页 return True else: # 没有下一页 return False# 开始的ID为0，每页增加15start = 0while start&lt;10000: #最多抽取1万部电影 request_url = base_url + str(start) # 下载数据，并返回是否有下一页 flag = download(request_url) if flag: start = start + 15 else: breakout.close()print('finished')爬取的代码在这里就不赘述了，其中有一点就是这里用到了selenium模拟打开窗口爬取。下面是爬取下来的数据：我们用获取到的少量宁浩数据，来做一次关联规则分析：12345678910111213141516171819# -*- coding: utf-8 -*-from efficient_apriori import aprioriimport csvdirector = u'宁浩'file_name = './'+director+'.csv'lists = csv.reader(open(file_name, 'r', encoding='utf-8-sig'))# 数据加载data = []for names in lists: name_new = [] for name in names: # 去掉演员数据中的空格 name_new.append(name.strip()) data.append(name_new[1:])# 挖掘频繁项集和关联规则itemsets, rules = apriori(data, min_support=0.5, min_confidence=1)print(itemsets)print(rules)代码中使用的 apriori 方法和开头中用 Apriori 获取购物篮规律的方法类似，比如代码中都设定了最小支持度和最小置信系数，这样我们可以找到支持度大于 50%，置信系数为 1 的频繁项集和关联规则。这是最后的运行结果：123&#123;1: &#123;('徐峥',): 5, ('黄渤',): 6&#125;, 2: &#123;('徐峥', '黄渤'): 5&#125;&#125;[&#123;徐峥&#125; -&gt; &#123;黄渤&#125;]你能看出来，宁浩导演喜欢用徐峥和黄渤，并且有徐峥的情况下，一般都会用黄渤。你也可以用上面的代码来挖掘下其他导演选择演员的规律。总结Apriori 算法的核心就是理解频繁项集和关联规则。在算法运算的过程中，还要重点掌握对支持度、置信度和提升度的理解。在工具使用上，你可以使用 efficient-apriori 这个工具包，它会把每一条数据中的项（item）放到一个集合（篮子）里来处理，不考虑项（item）之间的先后顺序。在实际运用中你还需要灵活处理，比如导演如何选择演员这个案例，虽然工具的使用会很方便，但重要的还是数据挖掘前的准备过程，也就是获取某个导演的电影数据集。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"}]},{"title":"数据分析 - EM聚类 实战","slug":"数据分析-EM聚类-实战","date":"2019-02-18T14:18:53.000Z","updated":"2020-04-05T14:30:47.934Z","comments":true,"path":"2019/02/18/数据分析-EM聚类-实战/","link":"","permalink":"cpeixin.cn/2019/02/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-EM%E8%81%9A%E7%B1%BB-%E5%AE%9E%E6%88%98/","excerpt":"","text":"今天进行 EM 的实战。上篇讲了 EM 算法的原理，EM 算法相当于一个聚类框架，里面有不同的聚类模型，比如 GMM 高斯混合模型，或者 HMM 隐马尔科夫模型。其中你需要理解的是 EM 的两个步骤，E 步和 M 步：E 步相当于通过初始化的参数来估计隐含变量，M 步是通过隐含变量来反推优化参数。最后通过 EM 步骤的迭代得到最终的模型参数。今天我们进行 EM 算法的实战，你需要思考的是：如何使用 EM 算法工具完成聚类？什么情况下使用聚类算法？我们用聚类算法的任务目标是什么？面对王者荣耀的英雄数据，EM 算法能帮助我们分析出什么？如何使用 EM 工具包在 Python 中有第三方的 EM 算法工具包。由于 EM 算法是一个聚类框架，所以你需要明确你要用的具体算法，比如是采用 GMM 高斯混合模型，还是 HMM 隐马尔科夫模型。这节课我们主要讲解 GMM 的使用，在使用前你需要引入工具包：1from sklearn.mixture import GaussianMixture我们看下如何在 sklearn 中创建 GMM 聚类。首先我们使用 gmm = GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100) 来创建 GMM 聚类，其中有几个比较主要的参数（GMM 类的构造参数比较多，我筛选了一些主要的进行讲解），我分别来讲解下：1.n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果你不指定 n_components，最终的聚类结果都会为同一个值。2.covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。协方差类型有 4 种取值：covariance_type=full，代表完全协方差，也就是元素都不为 0；covariance_type=tied，代表相同的完全协方差；covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。3.max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。创建完 GMM 聚类器之后，我们就可以传入数据让它进行迭代拟合。我们使用 fit 函数，传入样本特征矩阵，模型会自动生成聚类器，然后使用 prediction=gmm.predict(data) 来对数据进行聚类，传入你想进行聚类的数据，可以得到聚类结果 prediction。你能看出来拟合训练和预测可以传入相同的特征矩阵，这是因为聚类是无监督学习，你不需要事先指定聚类的结果，也无法基于先验的结果经验来进行学习。只要在训练过程中传入特征值矩阵，机器就会按照特征值矩阵生成聚类器，然后就可以使用这个聚类器进行聚类了。如何用 EM 算法对王者荣耀数据进行聚类了解了 GMM 聚类工具之后，我们看下如何对王者荣耀的英雄数据进行聚类。首先我们知道聚类的原理是“人以群分，物以类聚”。通过聚类算法把特征值相近的数据归为一类，不同类之间的差异较大，这样就可以对原始数据进行降维。通过分成几个组（簇），来研究每个组之间的特性。或者我们也可以把组（簇）的数量适当提升，这样就可以找到可以互相替换的英雄，比如你的对手选择了你擅长的英雄之后，你可以选择另一个英雄作为备选。我们先看下数据长什么样子：这里我们收集了 69 名英雄的 20 个特征属性，这些属性分别是最大生命、生命成长、初始生命、最大法力、法力成长、初始法力、最高物攻、物攻成长、初始物攻、最大物防、物防成长、初始物防、最大每 5 秒回血、每 5 秒回血成长、初始每 5 秒回血、最大每 5 秒回蓝、每 5 秒回蓝成长、初始每 5 秒回蓝、最大攻速和攻击范围等。现在我们需要对王者荣耀的英雄数据进行聚类，我们先设定项目的执行流程：首先我们需要加载数据源；在准备阶段，我们需要对数据进行探索，包括采用数据可视化技术，让我们对英雄属性以及这些属性之间的关系理解更加深刻，然后对数据质量进行评估，是否进行数据清洗，最后进行特征选择方便后续的聚类算法；聚类阶段：选择适合的聚类模型，这里我们采用 GMM 高斯混合模型进行聚类，并输出聚类结果，对结果进行分析。按照上面的步骤，我们来编写下代码。完整的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-import pandas as pdimport csvimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.mixture import GaussianMixturefrom sklearn.preprocessing import StandardScaler # 数据加载，避免中文乱码问题data_ori = pd.read_csv('./heros7.csv', encoding = 'gb18030')features = [u'最大生命',u'生命成长',u'初始生命',u'最大法力', u'法力成长',u'初始法力',u'最高物攻',u'物攻成长',u'初始物攻',u'最大物防',u'物防成长',u'初始物防', u'最大每5秒回血', u'每5秒回血成长', u'初始每5秒回血', u'最大每5秒回蓝', u'每5秒回蓝成长', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']data = data_ori[features] # 对英雄属性之间的关系进行可视化分析# 设置plt正确显示中文plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号# 用热力图呈现features_mean字段之间的相关性corr = data[features].corr()plt.figure(figsize=(14,14))# annot=True显示每个方格的数据sns.heatmap(corr, annot=True)plt.show() # 相关性大的属性保留一个，因此可以对属性进行降维features_remain = [u'最大生命', u'初始生命', u'最大法力', u'最高物攻', u'初始物攻', u'最大物防', u'初始物防', u'最大每5秒回血', u'最大每5秒回蓝', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']data = data_ori[features_remain]data[u'最大攻速'] = data[u'最大攻速'].apply(lambda x: float(x.strip('%'))/100)data[u'攻击范围']=data[u'攻击范围'].map(&#123;'远程':1,'近战':0&#125;)# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()data = ss.fit_transform(data)# 构造GMM聚类gmm = GaussianMixture(n_components=30, covariance_type='full')gmm.fit(data)# 训练数据prediction = gmm.predict(data)print(prediction)# 将分组结果输出到CSV文件中data_ori.insert(0, '分组', prediction)data_ori.to_csv('./hero_out.csv', index=False, sep=',')1234[28 14 8 9 5 5 15 8 3 14 18 14 9 7 16 18 13 3 5 4 19 12 4 12 12 12 4 17 24 2 7 2 2 24 2 2 24 6 20 22 22 24 24 2 2 22 14 20 14 24 26 29 27 25 25 28 11 1 23 5 11 0 10 28 21 29 29 29 17]同时你也能看到输出的聚类结果文件 hero_out.csv（它保存在你本地运行的文件夹里，程序会自动输出这个文件，你可以自己看下）。我来简单讲解下程序的几个模块。关于引用包首先我们会用 DataFrame 数据结构来保存读取的数据，最后的聚类结果会写入到 CSV 文件中，因此会用到 pandas 和 CSV 工具包。另外我们需要对数据进行可视化，采用热力图展现属性之间的相关性，这里会用到 matplotlib.pyplot 和 seaborn 工具包。在数据规范化中我们使用到了 Z-Score 规范化，用到了 StandardScaler 类，最后我们还会用到 sklearn 中的 GaussianMixture 类进行聚类。数据可视化的探索你能看到我们将 20 个英雄属性之间的关系用热力图呈现了出来，中间的数字代表两个属性之间的关系系数，最大值为 1，代表完全正相关，关系系数越大代表相关性越大。从图中你能看出来“最大生命”“生命成长”和“初始生命”这三个属性的相关性大，我们只需要保留一个属性即可。同理我们也可以对其他相关性大的属性进行筛选，保留一个。你在代码中可以看到，我用 features_remain 数组保留了特征选择的属性，这样就将原本的 20 个属性降维到了 13 个属性。关于数据规范化我们能看到“最大攻速”这个属性值是百分数，不适合做矩阵运算，因此我们需要将百分数转化为小数。我们也看到“攻击范围”这个字段的取值为远程或者近战，也不适合矩阵运算，我们将取值做个映射，用 1 代表远程，0 代表近战。然后采用 Z-Score 规范化，对特征矩阵进行规范化。在聚类阶段我们采用了 GMM 高斯混合模型，并将结果输出到 CSV 文件中。这里我将输出的结果截取了一段（设置聚类个数为 30）：第一列代表的是分组（簇），我们能看到张飞、程咬金分到了一组，牛魔、白起是一组，老夫子自己是一组，达摩、典韦是一组。聚类的特点是相同类别之间的属性值相近，不同类别的属性值差异大。因此如果你擅长用典韦这个英雄，不防试试达摩这个英雄。同样你也可以在张飞和程咬金中进行切换。这样就算你的英雄被别人选中了，你依然可以有备选的英雄可以使用。总结今天我带你一起做了 EM 聚类的实战，具体使用的是 GMM 高斯混合模型。从整个流程中可以看出，我们需要经过数据加载、数据探索、数据可视化、特征选择、GMM 聚类和结果分析等环节。聚类和分类不一样，聚类是无监督的学习方式，也就是我们没有实际的结果可以进行比对，所以聚类的结果评估不像分类准确率一样直观，那么有没有聚类结果的评估方式呢？这里我们可以采用 Calinski-Harabaz 指标，代码如下：123from sklearn.metrics import calinski_harabaz_scoreprint(calinski_harabaz_score(data, prediction))指标分数越高，代表聚类效果越好，也就是相同类中的差异性小，不同类之间的差异性大。当然具体聚类的结果含义，我们需要人工来分析，也就是当这些数据被分成不同的类别之后，具体每个类表代表的含义。另外聚类算法也可以作为其他数据挖掘算法的预处理阶段，这样我们就可以将数据进行降维了。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"}]},{"title":"数据分析 - EM聚类","slug":"数据分析-EM聚类","date":"2019-02-15T14:18:39.000Z","updated":"2020-04-05T14:30:45.641Z","comments":true,"path":"2019/02/15/数据分析-EM聚类/","link":"","permalink":"cpeixin.cn/2019/02/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-EM%E8%81%9A%E7%B1%BB/","excerpt":"","text":"今天要来学习 EM 聚类。EM 的英文是 Expectation Maximization，所以 EM 算法也叫最大期望算法。我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子 A 中，然后再把剩余的分到碟子 B 中，再来观察碟子 A 和 B 里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子 A 和 B 里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是 EM 算法的过程。EM 算法的工作原理说到 EM 算法，我们先来看一个概念“最大似然”，英文是 Maximum Likelihood，Likelihood 代表可能性，所以最大似然也就是最大可能性的意思。什么是最大似然呢？举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是最大似然的概念。最大似然估计是什么呢？它指的就是一件事情已经发生了，然后反推更有可能是什么因素造成的。还是用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性。最大似然估计是一种通过已知结果，估计参数的方法。那么 EM 算法是什么？它和最大似然估计又有什么关系呢？EM 算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。再回过来看下开头我给你举的分菜的这个例子，实际上最终我们想要的是碟子 A 和碟子 B 中菜的份量，你可以把它们理解为想要求得的模型参数。然后我们通过 EM 算法中的 E 步来进行观察，然后通过 M 步来进行调整 A 和 B 的参数，最后让碟子 A 和碟子 B 的参数不再发生变化为止。实际我们遇到的问题，比分菜复杂。我再给你举个一个投掷硬币的例子，假设我们有 A 和 B 两枚硬币，我们做了 5 组实验，每组实验投掷 10 次，然后统计出现正面的次数，实验结果如下：投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是 A 还是 B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：我们现在想要求得硬币 A 和 B 出现正面次数的概率，可以直接求得：而实际情况是我不知道每次投掷的硬币是 A 还是 B，那么如何求得硬币 A 和硬币 B 出现正面的概率呢？这里就需要采用 EM 算法的思想。1. 初始化参数。我们假设硬币 A 和 B 的正面概率（随机指定）是θA=0.5 和θB=0.9。2. 计算期望值。假设实验 1 投掷的是硬币 A，那么正面次数为 5 的概率为：所以实验 1 更有可能投掷的是硬币 A。然后我们对实验 2~5 重复上面的计算过程，可以推理出来硬币顺序应该是{A，A，B，B，A}。这个过程实际上是通过假设的参数来估计未知参数，即“每次投掷是哪枚硬币”。3. 通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。然后一直重复第二步和第三步，直到参数不再发生变化。简单总结下上面的步骤，你能看出 EM 算法中的 E 步骤就是通过旧的参数来计算隐藏变量。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。EM 聚类的工作原理上面你能看到 EM 算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是 EM 聚类的原理。相比于 K-Means 算法，EM 聚类更加灵活，比如下面这两种情况，K-Means 会得到下面的聚类结果。因为 K-Means 是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。你可以把 EM 算法理解成为是一个框架，在这个框架中可以采用不同的模型来用 EM 进行求解。常用的 EM 聚类有 GMM 高斯混合模型和 HMM 隐马尔科夫模型。GMM（高斯混合模型）聚类就是 EM 聚类的一种。比如上面这两个图，可以采用 GMM 来进行聚类。和 K-Means 一样，我们事先知道聚类的个数，但是不知道每个样本分别属于哪一类。通常，我们可以假设样本是符合高斯分布的（也就是正态分布）。每个高斯分布都属于这个模型的组成部分（component），要分成 K 类就相当于是 K 个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是 E 步骤。再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即 M 步骤。反复 EM 步骤，直到每个组成部分的高斯分布参数不变为止。这样也就相当于将样本按照 GMM 模型进行了 EM 聚类。总结EM 算法相当于一个框架，你可以采用不同的模型来进行聚类，比如 GMM（高斯混合模型），或者 HMM（隐马尔科夫模型）来进行聚类。GMM 是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而 HMM 用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM 在自然语言处理和语音识别领域中有广泛的应用。在 EM 这个框架中，E 步骤相当于是通过初始化的参数来估计隐含变量。M 步骤就是通过隐含变量反推来优化参数。最后通过 EM 步骤的迭代得到模型参数。在这个过程里用到的一些数学公式这节课不进行展开。你需要重点理解 EM 算法的原理。通过上面举的炒菜的例子，你可以知道 EM 算法是一个不断观察和调整的过程。通过求硬币正面概率的例子，你可以理解如何通过初始化参数来求隐含数据的过程，以及再通过求得的隐含数据来优化参数。通过上面 GMM 图像聚类的例子，你可以知道很多 K-Means 解决不了的问题，EM 聚类是可以解决的。在 EM 框架中，我们将潜在类别当做隐藏变量，样本看做观察值，把聚类问题转化为参数估计问题，最终把样本进行聚类。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"}]},{"title":"数据分析-K-Means_1","slug":"数据分析-K-Means-1","date":"2019-02-01T15:26:13.000Z","updated":"2020-04-04T17:38:15.018Z","comments":true,"path":"2019/02/01/数据分析-K-Means-1/","link":"","permalink":"cpeixin.cn/2019/02/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-K-Means-1/","excerpt":"","text":"数据分析 - K-Means 原理K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，你可以理解这个算法的本质是确定 K 类的中心点，当你找到了这些中心点，也就完成了聚类。那么请你和我思考以下三个问题：如何确定 K 类的中心点？如何将其他点划分到 K 类中？如何区分 K-Means 与 KNN？如果理解了上面这 3 个问题，那么对 K-Means 的原理掌握得也就差不多了。先请你和我思考一个场景，假设我有 20 支亚洲足球队，想要将它们按照成绩划分成 3 个等级，可以怎样划分？K-Means 的工作原理对亚洲足球队的水平，你可能也有自己的判断。比如一流的亚洲球队有谁？你可能会说伊朗或韩国。二流的亚洲球队呢？你可能说是中国。三流的亚洲球队呢？你可能会说越南。其实这些都是靠我们的经验来划分的，那么伊朗、中国、越南可以说是三个等级的典型代表，也就是我们每个类的中心点。所以回过头来，如何确定 K 类的中心点？一开始我们是可以随机指派的，当你确认了中心点后，就可以按照距离将其他足球队划分到不同的类别中。这也就是 K-Means 的中心思想，就是这么简单直接。你可能会问：如果一开始，选择一流球队是中国，二流球队是伊朗，三流球队是韩国，中心点选择错了怎么办？其实不用担心，K-Means 有自我纠正机制，在不断的迭代过程中，会纠正中心点。中心点在整个迭代过程中，并不是唯一的，只是你需要一个初始值，一般算法会随机设置初始的中心点。好了，那我来把 K-Means 的工作原理给你总结下：选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点；重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。如何给亚洲球队做聚类对于机器来说需要数据才能判断类中心点，所以我整理了 2015-2019 年亚洲球队的排名，如下表所示。我来说明一下数据概况。其中 2019 年国际足联的世界排名，2015 年亚洲杯排名均为实际排名。2018 年世界杯中，很多球队没有进入到决赛圈，所以只有进入到决赛圈的球队才有实际的排名。如果是亚洲区预选赛 12 强的球队，排名会设置为 40。如果没有进入亚洲区预选赛 12 强，球队排名会设置为 50。针对上面的排名，我们首先需要做的是数据规范化。你可以把这些值划分到[0,1]或者按照均值为 0，方差为 1 的正态分布进行规范化。我先把数值都规范化到[0,1]的空间中，得到了以下的数值表：如果我们随机选取中国、日本、韩国为三个类的中心点，我们就需要看下这些球队到中心点的距离。距离有多种计算的方式，有关距离的计算我在 KNN 算法中也讲到过：欧氏距离曼哈顿距离切比雪夫距离余弦距离欧氏距离是最常用的距离计算方式，这里我选择欧氏距离作为距离的标准，计算每个队伍分别到中国、日本、韩国的距离，然后根据距离远近来划分。我们看到大部分的队，会和中国队聚类到一起。这里我整理了距离的计算过程，比如中国和中国的欧氏距离为 0，中国和日本的欧式距离为 0.732003。如果按照中国、日本、韩国为 3 个分类的中心点，欧氏距离的计算结果如下表所示：然后我们再重新计算这三个类的中心点，如何计算呢？最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类，再根据球队的分类更新中心点的位置。计算过程这里不展开，最后一直迭代（重复上述的计算过程：计算中心点和划分分类）到分类不再发生变化，可以得到以下的分类结果：所以我们能看出来第一梯队有日本、韩国、伊朗、沙特、澳洲；第二梯队有中国、伊拉克、阿联酋、乌兹别克斯坦；第三梯队有卡塔尔、泰国、越南、阿曼、巴林、朝鲜、印尼、叙利亚、约旦、科威特和巴勒斯坦。如何使用 sklearn 中的 K-Means 算法sklearn 是 Python 的机器学习工具库，如果从功能上来划分，sklearn 可以实现分类、聚类、回归、降维、模型选择和预处理等功能。这里我们使用的是 sklearn 的聚类函数库，因此需要引用工具包，具体代码如下：1from sklearn.cluster import KMeans当然 K-Means 只是 sklearn.cluster 中的一个聚类库，实际上包括 K-Means 在内，sklearn.cluster 一共提供了 9 种聚类方法，比如 Mean-shift，DBSCAN，Spectral clustering（谱聚类）等。这些聚类方法的原理和 K-Means 不同，这里不做介绍。我们看下 K-Means 如何创建：1KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')我们能看到在 K-Means 类创建的过程中，有一些主要的参数：n_clusters: 即 K 值，一般需要多试一些 K 值来保证更好的聚类效果。你可以随机设置一些 K 值，然后选择聚类效果最好的作为最终的 K 值；max_iter： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长；n_init：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。如果 K 值比较大的时候，你可以适当增大 n_init 这个值；init： 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式。自己设置中心点一般是对于个性化的数据进行设置，很少采用。random 的方式则是完全随机的方式，一般推荐采用优化过的 k-means++ 方式；algorithm：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的”auto”。简单说下这三个取值的区别，如果你选择”full”采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。在创建好 K-Means 类之后，就可以使用它的方法，最常用的是 fit 和 predict 这个两个函数。你可以单独使用 fit 函数和 predict 函数，也可以合并使用 fit_predict 函数。其中 fit(data) 可以对 data 数据进行 k-Means 聚类。 predict(data) 可以针对 data 中的每个样本，计算最近的类。现在我们要完整地跑一遍 20 支亚洲球队的聚类问题。1234567891011121314151617181920212223# coding: utf-8from sklearn.cluster import KMeansfrom sklearn import preprocessingimport pandas as pdimport numpy as np# 输入数据data = pd.read_csv('data.csv', encoding='gbk')train_x = data[[\"2019年国际排名\",\"2018世界杯\",\"2015亚洲杯\"]]df = pd.DataFrame(train_x)# kmeans = KMeans(n_clusters=3)kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')# 规范化到[0,1]空间min_max_scaler=preprocessing.MinMaxScaler()train_x=min_max_scaler.fit_transform(train_x)# kmeans算法kmeans.fit(train_x)predict_y = kmeans.predict(train_x)# 合并聚类结果，插入到原数据中result = pd.concat((data,pd.DataFrame(predict_y)),axis=1)result.rename(&#123;0:u'聚类'&#125;,axis=1,inplace=True)print(result)结果：123456789101112131415161718192021 国家 2019年国际排名 2018世界杯 2015亚洲杯 聚类0 中国 73 40 7 21 日本 60 15 5 02 韩国 61 19 2 03 伊朗 34 18 6 04 沙特 67 26 10 05 伊拉克 91 40 4 26 卡塔尔 101 40 13 17 阿联酋 81 40 6 28 乌兹别克斯坦 88 40 8 29 泰国 122 40 17 110 越南 102 50 17 111 阿曼 87 50 12 112 巴林 116 50 11 113 朝鲜 110 50 14 114 印尼 164 50 17 115 澳洲 40 30 1 016 叙利亚 76 40 17 117 约旦 118 50 9 118 科威特 160 50 15 119 巴勒斯坦 96 50 16 1总结如何确定 K 类的中心点？其中包括了初始的设置，以及中间迭代过程中中心点的计算。在初始设置中，会进行 n_init 次的选择，然后选择初始中心点效果最好的为初始值。在每次分类更新后，你都需要重新确认每一类的中心点，一般采用均值的方式进行确认。如何将其他点划分到 K 类中？这里实际上是关于距离的定义，我们知道距离有多种定义的方式，在 K-Means 和 KNN 中，我们都可以采用欧氏距离、曼哈顿距离、切比雪夫距离、余弦距离等。对于点的划分，就看它离哪个类的中心点的距离最近，就属于哪一类。如何区分 K-Means 和 KNN 这两种算法呢？刚学过 K-Means 和 KNN 算法的同学应该能知道两者的区别，但往往过了一段时间，就容易混淆。所以我们可以从三个维度来区分 K-Means 和 KNN 这两个算法：首先，这两个算法解决数据挖掘的两类问题。K-Means 是聚类算法，KNN 是分类算法。这两个算法分别是两种不同的学习方式。K-Means 是非监督学习，也就是不需要事先给出分类标签，而 KNN 是有监督学习，需要我们给出训练数据的分类标识。最后，K 值的含义不同。K-Means 中的 K 值代表 K 类。KNN 中的 K 值代表 K 个最接近的邻居。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"K-Means","slug":"K-Means","permalink":"cpeixin.cn/tags/K-Means/"}]},{"title":"数据分析-KNN_2","slug":"数据分析-KNN-2","date":"2019-01-26T16:14:49.000Z","updated":"2020-04-04T17:37:45.109Z","comments":true,"path":"2019/01/27/数据分析-KNN-2/","link":"","permalink":"cpeixin.cn/2019/01/27/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-2/","excerpt":"","text":"通过 sklearn 中自带的手写数字数据集来进行实战。如何在 sklearn 中使用 KNN在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。如果是做分类，你需要引用：1from sklearn.neighbors import KNeighborsClassifier如果是做回归，你需要引用：1from sklearn.neighbors import KNeighborsRegressor从名字上你也能看出来 Classifier 对应的是分类，Regressor 对应的是回归。一般来说如果一个算法有 Classifier 类，都能找到相应的 Regressor 类。比如在决策树分类中，你可以使用 DecisionTreeClassifier，也可以使用决策树来做回归 DecisionTreeRegressor。好了，我们看下如何在 sklearn 中创建 KNN 分类器。这里，我们使用构造函数1KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)这里有几个比较主要的参数，我分别来讲解下：1.n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。2.weights：是用来确定邻居的权重，有三种方式：weights=uniform，代表所有邻居的权重相同；weights=distance，代表权重是距离的倒数，即与距离成反比；自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。3.algorithm：用来规定计算邻居的方法，它有四种方式：algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。4.leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。如何用 KNN 对手写数字进行识别分类手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。今天我们用 sklearn 自带的手写数字数据集做 KNN 分类，你可以把这个数据集理解成一个简版的 MNIST 数据集，它只包括了 1797 幅数字图像，每幅图像大小是 8*8 像素。好了，我们先来规划下整个 KNN 分类的流程：整个训练过程基本上都会包括三个阶段：数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8*8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。好了，按照上面的步骤，我们一起来实现下这个项目。首先是加载数据和对数据的探索：1234567891011121314# 加载数据digits = load_digits()data = digits.data# 数据探索print(data.shape)# 查看第5幅图像print(digits.images[4])# 第5幅图像代表的数字含义print(digits.target[4])# 将第5幅图像显示出来plt.gray()plt.imshow(digits.images[4])plt.show()结果：12345678910(1797, 64)[[ 0. 0. 0. 1. 11. 0. 0. 0.] [ 0. 0. 0. 7. 8. 0. 0. 0.] [ 0. 0. 1. 13. 6. 2. 2. 0.] [ 0. 0. 7. 15. 0. 9. 8. 0.] [ 0. 5. 16. 10. 0. 16. 6. 0.] [ 0. 4. 15. 16. 13. 16. 1. 0.] [ 0. 0. 0. 3. 15. 10. 0. 0.] [ 0. 0. 0. 2. 16. 4. 0. 0.]]4对应的手写图像数据：我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个 88 的像素矩阵，上面这幅图像是一个“4”，从训练集的分类标注中我们也可以看到分类标注为“4”。sklearn 自带的手写数字数据集一共包括了 1797 个样本，每幅图像都是 88 像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化，代码如下：1234567# 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# 采用Z-Score规范化ss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)注：上面代码中，在train的时候用到了：train_ss_x = ss.fit_transform(train_x)实际上：fit_transform是fit和transform两个函数都执行一次。所以ss是进行了fit拟合的。只有在fit拟合之后，才能进行transform，在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。另外，如果我们没有fit，直接进行transform会报错，因为需要先fit拟合，才可以进行transform。然后我们构造一个 KNN 分类器 knn，把训练集的数据传入构造好的 knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到 KNN 分类器准确率，代码如下：123456# 创建KNN分类器knn = KNeighborsClassifier() knn.fit(train_ss_x, train_y) predict_y = knn.predict(test_ss_x) print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))运行结果：12KNN准确率: 0.9756好了，这样我们就构造好了一个 KNN 分类器。之前我们还讲过 SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：1234567891011121314151617181920# 创建SVM分类器svm = SVC()svm.fit(train_ss_x, train_y)predict_y=svm.predict(test_ss_x)print('SVM准确率: %0.4lf' % accuracy_score(test_y, predict_y))# 采用Min-Max规范化mm = preprocessing.MinMaxScaler()train_mm_x = mm.fit_transform(train_x)test_mm_x = mm.transform(test_x)# 创建Naive Bayes分类器mnb = MultinomialNB()mnb.fit(train_mm_x, train_y) predict_y = mnb.predict(test_mm_x) print(\"多项式朴素贝叶斯准确率: %.4lf\" % accuracy_score(test_y, predict_y))# 创建CART决策树分类器dtc = DecisionTreeClassifier()dtc.fit(train_mm_x, train_y) predict_y = dtc.predict(test_mm_x) print(\"CART决策树准确率: %.4lf\" % accuracy_score(test_y, predict_y))结果：1234SVM准确率: 0.9867多项式朴素贝叶斯准确率: 0.8844CART决策树准确率: 0.8556这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为 Z-Score 会将数值规范化为一个标准的正态分布，即均值为 0，方差为 1，数值会包含负数。因此我们需要采用 Min-Max 规范化，将数据规范化到[0,1]范围内。你能看出来 KNN 的准确率还是不错的，和 SVM 不相上下。完整代码：123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*-# 手写数字分类from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as plt# 加载数据digits = load_digits()data = digits.data# 数据探索print(data.shape)# 查看第一幅图像print(digits.images[0])# 第一幅图像代表的数字含义print(digits.target[0])# 将第一幅图像显示出来plt.gray()plt.imshow(digits.images[0])plt.show()# 分割数据，将25%的数据作为测试集，其余作为训练集train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# 采用Z-Score规范化ss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# 创建KNN分类器knn = KNeighborsClassifier()knn.fit(train_ss_x, train_y) predict_y = knn.predict(test_ss_x) print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))总结手写数字分类识别的实战，分别用 KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在这个过程中你应该对数据探索、数据可视化、数据规范化、模型训练和结果评估的使用过程有了一定的体会。在数据量不大的情况下，使用 sklearn 还是方便的。如果数据量很大，比如 MNIST 数据集中的 6 万个训练数据和 1 万个测试数据，那么采用深度学习 +GPU 运算的方式会更适合。因为深度学习的特点就是需要大量并行的重复计算，GPU 最擅长的就是做大量的并行计算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"}]},{"title":"数据分析-KNN_1","slug":"数据分析-KNN-1","date":"2019-01-25T16:14:49.000Z","updated":"2020-04-04T17:37:43.047Z","comments":true,"path":"2019/01/26/数据分析-KNN-1/","link":"","permalink":"cpeixin.cn/2019/01/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-1/","excerpt":"","text":"数据分析 - KNN 原理KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行分类，统计了电影中打斗次数、接吻次数，当然还有其他的指标也可以被统计到，如下表所示。我们很容易理解《战狼》《红海行动》《碟中谍 6》是动作片，《前任 3》《春娇救志明》《泰坦尼克号》是爱情片，但是有没有一种方法让机器也可以掌握这个分类的规则，当有一部新电影的时候，也可以对它的类型自动分类呢？我们可以把打斗次数看成 X 轴，接吻次数看成 Y 轴，然后在二维的坐标轴上，对这几部电影进行标记，如下图所示。对于未知的电影 A，坐标为 (x,y)，我们需要看下离电影 A 最近的都有哪些电影，这些电影中的大多数属于哪个分类，那么电影 A 就属于哪个分类。实际操作中，我们还需要确定一个 K 值，也就是我们要观察离电影 A 最近的电影有多少个。KNN 的工作原理“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。整个计算过程分为三步：计算待分类物体与其他物体之间的距离；统计距离最近的 K 个邻居；对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。K 值如何选择你能看出整个 KNN 的分类过程，K 值的选择还是很重要的。那么问题来了，K 值选择多少是适合的呢？如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。距离如何计算在 KNN算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。关于距离的计算方式有下面五种方式：欧氏距离；曼哈顿距离；闵可夫斯基距离；切比雪夫距离；余弦距离。其中前三种距离是 KNN 中最常用的距离，我给你分别讲解下。欧氏距离是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：同理，我们也可以求得两点在 n 维空间中的距离：曼哈顿距离在几何空间中用的比较多。以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：闵可夫斯基距离不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。那么切比雪夫距离怎么计算呢？二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。其公式为p为极限无穷的情况：余弦距离实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。KD 树其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。用 KNN 做回归KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。那么 KNN 如何做回归呢？对于一个新电影 X，我们要预测它的某个属性值，比如打斗次数，具体特征属性和数值如下所示。此时，我们会先计算待测点（新电影 X）到已知点的距离，选择距离最近的 K 个点。假设 K=3，此时最近的 3 个点（电影）分别是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数就是这 3 个点的该属性值的平均值，即 (100+95+105)/3=100 次。总结今天我给你讲了 KNN 的原理，以及 KNN 中的几个关键因素。比如针对 K 值的选择，我们一般采用交叉验证的方式得出。针对样本点之间的距离的定义，常用的有 5 种表达方式，你也可以自己来定义两个样本之间的距离公式。不同的定义，适用的场景不同。比如在搜索关键词推荐中，余弦距离是更为常用的。另外你也可以用 KNN 进行回归，通过 K 个邻居对新的点的属性进行值的预测。KNN 的理论简单直接，针对 KNN 中的搜索也有相应的 KD 树这个数据结构。KNN 的理论成熟，可以应用到线性和非线性的分类问题中，也可以用于回归分析。不过 KNN 需要计算测试点与样本点之间的距离，当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间。另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。当然在实际工作中，我们需要考虑到各种可能存在的情况，比如针对某类样本少的情况，可以增加该类别的权重。同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"}]},{"title":"数据分析-SVM_2","slug":"数据分析-SVM-2","date":"2019-01-25T12:27:13.000Z","updated":"2020-04-04T17:34:30.961Z","comments":true,"path":"2019/01/25/数据分析-SVM-2/","link":"","permalink":"cpeixin.cn/2019/01/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-2/","excerpt":"","text":"SVM 是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。sklearn 中使用 SVM在 Python 的 sklearn 工具包中有 SVM 算法，首先需要引用工具包：1from sklearn import svmSVM 既可以做回归，也可以做分类器。当用 SVM 做回归的时候，我们可以使用 SVR 或 LinearSVR。SVR 的英文是 Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。当做分类器的时候，我们使用的是 SVC 或者 LinearSVC。SVC 的英文是 Support Vector Classification。我简单说一下这两者之前的差别。从名字上你能看出 LinearSVC 是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到 SVM 是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。如果是针对非线性的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。创建一个 SVM 分类器我们首先使用 SVC 的构造函数：1model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)这里有三个重要的参数 kernel、C 和 gamma。kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。linear：线性核函数poly：多项式核函数rbf：高斯核函数（默认）sigmoid：sigmoid 核函数这四种函数代表不同的映射方式，你可能会问，在实际工作中，如何选择这 4 种核函数呢？我来给你解释一下：线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。了解深度学习的同学应该知道 sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。上面介绍的 4 种核函数，除了第一种线性核函数外，其余 3 种都可以处理线性不可分的数据。参数 C 代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。参数 gamma 代表核函数的系数，默认为样本特征数的倒数，即 gamma = 1 / n_features。在创建 SVM 分类器之后，就可以输入训练集对它进行训练。我们使用 model.fit(train_X,train_y)，传入训练集中的特征值矩阵 train_X 和分类标识 train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用 prediction=model.predict(test_X) 来对结果进行预测，传入测试集中的样本特征矩阵 test_X，可以得到测试集的预测分类结果 prediction。同样我们也可以创建线性 SVM 分类器，使用 model=svm.LinearSVC()。在 LinearSVC 中没有 kernel 这个参数，限制我们只能使用线性核函数。由于 LinearSVC 对线性分类做了优化，对于数据量大的线性可分问题，使用 LinearSVC 的效率要高于 SVC。如果你不知道数据集是否为线性，可以直接使用 SVC 类创建 SVM 分类器。在训练和预测中，LinearSVC 和 SVC 一样，都是使用 model.fit(train_X,train_y) 和 model.predict(test_X)。SVM 进行乳腺癌检测在了解了如何创建和使用 SVM 分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集，点击这里进行下载 https://github.com/cystanford/breast_cancer_data/blob/master/data.csv。医疗人员采集了患者乳腺肿块经过细针穿刺 (FNA) 后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：数据表一共包括了 32 个字段，代表的含义如下：上面的表格中，mean 代表平均值，se 代表标准差，worst 代表最大值（3 个最大值的平均值）。每张图像都计算了相应的特征，得出了这 30 个特征值（不包括 ID 字段和分类标识结果字段 diagnosis），实际上是 10 个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry 和 fractal_dimension_mean）的 3 个维度，平均、标准差和最大值。这些特征值都保留了 4 位数字。字段中没有缺失的值。在 569 个患者中，一共有 357 个是良性，212 个是恶性。好了，我们的目标是生成一个乳腺癌诊断的 SVM 分类器，并计算这个分类器的准确率。首先设定项目的执行流程：首先我们需要加载数据源；在准备阶段，需要对加载的数据源进行探索，查看样本特征和特征值，这个过程你也可以使用数据可视化，它可以方便我们对数据及数据之间的关系进一步加深了解。然后按照“完全合一”的准则来评估数据的质量，如果数据质量不高就需要做数据清洗。数据清洗之后，你可以做特征选择，方便后续的模型训练；在分类阶段，选择核函数进行训练，如果不知道数据是否为线性，可以考虑使用 SVC(kernel=‘rbf’) ，也就是高斯核函数的 SVM 分类器。然后对训练好的模型用测试集进行评估。按照上面的流程，我们来编写下代码，加载数据并对数据做部分的探索：123456789# 加载数据集，你需要把数据放到目录中data = pd.read_csv(\"./data.csv\")# 数据探索# 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来pd.set_option('display.max_columns', None)print(data.columns)print(data.head(5))print(data.describe())运行结果中，你能看到 32 个字段里，id 是没有实际含义的，可以去掉。diagnosis 字段的取值为 B 或者 M，我们可以用 0 和 1 来替代。另外其余的 30 个字段，其实可以分成三组字段，下划线后面的 mean、se 和 worst 代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。12345678910# 将特征字段分成3组features_mean= list(data.columns[2:12])features_se= list(data.columns[12:22])features_worst=list(data.columns[22:32])# 数据清洗# ID列没有用，删除该列data.drop(\"id\",axis=1,inplace=True)# 将B良性替换为0，M恶性替换为1data['diagnosis']=data['diagnosis'].map(&#123;'M':1,'B':0&#125;)然后我们要做特征字段的筛选，首先需要观察下 features_mean 各变量之间的关系，这里我们可以用 DataFrame 的 corr() 函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。12345678910# 将肿瘤诊断结果可视化sns.countplot(data['diagnosis'],label=\"Count\")plt.show()# 用热力图呈现features_mean字段之间的相关性corr = data[features_mean].corr()plt.figure(figsize=(14,14))# annot=True显示每个方格的数据sns.heatmap(corr, annot=True)plt.show()图表展示：热力图中对角线上的为单变量自身的相关系数是 1。颜色越浅代表相关性越大。所以你能看出来 radius_mean、perimeter_mean 和 area_mean 相关性非常大，compactness_mean、concavity_mean、concave_points_mean 这三个字段也是相关的，因此我们可以取其中的一个作为代表。那么如何进行特征选择呢？特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。我们能看到 mean、se 和 worst 这三组特征是对同一组内容的不同度量方式，我们可以保留 mean 这组特征，在特征选择中忽略掉 se 和 worst。同时我们能看到 mean 这组特征中，radius_mean、perimeter_mean、area_mean 这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean 这三个属性相关性大。我们分别从这 2 类中选择 1 个属性作为代表，比如 radius_mean 和 compactness_mean。这样我们就可以把原来的 10 个属性缩减为 6 个属性，代码如下：123# 特征选择features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']对特征进行选择之后，我们就可以准备训练集和测试集：12345678# 抽取30%的数据作为测试集，其余作为训练集train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test# 抽取特征选择的数值作为训练和测试数据train_X = train[features_remain]train_y=train['diagnosis']test_X= test[features_remain]test_y =test['diagnosis']在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：12345# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()train_X = ss.fit_transform(train_X)test_X = ss.transform(test_X)最后我们可以让 SVM 做训练和预测了：12345678# 创建SVM分类器model = svm.SVC()# 用训练集做训练model.fit(train_X,train_y)# 用测试集做预测prediction=model.predict(test_X)print('准确率: ', metrics.accuracy_score(prediction,test_y))运行结果：12准确率: 0.9181286549707602乳腺癌诊断分类的 SVM 实战，从这个过程中整个执行的流程，包括数据加载、数据探索、数据清洗、特征选择、SVM 训练和结果评估等环节。sklearn 已经为我们提供了很好的工具，对上节课中讲到的 SVM 的创建和训练都进行了封装，让我们无需关心中间的运算细节。但正因为这样，我们更需要对每个流程熟练掌握，通过实战项目训练数据化思维和对数据的敏感度。全部代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pandasimport matplotlib.pyplot as pltfrom sklearn import svm, metricsimport seaborn as snsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerdata = pandas.read_csv(\"data.csv\")pandas.set_option('display.max_columns', None)# 将特征字段分成3组features_mean= list(data.columns[2:12])features_se= list(data.columns[12:22])features_worst=list(data.columns[22:32])# 数据清洗# ID列没有用，删除该列data.drop(\"id\",axis=1,inplace=True)# 将B良性替换为0，M恶性替换为1data['diagnosis']=data['diagnosis'].map(&#123;'M':1,'B':0&#125;)# # 将肿瘤诊断结果可视化# sns.countplot(data['diagnosis'],label=\"Count\")# plt.show()# # 用热力图呈现features_mean字段之间的相关性# corr = data[features_mean].corr()# plt.figure(figsize=(14,14))# # annot=True显示每个方格的数据# sns.heatmap(corr, annot=True)# plt.show()# 特征选择features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean','symmetry_mean', 'fractal_dimension_mean']# 抽取30%的数据作为测试集，其余作为训练集train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test# 抽取特征选择的数值作为训练和测试数据train_X = train[features_remain]train_y=train['diagnosis']test_X= test[features_remain]test_y =test['diagnosis']# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()train_X = ss.fit_transform(train_X)test_X = ss.transform(test_X)# 创建SVM分类器model = svm.SVC()# 用训练集做训练model.fit(train_X,train_y)# 用测试集做预测prediction=model.predict(test_X)print('准确率: ', metrics.accuracy_score(prediction,test_y))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"}]},{"title":"数据分析-SVM_1","slug":"数据分析-SVM-1","date":"2019-01-24T14:27:13.000Z","updated":"2020-04-04T17:34:33.868Z","comments":true,"path":"2019/01/24/数据分析-SVM-1/","link":"","permalink":"cpeixin.cn/2019/01/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-1/","excerpt":"","text":"SVM 支持向量机SVM 的英文叫 Support Vector Machine，中文名为支持向量机。它是常见的一种分类方法，在机器学习中，SVM 是有监督的学习模型。什么是有监督的学习模型呢？它指的是我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。桌子上我放了红色和蓝色两种球，请你用一根棍子将这两种颜色的球分开。你可以很快想到解决方案，在红色和蓝色球之间画条直线就好了，如下图所示：这次难度升级，桌子上依然放着红色、蓝色两种球，但是它们的摆放不规律，如下图所示。如何用一根棍子把这两种颜色分开呢？你可能想了想，认为一根棍子是分不开的。除非把棍子弯曲，像下面这样：所以这里直线变成了曲线。如果在同一个平面上来看，红蓝两种颜色的球是很难分开的。那么有没有一种方式，可以让它们自然地分开呢？这里你可能会灵机一动，猛拍一下桌子，这些小球瞬间腾空而起，如下图所示。在腾起的那一刹那，出现了一个水平切面，恰好把红、蓝两种颜色的球分开。在这里，二维平面变成了三维空间。原来的曲线变成了一个平面。这个平面，我们就叫做超平面。SVM 的工作原理用SVM 计算的过程就是帮我们找到那个超平面的过程，这个超平面就是我们的 SVM 分类器。我们再过头来看最简单的练习 1，其实我们可以有多种直线的划分，比如下图所示的直线 A、直线 B 和直线 C，究竟哪种才是更好的划分呢？很明显图中的直线 B 更靠近蓝色球，但是在真实环境下，球再多一些的话，蓝色球可能就被划分到了直线 B 的右侧，被认为是红色球。同样直线 A 更靠近红色球，在真实环境下，如果红色球再多一些，也可能会被误认为是蓝色球。所以相比于直线 A 和直线 B，直线 C 的划分更优，因为它的鲁棒性更强。那怎样才能寻找到直线 C 这个更优的答案呢？这里，我们引入一个 SVM 特有的概念：分类间隔。实际上，我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：如图中的决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。点到超平面的距离公式**在上面这个例子中，如果我们把红蓝两种颜色的球放到一个三维空间里，你发现决策面就变成了一个平面。这里我们可以用线性函数来表示，如果在一维空间里就表示一个点，在二维空间里表示一条直线，在三维空间中代表一个平面，当然空间维数还可以更多，这样我们给这个线性函数起个名称叫做“超平面”。超平面的数学表达可以写成：在这个公式里，w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。在这个过程中，支持向量就是离分类超平面最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。所以说， SVM 就是求解最大分类间隔的过程，我们还需要对分类间隔的大小进行定义。首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用 di 代表点 xi 到超平面 wxi+b=0 的欧氏距离。因此我们要求 di 的最小值，用它来代表这个样本到超平面的最短距离。di 可以用公式计算得出：其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导最大间隔的优化模型我们的目标就是找出所有分类间隔中最大的那个值对应的超平面。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开）。通过凸优化问题，最后可以求出最优的 w 和 b，也就是我们想要找的最优超平面。中间求解的过程会用到拉格朗日乘子，和 KKT（Karush-Kuhn-Tucker）条件。数学公式比较多，这里不进行展开。硬间隔、软间隔和非线性 SVM假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。我们知道，实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点。所以线性可分是个理想情况。这时，我们需要使用到软间隔 SVM（近似线性可分），比如下面这种情况：另外还存在一种情况，就是非线性支持向量机。比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状。那么这种情况下，不论是多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念：核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。用 SVM 如何解决多分类问题SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。一对多法假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：（1）样本 A 作为正集，B，C，D 作为负集；（2）样本 B 作为正集，A，C，D 作为负集；（3）样本 C 作为正集，A，B，D 作为负集；（4）样本 D 作为正集，A，B，C 作为负集。这种方法，针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。2. 一对一法一对一法的初衷是想在训练的时候更加灵活。我们可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：（1）分类器 1：A、B；（2）分类器 2：A、C；（3）分类器 3：B、C。当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。总结SVM 分类器，它在文本分类尤其是针对二分类任务性能卓越。同样，针对多分类的情况，我们可以采用一对多，或者一对一的方法，多个二值分类器组合成一个多分类器。另外关于 SVM 分类器的概念，我希望你能掌握以下的三个程度：完全线性可分情况下的线性分类器，也就是线性可分的情况，是最原始的 SVM，它最核心的思想就是找到最大的分类间隔；大部分线性可分情况下的线性分类器，引入了软间隔的概念。软间隔，就是允许一定量的样本分类错误；线性不可分情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到了一个高维的空间中，从而变得线性可分。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"}]},{"title":"数据分析-朴素贝叶斯（下）","slug":"数据分析-朴素贝叶斯（下）","date":"2019-01-21T02:34:38.000Z","updated":"2020-04-04T18:08:45.066Z","comments":true,"path":"2019/01/21/数据分析-朴素贝叶斯（下）/","link":"","permalink":"cpeixin.cn/2019/01/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。今天我带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是 sklearn 这个机器学习神器。sklearn机器学习包sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。TF-IDF 值我在多项式朴素贝叶斯中提到了“词的 TF-IDF 值”，如何理解这个概念呢？TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。词频 TF 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。TF-IDF 如何计算首先我们看下词频 TF 和逆向文档概率 IDF 的公式。为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。TF-IDF=TF*IDF你可以看到，TF-IDF 值就是 TF 与 IDF 的乘积, 这样可以更准确地对文档进行分类。比如“我”这样的高频单词，虽然 TF 词频高，但是 IDF 值很低，整体的 TF-IDF 也不高。我在这里举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。针对“this”，计算 TF-IDF 值：所以 TF-IDF=0.02(-0.0414)=-8.28e-4。针对“bayes”，计算 TF-IDF 值：TF-IDF=0.0050.5229=2.61e-3。很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。如何求 TF-IDF在 sklearn 中我们直接使用 TfidfVectorizer 类，它可以帮我们计算单词 TF-IDF 向量的值。在这个类中，取 sklearn 计算的对数 log 时，底数是 e，不是 10。下面我来讲下如何创建 TfidfVectorizer 类。创建 TfidfVectorizer 的方法是：1TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)我们在创建的时候，有两个构造参数，可以自定义停用词 stop_words 和规律规则 token_pattern。需要注意的是传递的数据结构，停用词 stop_words 是一个列表 List 类型，而过滤规则 token_pattern 是正则表达式。什么是停用词？停用词就是在分类中没有用的词，这些词一般词频 TF 高，但是 IDF 很低，起不到分类的作用。为了节省空间和计算时间，我们把这些词作为停用词 stop words，告诉机器这些词不需要帮我计算。当我们创建好 TF-IDF 向量类型时，可以用 fit_transform 帮我们计算，返回给我们文本矩阵，该矩阵表示了每个单词在每个文档中的 TF-IDF 值。在我们进行 fit_transform 拟合模型后，我们可以得到更多的 TF-IDF 向量属性，比如，我们可以得到词汇的对应关系（字典类型）和向量的 IDF 值，当然也可以获取设置的停用词 stop_words。如何对文档进行分类如果我们要对文档进行分类，有两个重要的阶段：基于分词的数据准备，包括分词、单词权重计算、去掉停用词；应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。整个流程将分成下面几个模块：模块 1：对文档进行分词在准备阶段里，最重要的就是分词。那么如果给文档进行分词呢？英文文档和中文文档所使用的分词工具不同。在英文文档中，最常用的是 NTLK 包。NTLK 包中包含了英文的停用词 stop words、分词和标注方法。1234import nltkword_list = nltk.word_tokenize(text) #分词nltk.pos_tag(word_list) #标注单词的词性在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。123import jiebaword_list = jieba.cut (text) #中文分词模块 2：加载停用词表我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。1stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]模块 3：计算单词的权重这里我们用到 sklearn 里的 TfidfVectorizer 类，上面我们介绍过它使用的方法。直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。123tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)features = tf.fit_transform(train_contents)这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。一般很少设置 min_df，因为 min_df 通常都会很小。模块 4：生成朴素贝叶斯分类器我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。当 0&lt; alpha &lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。1234# 多项式贝叶斯分类器from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)模块 5：使用生成的分类器做预测首先我们需要得到测试集的特征矩阵。方法是用训练集的分词创建一个 TfidfVectorizer 类，使用同样的 stop_words 和 max_df，然后用这个 TfidfVectorizer 类对测试集的内容进行 fit_transform 拟合，得到测试集的特征矩阵 test_features。123test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)test_features=test_tf.fit_transform(test_contents)然后我们用训练好的分类器对新数据做预测。方法是使用 predict 函数，传入测试集的特征矩阵 test_features，得到分类结果 predicted_labels。predict 函数做的工作就是求解所有后验概率并找出最大的那个。12predicted_labels=clf.predict(test_features)模块 6：计算准确率计算准确率实际上是对分类模型的评估。我们可以调用 sklearn 中的 metrics 包，在 metrics 中提供了 accuracy_score 函数，方便我们对实际结果和预测的结果做对比，给出模型的准确率。使用方法如下：123from sklearn import metricsprint metrics.accuracy_score(test_labels, predicted_labels)数据挖掘神器 sklearn从数据挖掘的流程来看，一般包括了获取数据、数据清洗、模型训练、模型评估和模型部署这几个过程。sklearn 中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。在文档分类的项目中，我们针对文档的特点，给出了基于分词的准备流程。一般来说 NTLK 包适用于英文文档，而 jieba 适用于中文文档。我们可以根据文档选择不同的包，对文档提取分词。这些分词就是贝叶斯分类中最重要的特征属性。基于这些分词，我们得到分词的权重，即特征矩阵。通过特征矩阵与分类结果，我们就可以创建出朴素贝叶斯分类器，然后用分类器进行预测，最后预测结果与实际结果做对比即可以得到分类器在测试集上的准确率。实战文档共有 4 种类型：女性、体育、文学、校园根据下面给定的训练数据和测试数据进行 朴素贝叶斯文本分类实战。数据地址：https://github.com/cystanford/text_classification/tree/master/text classification12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding: utf-8# 中文文本分类import osimport jiebaimport warningsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn import metricswarnings.filterwarnings('ignore')def cut_words(file_path): \"\"\" 对文本进行切词 :param file_path: txt文本路径 :return: 用空格分词的字符串 \"\"\" text_with_spaces = '' with open(file_path, 'r', encoding='gb18030', errors='ignore') as f: text = f.read() textcut = jieba.cut(text) for word in textcut: text_with_spaces += word + ' ' return text_with_spacestext_category = ['女性', '体育', '文学', '校园']def loadfile(data_path): \"\"\" 将路径下的所有文件加载 :param file_dir: 保存txt文件目录 :param label: 文档标签 :return: 分词后的文档列表和标签 \"\"\" words_list = [] labels = [] for category in text_category: file_path = data_path + category + '/' file_list = os.listdir(file_path) for file in file_list: words_list.append(cut_words(file_path+file)) labels.append(category) return words_list, labelsdef get_stop_words(stop_words_path): stop_words = open(stop_words_path, 'r', encoding='utf-8').read() stop_words = stop_words.encode('utf-8').decode('utf-8-sig') # 列表头部\\ufeff处理 stop_words = stop_words.split('\\n') # 根据分隔符分隔 return stop_wordsif __name__ == '__main__': train_data_path = 'train' + '/' test_data_path = 'test' + '/' stop_words_path = 'stop/stopword.txt' train_words_list, train_labels = loadfile(train_data_path) test_words_list, test_labels = loadfile(test_data_path) stop_words = get_stop_words(stop_words_path) tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5) train_features = tf.fit_transform(train_words_list) # 上面fit过了，这里transform test_features = tf.transform(test_words_list) # 多项式贝叶斯分类器 from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels) predicted_labels = clf.predict(test_features) # 计算准确率 print('准确率为：', metrics.accuracy_score(test_labels, predicted_labels))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"}]},{"title":"数据分析-朴素贝叶斯（上）","slug":"数据分析-朴素贝叶斯（上）","date":"2019-01-20T15:19:16.000Z","updated":"2020-04-04T18:01:51.620Z","comments":true,"path":"2019/01/20/数据分析-朴素贝叶斯（上）/","link":"","permalink":"cpeixin.cn/2019/01/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"贝叶斯原理贝叶斯原理是怎么来的呢？贝叶斯为了解决一个叫“逆向概率”问题写了一篇文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。什么是“逆向概率”呢？所谓“逆向概率”是相对“正向概率”而言。正向概率的问题很容易理解，比如我们已经知道袋子里面有 N 个球，不是黑球就是白球，其中 M 个是黑球，那么把手伸进去摸一个球，就能知道摸出黑球的概率是多少。但这种情况往往是上帝视角，即了解了事情的全貌再做判断。在现实生活中，我们很难知道事情的全貌。贝叶斯则从实际场景出发，提了一个问题：如果我们事先不知道袋子里面黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里面黑白球的比例么？正是这样的一个问题，影响了接下来近 200 年的统计学理论。这是因为，贝叶斯原理与其他统计学推断方法截然不同，它是建立在主观判断的基础上：在我们不了解所有客观事实的情况下，同样可以先估计一个值，然后根据实际结果不断进行修正。我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那是不是实际上患“贝叶死”的概率也是 99.9% 呢？实际上不是的。你自己想想，在 10000 个人中，还存在 0.1% 的误查的情况，也就是 10 个人没有患病但是被诊断成阳性。当然 10000 个人中，也确实存在一个患有贝叶死的人，他有 99.9% 的概率被检查出来。所以你可以粗算下，患病的这个人实际上是这 11 个人里面的一员，即实际患病比例是 1/11≈9%。上面这个例子中，实际上涉及到了贝叶斯原理中的几个概念：先验概率：通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。后验概率：后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。条件概率：事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。似然函数（likelihood function）：你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。介绍完贝叶斯原理中的这几个概念，我们再来看下贝叶斯原理，实际上贝叶斯原理就是求解后验概率，我们假设：A 表示事件 “测出为阳性”, 用 B1 表示“患有贝叶死”, B2 表示“没有患贝叶死”。根据上面那道题，我们可以得到下面的信息。患有贝叶死的情况下，测出为阳性的概率为 P(A|B1)=99.9%，没有患贝叶死，但测出为阳性的概率为 P(A|B2)=0.1%。另外患有贝叶死的概率为 P(B1)=0.01%，没有患贝叶死的概率 P(B2)=99.99%。那么我们检测出来为阳性，而且是贝叶死的概率 P(B1，A）=P(B1)_P(A|B1)=0.01%_99.9%=0.00999%。这里 P(B1,A) 代表的是联合概率，同样我们可以求得 P(B2,A)=P(B2)_P(A|B2)=99.99%_0.1%=0.09999%。然后我们想求得是检查为阳性的情况下，患有贝叶死的概率，也即是 P(B1|A)。所以检查出阳性，且患有贝叶死的概率为：检查出是阳性，但没有患有贝叶死的概率为：这里我们能看出来 0.01%+0.1% 均出现在了 P(B1|A) 和 P(B2|A) 的计算中作为分母。我们把它称之为论据因子，也相当于一个权值因子。其中 P(B1）、P(B2) 就是先验概率，我们现在知道了观测值，就是被检测出来是阳性，来求患贝叶死的概率，也就是求后验概率。求后验概率就是贝叶斯原理要求的，基于刚才求得的 P(B1|A)，P(B2|A)，我们可以总结出贝叶斯公式为：由此，我们可以得出通用的贝叶斯公式：朴素贝叶斯讲完贝叶斯原理之后，我们再来看下今天重点要讲的算法，朴素贝叶斯。它是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。朴素贝叶斯模型由两种类型的概率组成：每个类别的概率P(Cj)；每个属性的条件概率P(Ai|Cj)。我来举个例子说明下什么是类别概率和条件概率。假设我有 7 个棋子，其中 3 个是白色的，4 个是黑色的。那么棋子是白色的概率就是 3/7，黑色的概率就是 4/7，这个就是类别概率。假设我把这 7 个棋子放到了两个盒子里，其中盒子 A 里面有 2 个白棋，2 个黑棋；盒子 B 里面有 1 个白棋，2 个黑棋。那么在盒子 A 中抓到白棋的概率就是 1/2，抓到黑棋的概率也是 1/2，这个就是条件概率，也就是在某个条件（比如在盒子 A 中）下的概率。在朴素贝叶斯中，我们要统计的是属性的条件概率，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。另外我想告诉你的是，贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。朴素贝叶斯分类工作原理朴素贝叶斯分类是常用的贝叶斯分类方法。我们日常生活中看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。离散数据案例我们遇到的数据可以分为两种，一种是离散数据，另一种是连续数据。那什么是离散数据呢？离散就是不连续的意思，有明确的边界，比如整数 1，2，3 就是离散数据，而 1 到 3 之间的任何数，就是连续数据，它可以取在这个区间里的任何数值。我以下面的数据为例，这些是根据你之前的经验所获得的数据。然后给你一个新的数据：身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？针对这个问题，我们先确定一共有 3 个属性，假设我们用 A 代表属性，用 A1, A2, A3 分别为身高 = 高、体重 = 中、鞋码 = 中。一共有两个类别，假设用 C 代表类别，那么 C1,C2 分别是：男、女，在未知的情况下我们用 Cj 表示。那么我们想求在 A1、A2、A3 属性下，Cj 的概率，用条件概率表示就是 P(Cj|A1A2A3)。根据上面讲的贝叶斯的公式，我们可以得出：因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。在这个公式里，因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。我们假定 Ai 之间是相互独立的，那么：P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出 P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。我分别求下这些条件下的概率：P(A1|C1)=1/2, P(A2|C1)=1/2, P(A3|C1)=1/4，P(A1|C2)=0, P(A2|C2)=1/2, P(A3|C2)=1/2，所以 P(A1A2A3|C1)=1/16, P(A1A2A3|C2)=0。因为 P(A1A2A3|C1)P(C1)&gt;P(A1A2A3|C2)P(C2)，所以应该是 C1 类别，即男性。连续数据案例我们做了一个离散的数据案例，实际生活中我们得到的是连续的数值，比如下面这组数据：那么如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？公式还是上面的公式，这里的困难在于，由于身高、体重、鞋码都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办呢？这时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。比如，男性的身高是均值 179.5、标准差为 3.697 的正态分布。所以男性的身高为 180 的概率为 0.1069。怎么计算得出的呢? 你可以使用 EXCEL 的 NORMDIST(x,mean,standard_dev,cumulative) 函数，一共有 4 个参数：x：正态分布中，需要计算的数值；Mean：正态分布的平均值；Standard_dev：正态分布的标准差；Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE 时，函数结果为累积分布；为 False 时，函数结果为概率密度。这里我们使用的是 NORMDIST(180,179.5,3.697,0)=0.1069。同理我们可以计算得出男性体重为 120 的概率为 0.000382324，男性鞋码为 41 号的概率为 0.120304111。所以我们可以计算得出：P(A1A2A3|C1)=P(A1|C1)P(A2|C1)P(A3|C1)=0.10690.0003823240.120304111=4.9169e-6同理我们也可以计算出来该人为女的可能性：P(A1A2A3|C2)=P(A1|C2)P(A2|C2)P(A3|C2)=0.000001474890.0153541440.120306074=2.7244e-9很明显这组数据分类为男的概率大于分类为女的概率。当然在 Python 中，有第三方库可以直接帮我们进行上面的操作，这个我们会在下节课中介绍。这里主要是给你讲解下具体的运算原理。朴素贝叶斯分类器工作流程朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。流程可以用下图表示：从图片你也可以看出来，朴素贝叶斯分类器需要三个流程，我来给你一一讲解下这几个流程。第一阶段：准备阶段在这个阶段我们需要确定特征属性，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。第二阶段：训练阶段这个阶段就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。输入是特征属性和训练样本，输出是分类器。第三阶段：应用阶段这个阶段是使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"}]},{"title":"数据分析-决策树实战-泰坦尼克号乘客生存预测","slug":"数据分析-决策树实战-泰坦尼克号乘客生存预测","date":"2019-01-15T14:12:49.000Z","updated":"2020-04-04T15:10:20.269Z","comments":true,"path":"2019/01/15/数据分析-决策树实战-泰坦尼克号乘客生存预测/","link":"","permalink":"cpeixin.cn/2019/01/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/","excerpt":"","text":"在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。今天我来带你用决策树进行项目的实战。决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。在了解决策树的原理后，今天我们用 sklearn 工具解决一个实际的问题：泰坦尼克号乘客的生存预测。sklearn 中的决策树模型首先，我们需要掌握 sklearn 中自带的决策树分类器 DecisionTreeClassifier，方法如下：1clf = DecisionTreeClassifier(criterion='entropy')到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 ginientropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。我们通过设置 criterion=’entropy’可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在 sklearn 中是个什么东西？123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。我整理了这些参数代表的含义：![](../images/数据分析-决策树实战-泰坦尼克号乘客生存预测/ol-1586012917234.jpg)在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。下面这个表格是 fit 方法、predict 方法和 score 方法的作用:![](../images/数据分析-决策树实战-泰坦尼克号乘客生存预测/ol-1586012917234.jpg) ### Titanic 乘客生存预测 #### 问题描述 泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据，具体数据你可以从 GitHub 上下载：[点我](https://github.com/cystanford/Titanic_Data)其中数据集格式为 csv，一共有两个文件：train.csv 是训练数据集，包含特征信息和存活与否的标签；test.csv: 测试数据集，只包含特征信息。现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。在训练集中，包括了以下字段，它们具体为：![](../images/数据分析-决策树实战-泰坦尼克号乘客生存预测/ol-1586012917234.jpg) #### 生存预测的关键流程 我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：![](../images/数据分析-决策树实战-泰坦尼克号乘客生存预测/ol-1586012917234.jpg)**准备阶段**：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；**分类阶段**：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。下面，我分别对这些模块进行介绍。**模块 1**：数据探索数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。那么如何进行数据探索呢？这里有一些函数你需要了解：使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；使用 head 查看前几行数据（默认是前 5 行）；使用 tail 查看后几行数据（默认是最后 5 行）。我们可以使用 Pandas 便捷地处理这些问题：1234567891011121314151617181920import pandas as pdtrain_data = pd.read_csv('train.csv')print(train_data.info())print(\"===========================================\")print(train_data.describe())print(\"===========================================\")print(train_data.describe(include=['O']))print(\"===========================================\")print(train_data.head())print(\"===========================================\")print(train_data.tail())运行结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KBNone=========================================== PassengerId Survived Pclass ... SibSp Parch Farecount 891.000000 891.000000 891.000000 ... 891.000000 891.000000 891.000000mean 446.000000 0.383838 2.308642 ... 0.523008 0.381594 32.204208std 257.353842 0.486592 0.836071 ... 1.102743 0.806057 49.693429min 1.000000 0.000000 1.000000 ... 0.000000 0.000000 0.00000025% 223.500000 0.000000 2.000000 ... 0.000000 0.000000 7.91040050% 446.000000 0.000000 3.000000 ... 0.000000 0.000000 14.45420075% 668.500000 1.000000 3.000000 ... 1.000000 0.000000 31.000000max 891.000000 1.000000 3.000000 ... 8.000000 6.000000 512.329200[8 rows x 7 columns]=========================================== Name Sex Ticket Cabin Embarkedcount 891 891 891 204 889unique 891 2 681 147 3top Wick, Miss. Mary Natalie male 347082 B96 B98 Sfreq 1 577 7 4 644=========================================== PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]=========================================== PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]模块 2：数据清洗通过数据探索，我们发现 Age 和 Cabin 这三个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐；具体实现的代码如下：12train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。首先观察下 Embarked 字段的取值，方法如下：首先观察下 Embarked 字段的取值，方法如下：1print(train_data['Embarked'].value_counts())结果如下：123S 644C 168Q 77我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：1train_data['Embarked'].fillna('S', inplace=True)模块 3：特征选择特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？通过数据探索我们发现，PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；Cabin 字段缺失值太多，可以放弃； Ticket 字段为船票号码，杂乱无章且无规律，可以放弃。其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。12345# 特征选择features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']train_features = train_data[features]train_labels = train_data['Survived']test_features = test_data[features]特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。那该如何操作呢?我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下：123from sklearn.feature_extraction import DictVectorizerdvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record'))你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：12print(dvec.feature_names_)['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']你可以看到原本是一列的 Embarked，变成了“Embarked=C”“Embarked=Q”“Embarked=S”三列。Sex 列变成了“Sex=female”“Sex=male”两列。这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。模块 4：决策树模型刚才我们已经讲了如何使用 sklearn 中的决策树模型。现在我们使用 ID3 算法，即在创建 DecisionTreeClassifier 时，设置 criterion=‘entropy’，然后使用 fit 进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。12345from sklearn.tree import DecisionTreeClassifier# 构造ID3决策树clf = DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels)模块 5：模型预测 &amp; 评估在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labels：123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels = clf.predict(test_features)在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的 score 函数计算下得到的结果：123# 得到决策树准确率acc_decision_tree = round(clf.score(train_features, train_labels), 6)print(u'score准确率为 %.4lf' % acc_decision_tree)运行结果：1score准确率为 0.9820你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。这是为什么呢？因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比(test.csv数据中没有Survived)。如果我们使用 score 函数对训练集的准确率进行统计，正确率会接近于 100%（如上结果为 98.2%），无法对分类器的在实际环境下做准确率的评估。那么有什么办法，来统计决策树分类器的准确率呢？这里可以使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。K 折交叉验证的原理是这样的：将数据集平均分割成 K 个等份；使用 1 份数据作为测试数据，其余作为训练数据；计算测试准确率；使用不同的测试集，重复 2、3 步骤。在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score 和 cross_val_score 两种函数的正确率的评估结果：1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证 统计决策树准确率print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))1cross_val_score准确率为 0.7835你可以看到，score 函数的准确率为 0.9820，cross_val_score 准确率为 0.7835。这里很明显，对于不知道测试集实际结果的，要使用 K 折交叉验证才能知道模型的准确率。模块 6：决策树可视化sklearn 的决策树模型对我们来说，还是比较抽象的。我们可以使用 Graphviz 可视化工具帮我们把决策树呈现出来。安装 Graphviz 库需要下面的几步：安装 graphviz 工具，这里是它的下载地址；http://www.graphviz.org/download/将 Graphviz 添加到环境变量 PATH 中；需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。这样你就可以在程序里面使用 Graphviz 对决策树模型进行呈现，最后得到一个决策树可视化的 PDF 文件，可视化结果文件 Source.gv.pdf 你可以在 GitHub 上下载：https://github.com/cystanford/Titanic_Data决策树模型使用技巧总结今天我用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。我上面讲了泰坦尼克乘客生存预测的六个关键模块，请你用 sklearn 中的决策树模型独立完成这个项目，对测试集中的乘客是否生存进行预测，并给出模型准确率评估。数据从 GitHub 上下载即可。最后给你留一个思考题吧，我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，那么这两个函数有什么区别?项目完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import pandas as pdfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.feature_extraction import DictVectorizertrain_data = pd.read_csv('train.csv')test_data = pd.read_csv('test.csv')train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\"\"\"将空值用此列最多值来补齐index 获取索引值。 [num] 下标获取返回值 ，ascending=True 降序排列\"\"\"train_data['Embarked'].fillna(train_data['Embarked'].value_counts().index[0], inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().index[0], inplace=True)features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']train_features = train_data[features]train_labels = train_data['Survived']test_features = test_data[features]dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record'))# 构造ID3决策树clf = DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels)test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测# pred_labels = clf.predict(test_features)# # 得到决策树准确率# acc_decision_tree = round(clf.score(train_features, train_labels), 6)# print(u'score准确率为 %.4lf' % acc_decision_tree)import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证 统计决策树准确率print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树项目-用户流失预警","slug":"数据分析-决策树项目-用户流失预警","date":"2019-01-14T11:19:49.000Z","updated":"2020-04-04T11:57:28.727Z","comments":true,"path":"2019/01/14/数据分析-决策树项目-用户流失预警/","link":"","permalink":"cpeixin.cn/2019/01/14/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E9%A1%B9%E7%9B%AE-%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E8%AD%A6/","excerpt":"","text":"https://www.zhihu.com/question/340500005https://zhuanlan.zhihu.com/p/26332219https://zhuanlan.zhihu.com/p/45435103音乐网站用户流失 ** https://zhuanlan.zhihu.com/p/29598241","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-CART","slug":"数据分析-决策树分类算法-CART","date":"2019-01-13T15:12:49.000Z","updated":"2020-04-04T11:21:53.573Z","comments":true,"path":"2019/01/13/数据分析-决策树分类算法-CART/","link":"","permalink":"cpeixin.cn/2019/01/13/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-CART/","excerpt":"","text":"CART算法上节课我们讲了决策树，基于信息度量的不同方式，我们可以把决策树分为 ID3 算法、C4.5 算法和 CART 算法。今天我来带你学习 CART 算法。CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。那么你首先需要了解的是，什么是分类树，什么是回归树呢？我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。不管是分类，还是回归，其本质是一样的，都是对输入做出预测，并且都是监督学习。说白了，就是根据特征，分析输入的内容，判断它的类别，或者预测其值。分类问题输出的是物体所属的类别，回归问题输出的是物体的值分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。CART 分类树的工作流程通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。在属性选择上，我们是通过统计“不纯度”来做判断的，ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。我们接下来详解了解一下基尼系数。基尼系数不好懂，你最好跟着例子一起手动计算下。假设 t 为节点，那么该节点的 GINI 系数的计算公式为：这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：集合 1：6 个都去打篮球；集合 2：3 个去打篮球，3 个不去打篮球。针对集合 1，所有人都去打篮球，所以 p(Ck|t)=1，因此 GINI(t)=1-1=0。针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5_0.5+0.5_0.5）=0.5。通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到：所以在属性 A 的划分下，节点 D 的基尼系数为：节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。如何使用 CART 算法来创建分类树通过上面的讲解你可以知道，CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。下面，我们来用 CART 分类树，给 iris 数据集构造一棵分类决策树。iris 这个数据集，我在 Python 可视化中讲到过，实际上在 sklearn 中也自带了这个数据集。基于 iris 数据集，构造 CART 分类树的代码如下：1234567891011121314151617181920212223# encoding=utf-8from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_iris# 准备数据集iris=load_iris()# 获取特征集和分类标识features = iris.datalabels = iris.target# 随机抽取33%的数据作为测试集，其余为训练集train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)# 创建CART分类树clf = DecisionTreeClassifier(criterion='gini')# 拟合构造CART分类树clf = clf.fit(train_features, train_labels)# 用CART分类树做预测test_predict = clf.predict(test_features)# 预测结果与测试集结果作比对score = accuracy_score(test_labels, test_predict)print(\"CART分类树准确率 %.4lf\" % score)CART分类树准确率 0.9600如果我们把决策树画出来，可以得到下面的图示：首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。使用 clf = DecisionTreeClassifier(criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。使用 clf.fit(train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。使用 clf.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。最后使用 accuracy_score(test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。我们能看到 sklearn 帮我们做了 CART 分类树的使用封装，使用起来还是很方便的。CART 回归树的工作流程CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。其中差值的绝对值为样本值减去样本均值的绝对值：方差为每个样本值减去样本均值的平方和除以样本个数：所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。我们可以通过一个例子来看下如何创建一棵 CART 回归树来做预测。如何使用 CART 回归树做预测这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。根据这些指标，我们使用 CART 回归树对波士顿房价进行预测，代码如下：123456789101112131415161718192021222324# encoding=utf-8from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor# 准备数据集boston=load_boston()# 探索数据print(boston.feature_names)# 获取特征集和房价features = boston.dataprices = boston.target# 随机抽取33%的数据作为测试集，其余为训练集train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)# 创建CART回归树dtr=DecisionTreeRegressor()# 拟合构造CART回归树dtr.fit(train_features, train_price)# 预测测试集中的房价predict_price = dtr.predict(test_features)# 测试集的结果评价print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price))运行结果（每次运行结果可能会有不同）：[‘CRIM’ ‘ZN’ ‘INDUS’ ‘CHAS’ ‘NOX’ ‘RM’ ‘AGE’ ‘DIS’ ‘RAD’ ‘TAX’ ‘PTRATIO’ ‘B’ ‘LSTAT’]回归树二乘偏差均值: 23.80784431137724回归树绝对值偏差均值: 3.040119760479042如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）：我们来看下这个例子，首先加载了波士顿房价数据集，得到特征集和房价。然后通过 train_test_split 帮助我们把数据集抽取一部分作为测试集，其余作为训练集。使用 dtr=DecisionTreeRegressor() 初始化一棵 CART 回归树。使用 dtr.fit(train_features, train_price) 函数，将训练集的特征值和结果作为参数进行拟合，得到 CART 回归树。使用 dtr.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到预测结果 predict_price。最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。我们能看到 CART 回归树的使用和分类树类似，只是最后求得的预测值是个连续值。CART 决策树的剪枝CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。总结今天我给你讲了 CART 决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。最后我们来整理下三种决策树之间在属性选择标准上的差异：ID3 算法，基于信息增益做判断；C4.5 算法，基于信息增益率做判断；CART 算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。实际上这三个指标也是计算“不纯度”的三种计算方式。在工具使用上，我们可以使用 sklearn 中的 DecisionTreeClassifier 创建 CART 分类树，通过 DecisionTreeRegressor 创建 CART 回归树。你可以用代码自己跑一遍我在文稿中举到的例子。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-C4.5","slug":"数据分析-决策树分类算法-C4.5","date":"2019-01-12T06:12:39.000Z","updated":"2020-04-04T17:53:23.044Z","comments":true,"path":"2019/01/12/数据分析-决策树分类算法-C4.5/","link":"","permalink":"cpeixin.cn/2019/01/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-C4.5/","excerpt":"","text":"在 ID3 算法上进行改进的 C4.5 算法那么 C4.5 都在哪些方面改进了 ID3 呢？采用信息增益率因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。采用悲观剪枝ID3构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。离散化处理连续属性C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。处理缺失值针对数据集不完整的情况，C4.5 也可以进行处理。假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。温度 = 高：D1={2-,3+,4+} ；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。这里 + 号代表打篮球，- 号代表不打篮球。比如 ID=2 时，决策是不打篮球，我们可以记录为 2-。针对将属性选择为温度的信息增益为：Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208属性熵 =1.459, 信息增益率 Gain_ratio(D′, 温度)=0.208/1.459=0.1426。下图是计算过程：D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重比例为 6/7，所以 Gain(D′，温度) 所占权重比例为 6/7，所以：Gain_ratio(D, 温度)=6/7*0.1426=0.122。这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。Cart 算法在这里不做介绍，我会在下一讲给你讲解这个算法。现在我们总结下 ID3 和 C4.5 算法。首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。总结前面我们讲了两种决策树分类算法 ID3 和 C4.5，了解了它们的数学原理。你可能会问，公式这么多，在实际使用中该怎么办呢？实际上，我们可以使用一些数据挖掘工具使用它们，比如 Python 的 sklearn，或者是 Weka（一个免费的数据挖掘工作平台），它们已经集成了这两种算法。只是我们在了解了这两种算法之后，才能更加清楚这两种算法的优缺点。我们总结下，这次都讲到了哪些知识点呢？首先我们采用决策树分类，需要了解它的原理，包括它的构造原理、剪枝原理。另外在信息度量上，我们需要了解信息度量中的纯度和信息熵的概念。在决策树的构造中，一个决策树包括根节点、子节点、叶子节点。在属性选择的标准上，度量方法包括了信息增益和信息增益率。在算法上，我讲解了两种算法：ID3 和 C4.5，其中 ID3 是基础的决策树算法，C4.5 在它的基础上进行了改进，也是目前决策树中应用广泛的算法。然后在了解这些概念和原理后，强烈推荐你使用工具，具体工具的使用我会在后面进行介绍。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-ID3","slug":"数据分析-决策树分类算法-ID3","date":"2019-01-11T11:12:49.000Z","updated":"2020-04-04T11:21:41.118Z","comments":true,"path":"2019/01/11/数据分析-决策树分类算法-ID3/","link":"","permalink":"cpeixin.cn/2019/01/11/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-ID3/","excerpt":"","text":"在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，你会发现它实际上是一个树状图，这就是我们今天要讲的决策树决策树的工作原理决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去打篮球？还是不去？上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：构造和剪枝。构造什么是构造呢？构造就是生成一棵完整的决策树。简单来说，构造的过程就是选择什么属性作为节点的过程，那么在构造过程中，会存在三种节点：根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；叶节点：就是树最底部的节点，也就是决策结果。节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：选择哪个属性作为根节点；选择哪些属性作为子节点；什么时候停止并得到目标状态，即叶节点。剪枝决策树构造出来之后是不是就万事大吉了呢？也不尽然，我们可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止“过拟合”（Overfitting）现象的发生。“过拟合”这个概念你一定要理解，它指的就是模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果“太好“，反而在实际应用过程中会导致分类错误。造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。fitting:拟合，就是说这个曲线能不能很好的描述这个样本，有比较好的泛化能力过拟合（OverFititing）：太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平。欠拟合(UnderFitting)：样本不够或者算法不精确，测试样本特性没有学到，不具泛化性，拿到新样本后没有办法去准确的判断泛化能力指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。既然要对决策树进行剪枝，具体有哪些方法呢？一般来说，剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）**。预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。如何判断要不要去打篮球？我给你准备了打篮球的数据集，训练数据如下：我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：纯度和信息熵。先来说一下纯度。你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。我在这里举个例子，假设有 3 个集合：集合 1：6 次都去打篮球；集合 2：4 次去打篮球，2 次不去打篮球；集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1&gt; 集合 2&gt; 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。然后我们再来介绍信息熵（entropy）的概念，它表示了信息的不确定度**在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：大写Σ用于数学上的总和符号，比如：∑Pi，其中i=1,2,…,T，即为求P1 + P2 + … + PT的和。小写σ用于统计学上的标准差。这种写法表示的就是∑j=1+2+3+…+n。下图：其中i表示下界，n表示上界， k从i开始取数，一直取到n,全部加起来。p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。我举个简单的例子，假设有 2 个集合集合 1：5 次去打篮球，1 次不去打篮球；集合 2：3 次去打篮球，3 次不去打篮球。在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入上述信息熵公式可以计算得出：同样，集合 2 中，也是一共 6 次决策，其中类别 1 中“打篮球”的次数是 3，类别 2“不打篮球”的次数也是 3，那么信息熵为多少呢？我们可以计算得出：从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。ID3算法我们先看下 ID3 算法。ID3 算法计算的是信息增益，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。假设天气 = 晴的时候，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。那么 a 代表节点的属性，即天气 = 晴。你可以在下面的图例中直观地了解这几个概念。比如针对图上这个例子，D 作为节点的信息增益为：也就是 D 节点的信息熵 -2 个子节点的归一化信息熵。2 个子节点归一化信息熵 =3/10 的 D1 信息熵 +7/10 的 D2 信息熵。我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，于是我们可以用下面的方式来记录 D1，D2，D3：D1(天气 = 晴天)={1-,2-,6+}D2(天气 = 阴天)={3+,7-}D3(天气 = 小雨)={4+,5-}我们先分别计算三个叶子节点的信息熵：因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（父节点）中的概率是 3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是 2/7。那么作为子节点的归一化信息熵 = 3/70.918+2/71.0+2/7*1.0=0.965。因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。天气作为属性节点的信息增益为，Gain(D , 天气)=0.985-0.965=0.020。同理我们可以计算出其他属性作为根节点的信息增益，它们分别为 ：Gain(D , 温度)=0.128Gain(D , 湿度)=0.020Gain(D , 刮风)=0.020下图为计算Gain(D , 温度)过程：我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：然后我们要将上图中第一个叶节点，也就是 D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：Gain(D , 湿度)=1Gain(D , 天气)=1Gain(D , 刮风)=0.3115我们能看到湿度，或者天气为 D1 的节点都可以得到最大的信息增益，这里我们选取湿度作为节点的属性划分。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下：于是我们通过 ID3 算法得到了一棵决策树。ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3 算法倾向于选择取值比较多的属性。这样，如果我们把“编号”作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。所以 ID3 有一个缺陷就是，有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-使用sklearn优雅地进行数据挖掘","slug":"数据分析-使用sklearn优雅地进行数据挖掘","date":"2019-01-10T11:12:49.000Z","updated":"2020-04-04T17:49:11.900Z","comments":true,"path":"2019/01/10/数据分析-使用sklearn优雅地进行数据挖掘/","link":"","permalink":"cpeixin.cn/2019/01/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%BD%BF%E7%94%A8sklearn%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","excerpt":"","text":"https://www.cnblogs.com/jasonfreak/p/5448462.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"cpeixin.cn/tags/sklearn/"}]},{"title":"数据分析-特征选择","slug":"数据分析-特征选择","date":"2019-01-09T15:12:49.000Z","updated":"2020-04-04T17:40:15.397Z","comments":true,"path":"2019/01/09/数据分析-特征选择/","link":"","permalink":"cpeixin.cn/2019/01/09/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"","text":"数据分析-特征选择特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。根据特征选择的形式又可以将特征选择方法分为3种：Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。我们可以使用sklearn中的feature_selection库来进行特征选择。Filter方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：12345from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data)相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：123456789101112from sklearn.feature_selection import SelectKBestfrom minepy import MINE #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5 def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5)#选择K个最好的特征，返回特征选择后的数据SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)WrapperWrapper递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：1234567from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression#递归特征消除法，返回特征选择后的数据#参数estimator为基模型#参数n_features_to_select为选择的特征个数RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)Embedded基于惩罚项的特征选择法使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：12345from sklearn.feature_selection import SelectFromModelfrom sklearn.linear_model import LogisticRegression#带L1惩罚项的逻辑回归作为基模型的特征选择SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)实际上，L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn.linear_model import LogisticRegressionclass LR(LogisticRegression): def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1): #权值相近的阈值 self.threshold = threshold LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) #使用同样的参数创建L2逻辑回归 self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) def fit(self, X, y, sample_weight=None): #训练L1逻辑回归 super(LR, self).fit(X, y, sample_weight=sample_weight) self.coef_old_ = self.coef_.copy() #训练L2逻辑回归 self.l2.fit(X, y, sample_weight=sample_weight) cntOfRow, cntOfCol = self.coef_.shape #权值系数矩阵的行数对应目标值的种类数目 for i in range(cntOfRow): for j in range(cntOfCol): coef = self.coef_[i][j] #L1逻辑回归的权值系数不为0 if coef != 0: idx = [j] #对应在L2逻辑回归中的权值系数 coef1 = self.l2.coef_[i][j] for k in range(cntOfCol): coef2 = self.l2.coef_[i][k] #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0 if abs(coef1-coef2) &lt; self.threshold and j != k and self.coef_[i][k] == 0: idx.append(k) #计算这一类特征的权值系数均值 mean = coef / len(idx) self.coef_[i][idx] = mean return self 使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：from sklearn.feature_selection import SelectFromModel #带L1和L2惩罚项的逻辑回归作为基模型的特征选择#参数threshold为权值系数之差的阈值SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)基于树模型的特征选择法树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：123456from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifier#GBDT作为基模型的特征选择SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)降维当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。主成分分析法（PCA）使用decomposition库的PCA类选择特征的代码如下：12345from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(iris.data)线性判别分析法（LDA）使用lda库的LDA类选择特征的代码如下：12345from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(iris.data, iris.target)总结再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-特征预处理","slug":"数据分析-特征预处理","date":"2019-01-08T12:12:49.000Z","updated":"2020-04-04T17:39:45.967Z","comments":true,"path":"2019/01/08/数据分析-特征预处理/","link":"","permalink":"cpeixin.cn/2019/01/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"","text":"特征工程华盛顿大学教授、《终极算法》（The Master Algorithm）的作者佩德罗·多明戈斯曾在 Communications of The ACM 第 55 卷第 10 期上发表了一篇名为《机器学习你不得不知的那些事》（A Few Useful Things to Know about Machine Learning）的小文，介绍了 12 条机器学习中的“金科玉律”，其中的 7/8 两条说的就是对数据的作用的认识。多明戈斯的观点是：数据量比算法更重要。即使算法本身并没有什么精巧的设计，但使用大量数据进行训练也能起到填鸭的效果，获得比用少量数据训练出来的聪明算法更好的性能。这也应了那句老话：数据决定了机器学习的上限，而算法只是尽可能逼近这个上限。但多明戈斯嘴里的数据可不是硬件采集或者软件抓取的原始数据，而是经过特征工程处理之后的精修数据，在他看来，特征工程（feature engineering）才是机器学习的关键**。通常来说，原始数据并不直接适用于学习，而是特征筛选、构造和生成的基础。一个好的预测模型与高效的特征提取和明确的特征表示息息相关，如果通过特征工程得到很多独立的且与所属类别相关的特征，那学习过程就变成小菜一碟。特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码。机器学习中的很多具体算法都可以归纳到特征工程的范畴之中，比如使用 L1 正则化项的 LASSO 回归，就是通过将某些特征的权重系数缩小到 0 来实现特征的过滤；再比如主成分分析，将具有相关性的一组特征变换为另一组线性无关的特征。这些方法本质上完成的都是特征工程的任务。本质上讲，特征工程是一个表示和展现数据的过程；实际工作中，特征工程的目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。特征工程的重要性有以下几点：特征越好，灵活性越强。好的特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易和维护。特征越好，构建的模型越简单。好的特征可以在参数不是最优的情况，依然得到很好的性能，减少调参的工作量和时间，也就可以大大降低模型复杂度。特征越好，模型的性能越出色。特征工程的目的本来就是为了提升模型的性能。但是今天，我将不会讨论这些，而是把关注点放在算法之外，看一看在特征工程之前，数据的特征需要经过哪些必要的预处理（preprocessing）。特征预处理通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：不属于同一量纲即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。信息冗余对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。定性特征不能直接使用某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。存在缺失值：缺失值需要补充。信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。首先介绍数据预处理中，比较简单的部分：处理缺失值数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确。缺失值产生的原因信息暂时无法获取，或者获取信息的代价太大。信息被遗漏，人为的输入遗漏或者数据采集设备的遗漏。属性不存在，在某些情况下，缺失值并不意味着数据有错误，对一些对象来说某些属性值是不存在的，如未婚者的配偶姓名、儿童的固定收入等。缺失值的影响数据挖掘建模将丢失大量的有用信息。数据挖掘模型所表现出的不确定性更加显著，模型中蕴含的规律更难把握。包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。缺失值的处理方法直接使用含有缺失值的特征：当仅有少量样本缺失该特征的时候可以尝试使用；删除含有缺失值的特征：这个方法一般适用于大多数样本都缺少该特征，且仅包含少量有效值是有效的；插值补全缺失值插值补全缺失值最常使用的还是第三种插值补全缺失值的做法，这种做法又可以有多种补全方法。均值/中位数/众数补全如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。同类均值/中位数/众数补全对样本进行分类后，根据同类其他样本该属性的均值补全缺失值，当然同第一种方法类似，如果均值不可行，可以尝试众数或者中位数等统计数据来补全。固定值补全利用固定的数值补全缺失的属性值。建模预测利用机器学习方法，将缺失属性作为预测目标进行预测，具体为将样本根据是否缺少该属性分为训练集和测试集，然后采用如回归、决策树等机器学习算法训练模型，再利用训练得到的模型预测测试集中样本的该属性的数值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。高维映射将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。多重插补多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。压缩感知和矩阵补全压缩感知通过利用信号本身所具有的稀疏性，从部分观测样本中回复原信号。压缩感知分为感知测量和重构恢复两个阶段。感知测量：此阶段对原始信号进行处理以获得稀疏样本表示。常用的手段是傅里叶变换、小波变换、字典学习、稀疏编码等重构恢复：此阶段基于稀疏性从少量观测中恢复原信号。这是压缩感知的核心矩阵补全可以查看知乎上的问题手动补全除了手动补全方法，其他插值补全方法只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。但这种方法需要对问题领域有很高的认识和理解，要求比较高，如果缺失数据较多，会比较费时费力。最近邻补全寻找与该样本最接近的样本，使用其该属性数值来补全。处理异常值异常值分析是检验数据是否有录入错误以及含有不合常理的数据。忽视异常值的存在是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响。异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也称为离群点，异常值分析也称为离群点分析。异常值检测简单统计比如利用pandas库的describe()方法观察数据的统计性描述，或者简单使用散点图也能观察到异常值的存在，如下图所示：3∂原则这个原则有个条件：数据需要服从正态分布。在 3∂ 原则下，异常值如超过 3 倍标准差，那么可以将其视为异常值。正负3∂ 的概率是 99.7%，那么距离平均值 3∂ 之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。如下图所示：箱型图这种方法是利用箱型图的四分位距（IQR）对异常值进行检测，也叫Tukey‘s test。箱型图的定义如下：四分位距(IQR)就是上四分位与下四分位的差值。而我们通过IQR的1.5倍为标准，规定：超过上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离的点为异常值。下面是Python中的代码实现，主要使用了numpy的percentile方法。1234Percentile = np.percentile(df['length'],[0,25,50,75,100])IQR = Percentile[3] - Percentile[1]UpLimit = Percentile[3]+ageIQR*1.5DownLimit = Percentile[1]-ageIQR*1.5也可以使用seaborn的可视化方法boxplot来实现：123f,ax=plt.subplots(figsize=(10,8))sns.boxplot(y='length',data=df,ax=ax)plt.show()上面三种方法是比较简单的异常值检测方法，接下来是一些较复杂的异常值检测方法，因此这里简单介绍下这些方法的基本概念。基于模型预测顾名思义，该方法会构建一个概率分布模型，并计算对象符合该模型的概率，将低概率的对象视为异常点。如果模型是簇的组合，则异常点是不在任何簇的对象；如果模型是回归，异常点是远离预测值的对象(就是第一个方法的图示例子)。优缺点：有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。基于近邻度的离群点检测一个对象的离群点得分由到它的 k-最近邻（KNN）的距离给定。这里需要注意 k 值的取值会影响离群点得分，如果 k 太小，则少量的邻近离群点可能会导致较低的离群点得分；如果 k 太大，则点数少于 k 的簇中所有的对象可能都成了离群点。为了增强鲁棒性，可以采用 k 个最近邻的平均距离。优缺点：简单;基于邻近度的方法需要 O(m2) 时间，大数据集不适用；k 值的取值导致该方法对参数的选择也是敏感的；不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。基于密度的离群点检测一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。优缺点：给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；与基于距离的方法一样，这些方法必然具有 O(m2) 的时间复杂度。对于低维数据使用特定的数据结构可以达到 O(mlogm) ；参数选择是困难的。虽然 LOF 算法通过观察不同的 k 值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。基于聚类的离群点检测一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。这也是 k-means 算法的缺点，对离群点敏感。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。优缺点：基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；簇的定义通常是离群点的补集，因此可能同时发现簇和离群点；产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。专门的离群点检测除了以上提及的方法，还有两个专门用于检测异常点的方法比较常用：One Class SVM和Isolation Forest异常值处理删除含有异常值的记录：直接将含有异常值的记录删除；视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；平均值修正：可用前后两个观测值的平均值修正该异常值；不处理：直接在具有异常值的数据集上进行数据挖掘；将含有异常值的记录直接删除的方法简单易行，但缺点也很明显，在观测值很少的情况下，这种删除会造成样本量不足，可能会改变变量的原有分布，从而造成分析结果的不准确。视为缺失值处理的好处是可以利用现有变量的信息，对异常值（缺失值）进行填补。在很多情况下，要先分析异常值出现的可能原因，在判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。处理类别不平衡问题什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。那么如何解决类别不平衡问题呢？这里介绍八大解决办法。扩充数据集首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。尝试其他评价指标一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。其他可以推荐的评价指标有以下几种混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量；精确度(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，P=TP / (TP+FP)召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，R = TP / (TP+FN)F1 得分(F1 Score)：精确度和召回率的加权平均，F1=2PR / (P+R)Kappa (Cohen kappa)ROC 曲线(ROC Curves):常被用于评价一个二值分类器的优劣，而且对于正负样本分布变化的时候，ROC 曲线可以保持不变，即不受类别不平衡的影响。其中 TP、FP、TN、FN 分别表示正确预测的正类、错误预测的正类、预测正确的负类以及错误预测的负类。图例如下：对数据集进行重采样可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。对小类的数据样本进行采样来增加小类的数据样本个数，即过采样（over-sampling ，采样的个数大于该类样本的个数）。对大类的数据样本进行采样来减少该类数据样本的个数，即欠采样（under-sampling，采样的次数少于该类样本的个素）。采样算法往往很容易实现，并且其运行速度快，并且效果也不错。 一些经验法则：考虑对大类下的样本（超过 1 万、十万甚至更多）进行欠采样，即删除部分样本；考虑对小类下的样本（不足 1万甚至更少）进行过采样，即添加部分样本的副本；考虑尝试随机采样与非随机采样两种采样方法；考虑对各类别尝试不同的采样比例，比一定是 1:1，有时候 1:1 反而不好，因为与现实情况相差甚远；考虑同时使用过采样与欠采样。尝试人工生成数据样本一种简单的人工样本数据产生的方法便是，对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样。你可以使用基于经验对属性值进行随机采样而构造新的人工样本，或者使用类似朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。有一个系统的构造人工数据样本的方法 SMOTE(Synthetic Minority Over-sampling Technique)。SMOTE 是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。它基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本，然后对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。python 实现的 SMOTE 算法代码地址如下，它提供了多种不同实现版本，以及多个重采样算法。https://github.com/scikit-learn-contrib/imbalanced-learn尝试不同分类算法强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。决策树往往在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。尝试对模型进行惩罚你可以使用相同的分类算法，但使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 penalized-SVM 和 penalized-LDA 算法。如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用惩罚模型来解决不平衡性问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。尝试一个新的角度理解问题从一个新的角度来理解问题，比如我们可以将小类的样本作为异常点，那么问题就变成异常点检测与变化趋势检测问题。异常点检测：即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。变化趋势检测：类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。尝试创新仔细对问题进行分析和挖掘，是否可以将问题划分为多个更小的问题，可以尝试如下方法：将你的大类压缩成小类；使用 One Class 分类器（将小类作为异常点）；使用集成方式，训练多个分类器，然后联合这些分类器进行分类；对于类别不平衡问题，还是需要具体问题具体分析，如果有先验知识可以快速挑选合适的方法来解决，否则最好就是逐一测试每一种方法，然后挑选最好的算法。最重要的还是多做项目，多积累经验，这样遇到一个新的问题，也可以快速找到合适的解决方法。以上针对异常值，缺失数据，分类不均问题做了详细的描述，那么接下来，就要转化数据，使之成为有效的特征。常用的方法是标准化，归一化，特征的离散化等。无量纲化量纲：就是单位，特征的单位不一致，特征就不能放在一起比较。无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。标准化比方说有一些数字的单位是千克，有一些数字的单位是克，这个时候需要统一单位。如果没有标准化，两个变量混在一起搞，那么肯定就会不合适。0-1标准化是对原始数据进行线性变换，将特征值映射成区间为［0，1］的标准值中：Z标准化基于特征值的均值（mean）和标准差（standard deviation）进行数据的标准化。它的计算公式为：标准化的代码如下：123456789101112131415161718import pandasdata = pandas.read_csv('路径.csv')#（一）Min-Max 标准化from sklearn.preprocessing import MinMaxScaler#初始化一个scaler对象scaler = MinMaxScaler()#调用scaler的fit_transform方法，把我们要处理的列作为参数传进去data['标准化后的A列数据'] = scaler.fit_transform(data['A列数据'])data['标准化后的B列数据'] = scaler.fit_transform(data['B列数据'])#（二）Z-Score标准化 （可在scale中直接实现）from sklearn.preprocessing import scaledata['标准化后的A列数据'] = scale(data['A列数据'])data['标准化后的B列数据'] = scale(data['B列数据'])区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data)归一化归一化是因为在特征会在不同的尺度下有不同的表现形式，归一化会使得各个特征能够同时以恰当的方式表现。比方说某个专辑的点击播放率一般不会超过0.2，但是专辑的播放次数可能会达到几千次，所以说为了能够在模型里面得到更合适结果，需要先把一些特征在尺度上进行归一化，然后进行模型训练。与标准化的区别，简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：使用preproccessing库的Normalizer类对数据进行归一化的代码如下：12345from sklearn.preprocessing import Normalizerscaler = Normalizer()#归一化可以同时处理多个列，所以[0]第一个进行赋值data['归一化后的A列数据'] = scaler.fit_transform(data['A列数据'])[0]data['归一化后的B列数据'] = scaler.fit_transform(data['B列数据'])[0]对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：使用preproccessing库的Binarizer类对数据进行二值化的代码如下：1234from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data)特征的离散化在进行建模时，变量中经常会有一些变量为离散型变量，例如性别。这些变量我们一般无法直接放到模型中去训练模型。因此在使用之前，我们往往会对此类变量进行处理。一般是对离散变量进行one-hot编码。下面具体介绍通过python对离散变量进行one-hot的方法。离散化是把连续型的数值型特征分段，每一段内的数据都可以当做成一个新的特征。具体又可分为等步长方式离散化和等频率的方式离散化，等步长的方式比较简单，等频率的方式更加精准，会跟数据分布有很大的关系。 代码层面，可以用pandas中的cut方法进行切分。总之，离散化的特征能够提高模型的运行速度以及准确率。比方说年龄特征是一个连续的特征，但是把年龄层分成5－18岁（中小学生），19－23岁（大学生），24－29岁（工作前几年），30－40岁（成家立业），40－60岁（中年人）从某些层面来说比连续的年龄数据（比如说某人年龄是20岁1月3日之类的）更容易理解不同年龄层人的特性。典型的离散化步骤：对特征做排序－&gt; 选择合适的分割点－&gt; 作出区间的分割 －&gt; 作出区间分割－&gt; 查看是否能够达到停止条件。分桶1.离散化的常用方法是分桶：2.分桶的数量和边界通常需要人工指定。一般有两种方法：根据业务领域的经验来指定。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。年收入超过 260 万元，则为分桶 4 。根据模型指定。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。3.选择分桶大小时，有一些经验指导：分桶大小必须足够小，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。分桶大小必须足够大，使每个桶内都有足够的样本。如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。每个桶内的样本尽量分布均匀。离散特征的两种数据类型离散特征的取值之间有大小的意义：例如：尺寸（L、XL、XXL）离散特征的取值之间没有大小的意义：例如：颜色（Red、Blue、Green）离散特征值有大小意义的虚拟变量处理离散特征的取值之间有大小意义的处理函数，我们只需要把大小值以字典的方式，作为第一个参数传入即可；(1) dict映射的字典pandas.Series.map(dict)离散特征值没有大小意义的虚拟变量处理离散特征的取值之间没有大小意义的处理方法，我们可以使用get_dummies方法处理，它有6个常用的参数(1) data 要处理的DataFrame(2) prefix 列名的前缀，在多个列有相同的离散项时候使用(3) prefix_sep 前缀和离散值的分隔符，默认为下划线，默认即可(4) dummy_na 是否把NA值，作为一个离散值进行处理，默认不处理(5) columns 要处理的列名，如果不指定该列，那么默认处理所有列(6) drop_first 是否从备选项中删第一个，建模的时候为避免共线性使用pandas.getdummies(data,prefix=None,prefix_sep=’‘,dummy_na=False,columns=None,drop_first=False)举例：123456import pandas#有些朋友也可能是encoding='utf8'或其他data=pandas.read_csv('file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK')print(data)其实，虚拟变量的实质就是要把离散型的数据转化为连续型的数据，因为第1列年龄已经是连续值，所以我们就不需要处理了。我们看看如何处理学历和性别？因为不同学历之间是有高低之分的，因此我们要使用Map方法来处理这种类型的离散型数据；第1步： 首先我们要处理不同学历之间的大小值我们使用drop_duplicates方法，来看看数据列都有哪些学历12#查看学历去重之后的情况data['学历'].drop_duplicates()第2步：理解数据值背后的意义，作出我们自己的解析，对每个学历进行评分#构建学历字典123educationLevelDict=&#123;'博士':4,'硕士':3,'大学':2,'大专':1&#125;#调用Map方法进行虚拟变量的转换data['Education Level Map']=data['Education Level'].map(educationLevelDict)第3步 对于性别这种没有大小比较的离散变量，我们使用get_dummies方法，来进行调用处理即可；1234567dummies=pandas.get_dummies(data,columns=['性别'],prefix=['性别'],prefix_sep='_',dummy_na=False,drop_first=False)sklearn 处理离散变量preprocessing.LabelEncoder：用于标签，将分类转换为分类数值 一般就是一维的preprocessing.OrdinalEncoder ：特征专用，要求至少为二维矩阵preprocessing.OneHotEncoder虽然上面已经将文字类型转换成了数值类型，但是考虑下舱门这特征Embarked（S,C,Q），表示成了[0,1,2]这样合适吗?这三个数字在算法看来，是连续且可以计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所以算法会把舱门，学历这样的分类特征，都误会成是体重这样的分类特征。这是说，我们把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模。类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息：1234567#独热编码 创建哑变量from sklearn.preprocessing import OneHotEncoderdata_ = data.copy()encoder=OneHotEncoder()encoder.fit(data_.iloc[:,1:-1])res=encoder.transform(data_.iloc[:,1:-1]).toarray()#这里要转换成数组，否则得到的是一个对象resscikit DictVectorizer 热编码（只处理类别型变量）123456789101112# 数据预处理df.transpose().to_dict().values()feature = df.iloc[:, :-1]feature# 热编码from sklearn.feature_extraction import DictVectorizerdvec = DictVectorizer(sparse=False)X = dvec.fit_transform(feature.transpose().to_dict().values())pd.DataFrame(X, columns=dvec.get_feature_names())哑编码我们针对类别型的特征，通常采用哑编码（One_Hot Encodin）的方式。所谓的哑编码，直观的讲就是用N个维度来对N个类别进行编码，并且对于每个类别，只有一个维度有效，记作数字1 ；其它维度均记作数字0。但有时使用哑编码的方式，可能会造成维度的灾难，所以通常我们在做哑编码之前，会先对特征进行Hash处理，把每个维度的特征编码成词向量。以上为大家介绍了几种较为常见、通用的数据预处理方式，但只是浩大特征工程中的冰山一角。往往很多特征工程的方法需要我们在项目中不断去总结积累比如：针对缺失值的处理，在不同的数据集中，用均值填充、中位数填充、前后值填充的效果是不一样的；对于类别型的变量，有时我们不需要对全部的数据都进行哑编码处理；对于时间型的变量有时我们有时会把它当作是离散值，有时会当成连续值处理等。所以很多情况下，我们要根据实际问题，进行不同的数据预处理。参考：https://mp.weixin.qq.com/s/BnTXjzHSb5-4s0O0WuZYlg","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-特征提取","slug":"数据分析-特征提取","date":"2019-01-07T14:07:12.000Z","updated":"2020-04-04T17:42:58.760Z","comments":true,"path":"2019/01/07/数据分析-特征提取/","link":"","permalink":"cpeixin.cn/2019/01/07/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","excerpt":"","text":"特征工程当数据预处理完成后，我们就要开始进行特征工程了。主要包含以下几个方面：特征提取特征创造特征选择特征提取和特征选择的区别特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。特征分类下图就是将我们所能遇到的特征数据进行一个分类：首先是基本特征，而后统计和复杂特征层层递进。其中针对图像语音等抽提特征有专用的知识方法掌握了这套特征设计的思路，在复杂数据上几乎可以设计出无穷无尽的特征。而怎么在最短的时间内，把数据中最有价值的特征提炼出来，就要考验数据挖掘工程师的功底。特征提取特征抽取或者特征提取大概可以分为；字典特征抽取，应用DiceVectorizer实现对类别特征进行数值化、离散化文本特征抽取，应用CounterVertorize/TfIdfVectorize实现对文本特征数值化图像特征抽取(深度学习)对于以上不同类别数据的特征提取，这里不一一介绍，等以后遇到了对应问题，再详细的举例用哪些相应的算法来处理。特征提取也可以说是将任意数据（文本或者图像）转化成适用于机器学习的数字特征应用 sklearn.feature_extraction可以轻松完成上述任务","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-数据转换","slug":"数据分析-数据转换","date":"2019-01-06T15:26:15.000Z","updated":"2020-04-04T17:47:02.217Z","comments":true,"path":"2019/01/06/数据分析-数据转换/","link":"","permalink":"cpeixin.cn/2019/01/06/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2/","excerpt":"","text":"数据变换。如果一个人在百分制的考试中得了 95 分，你肯定会认为他学习成绩很好，如果得了 65 分，就会觉得他成绩不好。如果得了 80 分呢？你会觉得他成绩中等，因为在班级里这属于大部分人的情况。为什么会有这样的认知呢？这是因为我们从小到大的考试成绩基本上都会满足正态分布的情况。什么是正态分布呢？正态分布也叫作常态分布，就是正常的状态下，呈现的分布情况。比如你可能会问班里的考试成绩是怎样的？这里其实指的是大部分同学的成绩如何。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。正态分布的另一个好处就是，如果你知道了自己的成绩，和整体的正态分布情况，就可以知道自己的成绩在全班中的位置。另一个典型的例子就是，美国 SAT 考试成绩也符合正态分布。而且美国本科的申请，需要中国高中生的 GPA 在 80 分以上（百分制的成绩），背后的理由也是默认考试成绩属于正态分布的情况。为了让成绩符合正态分布，出题老师是怎么做的呢？他们通常可以把考题分成三类：第一类：基础题，占总分 70%，基本上属于送分题；第二类：灵活题，基础范围内 + 一定的灵活性，占 20%；第三类：难题，涉及知识面较广的难题，占 10%；那么，你想下，如果一个出题老师没有按照上面的标准来出题，而是将第三类难题比重占到了 70%，也就是我们说的“超纲”，结果会是怎样呢？你会发现，大部分人成绩都“不及格”，最后在大家激烈的讨论声中，老师会将考试成绩做规范化处理，从而让成绩满足正态分布的情况。因为只有这样，成绩才更具有比较性。所以正态分布的成绩，不仅可以让你了解全班整体的情况，还能了解每个人的成绩在全班中的位置。数据变换在数据分析中的角色我们再来举个例子，假设 A 考了 80 分，B 也考了 80 分，但前者是百分制，后者 500 分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。你现在可以理解为什么 80% 的工作时间会花在前期的数据准备上了吧。那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。我来介绍下这些常见的变换方法：数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。在这些变换方法中，最简单易用的就是对数据进行规范化处理。下面我来给你讲下如何对数据进行规范化处理。数据规范化的几种方法Min-max 规范化Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。Z-Score 规范化假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义。那么如何用相同的标准来比较 A 与 B 的成绩呢？Z-Score 就是用来可以解决这一问题的。我们定义：新数值 =（原数值 - 均值）/ 标准差。假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为 400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值 =(80-400)/100=-3.2。那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。我们能看到 Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。小数定标规范化小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。上面这三种是数值规范化中常用的几种方式。Python的SciKit-Learn库使用SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。我现在来讲下如何使用 SciKit-Learn 进行数据规范化。Min-max 规范化我们可以让原始数据投射到指定的空间[min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。默认情况下[min,max]是[0,1]，也就是把原始数据投放到[0,1]范围内。我们来看下下面这个例子：123456789101112# coding:utf-8from sklearn import preprocessingimport numpy as np# 初始化数据，每一行表示一个样本，每一列表示一个特征x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 将数据进行[0,1]规范化min_max_scaler = preprocessing.MinMaxScaler()minmax_x = min_max_scaler.fit_transform(x)print minmax_x结果：1234[[0. 0. 0.66666667] [1. 1. 1. ] [0. 1. 0. ]]Z-Score 规范化在 SciKit-Learn 库中使用 preprocessing.scale() 函数，可以直接将给定数据进行 Z-Score 规范化。12345678910from sklearn import preprocessingimport numpy as np# 初始化数据x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 将数据进行Z-Score规范化scaled_x = preprocessing.scale(x)print scaled_x结果：1234[[-0.70710678 -1.41421356 0.26726124] [ 1.41421356 0.70710678 1.06904497] [-0.70710678 0.70710678 -1.33630621]]1这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。我们看到 Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。小数定标规范化我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。这里我们看下运行代码：123456789101112# coding:utf-8from sklearn import preprocessingimport numpy as np# 初始化数据x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 小数定标规范化j = np.ceil(np.log10(np.max(abs(x))))scaled_x = x/(10**j)print scaled_x结果：1234[[ 0. -0.3 0.1] [ 0.3 0.1 0.2] [ 0. 0.1 -0.1]]数据挖掘中数据变换比算法选择更重要在考试成绩中，我们都需要让数据满足一定的规律，达到规范性的要求，便于进行挖掘。这就是数据变换的作用。如果不进行变换的话，要不就是维数过多，增加了计算的成本，要不就是数据过于集中，很难找到数据之间的特征。在数据变换中，重点是如何将数值进行规范化，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 Z-Score 规范化可以直接将数据转化为正态分布的情况，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。在最后我给大家推荐了 Python 的 sklearn 库，它和 NumPy, Pandas 都是非常有名的 Python 库，在数据统计工作中起了很大的作用。SciKit-Learn 不仅可以用于数据变换，它还提供了分类、聚类、预测等数据挖掘算法的 API 封装。后面我会详细给你讲解这些算法，也会教你如何使用 SciKit-Learn 工具来完成数据挖掘算法的工作。Q&amp;A数据规范化、归一化、标准化是同一个概念么？数据规范化是更大的概念，它指的是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。数据归一化和数据标准化都是数据规范化的方式。不同点在于数据归一化会让数据在一个[0,1]或者[-1,1]的区间范围内。而数据标准化会让规范化的数据呈现正态分布的情况，所以你可以这么记：归一化的“一”，是让数据在[0,1]的范围内。而标准化，目标是让数据呈现标准的正态分布。什么时候会用到数据规范化（Min-max、Z-Score 和小数定标）？刚才提到了，进行数据规范化有两个作用：一是让数据之间具有可比较性，二是加快后续算法的迭代收敛速度。实际上你能看到 Min-max、Z-Score 和小数定标规范化都是一种线性映射的关系，将原来的数值投射到新的空间中。这样变换的好处就是可以看到在特定空间内的数值分布情况，比如通过 Min-max 可以看到数据在[0,1]之间的分布情况，Z-Score 可以看到数值的正态分布情况等。不论是采用哪种数据规范化方法，规范化后的数值都会在同一个数量的级别上，这样方便后续进行运算。那么回过头来看，在数据挖掘算法中，是否都需要进行数据规范化呢？一般情况下是需要的，尤其是针对距离相关的运算，比如在 K-Means、KNN 以及聚类算法中，我们需要有对距离的定义，所以在做这些算法前，需要对数据进行规范化。另外还有一些算法用到了梯度下降作为优化器，这是为了提高迭代收敛的效率，也就是提升找到目标函数最优解的效率。我们也需要进行数据规范化，比如逻辑回归、SVM 和神经网络算法。在这些算法中都有目标函数，需要对目标函数进行求解。梯度下降的目标是寻找到目标函数的最优解，而梯度的方法则指明了最优解的方向，如下图所示。当然不是所有的算法都需要进行数据规范化。在构造决策树的时候，可以不用提前做数据规范化，因为我们不需要关心特征值的大小维度，也没有使用到梯度下降来做优化，所以数据规范化对决策树的构造结果和构造效率影响不大。除此之外，还是建议你在做数据挖掘算法前进行数据规范化。如何使用 Z-Score 规范化，将分数变成正态分布？假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。这里假设 A 和 B 的考试成绩都是成正态分布，可以直接采用 Z-Score 的线性化规范化方法。在专栏的讨论区中，有个同学提出了“Z-Score”的非线性计算方式，大家可以一起了解下：先按公式计算出百分等级。百分等级（年级）=100-(100x 年级名次 -50)/ 有效参加考试人数。这里百分等级是每个学生在该批学生中的相对位置，其中百分等级是按照正态分布图的所占面积比例求得的；按照百分等级数去标准正态分布表中查询得出 Z-Score 值，这样最终得出的 Z 分便是标准的正态分布，能够将偏态转化成标准正态。因为在很多情况下，数值如果不是正态分布，而是偏态分布，直接使用 Z-Score 的线性计算方式无法将分数转化成正态分布。采用以上的方法可以解决这一个问题，大家可以了解下。这里偏态分布指的是非对称分布的偏斜状态，包括了负偏态，也就是左偏态分布，以及正偏态，也就是右偏态分布。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-数据清洗","slug":"数据分析-数据清洗","date":"2019-01-05T15:53:22.000Z","updated":"2020-04-04T17:49:08.349Z","comments":true,"path":"2019/01/05/数据分析-数据清洗/","link":"","permalink":"cpeixin.cn/2019/01/05/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/","excerpt":"","text":"数据分析80%时间都花费在了数据清洗任务上？做完数据采集就可以直接进行挖掘了吗？肯定不是的。就拿做饭打个比方吧，对于很多人来说，热油下锅、掌勺翻炒一定是做饭中最过瘾的环节，但实际上炒菜这个过程只占做饭时间的 20%，剩下 80% 的时间都是在做准备，比如买菜、择菜、洗菜等等。在数据挖掘中，数据清洗就是这样的前期准备工作。对于数据科学家来说，我们会遇到各种各样的数据，在分析前，要投入大量的时间和精力把数据“整理裁剪”成自己想要或需要的样子。为什么呢？因为我们采集到的数据往往有很多问题。我们先看一个例子，假设老板给你以下的数据，让你做数据分析，你看到这个数据后有什么感觉呢？你刚看到这些数据可能会比较懵，因为这些数据缺少标注。我们在收集整理数据的时候，一定要对数据做标注，数据表头很重要。比如这份数据表，就缺少列名的标注，这样一来我们就不知道每列数据所代表的含义，无法从业务中理解这些数值的作用，以及这些数值是否正确。我简单解释下上图这些数据代表的含义。这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。列坐标中，第 0 列代表的是序号，第 1 列代表的会员的姓名，第 2 列代表年龄，第 3 列代表体重，第 46 列代表男性会员的三围尺寸，第 79 列代表女性会员的三围尺寸。了解含义以后，我们再看下中间部分具体的数据，你可能会想，这些数据怎么这么“脏乱差”啊，有很多值是空的（NaN），还有空行的情况。是的，这还仅仅是一家商店的部分会员数据，我们一眼看过去就能发现一些问题。日常工作中的数据业务会复杂很多，通常我们要统计更多的数据维度，比如 100 个指标，数据量通常都是超过 TB、EB 级别的，所以整个数据分析的处理难度是呈指数级增加的。这个时候，仅仅通过肉眼就很难找到问题所在了。我举了这样一个简单的例子，带你理解在数据分析之前为什么要有数据清洗这个重要的准备工作。有经验的数据分析师都知道，好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。但在实际工作中，也可能像这个案例一样，数据是缺少标注的。但是大多数的情况下，数据表头都是存在的，即使不存在，我们也会首先的想倒要去处理表头信息，例如：如果数据来源是python爬虫获取的，那么在爬取之前，我们就会定义好，解析获得到的每个字段是什么意思。如果数据源是日志采集而来的，那么在我们接收到前端传输过来数据流，一般都是带有字段信息的，例如，前端埋点数据打到 kafka消息队 列中，是json格式，那么字段信息则为key值，如果埋点数据并不是标准的json格式，那么前端人员也是会给一份数据字典与之对应的。数据源来自某个业务数据库，那么，数据表头信息则是数据库中数据的列名。数据质量的准则在上面这个服装店会员数据的案例中，一看到这些数据，你肯定能发现几个问题。你是不是想知道，有没有一些准则来规范这些数据的质量呢？准则肯定是有的。不过如果数据存在七八种甚至更多的问题，我们很难将这些规则都记住。有研究说一个人的短期记忆，最多可以记住 7 条内容或信息，超过 7 条就记不住了。而数据清洗要解决的问题，远不止 7 条，我们万一漏掉一项该怎么办呢？有没有一种方法，我们既可以很方便地记住，又能保证我们的数据得到很好的清洗，提升数据质量呢？在这里，我将数据清洗规则总结为以下 4 个关键点，统一起来叫“完全合一”，下面我来解释下。完整性：单条数据是否存在空值，统计的字段是否完善。全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。拿接触过的实际项目举例，检查数据质量是有多么的重要，工作初期，经常会使用hive，spark做一些数据报表，面对几百GB的数据，经常是直接忽略过数据质量审查的这一步骤，所以导致的结果就是，在程序计算过程中，会出现很对关于数值的错误异常，还有空值异常，极大异常值等，例如 前端生日栏位出现bug,用户自定义输入生日可以为‘3000-14-03’,用户性别值，前端居然给了‘未知选项’等等让人哭笑不得的数值错误，反而就是因为没有去检查数据质量，所以会导致在后面的编程过程中，要花费更多的时间去DEBUG，去写更多的容错逻辑。在很多数据挖掘的教学中，数据准则通常会列出来 7~8 项，在这里我们归类成了“完全合一” 4 项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。清洗数据一一击破了解了数据质量准则之后，我们针对上面服装店会员数据案例中的问题进行一一击破。这里你就需要 Python 的 Pandas 工具了。这个工具我们之前介绍过。它是基于 NumPy 的工具，专门为解决数据分析任务而创建。Pandas 纳入了大量库，我们可以利用这些库高效地进行数据清理工作。这里我补充说明一下，如果你对 Python 还不是很熟悉，但是很想从事数据挖掘、数据分析相关的工作，那么花一些时间和精力来学习一下 Python 是很有必要的。Python 拥有丰富的库，堪称数据挖掘利器。当然了，数据清洗的工具也还有很多，这里我们只是以 Pandas 为例，帮你应用数据清洗准则，带你更加直观地了解数据清洗到底是怎么回事儿。下面，我们就依照“完全合一”的准则，使用 Pandas 来进行清洗。完整性问题 1：缺失值在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：删除：删除数据缺失的记录；均值：使用当前列的均值；高频：使用当前列出现频率最高的数据。问题 2：空行我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。全面性问题：列数据的单位不统一观察 weight 列的数值，我们能发现 weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）合理性问题：非 ASCII 字符我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法唯一性问题 1：一列有多个参数在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname 两个字段。我们使用 Python 的 split 方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name 列删除。问题 2：重复数据我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。养成数据审核的习惯现在，你是不是能感受到数据问题不是小事，上面这个简单的例子里都有 6 处错误。所以我们常说，现实世界的数据是“脏的”，需要清洗。第三方的数据要清洗，自有产品的数据，也需要数据清洗。比如美团自身做数据挖掘的时候，也需要去除爬虫抓取，作弊数据等。可以说没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有“数据审核”的“职业病”。这就好比编辑非常在意文章中的错别字、语法一样。数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。当然了，这也是一门需要不断修炼的功夫。终有一天，你会进入这样一种境界：看一眼数据，差不多 7 秒钟的时间，就能知道这个数据是否存在问题。为了这一眼的功力，我们要做很多练习。刚开始接触数据科学工作的时候，一定会觉得数据挖掘是件很酷、很有价值的事。确实如此，不过今天我还要告诉你，再酷炫的事也离不开基础性的工作，就像我们今天讲的数据清洗工作。对于这些基础性的工作，我们需要耐下性子，一个坑一个坑地去解决。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据清洗","slug":"数据清洗","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"}]},{"title":"数据分析-如何采集数据","slug":"数据分析-如何采集数据","date":"2019-01-04T15:12:49.000Z","updated":"2020-04-04T17:51:07.262Z","comments":true,"path":"2019/01/04/数据分析-如何采集数据/","link":"","permalink":"cpeixin.cn/2019/01/04/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A6%82%E4%BD%95%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE/","excerpt":"","text":"如何对用户画像建模，而建模之前我们都要进行数据采集。数据采集是数据挖掘的基础，没有数据，也没有下一阶段的数据挖掘。很多时候，我们拥有多少数据源，多少数据量，以及数据质量如何，将决定我们挖掘产出的成果会怎样。有一句话 垃圾进，垃圾出， 指的就是你利用一批垃圾数据，那么挖掘出来的结论，也是如同垃圾一样，毫无价值。举个例子，你做量化投资，基于大数据预测未来股票的波动，根据这个预测结果进行买卖。你当前能够拿到以往股票的所有历史数据，是否可以根据这些数据做出一个预测率高的数据分析系统呢？实际上，如果你只有股票历史数据，你仍然无法理解股票为什么会产生大幅的波动。比如，当时可能是爆发了 SARS 疫情，或者某地区发生了战争等。这些重大的社会事件对股票的影响也是巨大的。因此我们需要考虑到，一个数据的走势，是由多个维度影响的。我们需要通过多源的数据采集，收集到尽可能多的数据维度，同时保证数据的质量，这样才能得到高质量的数据挖掘结果。数据源分类那么，从数据采集角度来说，都有哪些数据源呢？我将数据源分成了以下的四类这四类数据源包括了：开放数据源、爬虫抓取、传感器和日志采集。开放数据源一般是针对行业的数据库。比如美国人口调查局开放了美国的人口信息、地区分布和教育情况数据。除了政府外，企业和高校也会开放相应的大数据，这方面北美相对来说做得好一些。国内，贵州做了不少大胆尝试，搭建了云平台，逐年开放了旅游、交通、商务等领域的数据量。要知道很多研究都是基于开放数据源进行的，否则每年不会有那么多论文发表，大家需要相同的数据集才能对比出算法的好坏。爬虫抓取，一般是针对特定的网站或 App。如果我们想要抓取指定的网站数据，比如购物网站上的购物评价, 以及微博实时热点等，就需要我们做特定的爬虫抓取。第三类数据源是传感器，也就是目前比较火的IOT， 例如小米智能手环，智能体重秤，街边摄像头录下来的录像，它基本上采集的是物理信息。最后是日志采集，这个是统计用户的操作。我们可以在前端进行埋点，在后端进行脚本收集、统计，来分析网站的访问情况，以及使用瓶颈等。数据采集知道了有四类数据源，那如何采集到这些数据呢？收集开源数据我们先来看下开放数据源，教你个方法，开放数据源可以从两个维度来考虑，一个是单位的维度，比如政府、企业、高校；一个就是行业维度，比如交通、金融、能源等领域。这方面，国外的开放数据源比国内做得好一些，当然近些年国内的政府和高校做开放数据源的也越来越多。一方面服务社会，另一方面自己的影响力也会越来越大。比如，下面这张表格列举的就是单位维度的数据源。所以如果你想找某个领域的数据源，比如金融领域，你基本上可以看下政府、高校、企业是否有开放的数据源。当然你也可以直接搜索金融开放数据源。爬虫采集使用爬虫做抓取爬虫抓取应该属于最常见的需求，也是我最常用的获取数据的方式。比如你想要餐厅的评价数据。当然这里要注重版权问题，而且很多网站也是有反爬机制的。最直接的方法就是使用 Python 编写爬虫代码，当然前提是你需要会 Python 的基本语法。除此之外，PHP 也可以做爬虫，只是功能不如 Python 完善，尤其是涉及到多线程的操作。在 Python 爬虫中，基本上会经历三个过程。发请求，获取内容。使用 Requests 爬取内容。我们可以使用 Requests 库来抓取网页信息。Requests 库可以说是 Python 爬虫的利器，也就是 Python 的 HTTP 库，通过这个库爬取网页中的数据，非常方便，可以帮我们节约大量的时间。同时也可以使用目前比较成熟的爬虫框架 scrapy 来进行数据爬取。解析内容BeautifulSoup 是爬虫必学的技能。BeautifulSoup最主要的功能是从网页解析数据,其次还可以使用 XPath 解析内容。XPath 是 XML Path 的缩写，也就是 XML 路径语言。它是一种用来确定 XML 文档中某部分位置的语言，在开发中经常用来当作小型查询语言。XPath 可以通过元素和属性进行位置索引。保存数据根据你所爬取的数据量，可以选择不同的存储方式。如果你爬取的只是单次小型数据集，那么可以直接存储到txt,csv等文件中，也可以存储到，mysql数据库中，方便以后分析使用。如果你爬取的是大型数据集，可以使用 Pandas 保存数据。Pandas 是让数据分析工作变得更加简单的高级数据结构，我们可以用 Pandas 保存爬取的数据。最后通过 Pandas 再写入到 XLS 或者 MySQL 等数据库中。如果你的爬虫任务是长期执行的，那么你就要考虑好你要用什么存储工具来存储数据了 （eg: mongoDB??? elasticsearch ????）当然做 Python 爬虫还有很多利器，比如 Selenium，PhantomJS，或者用 Puppteteer 这种无头模式, 以上所说的三个工具，都是对付那些反爬虫比较复杂的数据源。日志采集为什么要做日志采集呢？日志采集最大的作用，就是通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，也可以方便技术人员基于用户实际的访问情况进行优化。日志采集也是运维人员的重要工作之一，那么日志都包括哪些呢，又该如何对日志进行采集呢？日志就是日记的意思，它记录了用户访问网站的全过程：哪些人在什么时间，通过什么渠道（比如搜索引擎、网址输入）来过，都执行了哪些操作；系统是否产生了错误；甚至包括用户的 IP、HTTP 请求的时间，用户代理等。这些日志数据可以被写在一个日志文件中，也可以分成不同的日志文件，比如访问日志、错误日志等。日志采集可以分两种形式通过 Web 服务器采集，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的 Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。自定义采集用户行为，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。埋点是什么？？埋点是日志采集的关键步骤，那什么是埋点呢？埋点就是在有需要的位置采集相应的信息，进行上报。比如某页面的访问情况，包括用户信息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。那我们要如何进行埋点呢？埋点就是在你需要统计数据的地方植入统计代码，当然植入代码可以自己写，也可以使用第三方统计工具。我之前讲到“不重复造轮子”的原则，一般来说需要自己写的代码，一般是主营核心业务，对于埋点这类监测性的工具，市场上已经比较成熟，这里推荐你使用第三方的工具，比如友盟、Google Analysis、Talkingdata 等。他们都是采用前端埋点的方式，然后在第三方工具里就可以看到用户的行为数据。但如果我们想要看到更深层的用户操作行为，就需要进行自定义埋点。总结一下，日志采集有助于我们了解用户的操作数据，适用于运维监控、安全审计、业务数据分析等场景。一般 Web 服务器会自带日志功能，也可以使用 Flume 从不同的服务器集群中采集、汇总和传输大容量的日志数据。当然我们也可以使用第三方的统计工具或自定义埋点得到自己想要的统计内容。传感器采集基本上是基于特定的设备，将设备采集的信息通过进行收集即可总结数据采集是数据分析的关键，很多时候我们会想到 Python 网络爬虫，实际上数据采集的方法、渠道很广，有些可以直接使用开放的数据源，比如想获取比特币历史的价格及交易数据，可以直接从 Kaggle 上下载，不需要自己爬取。另一方面根据我们的需求，需要采集的数据也不同，比如交通行业，数据采集会和摄像头或者测速仪有关。对于运维人员，日志采集和分析则是关键。所以我们需要针对特定的业务场景，选择适合的采集工具。今天我讲了数据采集的不同渠道以及相关的工具。给你留一个思考题，假如你想预测比特币的未来走势，都需要哪些维度的数据源呢？怎样收集到它们呢？","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据采集","slug":"数据采集","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"}]},{"title":"数据分析-用户画像项目","slug":"数据分析-用户画像项目","date":"2019-01-03T14:32:06.000Z","updated":"2020-04-04T17:38:20.096Z","comments":true,"path":"2019/01/03/数据分析-用户画像项目/","link":"","permalink":"cpeixin.cn/2019/01/03/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"用户标签： 性别，年龄，职业，星座，生活城市，家乡，用户注册渠道，邀请码消费标签： 存款，领取优惠金额，参与活动，会员星级，使用信用卡次数，储蓄卡使用次数， 数字货币使用次数，购买装备金额行为标签： 登录次数，游戏次数，在线时长，游戏时长，行为路径，用户点击热点，（搜索补充）内容标签： 论坛留言，浏览优惠活动查看，游戏聊天（搜索补充）针对单个用户的画像分析。针对某一属性群体的画像分析。平均值，最大值，最小值，众数，中位数，四分之三位数，六分之一位数 等数值在在实际中代表的意义。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"}]},{"title":"数据分析-用户画像标签","slug":"数据分析-用户画像标签","date":"2019-01-02T14:31:58.000Z","updated":"2020-04-04T17:39:39.021Z","comments":true,"path":"2019/01/02/数据分析-用户画像标签/","link":"","permalink":"cpeixin.cn/2019/01/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E6%A0%87%E7%AD%BE/","excerpt":"","text":"王兴说过，我们已经进入到互联网的下半场。在上半场，也就是早期的互联网时代，你永远不知道在对面坐的是什么样的人。那个年代大部分人还是 QQ 的早期用户。在下半场，互联网公司已经不新鲜了，大部分公司已经互联网化。他们已经在用网络进行产品宣传，使用电商销售自己的商品。这两年引领下半场发展的是那些在讲 “大数据”“赋能”的企业，他们有数据，有用户。通过大数据告诉政府该如何智慧地管理交通，做城市规划。通过消费数据分析，告诉企业该在什么时间生产什么产品，以最大化地满足用户的需求。通过生活大数据告诉我们餐饮企业，甚至房地产企业该如何选址。如果说互联网的上半场是粗狂运营，因为有流量红利不需要考虑细节。那么在下半场，精细化运营将是长久的主题。有数据，有数据分析能力才能让用户得到更好的体验。所以，用户是根本，也是数据分析的出发点。假如你进入到一家卖羊肉串的餐饮公司，老板说现在竞争越来越激烈，要想做得好就要明白顾客喜欢什么。于是上班第一天，老板问你：“你能不能分析下用户数据，给咱们公司的业务做个赋能啊？”听到这，你会怎么想？你说：“老板啊，咱们是卖羊肉串的，做数据挖掘没用啊。”估计老板听后，晚上就把你给开了。那该怎么办呢？如果你感觉一头懵，没关系，我们今天就来讲讲怎么一步步分析用户数据。用户画像的准则首先就是将自己企业的用户画像做个白描，告诉他这些用户“都是谁”“从哪来”“要去哪”。你可以这么和老板说：“老板啊，用户画像建模是个系统的工程，我们要解决三个问题。第一呢，就是用户从哪里来，这里我们需要统一标识用户 ID，方便我们对用户后续行为进行跟踪。我们要了解这些羊肉串的用户从哪里来，也就是所谓的用户来源渠道。他们是为了聚餐，还是自己吃宵夜，这些场景我们都要做统计分析。第二呢，这些用户是谁？这里指的是 用户生日，职业，性别，年龄，收入等基本信息， 我们需要对这些用户进行标签化，方便我们对用户行为进行理解。第三呢，就是用户要到哪里去？我们要将这些用户画像与我们的业务相关联，提升我们的转化率，或者降低我们的流失率。”听到这，老板给你竖起了大拇指，说：“不错，都需要什么资源，随时找我就行。”刚才说的这三个步骤，下面我一一给你做个梳理。首先，为什么要设计唯一标识？用户唯一标识是整个用户画像的核心。我们以一个 App 为例，它把“从用户开始使用 APP 到下单到售后整个所有的用户行为”进行串联，因为一个用户在这个APP中没操作一项，都可能会产生一条单独的数据，所以在后续的分析过程中，需要利用用户的唯一标识去抓数据，统计数据。这样就可以更好地去跟踪和分析一个用户的特征。设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。其次，给用户打标签。你可能会想，标签有很多，且不同的产品，标签的选择范围也不同，这么多的标签，怎样划分才能既方便记忆，又能保证用户画像的全面性呢？这里我总结了八个字，叫“用户消费行为分析”。我们可以从这 4 个维度来进行标签划分。用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。最后，当你有了用户画像，可以为企业带来什么业务价值呢？我们可以从用户生命周期的三个阶段来划分业务价值，包括：拉新、用户粘度和留存：如何进行拉新，通过更精准的营销获取客户。用户粘度：个性化推荐，搜索排序，场景运营等。留存：流失率预测，分析关键节点降低流失率。如果按照数据流处理的阶段来划分用户画像建模的过程，可以分为数据层、算法层和业务层。你会发现在不同的层，都需要打上不同的标签。数据层指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录。算法层指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识。业务层指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果。所以这个标签化的流程，就是通过数据层的“事实标签”，在算法层进行计算，打上“模型标签”的分类结果，最后指导业务层，得出“预测标签”。美团外卖的用户画像该设计刚才讲的是用户画像的三个阶段，以及每个阶段的准则。下面，我们来使用这些准则做个练习。如果你是美团外卖的数据分析师，你该如何制定用户标识 ID，制定用户画像，以及基于用户画像可以做哪些业务关联？首先，我们先回顾下美团外卖的产品背景。美团已经和大众点评进行了合并，因此在大众点评和美团外卖上都可以进行外卖下单。另外美团外卖针对的是高频 O2O 的场景，美团外卖是美团的核心产品，基本上有一半的市值都是由外卖撑起来的。基于用户画像实施的三个阶段，我们首先需要统一用户的唯一标识，那么究竟哪个字段可以作为用户标识呢？我们先看下美团和大众点评都是通过哪些方式登录的。我们看到，美团采用的是手机号、微信、微博、美团账号的登录方式。大众点评采用的是手机号、微信、QQ、微博的登录方式。这里面两个 APP 共同的登录方式都是手机号、微信和微博。那么究竟哪个可以作为用户的唯一标识呢？当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。当然，大家知道在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，尤其是多条产品线，在数据整合的过程中，会出现各种各样的数据格式问题，数据精度，数据的空值处理等。所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。然后我们思考下，有了用户，用户画像都可以统计到哪些标签。我们按照“用户消费行为分析”的准则来进行设计。用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。当你有了“用户消费行为分析”的标签之后，你就可以更好地理解业务了。比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。有了这些数据，我们就可以预测用户的行为。比如一个用户购买了“月子餐”后，更有可能购买婴儿水，同样婴儿相关的产品比如婴儿湿巾等的购买概率也会增大。具体在业务层上，我们都可以基于标签产生哪些业务价值呢？在拉新上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。在用户粘度上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。在留存上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。锻炼自己的抽象能力，将繁杂的事务简单化上面我们讲到的“用户消费行为标签”都是基于一般情况考虑的，除此之外，用户的行为也会随着营销的节奏产生异常值，比如双十一的时候，如果商家都在促销就会产生突发的大量订单。因此在做用户画像的时候，还要考虑到异常值的处理。总之，数据量是庞大的，会存在各种各样的使用情况。光是分析 EB 级别的大数据，我们就要花很长的时间。但我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。对数据的标签化能让我们快速理解一个用户，一个商品，乃至一个视频内容的特征，从而方便我们去理解和使用数据。对数据的标签化其实考验的是我们的抽象能力，在日常工作中，我们也要锻炼自己的抽象能力，它可以让我们很快地将一个繁杂的事物简单化，不仅方便理解，还有益后续的使用。我们今天讲了用户画像的流程，其中很重要的一个步骤就是给用户打标签，那么你不妨想想，如果给羊肉串连锁店进行用户画像分析，都可以从哪些角度进行标签化？最后，我们从现实生活中出发，打开你的手机，翻翻看你的微信通讯录，分析下你的朋友圈，都有哪些用户画像？如果你来给它设计标签，都有哪些种类需要统计呢。为了方便后续使用，你是如何将他们归类分组的？关于朋友圈的用户画像，我们按照 “用户消费行为习惯” 来设定标签的话，大体可以参考如下：用户标签:用户性别、用户年龄、用户所处位置、用户家乡、用户学历、用户角色、用户来源渠道消费标签：朋友圈广告点击情况、用户参与活动、使用小程序的类型行为标签：朋友圈发布频次、朋友圈可见设置、朋友圈点赞次数、朋友圈评论次数、朋友圈文字浏览次数、朋友圈权限设置内容分析：浏览文字类型、文字转发类型","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"}]},{"title":"数据分析-基本概念","slug":"数据分析-基本概念","date":"2019-01-01T14:31:58.000Z","updated":"2020-04-04T17:53:51.281Z","comments":true,"path":"2019/01/01/数据分析-基本概念/","link":"","permalink":"cpeixin.cn/2019/01/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"前言美国明尼苏达州一家 Target 百货被客户投诉，这名客户指控 Target 将婴儿产品优惠券寄给他的女儿，而他女儿还是一名高中生。但没多久这名客户就来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。Target 百货寄送婴儿产品优惠券绝非偶然之举，他们发现妇女在怀孕的情况下，购买的物品会发生变化，比如护手霜会从有香味的改成无味的，此外还会购买大量维生素等保健品。通过类似的关联分析，Target 构建了一个“怀孕预测指数”，通过这个指数预测到了顾客已经怀孕的情况，并把优惠券寄送给她。那么顾客怀孕与商品之间的关联关系是如何被发现的呢？实际上他们都是用的 Apriori 算法，该算法是由美国学者 Agrawal 在 1994 年提出的。他通过分析购物篮中的商品集合，找出商品之间的关联关系。利用这种隐性关联关系，商家就可以强化这类购买行为，从而提升销售额。这就是数据分析的力量，人们总是从数据分析中得到有价值的信息，啤酒和尿布的故事也是个经典的案例。如今在超市中，我们还能看到不少组合的套装打包在一起卖，比如宝洁的产品：飘柔洗发水 + 玉兰油沐浴露、海飞丝洗发水 + 舒肤佳沐浴露等等。商品的捆绑销售是个很有用的营销方式，背后都是数据分析在发挥作用。商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系开头中的百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。所以我们能在这个场景里看到三个重要的概念：商业智能、数据仓库和数据挖掘。商业智能的英文是 Business Intelligence，缩写是 BI。相比于数据仓库、数据挖掘，它是一个更大的概念。商业智能可以说是基于数据仓库，经过了数据挖掘后，得到了商业价值的过程。所以说数据仓库是个金矿，数据挖掘是炼金术，而商业报告则是黄金。数据仓库的英文是 Data Warehouse，缩写是 DW。它可以说是 BI 这个房子的地基，搭建好 DW 这个地基之后，才能进行分析使用，最后产生价值。数据仓库可以说是数据库的升级概念。数据仓库是企业级别的，而数据库则是业务系统和单个项目中所应用到的技术，从逻辑上理解，数据库和数据仓库没有什么区别，都是通过数据库技术来存储数据的。不过从数量上来讲，数据仓库的量更庞大，适用于数据挖掘和数据分析。数据库可以理解是一项技术。数据仓库将原有的多个数据来源中的数据进行汇总、整理而得。数据进入数据仓库前，必须消除数据中的不一致性，方便后续进行数据分析和挖掘。数据挖掘的英文是 Data Mining，缩写是 DM。在商业智能 BI 中经常会使用到数据挖掘技术。数据挖掘的核心包括分类、聚类、预测、关联分析等任务，通过这些炼金术，我们可以从数据仓库中得到宝藏，比如商业报告。很多时候，企业老板总是以结果为导向，他们认为商业报告才是他们想要的，但是这也是需要经过地基 DW、搬运工 ETL、科学家 DM 等共同的努力才得到的。元数据 VS 数据元我们前面提到了数据仓库，在数据仓库中，还有一类重要的数据是元数据，那么它和数据元有什么区别呢？元数据（MetaData）：描述其它数据的数据，也称为“中介数据”。数据元（Data Element）：就是最小数据单元。在生活中，只要有一类事物，就可以定义一套元数据。举个例子，比如一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。在图书这个元数据中，书名、作者、出版社就是数据元。你可以理解是最小的数据单元。元数据最大的好处是使信息的描述和分类实现了结构化，让机器处理起来很方便。元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息。而且整个数据仓库的运行都是基于元数据的，比如抽取调度数据、获取历史数据等。通过元数据，可以很方便地帮助我们管理数据仓库。数据挖掘的流程聊完了数据仓库，我们再来谈谈数据挖掘。数据挖掘不是凭空产生的，它与数据库技术的发展分不开。数据挖掘的一个英文解释叫 Knowledge Discovery in Database，简称 KDD，也就是数据库中的知识发现。在数据挖掘中，有几个非常重要的任务，就是分类、聚类、预测和关联分析。我来解释下这些概念。分类就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。这里需要说明下训练集和测试集的概念。一般来说数据可以划分为训练集和测试集。训练集是用来给机器做训练的，通常是人们整理好训练数据，以及这些数据对应的分类标识。通过训练，机器就产生了自我分类的模型，然后机器就可以拿着这个分类模型，对测试集中的数据进行分类预测。同样如果测试集中，人们已经给出了测试结果，我们就可以用测试结果来做验证，从而了解分类器在测试环境下的表现。聚类人以群分，物以类聚。聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。预测顾名思义，就是通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。关联分析就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。比如我们开头提到的那个案例。数据挖掘要怎么完成这些任务它需要将数据库中的数据经过一系列的加工计算，最终得出有用的信息。这个过程可以用以下步骤来描述。首先，输入我们收集到的数据，然后对数据进行预处理。预处理通常是将数据转化成我们想要的格式，然后我们再对数据进行挖掘，最后通过后处理得到我们想要的信息。那你可能想问，为什么不直接进行数据挖掘，还要进行数据预处理呢？因为在这个过程中，输入的数据通常是从不同渠道采集而来的，所以数据的格式以及质量是参差不齐的，所以我们需要对数据进行预处理。数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。数据清洗主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。数据集成是将多个数据源中的数据存放在一个统一的数据存储中。数据变换就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 01 之间。我会在后面的几节课给你讲解如何对数据进行预处理。数据后处理是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到的是 01 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"NLP word vector - word2vec","slug":"NLP-word-vector-word2vec","date":"2018-06-25T14:07:35.000Z","updated":"2020-04-04T11:05:38.947Z","comments":true,"path":"2018/06/25/NLP-word-vector-word2vec/","link":"","permalink":"cpeixin.cn/2018/06/25/NLP-word-vector-word2vec/","excerpt":"","text":"word vectorFirstly, in recent years, word vector is the basic knowledge of NLP, which is also very important.Among them, words or phrases from vocabulary are mapped to vectors of real Numbers, which is to translate human language symbols into Numbers that can be calculated by machines, which can improve the quality of machine translation.For example, word_1 is expressed as a vector [0,0,1] in articles and statements. There are many models or tools for word-to-vector transformation. Eg: one-hot, n-gram, word2vecword2vecWord2vec is a toolWord2vec contains two models skp-gram and CBOW, as well as two efficient training methods negative sampling and hiearchical softmax. Why introduce word2vec separately, because it can well express the similarity and analogy between different words顺便说说这两个语言模型。统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。","categories":[{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"cpeixin.cn/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"}]},{"title":"python - 优雅的程序设计","slug":"优雅的程序设计-python「工匠」","date":"2018-05-24T15:01:20.000Z","updated":"2020-04-04T12:00:22.701Z","comments":true,"path":"2018/05/24/优雅的程序设计-python「工匠」/","link":"","permalink":"cpeixin.cn/2018/05/24/%E4%BC%98%E9%9B%85%E7%9A%84%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-python%E3%80%8C%E5%B7%A5%E5%8C%A0%E3%80%8D/","excerpt":"","text":"今天是优秀博客的搬运工😂记录两篇看完很爽的文章，编程也是一门手艺，也可以说是一门艺术，编程的人要把自己当成工匠，对自己的作品不断的打磨，打磨成一个可以供人观赏的艺术品。Python 工匠：善用变量来改善代码质量Python 工匠：编写条件分支代码的技巧","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"Docker three elements","slug":"Docker-three-elements","date":"2018-01-02T13:01:05.000Z","updated":"2020-04-04T12:00:32.855Z","comments":true,"path":"2018/01/02/Docker-three-elements/","link":"","permalink":"cpeixin.cn/2018/01/02/Docker-three-elements/","excerpt":"","text":"whatbuild, ship and run any app anywhereDocker is designed to avoid a series of problems caused by inconsistencies between development and production environments during the process of program delivery and deployment between development engineers and operations engineers. Through docker, the development engineer packages the program code, running system, software version, description document and so on in a image file, and passes the image file to the operation and maintenance engineer for deployment. For example, if you buy a fish tank with water in it from a pet store, the fish can live in your home without any other operation.At present, there is no need to discuss the struggle between docker and virtual machine, because major Internet companies are using docker technology, and can expand capacity in a short time, we can know that in the current high concurrency business scenario, from practicality and convenience, virtual machine is unable to defeat docker.Simply, docker’s centos image is only 200MB, while the virtual machine image is about 2GB. Docker makes use of the resources of the host and only copies the Linux kernel, and docker is no need Hypervisor ,which can be said to be a stripped-down version of the virtual machinesoftware architectureFocus on understanding the three elementsHowDocker has many commands, which are not complicated. Just like Linux commands, it is recommended to find them on the official website and practice them a lotdocker imagesdocker run [options]docker build [options]and so on …Docker imagebase on UnionFSimage = fs1 + fs2 + … + fnIt looks like an image is a file system, but it’s actually an image that’s made up of layers of file systemsAn available image file is made up of multiple base image layers superimposed, all read-onlyFor example,Tomcat image = kernel image + centos image + jdk image + tomcat imageDocker containerdocker container = docker run -it imageWhen docker image is launched, a docker container will be instantiated. In principle, a layer of writable files is added to the top layer of multiple read-only docker images superimposed together, thus generating the running docker containerDocker volumes is very important and you can check the official website for detailed usageDockerfileDockerfile is similar to a key/value configuration file, which is composed of the construction word in the following figure. Starting From ‘From’, other construction words are listed according to requirements to tell the Dockerfile what to do and what image to generate.Note the difference between CMD and ENTRYPOINT build wordsDockerfile Docker image Docker container， So what is the relationship between these threeDockerfile–&gt;(build)–&gt;Docker image–&gt;(run)–&gt;Docker containerDocker RegistryA Repository is a place to store images. Like Node’s NPM; Python PyPi. Currently, Docker officially maintains a public repository, and most of the requirements can be implemented by directly downloading the image from the Docker Hub. Similar to GitHub, in the process of making image or Dockerfile, we draw the base image, i.e. From [base image], From the public library.","categories":[{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"}]},{"title":"Docker install and first Dockerfile","slug":"Docker-install-and-first-Dockerfile","date":"2018-01-01T12:00:05.000Z","updated":"2020-04-04T12:00:27.785Z","comments":true,"path":"2018/01/01/Docker-install-and-first-Dockerfile/","link":"","permalink":"cpeixin.cn/2018/01/01/Docker-install-and-first-Dockerfile/","excerpt":"","text":"prefaceThis article only explains the installation of Docker and the creation of Dockerfile. Dockerfile is composed of different commands in different requirements, which can be understood as a configuration file. However, only a demo is shown here to show the basic usage of DockerServer environmentlinux centos 7.6installstep 1If you have installed an older version of docker, you need to uninstall it first12345678910sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-enginestep 2install dependencies123sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2step 3set stable Repository123sudo yum-config-manager \\ --add-repo \\ https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;centos&#x2F;docker-ce.repostep 4install docker1sudo yum install docker-ce docker-ce-cli containerd.iostep 5start docker service1sudo systemctl start dockerstep 6Run a demo to see if the installation was successful: The hello-world image is not in your local, but when you run the command, docker will pulls the helloworld image from the remote repository1sudo docker run hello-worldcreate a demostep 1create dockerfileChoose any directory and create a dockerfile.It is suggested to name ‘Dockerfile’, because by default docker will run the file called ‘Dockerfile’ in the current directory. If you give it a different name, add the -f parameter and the path of the dockerfile12345678vim DockerfileFROM nginxMAINTAINER author &lt;email&gt;RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;&#x2F;h1&gt;&#39; &gt; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html:wqstep 2build image1docker build -t test&#x2F;hello-world: .-t is to set the repository and imagetest is repository namehello-world as the image namestep 3run image1docker run --name hello -d -p 8080:80 test&#x2F;hellostep 4Browser input http://localhost:8080/finalThis is a simple example of using Dockerfile to build the image and run the container!","categories":[{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"}]},{"title":"数据仓库 - 元数据管理","slug":"数据仓库-元数据管理","date":"2017-11-30T13:20:15.000Z","updated":"2020-04-04T11:23:20.355Z","comments":true,"path":"2017/11/30/数据仓库-元数据管理/","link":"","permalink":"cpeixin.cn/2017/11/30/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/","excerpt":"","text":"好文分享https://www.jianshu.com/p/9fe3ff2bbe99","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 数据分层","slug":"数据仓库数据分层","date":"2017-11-25T15:26:15.000Z","updated":"2020-04-04T17:54:37.540Z","comments":true,"path":"2017/11/25/数据仓库数据分层/","link":"","permalink":"cpeixin.cn/2017/11/25/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E5%88%86%E5%B1%82/","excerpt":"","text":"简述在上篇博客‘数据仓库建模方法’中我们探讨过数据模型的相关问题，实际场景中，我们团队在确定了数据模型使用经典的星型模型和3NF之后，接下来要进行讨论的问题则是，每天大批量的数据，应该怎样的放进库中，怎么存放合理。在数据对接业务之前，要经过哪些提炼的过程，在整个提炼的过程中，要分成几个步骤，还是一步到位的直接从原始数据ETL给业务人员。其中提到的这个提炼过程，也就是数据分层的过程，分层设计的好，数据分布合理，节约计算资源，避免多余的数据开发。所以，数据分层是数据仓库建设中，重要的一环。各种重复计算，严重浪费了计算资源，需要优化性能。为什么要数据分层我们对数据进行分层的一个主要原因就是希望在管理数据的时候，能对数据有一个更加清晰的掌控，详细来讲，主要有下面几个原因：清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。数据血缘追踪：简单来讲可以这样理解，我们最终给业务诚信的是一能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。把复杂问题简单化。讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。屏蔽原始数据的异常。屏蔽业务的影响，不必改一次业务就需要重新接入数据。怎么分层在这里，就拿我们团队对我们公司设计的数据仓库来举例。首先呢，我们公司的数据量级和数据主题复杂度，相当于一个二线互联网公司，数据主题目前这一版数据仓库可以分为用户行为，订单，游戏活动等7个主题数据，业务并不是很复杂。所以在数据分层上，总共分成三层。源数据层 （dw = staging）宽表数据层 （dw = dw）指标数据层 （dw = subjectName）源数据层源数据层拉取业务表有20张左右，使用spark读取底层RDBMS来获取数据，写入到Hive中，在这一层中，除了选择合适的分区字段，我们的分析业务中，时间粒度多为天为单位，业务数据来源于多个产品线，所以这一层，会以snapshot_date（‘YYYY-MM-DD’）和product_id为分区字段，对数据表进行分区外，不做任何数据处理，不做脏数据处理，不做合并，保持数据的原始性，随时可以做到追本溯源。我们对于近源数据层的定位是可以”快速”的构建基础数据平台. 不做业务相关的处理可以让这部分的工作专注在大数据架构正确性和稳定性的问题，近源数据层出现以后, 实际上我们已经可以开始主要的数据分析工作了。宽表数据层宽表数据层的数据是从源数据层经过ETL字段清洗，计算并且相关主题表，维度表进行join而来的。例如订单表：Orders表order_*为订单主题（order_id,order_number,order_amount,create_date…）t_users_dim 用户维度表(user_id,city,last_login_time,device_type…)t_activity_dim 活动维度表(activity_id,activity_name,create_date…)t_promotion_dim 优惠维度表(promotion_id,promotion_name,promotion_type…)t_deposit 充值表(deposit_id,deposit_type,channal,deposit_time…)以上五张表经过相关键join就组成了订单宽表 如下图所示：指标数据层指标数据层是从宽表数据层计算而来，这一层我们采用的策略是由总到分，关联时间维度表，缩小时间粒度到小时级别。可以对运营人员提供小时级别的数据指标。对于一些金额相关数据表，也会经过计算添加一些常用的数据模型，例如某用户近20次充值的max值和min值，中位数，四分位数等字段。这一层也是最不稳定的一层，经常要根据业务的变化增加字段或者新加指标数据表。由于的依赖Hive建设的数据仓库，那么对于增加字段的这件事，要小心对数据的影响，注意选择合适的Hive表格式。这层就不画图了 有点困了 困了～～～～这篇数据仓库数据分层设计，是根据我们公司的业务数据和主题来设计的，也是我们大数据工程团队的这几个渣渣工程师和架构师一起商讨的结果方案，对于其他行业和大规模海量数据肯定是不能全部适用的。在写这篇文章前，我有看过美团技术团队美团点评酒旅数据仓库建设实践这篇文章，发现业务复杂度和数据规模对数据建模和数据分层的影响很大，像我们公司的数据量量级要完全按照美团数仓去做，是完全没有必要的，而且会弄的更加复杂，所以在这里想表明的就是，对于数仓的建设，要选择适合自己的，对自己量身定制，照搬某个公司的，是没有意义的，反而可能会增加使用的难度。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 建模方法","slug":"数据仓库建模方法","date":"2017-11-24T12:26:15.000Z","updated":"2020-04-04T11:23:02.106Z","comments":true,"path":"2017/11/24/数据仓库建模方法/","link":"","permalink":"cpeixin.cn/2017/11/24/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/","excerpt":"","text":"背景接着上个文章数据仓库架构设计，想写一篇数据仓库常用模型的文章，但是自己对数据仓库模型的理解程度和建设架构并没有下面这个技术专家理解的深刻，并且自己去组织语言，可能会有不准确的地方，怕影响大家对数据仓库建模的理解，数据仓库属于一个工程学科，在设计上要体验出工程严谨性，所以这次向大家推荐这篇文章，毕竟IBM在数据仓库和数据集市方面已经做得很成熟了，已经有成型的商业数据仓库组件，这篇文章写的很好，可以让大家很好的理解数据仓库。版权作者 周三保(zhousb@cn.ibm.com) IBM 软件部信息技术专家.原文地址本文主要的主线就是回答下面三个问题：什么是数据模型为什么需要数据模型如何建设数据模型什么是数据模型数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图 1 所示。通过上面的图形，我们能够很容易的看出在整个数据仓库得建模过程中，我们需要经历一般四个过程：业务建模，生成业务模型，主要解决业务层面的分解和程序化。领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。为什么需要数据模型在数据仓库的建设中，我们一再强调需要数据模型，那么数据模型究竟为什么这么重要呢？首先我们需要了解整个数据仓库的建设的发展史。数据仓库的发展大致经历了这样的三个过程：简单报表阶段：这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。数据集市阶段：这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。数据仓库阶段：这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题：进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。如何建设数据模型建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。数据仓库数据模型架构数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。图 3. 数据仓库数据模型架构从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分：系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。数据仓库建模阶段划分我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容：图 4. 数据仓库建模阶段划分从上图我们可以清楚地看出，数据仓库的数据建模大致分为四个阶段：业务建模，这部分建模工作，主要包含以下几个部分：划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。深入了解各个业务部门的内具体业务流程并将其程序化。提出修改和改进业务部门工作流程的方法并程序化。数据建模的范围界定，整个数据仓库项目的目标和阶段划分。领域概念建模，这部分得建模工作，主要包含以下几个部分：抽取关键业务概念，并将之抽象化。将业务概念分组，按照业务主线聚合类似的分组概念。细化分组概念，理清分组概念内的业务流程并抽象化。理清分组概念之间的关联，形成完整的领域概念模型。逻辑建模，这部分的建模工作，主要包含以下几个部分：业务概念实体化，并考虑其具体的属性事件实体化，并考虑其属性内容说明实体化，并考虑其属性内容物理建模，这部分得建模工作，主要包含以下几个部分：针对特定物理化平台，做出相应的技术调整针对模型的性能考虑，对特定平台作出相应的调整针对管理的需要，结合特定的平台，做出相应的调整生成最后的执行脚本，并完善之。从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。数据仓库建模方法大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库得建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法，维度建模法，实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。范式建模法（Third Normal Form，3NF）范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :每个属性值唯一，不具有多义性 ;每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。根据 Inmon 的观点，数据仓库模型得建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例话。从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于：数据仓库的域模型应该包含企业数据模型得域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。以笔者的观点来看，Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。维度建模法维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模型 和 雪花模型。这里做一下 星型模型 和 雪花模型的扩充：星型模型星型模是一种多维的数据关系，它由一个事实表和一组维表组成。每个维表都有一个维作为主键，所有这些维的主键组合成事实表的主键。强调的是对维度进行预处理，将多个维度集合到一个事实表，形成一个宽表。这也是我们在使用hive时，经常会看到一些大宽表的原因，大宽表一般都是事实表，包含了维度关联的主键和一些度量信息，而维度表则是事实表里面维度的具体信息，使用时候一般通过join来组合数据，相对来说对OLAP的分析比较方便。雪花模型当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。雪花模型更加符合数据库范式，减少数据冗余，但是在分析数据的时候，操作比较复杂，需要join的表比较多所以其性能并不一定比星型模型高。星型模型和雪花模型的优劣对比应用场景星型模型的设计方式主要带来的好处是能够提升查询效率，因为生成的事实表已经经过预处理，主要的数据都在事实表里面，所以只要扫描实时表就能够进行大量的查询，而不必进行大量的join，其次维表数据一般比较少，在join可直接放入内存进行join以提升效率，除此之外，星型模型的事实表可读性比较好，不用关联多个表就能获取大部分核心信息，设计维护相对比较简答。雪花模型的设计方式是比较符合数据库范式的理念，设计方式比较正规，数据冗余少，但在查询的时候可能需要join多张表从而导致查询效率下降，此外规范化操作在后期维护比较复杂。总结通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。图 6. 维度建模法上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。实体建模法实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示：图 7. 实体建模法上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分：实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。说明，主要是针对实体和事件的特殊说明。由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。数据仓库数据模型与业务系统数据模型设计的区别数据仓库建模样例上面介绍得是一些抽象得建模方法和理论，可能理解起来相对有些难度，因此，笔者在这里举一个例子，读者可以跟着我们的这个样例，来初步了解整个数据仓库建模的大概过程。背景介绍熟悉社保行业的读者可以知道，目前我们国家的社保主要分为养老，失业，工伤，生育，医疗保险和劳动力市场这 6 大块主要业务领域。在这 6 大业务领域中，目前的状况养老和事业的系统已经基本完善，已经有一部分数据开始联网检测。而，对于工伤，生育，医疗和劳动力市场这一块业务，有些地方发展的比较成熟，而有些地方还不够成熟。1.业务建模阶段基于以上的背景介绍，我们在业务建模阶段，就很容易来划分相应的业务。因此，在业务建模阶段，我们基本上确定我们本次数据仓库建设的目标，建设的方法，以及长远规划等。如下图：图 8. 业务建模阶段在这里，我们将整个业务很清楚地划分成了几个大的业务主线，例如：养老，失业，工伤，生育，医疗，劳动力等着几个大的部分，然后我们可以根据这些大的模块，在每个业务主线内，考虑具体的业务主线内需要分析的业务主题。因此，业务建模阶段其实是一次和业务人员梳理业务的过程，在这个过程中，不仅能帮助我们技术人员更好的理解业务，另一方面，也能够发现业务流程中的一些不合理的环节，加以改善和改进。同时，业务建模阶段的另一个重要工作就是确定我们数据建模的范围，例如：在某些数据准备不够充分的业务模块内，我们可以考虑先不建设相应的数据模型。等到条件充分成熟的情况下，我们可以再来考虑数据建模的问题。领域概念建模阶段领域概念建模阶段是数据仓库数据建模的一个重要阶段，由于我们在业务建模阶段已经完全理清相应的业务范围和流程，因此，我们在这个领域概念建模阶段的最主要的工作就是进行概念的抽象，整个领域概念建模的工作层次如下图所示：图 9. 领域概念建模阶段从上图我们可以清楚地看到，领域概念建模就是运用了实体建模法，从纷繁的业务表象背后通过实体建模法，抽象出实体，事件，说明等抽象的实体，从而找出业务表象后抽象实体间的相互的关联性，保证了我们数据仓库数据按照数据模型所能达到的一致性和关联性。从图上看，我们可以把整个抽象过程分为四个层次，分别为：抽象方法层，整个数据模型的核心方法，领域概念建模的实体的划分通过这种抽象方法来实现。领域概念层，这是我们整个数据模型的核心部分，因为不同程度的抽象方法，决定了我们领域概念的不同。例如：在这里，我们可以使用“参与方”这个概念，同时，你也可以把他分成三个概念：“个人”，“公司”，和“经办机构”这三个概念。而我们在构建自己的模型的时候，可以参考业务的状况以及我们自己模型的需要，选择抽象程度高的概念或者是抽象程度低的概念。相对来说，抽象程度高的概念，理解起来较为复杂，需要专业的建模专家才能理解，而抽象程度低的概念，较适合于一般业务人员的理解，使用起来比较方便。笔者在这里建议读者可以选用抽象概念较低的实体，以方便业务人员和技术人员之间的交流和沟通。具体业务层，主要是解决具体的业务问题，从这张图我们可以看出，具体的业务层，其实只是领域概念模型中实体之间的一些不同组合而已。因此，完整的数据仓库的数据模型应该能够相应灵活多变的前端业务的需求，而其本身的模型架构具有很强的灵活性。这也是数据仓库模型所具备的功能之一。业务主线层，这个层次主要划分大的业务领域，一般在业务建模阶段即已经完成这方面的划分。我们一般通过这种大的业务主线来划分整个业务模型大的框架。通过领域概念建模，数据仓库的模型已经被抽象成一个个的实体，模型的框架已经搭建完毕，下面的工作就是给这些框架注入有效的肌体。逻辑建模阶段通过领域概念建模之后，虽然模型的框架已经完成，但是还有很多细致的工作需要完成。一般在这个阶段，我们还需要做非常多的工作，主要包括：实例话每一个抽象的实体，例如：在上面的概念模型之后，我们需要对“人”和“公司”等这些抽象实体进行实例化。主要是，我们需要考虑“人”的属性包括那些，在业务模块中，用到的所有跟“人”相关的属性是哪些，我们都需要将这些属性附着在我们数据模型的“人”这个实体上，例如“人”得年龄，性别，受教育程度等等。同理，我们对其他属性同样需要做这个工作。找出抽象实体间的联系，并将其实例话。这里，我们主要考虑是“事件”这个抽象概念的实例话，例如：对于养老金征缴这个“事件”的属性得考虑，对于失业劳动者培训这个“事件”的属性得考虑等等。找出抽象事件的关系，并对其进行说明。在这里我们主要是要针对“事件”进行完善的“说明”。例如：对于“事件”中的地域，事件等因素的考量等等。总而言之，在逻辑建模阶段，我们主要考虑得是抽象实体的一些细致的属性。通过逻辑建模阶段，我们才能够将整个概念模型完整串联成一个有机的实体，才能够完整的表达出业务之间的关联性。在这个阶段，笔者建议大家可以参考 3NF 的建模方法，表达出实体的属性，以及实体与实体之间的联系。例如：在这个阶段，我们可以通过采用 ERWIN 等建模工具等作出符合 3NF 的关系型数据模型来。物理建模阶段物理建模阶段是整个数据建模的最后一个过程，这个过程其实是将前面的逻辑数据模型落地的一个过程。考虑到数据仓库平台的不同，因此，数据模型得物理建模过程可能会稍微有一些不同，在这个阶段我们主要的工作是：生成创建表的脚本。不同的数据仓库平台可能生成不同的脚本。针对不同的数据仓库平台，进行一些相应的优化工作，例如对于 DB2 数据仓库来说，创建一些 MQT 表，来加速报表的生成等等。针对数据集市的需要，按照维度建模的方法，生成一些事实表，维表等工作。针对数据仓库的 ETL 车和元数据管理的需要，生成一些数据仓库维护的表，例如：日志表等。经过物理建模阶段，整个数据仓库的模型已经全部完成，我们可以按照自己的设计来针对当前的行业创建满足自己需要的数据模型来。这里，笔者通过一个数据建模的样例，希望能够给读者一个关于数据仓库建模的感性的认识。希望读者在利用这些数据仓库得建模方法创建自己的数据模型的时候，可以根据业务实际的需要和自己对抽象能力的把握来创建适合自己的数据模型。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 架构设计","slug":"数据仓库架构设计","date":"2017-11-22T15:26:15.000Z","updated":"2020-04-04T11:23:13.660Z","comments":true,"path":"2017/11/22/数据仓库架构设计/","link":"","permalink":"cpeixin.cn/2017/11/22/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"业务数据背景我所在的公司处理的数据主要是游戏过程数据，用户行为数据，游戏币消费数据还有其他业务部门维护的例如用户优惠数据，游戏活动等业务数据。其中对于业务系统DBMS中的数据，基本按照“天”作为时间粒度来采集到数据仓库中，对于实时性要求较强的游戏数据，则是需要采用埋点实时采集和处理的。需求的多样化和复杂性，运营人员希望做到精细化运营和数据指标的实时响应, 这要求数据仓库具有提供高效明细数据能力,也能随时提供最新的数据调取，为了满足不同层次的数据提出和分析, 就要求着数据仓库在设计之初，就要考虑到要同时支持离线和实时的情况。数据仓库架构理念设计数据仓库架构之初，我们大数据工程组团队开会讨论的第一件事就是：Who is the User of Data in Data Warehouse？这件事情还是很重要的，弄清楚了这件事，对于接下来架构设计和数仓数据主题的确定都是有影响的，其中对于架构设计来说，这决定了我们架构上层需要提供哪些接口，对接不同的业务系统，我们应该设计怎样的查询和索引，不同的使用者，能够接受的数据粒度或者是数据响应时间，整个数据仓库的数据流大致应该怎样流向，这都与使用用户有关。在架构设计方面，基本上都是由下到上的去设计，但是呢，也不妨从上层需求开始，向下层实现设计开始✌️架构图话不多说，po图👽架构层次&amp;技术选型数据源由于公司各业务部门的数据都由运维DBA统一管理和做权限控制，RDBMS中的数据的采集，需要由运维部门给出备库URL，避免直接来取主库数据，对线上业务产生影响。这部份的数据采集，主要涉及Sqoop，NIFI，Spark 这三个工具。Sqoop在上一版的数据仓库中，Sqoop是作为RDBMS -&gt; HDFS中主要的搬运工。主要原因是开发成本小，数据量小，使用简单。但是随着数仓原始层数据量激增了原来的三倍，Sqoop计算能力就捉襟见肘了，每天的第一个任务就是拉取原始层数据，如果继续使用Sqoop，会严重影响接下来其他例行任务的按时进行。所以，我们决定抛弃这个使用MapReduce作为计算框架的数据同步工具。Spark提升数仓原始数据层拉取速度，就要选择合适的拉去工具。面对拉去原始数据，基本上不需要对任何数据字段进行清洗，基本上直接下 select columns就可以满足要求， 所以选用Spark SQL， read jdbc -&gt; dataframe -&gt; writeInto -&gt; Hive。这样运行的速度相对Sqoop来说，提升了四倍✌️NIFINIFI这个工具，并没有作为主要的数据工具来使用，起初只是作为团队的技术调研对象，在几次的分享会上，大家对NIFI都比较感兴趣，可能是大家都想解放双手，用拖拽的形式来拒绝写代码 哈哈哈哈。就这样，NIFI作为一个技术储备，承担了test库中测试数据的同步，等以后团队人手充足了，有维护多框架的能力时可以考虑发挥NIFI的更多用处。另外对于埋点数据，产品方会将数据打到运维的kafka中，随后我们会使用fluentd将运维kafka中的数据实时转发到我们大数据平台中的kafka中，做接下来的处理Fluentd运维的kafka中，会收集这我们想要的日志数据和埋点数据，埋点数据一般为json格式，将数据解析和打到大数据平台kafka的这一段中，我们在Logstash,Fluentd两个工具中进行选择，Logstash是非常出名的ELK中的 ‘L’ , 成熟度和稳定性不用说。Fluentd则是我们第一次接触，我们并没有亲自对Fluentd和Logstash进行效率和资源消耗上做测试，但是根据网上几篇对比测评文章，Fluentd的支持度虽没有Logstash那么完整，但是Fluentd中的插件完全能满足我们的开发需求，最重要的，Fluentd在CPU和内存上的消耗，都要优于Logstash。所以最后我们选择了FluentdFlink &amp; Spark StreamingFlink，Spark Streaming这两个实时计算框架就不多介绍了，家喻户晓。在这里说一下，在实时计算的这一层面，团队为什么选择了两个计算框架。主要是因为对待不同的需求，使用更适合的工具，团队成员对Spark Streaming更加熟悉，在对实时性要求更高的项目中，例如游戏监控系统，对于数据状态管理，watermark的需求都是有要求的，在这方面，Flink的实现就是要比Spark Streaming好。数据存储层数据存储层分为离线和实时HiveHive是Hadoop的一个数据仓库工具，底层存储为HDFS，可以将我们按照主题设计好的结构化数据映射成表，提供SQL接口来分析数据。从存储层到共享层中，大部分的计算任务，都是在Hive表中，使用Spark作为计算框架，ETL将数据清洗到HBase,MySQL等结果库中。HBase&amp;Elasticsearch&amp;GrafanaHBaseHBase的用途基本上是存储流式ETL后的结果数据，或者是直接存入原始数据作为备份ElasticsearchES的用途基本上是搭配Fluentd和Kibana，组成EFK，做数据展示，多数情况下会存储日志数据，做详情的检索。还有一个常用的功能就是结合HBase做二级索引，会在ES索引里面存储一个hbase的关键字rowkey，根据查询条件定位到某个rowkey，直接去hbase里面get数据，秒级返回GrafanaGrafana是团队后期引入的，主要是被Grafana的UI所吸引，Grafana和Kibana对比起来，UI展示要强大的很多，对于Grafana，我们正在调研，使用Grafana来做数据质量的监控，等使用成熟了，也会分享出来。数据共享层共享层的数据，都是经过ETL清洗的，都是即拿即用的。业务系统 ，对接数据仓库的，都是通过GoApi结构进行调取的。报表数据 ，工程师会通过Spark写成CSV文件，提供给运营人员即席查询 ，我们还开放了Presto接口（presto的查询速度比hive快5-10倍），同时对数据仓库做了仅查权限控制，希望业务分析人员可以自主的通过presto进行数据调取，但是理想是丰满的，现实是骨感的。根本没人用 哈哈哈哈哈哈哈数据应用层略数据用户层略","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 实际作用与需求","slug":"数据仓库实际作用与需求","date":"2017-11-20T15:26:15.000Z","updated":"2020-04-04T11:22:56.342Z","comments":true,"path":"2017/11/20/数据仓库实际作用与需求/","link":"","permalink":"cpeixin.cn/2017/11/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AE%9E%E9%99%85%E4%BD%9C%E7%94%A8%E4%B8%8E%E9%9C%80%E6%B1%82/","excerpt":"","text":"数据仓库的产生背景数据仓库的起源可以追溯到计算机的发展初期，并且数据仓库是信息技术长期发展的产物，在以后也会一直发展。从工程角度来讲，在计算机领域出现了大型在线事务处理系统后，对于数据之间的交互和使用就逐渐的多了起来。随后出现的抽取程序，可以通过设置参数，在文件中搜索满足条件的数据，然后把这些数据传送到其他文件或者数据库中。慢慢的人们发现在抽取结果中，加上一些条件限制可以更方便的得到想要的数据，于是就出现了基于抽取之上的抽取。这样就造成了如下问题：无休止的抽取带来诸多问题，抽取程序过多，功能单一，不可复用，无公共使用数据源，数据之间会产生差异等，所以开始思考是否可以建立成体系的机构化环境，以减少数据的差异。这也就是数据仓库出现的原因。数据仓库从操作型数据库中抽取数据，通过规范的加工过程，得到粒度化数据，并且这些数据时面向主题、集成、不易失、随时间变化的从商业角度来讲，在一次大会上，马云爸爸谈到当今时代，是IT时代到DT时代的变革。数据资源是比石油还要重要的资源，数据就是生产资料。随着企业多年的经营，业务数据量的不断增大，如何能够更好地利用数据，将数据转化成商业价值，已经成为人们越来越关心的问题。举例来说，数据库系统可以很好地解决事务处理，实现对数据的“增删改查”等功能，但是却不能提供很好的决策分析支持。因为事务处理首先考虑响应的及时性，多数情况都是在处理当前数据，而决策分析需要考虑的是数据的集成性和历史性，可能对分析处理的时效性要求不高。所以为了提高决策分析的有效性和完整性，人们逐渐将一部分或者大部分数据从联机事物处理系统中剥离出来，形成今天的数据仓库。数据团队眼中的数据仓库从实习工作，到目前为止，我参与了两次数仓的建设，实习公司当时大数据团队刚刚成立，那时并没有建立数仓，而是依据当时的各个项目需求，直接建立数据集市，将每个审计局的业务需求，分别建立不同的业务表，数据来源有来自Excel文件的，也有来自Oracle数据库的。再一个就是当前公司，这次的数仓建设，是我完整的一次参与整体流程，建立数仓的目的就是来应对各个产品所会提出的各种数据需求和统一各个产品的数据标准，现在的产品方，确实具有一定的数据头脑，不再像以前只追求盈利，不去思考数据背后的价值和数据指标对经营带来的影响。我所在的大数据团队负责公司层面和下面8个产品线的数据驱动服务，公司层面经常会调取盈利相关指标，产品方的需求则是各种各样，时间长度也是从年到天都有。对于公司和产品方，他们并不关心数据的来源和数仓的重要性，因为他们是使用数据结果的人，但是对于大数据团队，建立数据仓库是一个必要的事情，数据仓库对企业的价值数据仓库对企业带来的价值是很难估算的，但是数据仓库的成本倒是可以估算出大概的。就我们公司而言，数据仓库数据主题分成两个大类来说，就是用户数据与业务数据，那么产品方最想看到的就是用户数据与业务数据联合起来，这就是数据仓库对公司产品运营方带来的价值。数据仓库具有历史性，其中存储的数据大多是结构化数据，这些数据并非企业全量数据，而是根据需求针对性抽取的，因此数据仓库对于业务的价值是各种各样的报表，但这些报表又无法实时产生。数据仓库报表虽然能够提供部分业务价值，但不能直接影响业务，需要数据开发人员对数据仓库中的数据进行ETL开发，给出指导性的数据分析结果。数据仓库中数据与数据库中数据的不同这里呢，简单的说，应该就是OLTP和OLAP的区别数据仓库需求分析阶段数据仓库的需求分析，在很多公司中，都是由数据团队或者数据仓库工程师几个技术人员协定下来的。但是一个合格的数据仓库，应该是由业务方，产品方，运营方，数据团队技术方一同来商定的。一个完整的流程应该是去了解数据的背景，细节，去了解用户需要用到哪些主题的数据和数据粒度的大小，去了解主题数据中数据的维度，最后才是数据仓库设计人员开始拿出数据仓库的架构方案和ETL方案。数据仓库作用整合公司所有业务数据，建立统一的数据中心产生业务报表，用于作出决策为网站运营提供运营上的数据支持可以作为各个业务的数据源，形成业务数据互相反馈的良性循环分析用户行为数据，通过数据挖掘来降低投入成本，提高投入效果开发数据产品，直接或间接地为公司盈利","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 简述","slug":"数据仓库简述","date":"2017-11-19T15:26:15.000Z","updated":"2020-04-04T11:23:08.091Z","comments":true,"path":"2017/11/19/数据仓库简述/","link":"","permalink":"cpeixin.cn/2017/11/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0/","excerpt":"","text":"前言说到数据仓库，在一年前，我的答案是非常模糊的，在两年前，我根本不知道数据仓库的存在。相信大多数的人都是经常接触数据库，上大学的时候学过Oracle，MySQL这两个关系型数据库，操作着各种增删改查，现在人们对这种类型的数据库都称作传统数据库，数据量偏小，每一张表都是为了某一系统功能而设计的。在之后从事大数据行业了，数据仓库这几个字也就总出现在各种博客中，各种技术方案中。现在正在和同事主导公司数据仓库的设计和搭建工作，所以准备把我这段工程经验，拿出来分享一下。数据库与数据仓库的区别数据库这里我们所指的数据库，也就是一系列经典的RDBMS，如Oracle，MySQL，SQL Server等关系型数据库。其中的一些设计过程如 ER图的设计，逻辑模型的设计，物理模型设计，还有规范化设计 如 至少要符合第一范式，尽可量的去符合第二范式，第三范式等设计细节，这里就不赘述了，但是以上所提到的都是关系型数据库的精华。关系型数据库的用途，现在也会被分成两大类（1）操作型数据库，主要用于业务支撑。一个公司往往会使用并维护若干个数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等；（2）分析型数据库，主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析；数据仓库接下来先给大家看百度百科给出的数据仓库概念数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。从上面的定义来看，数据仓库的主要功能是用于做企业各个业务层面的分析，企业层面的数据总集，曾经看过一个博主对数据仓库的定义是“面向分析的存储系统”，这个定义是很接地气的，如果按照这个思路往下去思考，数据仓库存储的数据是很庞大的，不是一个单一的业务数据集，不用精准到每一条数据，例如，淘宝在存储每一条下单记录的时候，在关系型数据库中，这条记录是不会存储到其他业务表里面，只会存储在下单记录表里面，并且下单记录表里面不会存储两条相同的订单数据，这是因为关系型数据库要严格满足完整性/参照性约束以及范式设计要求，但是在数据仓库中，这条订单数据可能会存在用户行为表中，也可能存在下单数据表中。也就是说，数据仓库不应让传统关系数据库来实现，因为关系数据库最少也要求满足第1范式，而数据仓库里的关系表可以不满足第1范式。也就是说，同样的记录在一个关系表里可以出现N次。但由于大多数数据仓库内的表的统计分析还是用SQL，因此很多人把它和关系数据库搞混了。数据仓库的特点面向主题面向主题特性是数据仓库和操作型数据库的根本区别。操作型数据库是为了支撑各种业务而建立，而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；集成性集成性是指数据仓库会将不同源数据库中的数据汇总到一起；企业范围数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来；历史性较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年；时变性时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告；数据仓库组件数据仓库的核心组件有四个：各源数据库，ETL，数据仓库，前端应用。如下图所示：业务系统业务系统包含各种源数据库或者网站前端埋点实时数据，数据源可以是离线的，也可以是实时的（时间粒度为小时）。比如我们数据仓库中的离线数据来源是公司业务系统的Oracle,这些源数据库既为业务系统提供数据支撑，同时也作为数据仓库的数据源，部分实时数据是通过kafka，flume，NIFI收集来的埋点数据。ETLETL分别代表：提取extraction、转换transformation、加载load。其中提取过程表示操作型数据库搜集指定数据，转换过程表示将数据转化为指定格式并进行数据清洗保证数据质量，加载过程表示将转换过后满足指定格式的数据加载进数据仓库。数据仓库会周期不断地从源数据库提取清洗好了的数据。（ETL应该是数据工程师必会的技能😂）前端应用数据仓库的数据，肯定是为了前端应用而准备的，前端应用则是为了使用数据的用户而准备的。目前我搭建的数仓一般不会直接对接前端应用，而是将数仓中的数据，按照用户方需求进行计算聚合到HBase或者ES中。中间加一层API,API可以是Go或者Python开发的，这样前端应用直接调用API,也有一些特殊场景，会直接调用presto来访问Hive数据集市数据集市是数据仓库下面衍生出来的概念，在这里，我举一个例子来帮助大家理解，我们可以把数据仓库理解成万达购物中心，其中每层都卖着不同种类的商品，其中，一层卖的服装，二层买的家电…这样就可以分成一层是一个服装主题的数据集市，二层是家电主题的数据集市。同时可以理解为是一种”小型数据仓库”，它只包含单个主题，且关注范围也非全局。集市可以分为两种，一种是独立数据集市(independent data mart)，这类数据集市有自己的源数据库和ETL架构；另一种是非独立数据集市(dependent data mart)，这种数据集市没有自己的源系统，它的数据来自数据仓库。当用户或者应用程序不需要/不必要/不允许用到整个数据仓库的数据时，非独立数据集市就可以简单为用户提供一个数据仓库的”子集”。数据仓库搭建流程大家先看这个流程图，其中的各个步骤，我都会去介绍，其中着重介绍的会是数据仓库建模和ETL工程，因为，建模是整个数据仓库的核心，ETL工程师整个数据仓库中最耗时耗力的。今天呢，先简单介绍一下，铺垫一下","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"python - 协程","slug":"python-协程","date":"2017-05-31T12:16:32.000Z","updated":"2020-04-04T11:03:54.388Z","comments":true,"path":"2017/05/31/python-协程/","link":"","permalink":"cpeixin.cn/2017/05/31/python-%E5%8D%8F%E7%A8%8B/","excerpt":"","text":"深入理解Python异步编程推荐推荐推荐！！！！协程，Python asyncio异步编程 只此一篇足矣，一览众山小！链接","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 生成器 2","slug":"python-生成器-2","date":"2017-05-30T11:08:35.000Z","updated":"2020-04-04T17:14:25.836Z","comments":true,"path":"2017/05/30/python-生成器-2/","link":"","permalink":"cpeixin.cn/2017/05/30/python-%E7%94%9F%E6%88%90%E5%99%A8-2/","excerpt":"","text":"生成器进化成协程生成器是由迭代器进化而来，所以生成器对象有 iter 和 next 方法，可以使用 for 循环获得值，注意这里所说的 “获得值” 指的是下文代码块里 yield 语句中 yield 关键字后面的 i 。这是在 Python 2.5 时出现的特性，在 Python 3.3 中出现 yield from 语法之前，生成器没有太大用途。但此时 yield 关键字还是实现了一些特性，且至关重要，就是生成器对象有 send 、throw 和 close 方法。这三个方法的作用分别是发送数据给生成器并赋值给 yield 语句、向生成器中抛入异常由生成器内部处理、终止生成器。这三个方法使得生成器进化成协程。协程有四种存在状态：GEN_CREATED 创建完成，等待执行GEN_RUNNING 解释器正在执行（这个状态在下面的示例程序中无法看到）GEN_SUSPENDED 在 yield 表达式处暂停GEN_CLOSE 执行结束，生成器停止可以使用 inspect.getgeneratorstate 方法查看协程的当前状态，举例如下：inspect模块用于收集python对象的信息，可以获取类或函数的参数的信息，源码，解析堆栈，对对象进行类型检查等等12345678910111213141516171819202122import inspectdef generator(): i = '激活生成器' while True: try: value = yield i except ValueError: print('OVER') i = valueg = generator() # 1inspect.getgeneratorstate(g) # 2print(next(g)) # 3inspect.getgeneratorstate(g)print(g.send('Hello Shiyanlou')) # 4g.throw(ValueError) # 5g.close() # 6inspect.getgeneratorstate(g)代码说明如下：1、创建生成器2、查看生成器状态3、这步操作叫做预激生成器（或协程），这是必须做的。在生成器创建完成后，需要将其第一次运行到 yield 语句处暂停4、暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值，也就是 yield 表达式等号前面的 value 变量的值变成 ‘Hello Shiyanlou’，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i5、向生成器抛入异常，异常会被 try except 捕获，作进一步处理6、close 方法终止生成器，异常不会被抛出因为生成器的调用方也就是程序员自己可以控制生成器的启动、暂停、终止，而且可以向生成器内部传入数据，所以这种生成器又叫做协程，generator 函数既可以叫做生成器函数，也可以叫协程函数，这是生成器向协程的过渡阶段。yield -&gt; yield from在 Python 3.3 中新增了 yield from 语法，如果将yield理解成“返回”，那么yield from就是“从什么（生成器）里面返回”,这是全新的语言结构，是 yield 的升级版。相比 yield ，该语法有两大优势，我们来举例说明它的用法。区别示例123456789101112def generator(): yield 'a' yield 'b' yield 'c' yield from generator1() #yield from iterable本质上等于 for item in iterable: yield item的缩写版 yield from [11,22,33,44] yield from (12,23,34) yield from range(3) for i in generator(): print(i,end=' , ')避免潜逃循环yield:12345678910def chain(*args): for iter_obj in args: for i in iter_obj: yield ichain(&#123;'one', 'two'&#125;, list('ace'))for i in c: print(i)yield from:123456789def chain(*args): for iter_obj in args: yield from iter_objc = chain(&#123;'one', 'two'&#125;, list('ace'))for i in c: print(i)可以看到 yield from 语句可以替代 for 循环，避免了嵌套循环。同 yield 一样，yield from 语句也只能出现在函数体内部，有 yield from 语句的函数叫做协程函数或生成器函数。yield from 后面接收一个可迭代对象，例如上面代码中的 iter_obj 变量，在协程中，可迭代对象往往是协程对象，这样就形成了嵌套协程。转移控制权123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import timefrom faker import Fakerfrom functools import wraps# 预激协程装饰器def coroutine(func): @wraps(func) def wrapper(*args, **kw): g = func(*args, **kw) next(g) return g return wrapper# 子生成器函数，这个生成器是真正做事的生成器def sub_coro(): l = [] # 创建空列表 while True: # 无限循环 value = yield # 调用方使用 send 方法发生数据并赋值给 value 变量 if value == 'CLOSE': # 如果调用方发生的数据是 CLOSE ，终止循环 break l.append(value) # 向列表添加数据 return sorted(l) # 返回排序后的列表# 使用预激协程装饰器# 带有 yield from 语句的父生成器函数@coroutinedef dele_coro(): # while True 可以多次循环，每次循环会创建一个新的子生成器 sub_coro() # 这里 while 只循环一次，这是由调用方，也就是 main 函数决定的 # while 循环可以捕获函数本身创建的父生成器终止时触发的 StopIteration 异常 while True: # yield from 会自动预激子生成器 sub_coro() # 所以 sub_coro 在定义时不可以使用预激协程装饰器 # yield from 将捕获子生成器终止时触发的 StopIteration 异常 # 并将异常的 value 属性值赋值给等号前面的变量 l # 也就是 l 变量的值等于 sub_coro 函数的 return 值 # yield from 还实现了一个重要功能 # 就是父生成器的 send 方法将发送值给子生成器 # 并赋值给子生成器中 yield 语句等号前面的变量 value l = yield from sub_coro() print('排序后的列表：', l) print('------------------')# 调用父生成器的函数，也叫调用方def main(): # 生成随机国家代号的方法 fake = Faker().country_code # 嵌套列表，每个子列表中有三个随机国家代号(字符串) nest_country_list = [[fake() for i in range(3)] for j in range(3)] for country_list in nest_country_list: print('国家代号列表：', country_list) c = dele_coro() # 创建父生成器 for country in country_list: c.send(country) # 父生成器的 send 方法将国家代号发送给子生成器 # CLOSE 将终止子生成器中的 while 循环 # 子生成器的 return 值赋值给父生成器 yield from 语句中等号前面的变量 l c.send('CLOSE')if __name__ == '__main__': main()所谓 “转移控制权” 就是 yield from 语法可以将子生成器的控制权交给调用方 main 函数，在 main 函数内部创建父生成器 c ，控制 c.send 方法传值给子生成器。这是一个巨大的进步，在此基础上，Python 3.4 新增了创建协程的装饰器，这样非生成器函数的协程函数就正式出现了。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 生成器","slug":"python-生成器","date":"2017-05-28T12:07:35.000Z","updated":"2020-04-04T17:13:37.299Z","comments":true,"path":"2017/05/28/python-生成器/","link":"","permalink":"cpeixin.cn/2017/05/28/python-%E7%94%9F%E6%88%90%E5%99%A8/","excerpt":"","text":"前言在开始讲解协程和生成器之前，说明一下协程和生成器之间的关系是很有必要的，生成器在学习python的时候，并没有深入的学习，所以感觉很抽象很难懂，和普通函数单向调用的逻辑思维并不一样。协程则是和线程，进程是同一类别的概念。首先要认识yield关键字，是yield关键词，yield放在函数中可以使得函数变成生成器，也可以变成协程。在生成器中, yield 只对外产出值，在协程中，yield能对外产出值，而且能接收通过send()方法传入值yielld构造的生成器可以作为协程使用，协程是指一个过程，这个过程与调用方协作，由调用方提供的值，来计算并产出。纯粹的生，这样可以交接给for调用。成器只输出值，和迭代有关协程与函数的区别，函数是一种上下级调用关系，而协程是通过_yield_方式转移执行权，对称而平级的调用对方，典型的有生产者和消费者。从调试过程中理解区别于其他教程，前几段都是云里雾里的讲概念，但是对于生成器，一上来就看概念，真的很难懂。所以我准备了三个实例，建议上来先通过打断点，分步调试过程中，通过看调用顺序和返回值来初步了解生成器的原理，接下来在看概念解释，则比较容易理解。1234567891011121314151617# encoding:UTF-8def yield_test(n): for i in range(n): yield call(i) print(\"i=\", i) # 做一些其它的事情 print(\"do something.\") print(\"end.\")def call(i): return i * 2# 使用for循环for i in yield_test(5): print(i, \",\")下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c)下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示12345678910def h(): print 'Wen Chuan', m = yield 5 print m d = yield 12 print 'We are together!'c = h()m = c.__next__() c.send('Fighting!')下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示经过上面的三个程序的调试步骤后，下面开始带着心中初步了解的程序运行顺序来看生成器相关的原理生成器what生成器是一次生成一个值的特殊类型函数。可以将其视为可恢复函数。调用该函数将返回一个可用于生成连续 x 值的生成【Generator】，简单的说就是在函数的执行过程中，yield语句会把你需要的值返回给调用生成器的地方，然后退出函数，下一次调用生成器函数的时候又从上次中断的地方开始执行，而生成器内的所有变量参数都会被保存下来供下一次使用。why列表所有数据都在内存中，如果有海量数据的话将会非常耗内存。如：仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。如果列表元素按照某种算法推算出来，那我们就可以在循环的过程中不断推算出后续的元素，这样就不必创建完整的list，从而节省大量的空间。简单一句话：我又想要得到庞大的数据，又想让它占用空间少，那就用生成器！How方法一123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;方法二如果一个函数中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。调用函数就是创建了一个生成器（generator）对象。yield关键字yield 的用法起源于对一般 function 中 return 的扩展。在一个 function 中，必须有一个返回值列于 return 之后，可以返回数字也可以返回空值，但是必须要有一个返回值，标志着这个 function 的结束。一旦它结束，那么这个 function 中产生的一切变量将被统统抛弃，有什么可以使一个 function 暂停下来，并且返回当前所在地方的值，当接收到继续的命令时可以继续前进呢？换个说法，就是 return 返回一个值，并且记住这个返回的位置。这个操作有点儿像Python2.5以前，yield是一个语句，但现在2.5中，yield是一个表达式(Expression)，比如：1m = yield 5大家不要认为，m值为5，而是表达式(yield 5)的返回值将赋值给m，那么yield 5的返回值是什么呢？yield 5的返回值是下面将要提到的send(msg)方法传递过来的msg参数。生成器的工作原理生成器(generator)能够迭代的关键是它有一个 next() 方法 。 工作原理就是通过重复调用next()方法，直到捕获一个异常。带有 yield 的函数不再是一个普通函数，而是一个生成器generator。可用next()调用生成器对象来取值。next 两种方式 t.next() | next(t)。 可用for 循环获取返回值（每执行一次，取生成器里面一个值） （基本上不会用next()来获取下一个返回值，而是直接使用for循环来迭代）。yield相当于 return 返回一个值，_迭代一次遇到yield时就返回yield后面(右边)的值_。并且记住这个返回的位置，下次迭代时，代码从yield的_下一条_语句开始执行。send() 和next()一样，都能让生成器继续往下走一步（下次遇到yield停），send()能传一个值，这个值作为yield表达式等号左边的值。——换句话说，就是send可以强行修改上一个yield表达式值。比如函数中有一个yield赋值，a = yield第一次迭代到这里会返回5，a还没有赋值。第二次迭代时，使用.send(10)，那么，就是强行修改yield 5表达式的值为10，本来是5的，那么a=10send(msg)与next()都有返回值，它们的返回值是当前迭代遇到yield时，yield后面表达式的值，其实就是当前迭代中yield后面的参数。感受下yield返回值的过程（关注点：每次停在哪，下次又开始在哪）及send()传参的通讯过程生成器还可以使用 next 方法迭代。生成器会在 yield 语句处暂停，这是至关重要的，未来协程中的 IO 阻塞就出现在这里。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 多进程","slug":"python-多进程","date":"2017-05-26T15:10:35.000Z","updated":"2020-04-04T11:05:03.269Z","comments":true,"path":"2017/05/26/python-多进程/","link":"","permalink":"cpeixin.cn/2017/05/26/python-%E5%A4%9A%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"多线程： 同一进程中，创建多个线程，执行添加的任务列表多进程： 创建任意个进程，各自执行添加的任务列表假如cpu只有一个（早期的计算机确实如此），也能保证支持（伪）并发的能力，将一个单独的cpu变成多个虚拟的cpu（多道技术：时间多路复用和空间多路复用+硬件上支持隔离），没有进程的抽象，现代计算机将不复存在。对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。如果我们并没有在程序中加入进程，线程，协程等，程序都是执行单任务的进程，也就是只有一个线程。如果我们要同时执行多个任务怎么办？有两种解决方案：一种是启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。还有一种方法是启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。当然还有第三种方法，就是启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。总结一下就是，多任务的实现有3种方式：多进程模式；多线程模式；多进程+多线程模式。同时执行多个任务通常各个任务之间并不是没有关联的，而是需要相互通信和协调，有时，任务1必须暂停等待任务2完成后才能继续执行，有时，任务3和任务4又不能同时执行，所以，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。什么时候用多进程1.多线程使用场景：IO密集型2.多进程使用场景：CPU密集型涉及并发的场景，大家想到使用多线程或多进程解决并发问题;一般情况下，解决多并发场景问题，多数语言采用多线程编程模式(线程是轻量级的进程，共用一份进程空间)。也同样适用于Python多并发处理吗? 不是的，针对并发处理，Python多线程和多进程是有很大差异的!Python多线程和多进程差异Python多线程不能使用CPU多核资源，即同一时刻，只有一个线程使用CPU资源，所以使用Python多线程不能算是并发。如果想要充分利用CPU多核资源，做到多并发，这就需要Python多进程的了!也就是说：只有Python多进程才能利用CPU多核资源，做到真正的多并发!Python多线程和多进程应用场景Python多线程适用于I/O密集型场景，如解决网络IO、磁盘IO阻塞问题，例如文件读写、网络数据传输等;而Python多进程更适用于计算密集型场景，多并发，大量计算任务等。注意：Python多线程和多进程在平时开发过程中，需要注意使用，如果使用Python多线程方式处理计算密集型任务，它比实际单进程处理性能还要慢!所以要注意，看场景类型。多进程-fork()Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程：123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid))结果：123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876.由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的，推荐大家用Mac学Python！有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。多进程-multiprocessing进程基础版本：1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.')执行结果如下：1234Parent process 928.Process will start.Run child process test (929)...Process end.进程池：如果要启动大量的子进程，可以用进程池的方式批量创建子进程：12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.')代码解读：对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：p = Pool(5)就可以同时跑5个进程。由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。进程池实例方法：apply(func[, args[, kwds]])：同步进程池apply_async(func[, args[, kwds[, callback[, error_callback]]]]) ：异步进程池Lock互斥锁当多个进程需要访问共享资源的时候，Lock可以用来避免访问的冲突进程之间数据隔离，但是共享一套文件系统，因而可以通过文件来实现进程直接的通信，但问题是必须自己加锁处理。注意：加锁的目的是为了保证多个进程修改同一块数据时，同一时间只能有一个修改，即串行的修改，没错，速度是慢了，牺牲了速度而保证了数据安全1234567891011121314151617181920212223242526272829303132333435363738import jsonimport timeimport randomimport osfrom multiprocessing import Process,Lockdef chakan(): dic = json.load(open('piao',)) # 先查看票数，也就是打开那个文件 print('剩余票数：%s' % dic['count']) # 查看剩余的票数def buy(): dic = json.load(open('piao',)) if dic['count']&gt;0: #如果还有票 dic['count']-=1 #就修改里面的值-1 time.sleep(random.randint(1,3)) #执行里面买票的一系列操作就先不执行了，让睡一会代替（并且随机的睡） json.dump(dic,open('piao','w')) print('%s 购票成功' % os.getpid()) # 当前的那个id购票成功def task(mutex): #抢票 # 第一种加锁： # mutex.acquire() #加锁 # chakan() # 因为查看的时候大家都可以看到，不需要加锁 # buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买 # mutex.release() #取消锁 # 第二种加锁： #with表示自动打开自动释放锁 with mutex: chakan() # 因为查看的时候大家都可以看到，不需要加锁 buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买if __name__ == '__main__': mutex = Lock() for i in range(50):#让50个人去访问那个票数 p = Process(target=task,args=(mutex,)) p.start()进程池加锁还是上面抢票的例子，这次使用了进程池，由于进程之间不共享内存，所以进程之间的通信不能像线程之间直接引用，使用进程池异步对共享变量进行操作，异步操作lock锁，会引起冲突，因而需要采取一些策略来完成进程之间的数据通信。所以要引入进程Manager来完成进程间通信的方式这里举两个例子：大家自行创建 piao 文件，内容为 {“count”:320}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import jsonimport timeimport randomimport osfrom multiprocessing import Process,Lock,Pooldef chakan(): dic = json.load(open('piao',)) # 先查看票数，也就是打开那个文件 print('剩余票数：%s' % dic['count']) # 查看剩余的票数def buy(): dic = json.load(open('piao',)) if dic['count']&gt;0: #如果还有票 dic['count']-=1 #就修改里面的值-1 time.sleep(random.randint(1,3)) #执行里面买票的一系列操作就先不执行了，让睡一会代替（并且随机的睡） json.dump(dic,open('piao','w')) print('%s 购票成功' % os.getpid()) # 当前的那个id购票成功def task(mutex): #抢票 # mutex.acquire() #加锁 # chakan() # 因为查看的时候大家都可以看到，不需要加锁 # buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买 # mutex.release() #取消锁 with mutex: chakan() # 因为查看的时候大家都可以看到，不需要加锁 buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买if __name__ == '__main__': mutex = Lock() pool = Pool(20) \"\"\"进程池加锁 \"\"\" from multiprocessing import Pool, Manager, Lock manager = Manager() mutex = manager.Lock() for i in range(50):#让50个人去访问那个票数 pool.apply_async(task,args=(mutex, )) pool.close() pool.join()12345678910111213141516171819202122232425from multiprocessing import Process,Managerimport os# 这里实现的就是多个进程之间共享内存，并修改数据# 这里不需要加锁，因为manager已经默认给你加锁了def f(d,l): d[1] = '1' d['2'] = 2 d[0.25] = None l.append(os.getpid()) print(l)if __name__ == '__main__': with Manager() as manager: d = manager.dict() #生成一个字典 l = manager.list(range(5)) #生成一个列表 p_list = [] for i in range(10): p = Process(target=f,args=(d,l)) p.start() p_list.append(p) for res in p_list: res.join() print(d) print(l)","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 多线程","slug":"python-多线程","date":"2017-05-24T15:08:35.000Z","updated":"2020-04-04T11:04:52.986Z","comments":true,"path":"2017/05/24/python-多线程/","link":"","permalink":"cpeixin.cn/2017/05/24/python-%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"巴拉巴拉最近在搞爬虫项目，架构和程序设计都是由我来决定和设计，所以发挥空间还是很自由的。程序语言选择的是python，在语言选型上也考虑过golang，golang的效率和速度肯定是要比python好很多的。但是在爬虫领域，python的易用性，超多的第三方扩展库，而且python支持协程后，效率速度方面上的表现，也是很不错的。那么在python做爬虫的过程中，多线程，多进程，协程，异步等肯定都是逃不过的。所以抽出一些时间，写一写线程，进程，协程，异步等方法在爬虫中的表现。多进程和多线程我们常见的 Linux、Windows、Mac OS 操作系统，都是支持多进程的多核操作系统。所谓多进程，就是系统可以同时运行多个任务。例如我们的电脑上运行着 QQ、浏览器、音乐播放器、影音播放器等。在操作系统中，每个任务就是一个进程。每个进程至少做一件事，多数进程会做很多事，例如影音播放器，要播放画面，同时要播放声音，在一个进程中，就有很多线程，每个线程做一件事，在一个进程中有多个线程运行就是多线程。可以在实验环境终端执行 ps -ef 命令来查看当前系统中正在运行的进程。计算机的两大核心为运算器和存储器。常说的手机配置四核、八核，指的就是 CPU 的数量，它决定了手机的运算能力；128G、256G 超大存储空间，指的就是手机存储数据的能力。当我们运行一个程序来计算 3 + 5，计算机操作系统会启动一个进程，并要求运算器派过来一个 CPU 来完成任务；当我们运行一个程序来打开文件，操作系统会启动存储器的功能将硬盘中的文件数据导入到内存中。一个 CPU 在某一时刻只能做一项任务，即在一个进程（或线程）中工作，当它闲置时，会被系统派到其它进程中。单核计算机也可以实现多进程，原理是第 1 秒的时间段内运行 A 进程，其它进程等待：第 2 秒的时间段内运行 B 进程，其它进程等待。。。第 5 秒的时间段内又运行 A 进程，往复循环。当然实际上 CPU 在各个进程间的切换是极快的，在毫秒（千分之一）、微秒（百万分之一）级，以至于我们看起来这些程序就像在同时运行。现代的计算机都是多核配置，四核八核等，但计算机启动的瞬间，往往就有几十上百个进程在运行了，所以进程切换是一定会发生的，CPU 在忙不迭停地到处赶场。注意，什么时候进行进程 、线程切换是由操作系统决定的，无法人为干预。线程安全我们都知道在 MySQL 中有 “原子操作” 的概念，打个比方：韩梅向李红转账 100 块钱，在 MySQL 中需要两步操作：韩梅账户减少 100 元，李红账户增加 100 元。如果第一步操作完成后，意外情况导致第二步没有做，这是不允许发生的，如何保证其不允许发生呢？将两步操作设计成一个事务，事务里可以有多个步骤，其中任何一步出现问题，事务都将失败，前面的步骤全部回滚，就像什么事都没发生。这种操作就叫做原子操作，这种特性就叫做原子性。在 Python 多线程中，变量是共享的，这也是相较多进程的一个优点，线程占用资源要少得多，但也导致多个 CPU 同时操作多个线程时会引起结果无法预测的问题，也就是说 Python 的线程不安全。在多线程与多进程的时候，因为一般情况下都是各自完成各自的任务，各个子线程或者各个子进程之前并没有太多的联系，如果需要通信的话我会使用队列或者数据库来完成，但是最近我在写一些多线程与多进程的代码时，发现如果它们需要用到共享变量的话，需要有一些注意的地方接下来用实例讲解一下线程共享的问题标准数据类型在线程间共享下面的实例，在主线程中创建变量d,在5个子线程中引用。123456789101112131415# coding:utf-8import threadingdef test(name, data): print(\"in thread &#123;&#125; name is &#123;&#125;\".format(threading.current_thread(), name)) print(\"data is &#123;&#125; id(data) is &#123;&#125;\".format(data, id(data)))if __name__ == '__main__': d = 5 name = \"cpeixin\" for i in range(5): th = threading.Thread(target=test, args=(name, d)) th.start()下面的结果中显示，5个子线程中打印出变量d的id相同，表示引用的同一个变量，所以说明在主线程中创建了变量d，在子线程中是可以共享的，在子线程中对共享元素的改变是会影响到其它线程的，所以如果要对共享变量进行修改时，也就是线程不安全的，需要加锁。1234567891011/Users/cpeixin/venv/pythonCode/bin/python /Users/cpeixin/PycharmProjects/pythonCode/thread/variable_thread.pyin thread &lt;Thread(Thread-1, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-2, started 123145524641792)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-3, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-4, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-5, started 123145524641792)&gt; name is cpeixindata is 5 id(data) is 4304846080自定义类型对象在线程间共享如果我们要自定义一个类呢，将一个对象作为变量在子线程中传递呢？会是什么效果呢？123456789101112131415161718192021222324252627# coding:utf-8import threadingclass Data: def __init__(self, data=None): self.data = data def get(self): return self.data def set(self, data): self.data = datadef test(name, data): print(\"in thread &#123;&#125; name is &#123;&#125;\".format(threading.current_thread(), name)) print(\"data is &#123;&#125; id(data) is &#123;&#125;\".format(data.get(), id(data)))if __name__ == '__main__': d = Data(10) name = \"cpeixin\" print(\"in main thread id(data) is &#123;&#125;\".format(id(d))) for i in range(5): th = threading.Thread(target=test, args=(name, d)) th.start()1234567891011in main thread id(data) is 4348194152in thread &lt;Thread(Thread-1, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-2, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-3, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-4, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-5, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152我们看到，在主线程和子线程中，这个对象的id是一样的，说明它们用的是同一个对象。无论是标准数据类型还是复杂的自定义数据类型，它们在多线程之间是共享同一个的以上就是在多线程中，变量共享的码上说明GIL 全局解释器锁如何解决线程安全问题？CPython 解释器使用了加锁的方法。每个进程有一把锁，启动线程先加锁，结束线程释放锁。打个比方，进程是一个厂房，厂房大门是开着的，门内有锁，工人进入大门后可以在内部上锁。厂房里面有 10 个车间对应 10 个线程，每个 CPU 就是一个工人。GIL（Global Interpreter Lock）全局锁就相当于厂房规定：工人要到车间工作，从厂房大门进去后要在里面反锁，完成工作后开锁出门，下一个工人再进门上锁。也就是说，任意时刻厂房里只能有一个工人，但这样就保证了工作的安全性，这就是 GIL 的原理。当然了，GIL 的存在有很多其它益处，包括简化 CPython 解释器和大量扩展的实现。根据上面的例子可以看出 GIL 实现了线程操作的安全性，但多线程的效率被大打折扣，一个工厂里只能有一个工人干活，很难想象。这也是 David Beazley（《Python 参考手册》和《Python Cookbook》的作者）说 “Python 线程毫无用处” 的原因。注意，GIL 不是语言特性，而是解释器的设计特点，有些 Python 解释器例如 JPython 就没有 GIL ，除了 Python 其它语言也有 GIL 设计，例如 Ruby 。线程锁为什么需要线程锁?多个线程对同一个数据进行修改时， 可能会出现不可预料的情况.例如实现银行转账功能，money += 1 这句其实有三个步骤 money; money+1; money=money+1;假如这三步骤还没完成money-=1的线程就开始执行了，后果可想而知，money的值肯定时乱的如何实现线程锁?实例化一个锁对象;lock = threading.Lock()操作变量之前进行加锁lock.acquire()操作变量之后进行解锁lock.release()12345678910111213141516171819202122232425262728293031323334353637import threading# 银行存钱和取钱def add(lock): global money # 生命money为全局变量 for i in range(1000000): # 2. 操作变量之前进行加锁 lock.acquire() money += 1 # money; money+1; money=money+1; # 3. 操作变量之后进行解锁 lock.release()def reduce(lock): global money for i in range(1000000): # 2. 操作变量之前进行加锁 lock.acquire() money -= 1 # 3. 操作变量之后进行解锁 lock.release()if __name__ == '__main__': money = 0 # 1. 实例化一个锁对象; lock = threading.Lock() t1 = threading.Thread(target=add, args=(lock,)) t2 = threading.Thread(target=reduce, args=(lock,)) t1.start() t2.start() t1.join() t2.join() print(\"当前金额:\", money)多线程提高工作效率实际情况并非上面讲得那么惨，Python 多线程可以成倍提高程序的运行速度，而且在多数情况下都是有效的。接着上面的例子说，一个工厂里同一时刻只能有一个工人在工作，如果这个工厂里各个车间的自动化程度极高且任务耦合度极低，工人进去只是按几下按钮，就可以等待机器完成其余工作，那情况就不一样了，这种场景下一个工人可以管理好多个车间，而且大多数时间都是等，甚至还能抽空打打羽毛球看场电影。比如爬虫程序爬取页面数据这个场景中，CPU 做的事就是发起页面请求和处理响应数据，这两步是极快的，中间网络传输数据的过程是耗时且不占用 CPU 的。一个工人可以在吃完早饭后一分钟内快速到 1000 个车间按下发起请求的按钮，吃完午饭睡一觉，日薄西山时差不多收到网络传回的数据，又用一分钟处理数据，整个程序完成。上面的场景中，CPU 再多也没有用处，一个 CPU 抽空就能完成整个任务了，毕竟程序中需要 CPU 做的事并不多。这就涉及复杂程序的分类：CPU 密集型和 IO 密集型。爬虫程序就是 IO 密集型程序。CPU 密集型程序全是手工操作，工人一刻也不能停歇，这种情况下 Python 多线程就真可以说是毫无用处了。我们可以使用 time.sleep 方法模拟 IO 操作来写一段程序证明多线程可以提高程序的运行效率：12345678910111213141516171819202122232425262728293031323334353637# File Name: thread.pyimport threadingimport timeimport requestsdef crawl_url(): # 假设这是爬虫程序，爬取一个 URL time.sleep(0.02) # 模拟 IO 操作 # res = requests.get(\"http://www.ip111.cn\").status_code # print(res)def main1(): # 单线程程序 for i in range(100): crawl_url()def main2(): # 多线程程序 thread_list = [] for i in range(100): t = threading.Thread(target=crawl_url) t.start() thread_list.append(t) for t in thread_list: t.join()if __name__ == '__main__': start = time.time() main1() end = time.time() print('单线程耗时：&#123;:.4f&#125;s'.format(end - start)) start = time.time() main2() end = time.time() print('多线程耗时：&#123;:.4f&#125;s'.format(end - start))12单线程耗时：2.4027s多线程耗时：0.0323s理论上，main1 的耗时是 main2 的 100 倍，考虑到 main2 创建多线程、线程切换的开销，这个结果也是相当可观的，IO 操作耗时越长，多线程的威力越大。线程池ThreadPool在使用多线程处理任务时也不是线程越多越好，由于在切换线程的时候，需要切换上下文环境，依然会造成cpu的大量开销。为解决这个问题，线程池的概念被提出来了。预先创建好一个较为优化的数量的线程，让过来的任务立刻能够使用，就形成了线程池。1234567891011121314# coding: utf-8from concurrent.futures import ThreadPoolExecutorimport timedef spider(page): time.sleep(page) print(f\"crawl task&#123;page&#125; finished\") return pagewith ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 task1 = t.submit(spider, 1) task2 = t.submit(spider, 2) # 通过submit提交执行的函数到线程池中 task3 = t.submit(spider, 3)使用 with 语句 ，通过 ThreadPoolExecutor 构造实例，同时传入 max_workers 参数来设置线程池中最多能同时运行的线程数目。使用 submit 函数来提交线程需要执行的任务到线程池中，并返回该任务的句柄（类似于文件、画图），注意 submit() 不是阻塞的，而是立即返回。multiprocessing.dummy.Poolmultiprocessing.dummy.Pool 是个什么东东？看起来像一个进程池的样子啊～～看了源码中的代码和注释123456789# Support for the API of the multiprocessing package using threads## multiprocessing/dummy/__init__.pyclass DummyProcess(threading.Thread)def Pool(processes=None, initializer=None, initargs=()): from ..pool import ThreadPool return ThreadPool(processes, initializer, initargs)这只是是以multiprocessing相同API实现的多线程模块。继承了Thread，内部是封装调用了ThreadPool，使用起来更加方便。异步和同步，阻塞和非阻塞上文的模拟爬虫示例代码中，main1 中的 for 循环运行 100 次爬取网页的操作，前一个完成后才能运行下一个，这就是同步的概念，在 crawl_url 函数内部的 IO 操作为阻塞操作，线程无法向下执行。main2 中的第一个 for 循环，_创建 100 个线程并启动，这步操作是非阻塞的_，不会等一个线程运行完成才创建下一个线程，它会一气儿创建 100 个线程；第二个 for 循环将主线程挂起，直到全部子线程完成，此时的主线程就是阻塞的。这种程序运行方式叫做异步，CPU 在遇到 IO 阻塞时不会站在那儿傻等，而是被操作系统派往其它线程中看看有什么事可做。所谓的异步，就是 CPU 在当前线程阻塞时可以去其它线程中工作，不管怎么设计，在一个线程内部代码都是顺序执行的，遇到 IO 都得阻塞，所谓的非阻塞，是遇到当前线程阻塞时，CPU 去其它线程工作。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"Spark - RDD讲解","slug":"Spark-RDD讲解","date":"2017-03-11T22:41:50.000Z","updated":"2020-04-12T06:24:14.815Z","comments":true,"path":"2017/03/12/Spark-RDD讲解/","link":"","permalink":"cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"What is RDD在讲RDD之前，先和大家说一下在Spark中，我们分析数据的过程，主要会碰到RDD，DataFrame，DataSet概念，这三种都是我们计算过程中的数据单元。在Spark 2.0之前，主要的数据操作都是操作RDD，而在Spark 2.0以后，官方开始呼吁大家迁移到基于DataSet，即使你是从Spark 2.0以后开始接触的spark，但是我也很负责人的告诉你，RDD你必须要懂～～定义弹性分布式数据集（Resilient Distributed Datasets : RDD），表示已被分区、不可变的，并能够被并行操作，可同错的数据集合。针对上面的定义，还说描述的很抽象，接下来根据每个RDD的属性，进行逐点说明分区分区这个概念在分布式计算中我们经常会看到，例如MapReduce中，数据在Map端写入到环形缓冲区，数据进行分区，reduce端读取相应的分区文件，还有在Kafka中，topic中的分区概念。在RDD中，本质上是一个只读的分区记录集合。也就是我们要处理的源数据的抽象。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同节点上，从而可以在集群中的不同节点上进行并行计算。这也是它可以被并行处理的前提。反向来思考的话，如果RDD不可分区，只是一个单独不可拆分的数据块，那么集群中的节点怎么对这个源数据进行分布式并行计算呢？逻辑上，我们可以认为 RDD 是一个大的数组。数组中的每个元素可以代表一个分区（Partition）。在物理存储中，每个分区指向一个存放在堆内内存和堆外内存或者磁盘中的数据块（Block），而这些数据块是独立的，它们可以被存放在系统中的不同节点。所以，RDD 只是抽象意义的数据集合，分区内部并不会存储具体的数据，仅保存了元数据信息。下图很好地展示了 RDD 的分区逻辑结构：RDD 中的每个分区存有它在该 RDD 中的 index。通过 RDD 的 ID 和分区的 index 可以唯一确定对应数据块的编号，从而通过底层存储层的接口中提取到数据进行处理。在集群中，各个节点上的数据块会尽可能地存放在内存中，只有当内存没有空间时才会存入硬盘。这样可以最大化地减少硬盘读写的开销。虽然 RDD 内部存储的数据是只读的，但是，我们可以去修改（例如通过 repartition 转换操作）并行计算单元的划分结构，也就是分区的数量。不可变性不可变性代表每一个 RDD 都是只读的，它所包含的分区信息不可以被改变。既然已有的 RDD 不可以被改变，我们只可以对现有的 RDD 进行转换（Transformation）操作，得到新的 RDD 作为中间计算的结果。上图也就是刚刚提到的针对RDD的Transformation操作中，包括的map算子，flatMap算子，filter算子，这些算子我们接下来的文章在一一讲解，这里我想给大家看的是，在举例的这三个算子实现方法中，我们可以看到都new MapPartitionsRDD（），也就是说，在对一个已有的RDD进行转换操作的过程中，并不是对这个RDD进行直接的修改，变换，而是读取父RDD，创建了一个新的RDD进行转换并且最后返回。那么这样会带来什么好处呢？显然，对于代表中间结果的 RDD，我们需要记录它是通过哪个 RDD 进行哪些转换操作得来，即依赖关系，而不用立刻去具体存储计算出的数据本身。这样做有助于提升 Spark 的计算效率，并且使错误恢复更加容易。试想，在一个有 N 步的计算模型中，如果记载第 N 步输出 RDD 的节点发生故障，数据丢失，我们可以从第 N-1 步的 RDD 出发，再次计算，而无需重复整个 N 步计算过程。这样的容错特性也是 RDD 为什么是一个“弹性”的数据集的原因之一。后边我们会提到 RDD 如何存储这样的依赖关系。并行操作由于单个 RDD 的分区特性，使得它天然支持并行操作，即不同节点上的数据可以被分别处理，然后产生一个新的 RDD。容错性为了保证RDD 中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。 相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的 特定数据转换（Transformation）操作（filter, map, join etc.)行为。当这个RDD的部分分区数据丢失时 ，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制 了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。另外，在RDD计算中，也通过checkpoint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是 logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通 过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算 生成丢失的分区数据。RDD 五大特性A list of partitionsRDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。A function for computing each split一个函数计算每一个分片，RDD的每个partition上面都会有function，也就是函数应用A list of dependencies on other RDDsRDD会记录它的依赖 ，依赖还具体分为宽依赖和窄依赖，RDD在计算的过程中，不断的转换，在内存中，不落地磁盘，如果某一环节出错，可以根据依赖来找回上一状态的RDD，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。Optionally,a Partitioner for Key-value RDDs可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面Optionally, a list of preferred locations to compute each split on我们的原则是移动计算，不移动数据，默认的是，磁盘中的数据是作为RDD加载到本机的内存中，但是，Spark这里给出了一个可选项，可以选择加载到指定的机器内存中，就是可以选择将数据放在那几台性能好的节点上RDD 结构通过上述讲解，我们了解了 RDD 的基本特性。而且，我们还提到每一个 RDD 里都会包括分区信息、所依赖的父 RDD 以及通过怎样的转换操作才能由父 RDD 得来等信息。实际上 RDD 的结构远比你想象的要复杂，让我们来看一个 RDD 的简易结构示意图：SparkContext 是所有 Spark 功能的入口，它代表了与 Spark 节点的连接，可以用来创建 RDD 对象以及在节点中的广播变量等。一个线程只有一个 SparkContext。SparkConf 则是一些参数配置信息。感兴趣的同学可以去阅读官方的技术文档，一些相对不重要的概念我就不再赘述了。Partitions 前文中我已经提到过，它代表 RDD 中数据的逻辑结构，每个 Partition 会映射到某个节点内存或硬盘的一个数据块。Partitioner 决定了 RDD 的分区方式，目前有两种主流的分区方式：Hash partitioner 和 Range partitioner。Hash，顾名思义就是对数据的 Key 进行散列分区，Range 则是按照 Key 的排序进行均匀分区。此外我们还可以创建自定义的 Partitioner。依赖关系Dependencies 是 RDD 中最重要的组件之一。如前文所说，Spark 不需要将每个中间计算结果进行数据复制以防数据丢失，因为每一步产生的 RDD 里都会存储它的依赖关系，即它是通过哪个 RDD 经过哪个转换操作得到的。细心的读者会问这样一个问题，父 RDD 的分区和子 RDD 的分区之间是否是一对一的对应关系呢？Spark 支持两种依赖关系：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）。窄依赖就是父 RDD 的分区可以一一对应到子 RDD 的分区，宽依赖就是父 RDD 的每个分区可以被多个子 RDD 的分区使用。显然，窄依赖允许子 RDD 的每个分区可以被并行处理产生，而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理。宽依赖本质就是shuffle,计算代价大,经过大量shuffle生成的RDD，建议进行缓存。这样避免失败后重新计算带来的开销如上图所示，一些转换操作如 map、filter 会产生窄依赖关系，而 Join、groupBy 则会生成宽依赖关系。这很容易理解，因为 map 是将分区里的每一个元素通过计算转化为另一个元素，一个分区里的数据不会跑到两个不同的分区。而 groupBy 则要将拥有所有分区里有相同 Key 的元素放到同一个目标分区，而每一个父分区都可能包含各种 Key 的元素，所以它可能被任意一个子分区所依赖。Spark 之所以要区分宽依赖和窄依赖是出于以下两点考虑：窄依赖可以支持在同一个节点上链式执行多条命令，例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。从失败恢复的角度考虑，窄依赖的失败恢复更有效，因为它只需要重新计算丢失的父分区即可，而宽依赖牵涉到 RDD 各级的多个父分区。DAG结合上篇spark运行原理和这篇RDD的讲述，我们来讲一下关于任务运行中，Job，Stage，Task的划分spark任务运行中，会存在一个或者多个Job，action算子的触发会生成一个Job, Job会提交给DAGScheduler,分解成Stage。上图是一个job被切割成三个Stage。job分割stage的规则是从G端向前开始分割，遇到宽依赖，就分割一个stage.。F–&gt;G 切割 stage2 和 stage3A–&gt;B stage1上图，程序的运行最小单元是Task，就拿stage2来举例：stage2有4个Task: C端有2个partition，E端有两个partition。每个partition为开始，最终到F端，作为一个Task。 其中B阶段呢，属于一个程序级别的优化操作。一般分布式程序中，为了让程序能平稳的执行，就要做一些优化操作。在以后的Spark开发中，我们在Web UI中会经常看到类似于下图的工作流程，这里展示了一个Job的划分和对应操作的细节。How use RDD上面讲了很长篇幅的RDD概念和属性，那么我们该如何开始实操RDD呢？我们的第一个RDD来自哪里？上面我说过了，其实RDD就是我们要进行处理的源数据集合。在实际的业务场景中，对于离线数据分析，大多数的场景下，源数据可以是Spark从Hive、HBase中读取，转化成RDD，再细小一点的场景，源数据可以是一个csv报表数据，读取目标CSV进行RDD的转换。那么在下面的讲解中，我们无需对接上游生产环境的数据源，我们可以在IDEA中直接进行RDD的创建，随后再进行各种转换算子和行动算子的操作演示创建RDDRDD的创建有三种方法利用内存中集合中创建利用外部文件创建由其他RDD创建新的RDD从集合中创建可以使用parallelize() 和 makeRDD()方法创建我们可以看到，这两个方法需要我们传入的参数是 Seq[T] 集合，返回的都是RDD[T]。注意，makeRDD给了两种方法，这里先记为第一种makeRDD和第二种makeRDD这里，先把源码po出来可以清晰的看到，第一种makeRDD的源码实现中，实际上是调用了parallelize（），并且都可以指定分区数量，而且在注释中清晰的写明了，第一种makeRDD和parallelize是identical（完全相同的）。再来看第二种makeRDD，第二种实现可以为数据提供位置信息，并且不能指定RDD的分区数量，除此之外的实现和parallelize函数也是一致的创建示例：123456789val make_rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))val make_rdd_1: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\"))val make_rdd_2: RDD[Int] = sc.makeRDD(List(1,2,3,4),2)val make_rdd_3: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\"),4)println(make_rdd.partitions.size)println(make_rdd_1.partitions.size)println(make_rdd_2.partitions.size)println(make_rdd_3.partitions.size)结果：12341124**外部文件创建****1234sc.textFile(\"hdfs://hadoop000:9000/xxx/data.txt\") // 读取hdfs文件sc.textFile(\"data.txt\") // 这里纯粹的本地文件是不推荐的， 因为这个文件访问是针对每一个Worker都要是能访问的 换言之,如果是本地文件,则必须保证每一个Worker的本地都有一份这个文件RDD 的转换操作RDD 的数据操作分为两种：转换（Transformation）和动作（Action）。顾名思义，转换是用来把一个 RDD 转换成另一个 RDD，而动作则是通过计算返回一个结果。下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）Transformation（转换）Meaning（含义）map(func)返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中的元素应用一个函数 func 来生成。filter(func)返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中应用一个函数 func 且返回值为 true 的元素来生成。flatMap(func)与 map 类似，但是每一个输入的 item 可以被映射成 0 个或多个输出的 items（所以 func 应该返回一个 Seq 而不是一个单独的 item）。mapPartitions(func)与 map 类似，但是单独的运行在在每个 RDD 的 partition（分区，block）上，所以在一个类型为 T 的 RDD 上运行时 func 必须是 Iterator=&gt; Iterator 类型。mapPartitionsWithIndex(func)与 mapPartitions 类似，但是也需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 func_，所以在一个类型为 T 的 RDD 上运行时 _func 必须是 (Int, Iterator) =&gt; Iterator 类型。sample(withReplacement, fraction, seed)样本数据，设置是否放回（withReplacement），采样的百分比（_fraction_）、使用指定的随机数生成器的种子（seed）。union(otherDataset)返回一个新的 dataset，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的并集。intersection(otherDataset)返回一个新的 RDD，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的交集。distinct([_numTasks_]))返回一个新的 dataset，它包含了 source dataset（源数据集）中去重的元素。groupByKey([_numTasks_])在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable) .Note: 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 reduceByKey 或 aggregateByKey 来计算性能会更好.Note: 默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 numTasks 参数来设置不同的任务数。reduceByKey(func, [_numTasks_])在 (K, V) pairs 的 dataset 上调用时，返回 dataset of (K, V) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的函数 func 来进行聚合的，它必须是 type (V,V) =&gt; V 的类型。像 groupByKey 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。aggregateByKey(zeroValue)(seqOp, combOp, [_numTasks_])在 (K, V) pairs 的 dataset 上调用时，返回 (K, U) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的 combine 函数以及一个 neutral “0” 值来进行聚合的。允许聚合值的类型与输入值的类型不一样，同时避免不必要的配置。像 groupByKey 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。sortByKey([_ascending_], [_numTasks_])在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset，由 boolean 类型的 ascending 参数来指定。join(otherDataset, [_numTasks_])在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 leftOuterJoin, rightOuterJoin 和 fullOuterJoin 来实现。cogroup(otherDataset, [_numTasks_])在一个 (K, V) 和的 dataset 上调用时，返回一个 (K, (Iterable, Iterable)) tuples 的 dataset。这个操作也调用了 groupWith。cartesian(otherDataset)在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) pairs 类型的 dataset（所有元素的 pairs，即笛卡尔积）。pipe(command, [envVars])通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回。coalesce(numPartitions)Decrease（降低）RDD 中 partitions（分区）的数量为 numPartitions。对于执行过滤后一个大的 dataset 操作是更有效的。repartition(numPartitions)Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀。该操作总是通过网络来 shuffles 所有的数据。repartitionAndSortWithinPartitions(partitioner)根据给定的 partitioner（分区器）对 RDD 进行重新分区，并在每个结果分区中，按照 key 值对记录排序。这比每一个分区中先调用 repartition 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行。下表列出了一些 Spark 常用的 actions 操作。详细请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）。Action（动作）Meaning（含义）reduce(func)使用函数 func 聚合 dataset 中的元素，这个函数 func 输入为两个元素，返回为一个元素。这个函数应该是可交换（commutative）和关联（associative）的，这样才能保证它可以被并行地正确计算。collect()在 driver 程序中，以一个 array 数组的形式返回 dataset 的所有元素。这在过滤器（filter）或其他操作（other operation）之后返回足够小（sufficiently small）的数据子集通常是有用的。count()返回 dataset 中元素的个数。first()返回 dataset 中的第一个元素（类似于 take(1)。take(n)将数据集中的前 n 个元素作为一个 array 数组返回。takeSample(withReplacement, num, [_seed_])对一个 dataset 进行随机抽样，返回一个包含 num 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。takeOrdered(n, [ordering])返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 n 个元素。saveAsTextFile(path)将 dataset 中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录。saveAsSequenceFile(path)(Java and Scala)将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int，Double，String 等等)。saveAsObjectFile(path)(Java and Scala)使用 Java 序列化（serialization）以简单的格式（simple format）编写数据集的元素，然后使用 SparkContext.objectFile() 进行加载。countByKey()仅适用于（K,V）类型的 RDD。返回具有每个 key 的计数的（K , Int）pairs 的 hashmap。foreach(func)对 dataset 中每个元素运行函数 _func_。这通常用于副作用（side effects），例如更新一个 Accumulator（累加器）或与外部存储系统（external storage systems）进行交互。Note：修改除 foreach()之外的累加器以外的变量（variables）可能会导致未定义的行为（undefined behavior）。详细介绍请阅读 Understanding closures（理解闭包） 部分。该 Spark RDD API 还暴露了一些 actions（操作）的异步版本，例如针对 foreach 的 foreachAsync，它们会立即返回一个FutureAction 到调用者，而不是在完成 action 时阻塞。这可以用于管理或等待 action 的异步执行。部分Transformation算子操作：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package rddimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object transformation_func &#123; def main(args: Array[String]): Unit =&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"transformation_func\") .setMaster(\"local\") val sc = new SparkContext(sparkConf) var original_rdd: RDD[(String, Int)] = sc.parallelize(Array((\"a\", 1), (\"b\", 1), (\"a\", 2),(\"c\",4),(\"c\",4)),2) var map_rdd: RDD[(String, Int)] = original_rdd.map(x =&gt;(x._1,x._2+1)) println(\"map操作：对original_rdd每个数据的第二个元素+1\") map_rdd.foreach(println) println(\"filter操作：过滤掉original_rdd中，第一个元素不为a的数据\") var filter_rdd: RDD[(String, Int)] = original_rdd.filter(x =&gt; x._1 == \"a\") filter_rdd.foreach(println) println(\"flatmap操作：对original_rdd做映射扁平化操作\") val flatmap_rdd: RDD[Char] = original_rdd.flatMap(x=&gt; x._1 + x._2) flatmap_rdd.foreach(println) println(\"mapPartitions操作：对original_rdd每个分区做相应操作\") val mapPartitions_rdd: RDD[(String, Int)] = original_rdd.mapPartitions(x=&gt;&#123;x.map(item=&gt;(item._1,item._2+1))&#125;) mapPartitions_rdd.foreach(println) println(\"sample操作：提取样本\") val sample_rdd: RDD[(String, Int)] = original_rdd.sample(true, 0.25) sample_rdd.foreach(println) println(\"distinct操作：去重\") val distinct_rdd: RDD[(String, Int)] = original_rdd.distinct() distinct_rdd.foreach(println) println(\"groupbykey操作：分组聚合\") val groupByKey_rdd: RDD[(String, Iterable[Int])] = original_rdd.groupByKey() groupByKey_rdd.foreach(println) println(\"reduceByKey操作：聚合\") val reduceByKey_rdd: RDD[(String, Int)] = original_rdd.reduceByKey(_+_) reduceByKey_rdd.foreach(println) println(\"sortByKey操作：排序\") val sortByKey_rdd: RDD[(String, Int)] = original_rdd.sortByKey() sortByKey_rdd.foreach(println) &#125;&#125;RDD 的持久化（缓存）每当我们对 RDD 调用一个新的 action 操作时，整个 RDD 都会从头开始运算。因此，如果某个 RDD 会被反复重用的话，每次都从头计算非常低效，我们应该对多次使用的 RDD 进行一个持久化操作。Spark 的 persist() 和 cache() 方法支持将 RDD 的数据缓存至内存或硬盘中，这样当下次对同一 RDD 进行 Action 操作时，可以直接读取 RDD 的结果，大幅提高了 Spark 的计算效率。12345678rdd = sc.parallelize([1, 2, 3, 4, 5])rdd1 = rdd.map(lambda x: x+5)rdd2 = rdd1.filter(lambda x: x % 2 == 0)rdd2.persist()count = rdd2.count() // 3first = rdd2.first() // 6rdd2.unpersist()在文中的代码例子中你可以看到，我们对 RDD2 进行了多个不同的 action 操作。由于在第四行我把 RDD2 的结果缓存在内存中，所以 Spark 无需从一开始的 rdd 开始算起了（持久化处理过的 RDD 只有第一次有 action 操作时才会从源头计算，之后就把结果存储下来，所以在这个例子中，count 需要从源头开始计算，而 first 不需要）。在缓存 RDD 的时候，它所有的依赖关系也会被一并存下来。所以持久化的 RDD 有自动的容错机制。如果 RDD 的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算。持久化可以选择不同的存储级别。正如我们讲 RDD 的结构时提到的一样，有 MEMORY_ONLY，MEMORY_AND_DISK，DISK_ONLY 等。cache() 方法会默认取 MEMORY_ONLY 这一级别。RDD CheckpointCheckpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，但是在正常生产环境下放在 HDFS 上，这就天然的借助HDFS 高可靠的特征来完成最大化的可靠的持久化数据的方式。在进行 RDD 的 Checkpoint 的时候，其所依赖的所有 RDD 都会清空掉；官方建议如果要进行 checkpoint 时，必需先缓存在内存中。但实际可以考虑缓存在本地磁盘上或者是第三方组件，e.g. Taychon 上。在进行 checkpoint 之前需要通过 SparkConetxt 设置 checkpoint 的文件夹作为最佳实践，一般在进行 checkpoint 方法调用前都要进行 persists 来把当前 RDD 的数据持久化到内存或者是磁盘上，这是因为 checkpoint 是 lazy 级别，必需有 Job 的执行且在Job 执行完成后才会从后往前回溯哪个 RDD 进行了Checkpoint 标记，然后对该标记了要进行 Checkpoint 的 RDD 新启动一个Job 执行具体 Checkpoint 的过程Why use RDD首先，它的数据可以尽可能地存在内存中，从而大大提高的数据处理的效率；其次它是分区存储，所以天然支持并行处理；而且它还存储了每一步骤计算结果之间的依赖关系，从而大大提升了数据容错性和错误恢复的正确率，使 Spark 更加可靠。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 运行原理","slug":"Spark-运行原理","date":"2017-03-10T13:41:50.000Z","updated":"2020-04-10T03:22:48.060Z","comments":true,"path":"2017/03/10/Spark-运行原理/","link":"","permalink":"cpeixin.cn/2017/03/10/Spark-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/","excerpt":"","text":"spark的运行原理对于spark的学习尤为重要，如果不了解其运行原理，也就不会从根本上将spark的程序写好。这是一篇spark理论的文章，如果这篇文章并不能让你理解，可以先从实战入手，等你能简单的用spark写一个word count，需要提交任务在本地或者扔到服务器上的时候，再回来看看这篇文章，也许理解的程度就会更高一些。运行架构运行架构12345释义：• Cluster Manager：控制整个集群，监控worker。在standalone模式中即为Master主节点，在YARN模式中为ResourceManager• Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。• Driver： 运行Application 的main()函数• Executor：执行器，是为某个Application运行在worker node上的一个进程,Executor中有线程池任务运行流程构建Spark Application的运行环境，启动SparkContextSparkContext向资源管理器（Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackendExecutor向SparkContext申请TaskSparkContext将应用程序分发给ExecutorSparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行Task在Executor上运行，运行完释放所有资源运行特点**每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行Task。这种Application隔离机制是有优势的，无论是从调度角度看（每个Driver调度他自己的任务），还是从运行角度看（来自不同Application的Task运行在不同JVM中），当然这样意味着Spark Application不能跨应用程序共享数据，除非将数据写入外部存储系统Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了提交SparkContext的Client应该靠近Worker节点（运行Executor的节点），最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换Task采用了数据本地性和推测执行的优化机制释义：Application: Appliction都是指用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码Driver: Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用SparkContext代表DriverExecutor: 某个Application运行在worker节点上的一个进程， 该进程负责运行某些Task， 并且负责将数据存到内存或磁盘上，每个Application都有各自独立的一批Executor， 在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象， 负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task， 这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型Standalone : spark原生的资源管理，由Master负责资源的分配Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架Hadoop Yarn: 主要是指Yarn中的ResourceManagerWorker: 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NodeManager节点Task: 被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责Job: 包含多个Task组成的并行计算，往往由Spark Action触发生成， 一个Application中往往会产生多个JobStage: 每个Job会被拆分成多组Task， 作为一个TaskSet， 其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方DAGScheduler: 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图)，并提交Stage给TASkScheduler。 其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法TASKSedulter: 将TaskSET提交给worker运行，每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet，当Executor向Driver发生心跳时，TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行标签，重试失败的Task。下图展示了DAGScheduler， TaskScheduler的作用Job=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency1234在不同运行模式中任务调度器具体为： a. Spark on Standalone模式为TaskScheduler b. YARN-Client模式为YarnClientClusterScheduler c. YARN-Cluster模式为YarnClusterScheduler将这些术语串起来的运行层次图如下：![512251993-5b043bbda8a04_articlex.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586437820952-5268cd17-13cb-41d9-ba92-5bb1208b34b7.png#align=left&display=inline&height=449&name=512251993-5b043bbda8a04_articlex.png&originHeight=449&originWidth=559&size=113090&status=done&style=none&width=559) ### ### 运行模式Spark的运行模式多种多样，灵活多变，部署在单机上时，既可以用本地模式运行，也可以用伪分布模式运行，而当以分布式集群的方式部署时，也有众多的运行模式可供选择，这取决于集群的实际情况，底层的资源调度即可以依赖外部资源调度框架，也可以使用Spark内建的Standalone模式。对于外部资源调度框架的支持，目前的实现包括相对稳定的Mesos模式，以及hadoop YARN模式本地模式：常用于本地开发测试，本地还分别 local 和 local clusterstandalone: 独立集群运行模式**Standalone模式使用Spark自带的资源调度框架，采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA。框架结构图如下:该模式主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的Yarn模式运行：Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-ClusterYarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://xxxx:4040访问，而YARN通过http:// xxxx:8088访问YARN-client的工作流程步骤为：YARN-cluster的工作流程步骤Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己Spark Client 和 Spark Cluster的区别理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 初识","slug":"Spark-初识","date":"2017-03-09T11:27:07.000Z","updated":"2020-04-10T03:22:37.781Z","comments":true,"path":"2017/03/09/Spark-初识/","link":"","permalink":"cpeixin.cn/2017/03/09/Spark-%E5%88%9D%E8%AF%86/","excerpt":"","text":"这篇分享从介绍Spark开始，算是入门前的入门，不涉及原理，不涉及技术，但是却很重要，重要的是理解Spark是什么？用来做什么？为什么要选择Spark?这将对以后工程师们在选择和使用Spark的时候，更加能知道，应该怎样的去用Spark，在什么样的场景下去使用什么样的Spark组件，来发挥它的价值。What - SparkSpark是一个通用数据处理引擎。适用于各种环境 ，这里介绍一下，主要应用于两种最常见的场景离线场景：可以是时间为维度，几年的数据集，或者是业务为维度，某个领域的大数据集，这种数据可以我们一般叫做离线数据，或者冷数据。实时场景：网站埋点，实时从前端页面传输过来的数据，或者业务系统，物理硬件实时传输过来的数据，硬件信号或者图像数据，我们需要实时的去计算处理并且返回结果。应用程序开发人员和数据科学家将Spark纳入其应用程序，以便快速查询，分析和转换数据。所以与Spark关联最频繁的任务包括跨大型数据集的交互式查询，处理来自传感器或金融系统的流数据以及机器学习任务。Spark于2009年开始在加利福尼亚大学伯克利分校的AMPLab项目中生活。更具体地说，它是由于需要证明Meso的概念而诞生的，这也是在AMPLab中创建的。（科普：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核。）从一开始，Spark就被优化成在内存中运行。它比Hadoop的MapReduce等替代方法更快速地处理数据，这往往会在每个处理阶段之间向计算机硬盘驱动器写入数据。Spark的支持者声称，Spark在内存中的运行速度比Hadoop MapReduce快100倍，而且在处理基于Hadoop MapReduce本身的磁盘数据时速度也快了10倍。这种比较并不完全公平，这不仅仅是因为对于Spark的典型用例，原始速度往往比批处理更重要，在这种情况下类似MapReduce的解决方案仍然非常出色。基于Hadoop基于YARN的体系结构为Spark和其他应用程序共享通用集群和数据集提供了基础，同时确保一致的服务和响应级别。下图是构成Spark的生态系统，强大的生态系统：其中 Spark Core是Spark的核心API，并且支持Scala,Python,Java编程语言，R,SQL分析语言,在以Spark Core为基础之上，有着Spark SQL，Spark Streaming，Spark Mlib,Spark Graphx四个亲儿子。这四个组件，我将在之后的文章详细的介绍，并且会从应用和代码示例来讲解。Spark do WhatSpark能够一次处理数PB的数据，分布在数千个协作的物理或虚拟服务器集群中。（科普：什么是分布式计算？所谓分布式计算是一门计算机科学，它研究如何把一个需要非常巨大的计算能力才能解决的问题分成许多小的部分，然后把这些部分分配给许多计算机进行处理，最后把这些计算结果综合起来得到最终的结果。分布式网络存储技术是将数据分散的存储于多台独立的机器设备上。分布式网络存储系统采用 可扩展的系统结构，利用多台存储服务器分担存储负荷，利用位置服务器定位存储信息，不但解决了传统集中式存储系统中单存储服务器的瓶颈问题，还提高了系统的可靠性、可用性和扩展性。）它有一套广泛的开发者库和API，并且支持Java，Python，R和Scala等语言; (现在在写spark应用程序时，最长使用的是Scala语言，因为spark的源码就是用scala来编写的，再其次就是python语言，python的第三方库很多，节省了程序员很多的时间要去自己实现某些功能，这两个语言的语法都很简洁，上手简单，支持函数式编程)它的灵活性使其非常适合于各种用例。Spark通常与Hadoop的数据存储模块HDFS一起使用，但它也可以与HBase，Cassandra，MapR-DB，MongoDB和Amazon S3 等其他流行的数据存储子系统集成，并且可以和Kafka，Flume等数据传输队列和数据采集工具一起搭配使用。Who Use SparkSpark是为数据科学设计的，其抽象使数据科学变得更加简单。数据科学家通常使用机器学习 - 一套可以从数据中学习的技术和算法。这些算法通常是迭代的，Spark将数据集缓存在内存中的能力大大加快了迭代数据处理速度，使得Spark成为实现这种算法的理想处理引擎。Spark是为大数据工程师设计的，在强大的计算能力和优秀的架构设计面前，可以让数据工程师在不管是离线情景下还是实时的业务需求下，都可以放心的选择使用Spark,一次读取，并行化处理，对数据集支持容错，操作灵活性，第三方社区的积极支持，虽然Spark还面对着缺点，但我相信Spark的明天会更好。Why Use Spark在技术不断高速更迭的程序圈，一个新工具的出现与流行，必然是因为它满足了很大一部分人长期未被满足的需求，或是解决了一个长期让很多人难受的痛点。这里就不能不提MapReduce了，既然已经有了看似很成熟的 Hadoop 和 MapReduce，为什么我们还需要 Spark？MapReduce 被硅谷一线公司淘汰的两大主要原因：高昂的维护成本、时间性能“达不到”用户的期待。除此之外，MapReduce 模型的抽象层次低，大量的底层逻辑都需要开发者手工完成。只提供 Map 和 Reduce 两个操作。在 Hadoop 中，每一个 Job 的计算结果都会存储在 HDFS 文件存储系统中，所以每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟。由于这一原因，MapReduce 对于迭代算法的处理性能很差，而且很耗资源。因为迭代的每一步都要对 HDFS 进行读写，所以每一步都需要差不多的等待时间。第四，只支持批数据处理，欠缺对流数据处理的支持。因此，在 Hadoop 推出后，有很多人想办法对 Hadoop 进行优化，其中发展到现在最成熟的就是 Spark。选择Spark有很多原因，但三个关键：简单性：Spark的功能可以通过一组丰富的API来访问，所有这些都是专门为大规模数据快速轻松地交互而设计的。这些API都有详细的文档和结构，使数据科学家和应用程序开发人员能够快速地将Spark工作。速度：Spark是为速度而设计的，可以在内存和磁盘上运行。来自Databricks的团队使用Spark在2014年Daytona Grey Sort 100TB Benchmark挑战赛中与加利福尼亚大学圣地亚哥分校的一队队员并列第一名。挑战包括处理静态数据集; Databricks团队能够在23分钟内处理存储在固态硬盘上的100TB的数据，而之前的获胜者通过使用Hadoop和不同的集群配置需要72分钟的时间。在支持存储在内存中的数据的交互式查询时，Spark可以执行得更好。在这种情况下，有人声称Spark可以比Hadoop的MapReduce快100倍。关于速度来好好讲解一下，Spark以速度为出名，所以要把Spark为什么这么快来聊一聊由于 Spark 可以把迭代过程中每一步的计算结果都缓存在内存中，所以非常适用于各类迭代算法。Spark 第一次启动时需要把数据载入到内存，之后的迭代可以直接在内存里利用中间结果做不落地的运算。所以，后期的迭代速度快到可以忽略不计。在当今机器学习和人工智能大热的环境下，Spark 无疑是更好的数据处理引擎。下图是在 Spark 和 Hadoop 上运行逻辑回归算法的运行时间对比。在任务（task）级别上，Spark 的并行机制是多线程模型，而 MapReduce 是多进程模型。多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间。而 Spark 同一节点上的任务以多线程的方式运行在一个 JVM 进程中，可以带来更快的启动速度、更高的 CPU 利用率，以及更好的内存共享。从前文中你可以看出，Spark 作为新的分布式数据处理引擎，对 MapReduce 进行了很多改进，使得性能大大提升，并且更加适用于新时代的数据处理场景。支持：Spark支持一系列编程语言，包括Java，Python，R和Scala。尽管通常与HDFS密切相关，但Spark还包括对Hadoop生态系统及其以后的许多领先存储解决方案的紧密集成的本地支持。此外，Apache Spark社区是大型的，活跃的和国际性的。包括Databricks，IBM以及所有主要Hadoop供应商在内的不断增长的商业提供商为Spark解决方案提供全面的支持。Spark Use Case随着 Apache Spark的发展势头继续增长，几乎所有一站式大数据平台都早已集成Spark,国外最为著名的CDH,HDP，国内的TDH等，所有行业用于实际应用。正在使用Spark来改善他们的业务，通过检测模式和提供可操作的洞察力来推动组织变革，并开始改变生活的某些方面。下面提供了一些从保险到互联网公司如何使用Spark的例子：保险行业： 通过使用Spark的机器学习功能来处理和分析所有索赔，优化索赔报销流程。医疗保健： 使用Spark Core，Streaming和SQL构建病人护理系统。零售业 ： 使用Spark分析销售点数据和优惠券使用情况。互联网 ： 使用Spark的ML功能来识别虚假的配置文件，并增强他们向客户展示的产品匹配。银行业 ： 使用机器学习模型来预测某些金融产品的零售银行客户的资料。政府 ： 分析地理，时间和财政支出。科学研究 ： 通过时间，深度，地理分析地震事件来预测未来的事件。投资银行 ： 分析日内股价以预测未来的价格走势。地理空间分析： 按时间和地理分析Uber旅行，以预测未来的需求和定价。Twitter情绪分析： 分析大量的推文，以确定特定组织和产品的积极，消极或中立的情绪。航空公司 ： 建立预测航空旅行延误的模型。设备 ： 预测建筑物超过临界温度的可能性。上面所举的应用实例是想让大家更直接的去理解，Spark到底在实际的生产环境中能带来什么样的作用和发挥什么样的价值，对以后的学习，能更好的指导方向！最后来纠正一个不正确的观点，貌似很多技术论坛和网站上都有一些标题党在说“Spark是Hadoop的代替者”，“Hadoop被Spark终结”等类似标题的文章，内行的人一看就是脑残一样的标题😄，Spark 并不是一个完全替代 Hadoop 的全新工具。因为 Hadoop 还包含了很多组件：数据存储层：分布式文件存储系统 HDFS，分布式数据库存储的 HBase；数据处理层：进行数据处理的 MapReduce，负责集群和资源管理的 YARN；数据访问层：Hive、Pig、Mahout……从狭义上来看，Spark 只是 MapReduce 的替代方案，大部分应用场景中，它还要依赖于 HDFS 和 HBase 来存储数据，依赖于 YARN 来管理集群和资源。当然，Spark 并不是一定要依附于 Hadoop 才能生存，它还可以运行在 Apache Mesos、Kubernetes、standalone 等其他云平台上。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Scala 方法和函数","slug":"Scala-方法和函数","date":"2017-01-04T09:23:59.000Z","updated":"2020-04-10T03:22:41.218Z","comments":true,"path":"2017/01/04/Scala-方法和函数/","link":"","permalink":"cpeixin.cn/2017/01/04/Scala-%E6%96%B9%E6%B3%95%E5%92%8C%E5%87%BD%E6%95%B0/","excerpt":"","text":"Scala 方法和函数方法与函数Scala 有方法与函数，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。Scala 中的方法跟 Java 的类似，方法是组成类的一部分。Scala 中的函数则是一个完整的对象，Scala 中的函数其实就是继承了 Trait 的类的对象。Scala 中使用 val 语句可以定义函数，def 语句定义方法。方法定义方法定义由一个 def 关键字开始，紧接着是可选的参数列表，一个冒号 : 和方法的返回类型，一个等于号 = ，最后是方法的主体。定义格式如下：1234def functionName ([参数列表]) : [return type] = &#123; function body return [expr]&#125;以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。以下方法的功能是将两个传入的参数相加并求和：1234567object add&#123; def addInt( a:Int, b:Int ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;函数Scala 也是一种函数式语言，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程，定义方式上，和scala方法是一样的。传值函数&amp;传名函数**Scala的解释器在解析函数参数(function arguments)时有两种方式：先计算参数表达式的值(reduce the arguments)，再应用到函数内部；或者是将未计算的参数表达式直接应用到函数内部。前者叫做传值调用（call-by-value），后者叫做传名调用（call-by-name）123456789101112131415object Test &#123; def main(args: Array[String]) &#123; delayed(time()); &#125; def time() = &#123; println(\"获取时间，单位为纳秒\") System.nanoTime &#125; def delayed( t: =&gt; Long ) = &#123; println(\"在 delayed 方法内\") println(\"参数： \" + t) 此时计算 time()函数 t &#125;&#125;注：scala函数体和方法体中的最后一行为返回值结果：1234在 delayed 方法内获取时间，单位为纳秒参数： 241550840475831获取时间，单位为纳秒=&gt; Unit 与 () =&gt;Unit的区别简单来说, =&gt; Unit是 传名函数, 只传入了一个表达式, 在调用时才会去执行, 使用 code调用() =&gt; 是传值函数, 传入的计算后的值，示例例如：1234567def function_1(t: () =&gt; Long): Unit = &#123; xxxx &#125; def function_2(t: =&gt; Long): Unit = &#123; xxxx &#125;指定函数参数名**不管在用什么语言在进行开发的过程中，对于函数的传参，我们几乎都是按照函数定义中，参数的顺序进行传参的，但是在Scala中会灵活一些，我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下：123456789object Test &#123; def main(args: Array[String]) &#123; printInt(b=5, a=7); &#125; def printInt( a:Int, b:Int ) = &#123; println(\"Value of a : \" + a ); println(\"Value of b : \" + b ); &#125;&#125;可变参数**Scala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。Scala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：123456789101112object Test &#123; def main(args: Array[String]) &#123; printStrings(\"Runoob\", \"Scala\", \"Python\"); &#125; def printStrings( args:String* ) = &#123; var i : Int = 0; for( arg &lt;- args )&#123; println(\"Arg value[\" + i + \"] = \" + arg ); i = i + 1; &#125; &#125;&#125;**递归函数**12345678910111213object Test &#123; def main(args: Array[String]) &#123; for (i &lt;- 1 to 10) println(i + \" 的阶乘为: = \" + factorial(i) ) &#125; def factorial(n: BigInt): BigInt = &#123; if (n &lt;= 1) 1 else n * factorial(n - 1) &#125;&#125;默认参数值Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：12345678910object Test &#123; def main(args: Array[String]) &#123; println( \"返回值 : \" + addInt() ); &#125; def addInt( a:Int=5, b:Int=7 ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;高阶函数高阶函数（Higher-Order Function）就是操作其他函数的函数。Scala 中允许使用高阶函数, 高阶函数可以使用其他函数作为参数，或者使用函数作为输出结果。以下实例中，apply() 函数使用了另外一个函数 f 和 值 v 作为参数，而函数 f 又调用了参数 v：**123456789101112object Test &#123; def main(args: Array[String]) &#123; println( apply( layout, 10) ) &#125; // 函数 f 和 值 v 作为参数，而函数 f 又调用了参数 v def apply(f: Int =&gt; String, v: Int) = f(v) def layout[A](x: A) = \"[\" + x.toString() + \"]\" &#125;匿名函数**匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。使用匿名函数后，我们的代码变得更简洁了。下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:1var inc = (x:Int) =&gt; x+1上述定义的匿名函数，其实是下面这种写法的简写：123def add2 = new Function1[Int,Int]&#123; def apply(x:Int):Int = x+1; &#125;以上实例的 inc 现在可作为一个函数，使用方式如下：1var x = inc(7)-1同样我们可以在匿名函数中定义多个参数：1var mul = (x: Int, y: Int) =&gt; x*ymul 现在可作为一个函数，使用方式如下：1println(mul(3, 4))我们也可以不给匿名函数设置参数，如下所示：1var userDir = () =&gt; &#123; System.getProperty(\"user.dir\") &#125;userDir 现在可作为一个函数，使用方式如下：1println( userDir() )偏应用函数偏应用函数也是一个蛮有意思的用法，**Scala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。1234567891011121314import java.util.Dateobject Test &#123; def main(args: Array[String]) &#123; val date = new Date log(date, \"message1\" ) Thread.sleep(1000) log(date, \"message2\" ) Thread.sleep(1000) log(date, \"message3\" ) &#125; def log(date: Date, message: String) = &#123; println(date + \"----\" + message) &#125;&#125;执行以上代码，输出结果为：12345$ scalac Test.scala$ scala TestMon Dec 02 12:52:41 CST 2018----message1Mon Dec 02 12:52:41 CST 2018----message2Mon Dec 02 12:52:41 CST 2018----message3实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。以上实例修改如下：123456789101112131415import java.util.Dateobject Test &#123; def main(args: Array[String]) &#123; val date = new Date val logWithDateBound = log(date, _ : String) logWithDateBound(\"message1\" ) Thread.sleep(1000) logWithDateBound(\"message2\" ) Thread.sleep(1000) logWithDateBound(\"message3\" ) &#125; def log(date: Date, message: String) = &#123; println(date + \"----\" + message) &#125;&#125;执行以上代码，输出结果为和上面的是一样的。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 基本语法","slug":"Scala-基本语法","date":"2017-01-03T09:21:58.000Z","updated":"2020-04-10T03:22:44.338Z","comments":true,"path":"2017/01/03/Scala-基本语法/","link":"","permalink":"cpeixin.cn/2017/01/03/Scala-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/","excerpt":"","text":"数据类型Scala 与 Java有着相同的数据类型，下表列出了 Scala 支持的数据类型：上表中列出的数据类型都是对象，也就是说scala没有java中的原生类型。在scala是可以对数字等基础类型调用方法的。这里就不多说了，其中需要注意的一点，就是字符与字符串的表示，因为平常使用python编程也是比较多的，在python中字符串是用单引号表示的，所以切换回scala，总会出现使用单引号定义字符串的情况在 Scala 字符变量使用单引号 ‘ 来定义，如下：‘a’而字符串字面量使用双引号 “ 来定义，如下：“Hello,World!”变量定义声明变量实例如下：1var myVar : String = \"Foo\"声明常量实例如下：1val myVal : String = \"Foo\"以上定义了常量 myVal，它是不能修改的。如果程序尝试修改常量 myVal 的值，程序将会在编译时报错。变量类型声明变量的类型在变量名之后的 ：声明。定义变量的类型的语法格式如下：12345var VariableName : DataType = Value或val VariableName : DataType = Value在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。上图则为省略变量类型，scala根据初始值进行判断。但是在项目开发中，建议大家不要省略变量类型。访问修饰符Scala 访问修饰符基本和Java的一样，分别有：private，protected，public。如果没有指定访问修饰符，默认情况下，Scala 对象的访问级别都是 public。Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。条件语句1234567891011121314151617181920if(布尔表达式)&#123; // 如果布尔表达式为 true 则执行该语句块&#125;if(布尔表达式)&#123; // 如果布尔表达式为 true 则执行该语句块&#125;else&#123; // 如果布尔表达式为 false 则执行该语句块&#125;if(布尔表达式 1)&#123; // 如果布尔表达式 1 为 true 则执行该语句块&#125;else if(布尔表达式 2)&#123; // 如果布尔表达式 2 为 true 则执行该语句块&#125;else if(布尔表达式 3)&#123; // 如果布尔表达式 3 为 true 则执行该语句块&#125;else &#123; // 如果以上条件都为 false 执行该语句块&#125;循环语句这里就不描述 while() do while() 语句了和其他语言基本一致，这里只描述和其他语言有差异的for循环语法如下：1234567891011for( var x &lt;- Range )&#123; statement(s);&#125;以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值。for( var x &lt;- List )&#123; statement(s);&#125;以上语法中， List 变量是一个集合，for 循环会迭代所有集合的元素。yield：你可以利用yield 将 for 循环中符合条件的值作为一个变量存储。语法格式如下：12345678910111213141516object Test &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6,7,8,9,10); // for 循环 var retVal = for&#123; a &lt;- numList if a != 3; if a &lt; 8 &#125;yield a // 输出返回值 for( a &lt;- retVal)&#123; println( \"Value of a: \" + a ); &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 创建程序文件","slug":"Scala-创建程序文件","date":"2017-01-02T09:20:44.000Z","updated":"2020-04-10T03:22:51.424Z","comments":true,"path":"2017/01/02/Scala-创建程序文件/","link":"","permalink":"cpeixin.cn/2017/01/02/Scala-%E5%88%9B%E5%BB%BA%E7%A8%8B%E5%BA%8F%E6%96%87%E4%BB%B6/","excerpt":"","text":"Scala 是一门类 Java 的编程语言，它结合了面向对象编程和函数式编程。Scala 是纯面向对象的，每个值都是一个对象，对象的类型和行为由类定义，不同的类可以通过混入(mixin)的方式组合在一起。Scala 的设计目的是要和两种主流面向对象编程语言Java 和 C#实现无缝互操作，这两种主流语言都非纯面向对象。Scala 也是一门函数式变成语言，每个函数都是一个值，原生支持嵌套函数定义和高阶函数。Scala 也支持一种通用形式的模式匹配，模式匹配用来操作代数式类型，在很多函数式语言中都有实现。Scala 被设计用来和 Java 无缝互操作（另一个修改的 Scala 实现可以工作在.NET上）。Scala 类可以调用 Java 方法，创建 Java 对象，继承 Java 类和实现 Java 接口。这些都不需要额外的接口定义或者胶合代码。前言我相信大多数接触scala的工程师，都是因为接触到了大数据技术栈，而被迫去学习scala的。scala这门语言完全可以理解成因为其开源项目的火爆，而将这门语言带到了大众的视野，例如计算框架Spark，消息队列Kafka， akka，使用Scala编写Spark程序真的是太便利了，简洁的函数编程让你欲罢不能，那么从另一方面来说，如果有一天，Spark被大数据技术栈淘汰了，那么Scala对于大数据工程师还是硬需求么？从现在来看，异军突起的Flink实时计算框架也是支持Scala的，同样相对于Java和python来说，Scala都有独特迷人的地方。创建文件对于有其他语言基础的工程师来讲，新学一门语言并不难，那么这里我就先写一些简明扼要的，工程师看完就可以直接创建文件，编写hello word就可以运行的。首先，我假定你已经配置好了scala的环境，编译器IDEA也已经配置完毕了，在选择好的目录中可以创建Scala文件了，如下图，我们该怎么选择呢？简单的来说：类class里无static类型，类里的属性和方法，必须通过new出来的对象来调用，所以有main主函数也没用。而object的特点是：可以拥有属性和方法，且默认都是”static”类型，可以直接用object名直接调用属性和方法，不需要通过new出来的对象（也不支持）。object里的main函数式应用程序的入口。object和class有很多和class相同的地方，可以extends父类或Trait，但object不可以extends object，即object无法作为父类。Scala的Trait相当于Java里的Interface根据上面的红字，我们也可以知道，如果想创建文件，并且运行文件，我们需要选择Object来写main()函数main函数怎么写呢？1def main(args: Array[String]) - Scala程序从main()方法开始处理，这是每一个Scala程序的强制程序入口部分。打印 hello word12345object helloword &#123; def main(args: Array[String]): Unit = &#123; println(\"hello word\") &#125;&#125;假如现在你对scala其他语法一无所知的情况下，你懂的了怎么创建文件，怎么去定义main()函数，还有和其他语言差不太多的打印函数 println(‘’)，就完成了编程语言第一课，HelloWorld这里有几个需要记住的点scala语言，每行语句后，不用 ； 结尾，java中需要使用 ；号结尾，python和scala一样，不需要分号。但是如果你想如果一行里写多个语句那么分号是需要的。例如val s = “哈哈哈”; println(s)Scala 使用 import 关键字引用包","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Hive 基本操作","slug":"Hive-基本操作","date":"2016-10-20T14:14:13.000Z","updated":"2020-04-05T14:32:51.398Z","comments":true,"path":"2016/10/20/Hive-基本操作/","link":"","permalink":"cpeixin.cn/2016/10/20/Hive-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Hive 交互命令刚刚安装好hive后，先来看一看基本的交互12345678910111213141516(base) [cpeixin@CpeixindeMBP:] ~ $ hive -helpHive Session ID &#x3D; 626af458-fe89-406e-9a63-5967e2962486usage: hive -d,--define &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. -d A&#x3D;B or --define A&#x3D;B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property&#x3D;value&gt; Use value for given property --hivevar &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. --hivevar A&#x3D;B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console)在不进入客户端的情况下，我们可以上面这些交互命令来操作hive-e 在linux命令行窗口执行sql语句[cpeixin@CpeixindeMBP]$ hive -e “hive -e “select * from test.car””注： 记得加库名-f 执行脚本中sql语句，并将结果写到指定文件[cpeixin@CpeixindeMBP]$ hive -f /opt/sql/test.sql &gt; /opt/data/test_result.txt查看hive的执行历史[cpeixin@CpeixindeMBP:] ~ $ cat .hivehistory在进入hive客户端中，我们还可以使用命令查看本地和HDFS文件系统的文件，这个还是蛮实用的Hive 数据类型基本的交互了解后呢，我们来看一下Hive中的数据类型基本数据类型：数据类型字节范围示例TINYINT1byte-128 ~ 127100YSMALLINT2byte-32,768 ~ 32,767100SINT/INTEGER4byte-2,147,483,648 ~ 2,147,483,647100BIGINT8byte-9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807100LFLOAT4byte单精度浮点数0.2DOUBLE8byte双精度浮点数0.2DECIMAL高精度浮点数DECIMAL(9,8)BOOLEANTRUE/FALSEtrueBINARY二进制类型TIMESTAMP时间戳DATE日期2016-08-08STRINGVARCHAR长度 1～65535CHAR最大长度255关于整型：默认情况下，整型数据默认为INT，除非数字超出INT的范围，在这种情况下它被表示为 BIGINT，或者直接指定100Y，100S, 100L 才会对应转换成TINYINT、SMALLINT、BIGINT。关于浮点型：浮点数假定为 DOUBLE，Decimal 字为 DOUBLE 类型提供精确值和浮点数的更大范围。 Decimal 数据类型存储数值的精确表示，而 DOUBLE 数据类型存储非常接近数值的近似值。DECIMAL不指定精度时默认为DECIMAL(10,0)；DOUBLE 的(非常接近)近似值不足以满足要求是，需要使用Decimal 类型，例如财务应用程序，相等和不等式检查以及舍入操作。对于处理 DOUBLE 范围(大约-10308 到 10308)或非常接近零(-10-308 到 10-308)之外的数的用例，也需要它们。另外，Decimal为专门为财务相关问题设计的数据类型。关于字符型：Strings 字符串数据可以用单引号(‘)或 双引号(“)表示.Hive 在 strings 中使用 C-style 转义。Varchar 使用长度说明符(介于 1 和 65535 之间)创建 Varchar 类型，该长度说明符定义字符 string 中允许的最大字符数。如果varchar value超过了长度说明符，则会以静默方式截断 string。字符长度由字符 串中包含的字符数决定。与 string 一样，尾随空格在 varchar 中很重要，会影响比较结果。Char 类型与 Varchar 类似，但它们是固定长度意味着短于指定长度 value 的值用空格填充，但尾部空格在比较期间不影响比较。最大长度固定为 255。关于时间类型：Timestampstimestamp表示UTC时间，可以是以秒为单位的整数；带精度的浮点数，最大精确到小数点后9位，纳秒级；java.sql.Timestamp格式的字符串 YYYY-MM-DD hh:mm:ss.fffffffffDateHive中的Date只支持YYYY-MM-DD格式的日期，其余写法都是错误的，如需带上时分秒，请使用timestamp复杂数据类型：数据类型释义ARRAYARRAY类型是由一系列相同数据类型的元素组成，这些元素可以通过下标来访问。比如有一个ARRAY类型的变量fruits，它是由[‘apple’,’orange’,’mango’]组成，那么我们可以通过fruits[1]来访问元素orange，因为ARRAY类型的下标是从0开始的；MAPMAP包含key-&gt;value键值对，可以通过key来访问元素。比如”userlist”是一个map类型，其中username是key，password是value；那么我们可以通过userlist[‘username’]来得到这个用户对应的password；STRUCTSTRUCT可以包含不同数据类型的元素。这些元素可以通过”点语法”的方式来得到所需要的元素，比如user是一个STRUCT类型，那么可以通过user.address得到这个用户的地址。UNIONUNIONTYPE&lt;data_type, data_type, …&gt;Hive DDL操作关于库：创建库CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name[COMMENT database_comment] //关于数据块的描述[LOCATION hdfs_path] //指定数据库在HDFS上的存储位置[WITH DBPROPERTIES (property_name=property_value, …)]; //指定数据块属性eg: create database t1;** create database if not exists t1;** create database if not exists t1 comment ‘comment dor t1’;**create database if not exists t3 with dbproperties(‘creator’=’cpeixin’,’date’=’2016-04-05’);查看库show databases;** desc database extended t1;**show create database t1;删除库** **drop database dbname;切换库use dbname;关于表：创建表下面为官网给出的建表参数：1234567891011121314CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [&lt;font&gt;&lt;&#x2F;font&gt; [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path]内部表和外部表默认情况下，Hive创建内部表，其中文件，元数据和统计信息由内部Hive进程管理这里我们需要知道最根本的区别就是，在删除内部表的时候，数据也会被删除，而外部表不会。STORED as 存储格式是指定文件的类型，保存在hive中的文件的类型有多种，一般简单就保存为文本格式，即TEXTFILE，但是企业中一般不使用这种格式来保存数据，主要是因为文本格式占的空间比较大，不利于大数据分析。企业中一般使用ORC和PARQUET，AVRO三种文件类型来保存，具体的会在后面讲解。ROW FORMAT DELIMITED 行分隔符ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘ ‘这句的意思是以空格来分隔行数据，那么这一行中的数据只要遇到一个空格就可以划分为一个数据。这里的分隔符可以是其他字符，比如”,”,”#”,”|”等，一般只要用数据文件中可以区分每一行中的不同数据即可。列与列直接的分隔符通常是以换行符来区分，可以用如下的语句来指定：`COLLECTION ITEMS TERMINATED BY ‘\\n’， 通常列与列直接的分隔符是不需要写的。LOCATION hdfs_path **可以在创建表的时候指定该表映射到到hdfs的文件路径，默认是映射到/user/hive/warehouse目录下。PARTITIONED BY 分区为了对表进行合理的管理以及提高查询效率，Hive可以将表组织成“分区”。一个分区实际上就是表下的一个目录，一个表可以在多个维度上进行分区，分区之间的关系就是目录树的关系。通过PARTITIONED BY子句指定，分区的顺序决定了谁是父目录，谁是子目录。在这里分区又分为 静态分区和动态分区。简单的来说，静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断下面进行建表，建表的示例也会尽可量的使用各种数据类型进行解释说明：建表实例：1.创建普通表，不添加任何参数**123456789101112create table t_user_details(user_name string,age tinyint,phone_number string,birth_date string,deposit_amount float,promotion_amount double,register_date date,last_login_time timestamp,user_level int,vip_flag boolean);**12345678910111213141516171819202122232425hive (test)&gt; show create table t_user_details;OKcreatetab_stmtCREATE TABLE &#96;t_user_details&#96;( &#96;user_name&#96; string, &#96;age&#96; tinyint, &#96;phone_number&#96; string, &#96;birth_date&#96; string, &#96;deposit_amount&#96; float, &#96;promotion_amount&#96; double, &#96;register_date&#96; date, &#96;last_login_time&#96; timestamp, &#96;user_level&#96; int, &#96;vip_flag&#96; boolean)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39; STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;LOCATION &#39;hdfs:&#x2F;&#x2F;localhost:8020&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;t_user_details&#39;TBLPROPERTIES ( &#39;bucketing_version&#39;&#x3D;&#39;2&#39;, &#39;transient_lastDdlTime&#39;&#x3D;&#39;1586087852&#39;)1insert into t_user_details values (&#39;brent&#39;,27,&#39;13611111111&#39;,&#39;1993-03-14&#39;,100.91,4000.56,2016-04-05,&#39;2016-04-07 14:20:36.345&#39;,6,True),(&#39;haylee&#39;,25,&#39;13211111111&#39;,&#39;1994-03-14&#39;,130.91,40000000.56,&#39;2016-04-06&#39;,&#39;2016-04-08 14:20:36.345&#39;,6,False);观察以上内容，是针对没有输入任何建表参数的情况下，所生成的结果，在show create table xx的结果中，体现出了完整的建表默认参数。关于ROW FORMAT SERDE，用于指定序列化和反序列化的规则，默认值：LazySimpleSerDe简单来说，就是它希望对于Deserialization，反序列化，可以lazy一点。对于Serialization，序列化，可以simple一点在没有指定字段之间的分隔符时，默认是用\\001 不可见字符进行分割的，我们也可以在建表的时候使用FIELDS TERMINATED BY ‘,’ 参数来指定字段之间使用逗号分割。除此之外，在使用hive的时候，存储格式的选择非常重要，不同存储格式直接在最底层影响着你的执行效率，所以这部分在之后用单独的一篇文章来说。这里大家先知道企业里面常用的ORC，Parquet，Avro等格式就可以了2.创建分区表**分区表在显示工作中非常常用，例如针对网站数据的存储，网站每天产生数据量过大的话，我们不能始终在表末尾进行数据的追加，而是应该利用动态分区或者静态分区，按月，按天的粒度进行分区存储123456789101112131415161718192021222324CREATE TABLE &#96;t_user_detail_partition&#96;( user_name string, age tinyint, phone_number string, birth_date string, deposit_amount float, promotion_amount double, register_date date, last_login_time timestamp, user_level int, vip_flag boolean)PARTITIONED BY ( &#96;snapshot_date&#96; string)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39; WITH SERDEPROPERTIES ( &#39;field.delim&#39;&#x3D;&#39;\\u0001&#39;, &#39;line.delim&#39;&#x3D;&#39;\\n&#39;, &#39;serialization.format&#39;&#x3D;&#39;\\u0001&#39;) STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;;1insert into t_user_detail_partition values (&#39;brent&#39;,27,&#39;13611111111&#39;,&#39;1993-03-14&#39;,100.91,4000.56,2016-04-05,&#39;2016-04-07 14:20:36.345&#39;,6,True,&#39;2016-04-04&#39;),(&#39;haylee&#39;,25,&#39;13211111111&#39;,&#39;1994-03-14&#39;,130.91,40000000.56,&#39;2016-04-06&#39;,&#39;2016-04-08 14:20:36.345&#39;,6,False,&#39;2016-04-05&#39;);上面的建表语句使用 PARTITIONED BY 指定了 snapshot_date 为分区字段，当然你还可以再添加分区字段，hive支持多分区，理论上最多支持8级分区在插入数据的语句中，将分区字段的值顺序的写在表中字段值的后面，则可以按照分区进行插入数据。在hdfs中，t_user_detail_partition表则按照分区字段进行划分，将数据存储到不同的分区目录下。Hive中的分区是使用的表外字段，MySQL使用的是表内字段静态分区和动态分区在创建表时，语句是一样的。只是在赋值的时候有区别动态分区和静态分区的区别加载数据的方式：静态分区可以通过load命令，向不同的分区加载数据，加载数据时要指定分区的值；静态分区只能通过select加载数据，并且不需要指定分区的名字，而是根据伪列的值，动态的确定分区值确定分区值的方式：两者在创建表的时候命令完全一致，只是在确定分区值的时候不同，静态分区需要手动指定分区值，而动态分区会自动识别伪列的属性，动态生成分区值Hive DML操作加载文件数据到表123LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)]复制代码LOCAL 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：从本地文件系统加载文件时， filepath 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；从 HDFS 加载文件时候，filepath 为文件完整的 URL 地址：如 hdfs://namenode:port/user/hive/project/ data1filepath 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；加载文件的格式必须与建表时使用 STORED AS 指定的存储格式相同。查询结果插入到表12345INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;复制代码Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 immutable 属性的影响）;可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；从 Hive 1.1.0 开始，TABLE 关键字是可选的；从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列；可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：12345FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...;**动态插入分区****12345INSERT OVERWRITE TABLE tablename PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) select_statement FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) select_statement FROM from_statement;复制代码在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。使用SQL语句插入值123INSERT INTO TABLE tablename [PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...)] VALUES ( value [, value ...] )复制代码使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；不支持支持复杂类型 (array, map, struct, union) 的插入。更新和删除数据更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。1234-- 更新UPDATE tablename SET column &#x3D; value [, column &#x3D; value ...] [WHERE expression]--删除DELETE FROM tablename [WHERE expression]查询结果写出到文件系统1234INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] SELECT ... FROM ...复制代码OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：1234567-- 定义列分隔符为&#39;\\t&#39; insert overwrite local directory &#39;.&#x2F;test-04&#39; row format delimited FIELDS TERMINATED BY &#39;\\t&#39;COLLECTION ITEMS TERMINATED BY &#39;,&#39;MAP KEYS TERMINATED BY &#39;:&#39;select * from src;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"Hive 初识","slug":"Hive-初识","date":"2016-10-15T14:11:24.000Z","updated":"2020-04-05T14:32:53.687Z","comments":true,"path":"2016/10/15/Hive-初识/","link":"","permalink":"cpeixin.cn/2016/10/15/Hive-%E5%88%9D%E8%AF%86/","excerpt":"","text":"What - Hivehive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。Why - HiveHive最初是Facebook为了满足对海量社交网络数据的管理和机器学习的需求而产生和发展的。大数据是现在互联网的趋势，而hadoop就是大数据时代里的核心技术，但是hadoop的mapreduce操作专业性太强，所以facebook在这些基础上开发了hive框架，业务人员在不学习编程语言的情况下，只要学会基本的SQL语句，就可以对大数据平台的数据进行分析。How - Hive在hadoop集群中，安装配置好hive，就可以在命令行中直接输入hive，进入hive客户端接下来的操作就和操作数据库的SQL几乎一样。具体操作将在后面的文章里进行介绍。Hive - 优缺点优点：简单容易上手：提供了类SQL查询语言HQL可扩展：为超大数据集设计了计算/扩展能力（MR作为计算引擎，HDFS作为存储系统，Yarn作为资源调度）提供统一的元数据管理延展性：Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数容错：良好的容错性，节点出现问题SQL仍可完成执行支持用户自定义函数缺点：hive的HQL表达能力有限hive的效率比较低（后面可用spark计算框架代替Hive分析）hive调优比较困难，粒度较粗Hive - 架构","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"HDFS-三思","slug":"HDFS-三思","date":"2016-10-06T07:30:21.000Z","updated":"2020-04-04T17:25:03.805Z","comments":true,"path":"2016/10/06/HDFS-三思/","link":"","permalink":"cpeixin.cn/2016/10/06/HDFS-%E4%B8%89%E6%80%9D/","excerpt":"","text":"读写流程写流程具体过程如下：Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象；通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入；调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。**读流程**![p2.gif](https://cdn.nlark.com/yuque/0/2020/gif/1072113/1586020955556-6aa55757-4f0d-4d46-91bb-90e9963517fd.gif#align=left&display=inline&height=541&name=p2.gif&originHeight=541&originWidth=960&size=1319443&status=done&style=none&width=960)具体过程：Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；NameNode 返回存储的每个块的 DataNode 列表；Client 将连接到列表中最近的 DataNode；Client 开始从 DataNode 并行读取数据；一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。写流程中备份三，其中一个写失败了怎么办？只要成功写入的节点数量达到dfs.replication.min(默认为1)，那么就任务是写成功的。然后NameNode会通过异步的方式将block复制到其他节点，使数据副本达到dfs.replication参数配置的个数HDFS HA 启动流程①开启zookeeper服务1zkServer.sh start②开启`journalNode`守护进程（在`journal`协议指定的节点上执行）[ˈdʒɜːnl]1hadoop-daemon.sh start journalnode③开启namenode守护进程（在nn1和nn2执行）1hadoop-daemon.sh start namenode④开启datanode守护进程123hadoop-daemons.sh start datanode（在namenode节点上执行开启全部datanode）⑤开启zkfc守护进程1hadoop-daemon.sh --script $HADOOP_PREFIX&#x2F;bin&#x2F;hdfs start zkfcHDFS 存储类型HDFS支持如下4种存储类型：DISK：表示普通磁盘(机械磁盘)SSD：表示固态硬盘RAM_DISK：表示内存硬盘，参考虚拟内存盘，说白了就是内存ARCHIVE：这个并不是特指某种存储介质，而是为了满足高密度存储而定义的一种存储类型，一般对于归档的、访问不怎么频繁的数据可以以 ARCHIVE 的形式存储。以上四种的存储类型的存取的速度大小为：RAM_DISK-&gt;SSD-&gt;DISK-&gt;ARCHIVE。但是单bit存储成本由高到低那么我们在配置DataNode的存储路径的时候，我们可以分别为上面四种存储类型配置存储位置，如下图：12345&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;[RAM_DISK]file:///ram_disk,[SSD]file:///ssd1/dn,[DISK]file:///disk1/dn,[ARCHIVE]file:///archive1/dn&lt;/value&gt; &lt;description&gt;DataNode存放数据的地方&lt;/description&gt;&lt;/property&gt;上面配置的DataNode的多个存储位置由逗号隔开，每一个存储位置由存储类型和存储物理路径组成。HDFS通过该配置感知底层存储的位置和类型HDFS是否有异步访问模式？在现有HDFS的RPC调用方式上,采用的基本是blocking call的形式,也就是阻塞式的调用方式.阻塞方式的一个明显的缺点是它的请求过程是同步的,也就是说,客户端必须等待当前请求结果的返回,才能接着发送下一次请求.如果此客户端打算在一个线程中发送大量请求的话,阻塞式的RPC调用将会非常耗时.但是如果为了每一次请求调用而专门单独开一个线程的话,系统资源将会被大幅度的使用,显然这也不是一个好的解决的办法.那么有没有什么好的办法呢,在HDFS中是否存在有异步模式的RPC请求接口呢本文我们就来聊聊HDFS的异步访问模式.HDFS异步访问模式老实说,在目前Hadoop的发布版本中,确实还不存在HDFS异步访问的模式,但是这并不代表社区没有在关注这方面的问题.在许多特殊的场景下,HDFS的异步访问模式还是有它独到的用处的.社区在JIRA HDFS-9924([umbrella] Nonblocking HDFS Access)上对此功能特性进行了实现.在本文中,我们姑且取名”HDFS异步访问模式”为AsyncDistributedFileSystem,与DistributedFileSystem相对应.HDFS异步访问模式原理在HDFS异步访问模式的设计文档中,给出了新的异步的RPC调用模式,采用了Future-Get的异步调用模式,以FileSystem的rename方法客户端异步请求的控制在前面HDFS异步访问模式的过程中,有一点必须格外引起注意:客户端异步请求的控制.客户端应有异步请求数的限制,以此防止客户端利用大量的异步请求冲垮服务端.如果超过了此限制阈值,客户端的请求将会处于阻塞状态.这点必须要引起足够重视,否则客户端随随便便发起的请求将会摧毁NameNode.HDFS异步访问模式的优化点第一, 保证异步请求的有序性.在某些场景下,我们需要保证异步请求能够按照请求发起的时间,顺序执行.第二, 客户端对HDFS读写异步请求的支持.总结HDFS Async调用模式的出现将会带给用户更灵活的RPC请求方式的选择,但是可能考虑到此种方式对比之前的方式而言,改动较大,目前这些异步调用相关的方法许多是打上了@Unstable标记的.HDFS异步调用的方式同样可以很好的运用在单元测试上.鉴于此特性是还暂未发布,大家可以根据自己的需要,进行部分的合入.调整数据块大小会有什么影响？Hadoop 1.x 中， 默认的数据块大小是 64MHadoop 2.x 中， 默认的数据块大小是 128M在HDFS中，数据块不宜设置的过大，也不适宜设置的过小。主要是从 减少寻址时间和MR任务并行方面去考虑为什么HDFS中块（block）不能设置太大，也不能设置太小？如果块设置过大，一方面，从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢；另一方面，mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。2. 如果块设置过小，一方面存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；另一方面文件块过小，寻址时间增大，导致程序一直在找block的开始位置。因而，块适当设置大一些，减少寻址时间，那么传输一个由多个块组成的文件的时间主要取决于磁盘的传输速率。HDFS中块（block）的大小为什么设置为128M？HDFS中平均寻址时间大概为10ms；2. 经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态；所以最佳传输时间为10ms/0.01=1000ms=1s3. 目前磁盘的传输速率普遍为100MB/s；计算出最佳block大小：100MB/s x 1s = 100MB所以我们设定block大小为128MB。实际在工业生产中，磁盘传输速率为200MB/s时，一般设定block大小为256MB，磁盘传输速率为400MB/s时，一般设定block大小为512MB以后随着新一代磁盘驱动器传输速率的提升，块的大小将被设置得更大","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"Hadoop 2.x - HDFS","slug":"Hadoop-2-x-HDFS","date":"2016-10-06T03:33:21.000Z","updated":"2020-04-04T17:26:26.578Z","comments":true,"path":"2016/10/06/Hadoop-2-x-HDFS/","link":"","permalink":"cpeixin.cn/2016/10/06/Hadoop-2-x-HDFS/","excerpt":"","text":"在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。NameNode 高可用整体架构概述在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。HDFS NameNode 和 YARN ResourceManger 的高可用 (High Availability，HA) 方案基本类似，两者也复用了部分代码，但是由于 HDFS NameNode 对于数据存储和数据一致性的要求比 YARN ResourceManger 高得多，所以 HDFS NameNode 的高可用实现更为复杂一些，本文从内部实现的角度对 HDFS NameNode 的高可用机制进行详细的分析。HDFS NameNode 的高可用整体架构如图 1 所示图 1.HDFS NameNode 高可用整体架构从上图中，我们可以看出 NameNode 的高可用架构主要分为下面几个部分：Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。Zookeeper 集群：为主备切换控制器提供主备选举支持。共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。下面开始分别介绍 NameNode 的主备切换实现和共享存储系统的实现，在文章的最后会结合笔者的实践介绍一下在 NameNode 的高可用运维中的一些注意事项。NameNode 的主备切换实现NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现：ZKFailoverController 作为 NameNode 机器上一个独立的进程启动 (在 hdfs 启动脚本之中的进程名为 zkfc)，启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，也会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调方法。HealthMonitor 主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举。ActiveStandbyElector 主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。NameNode 实现主备切换的流程如图 2 所示，有以下几步：HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。图 2.NameNode 的主备切换流程下面分别对 HealthMonitor、ActiveStandbyElector 和 ZKFailoverController 的实现细节进行分析：HealthMonitor 实现分析ZKFailoverController 在初始化的时候会创建 HealthMonitor，HealthMonitor 在内部会启动一个线程来循环调用 NameNode 的 HAServiceProtocol RPC 接口的方法来检测 NameNode 的状态，并将状态的变化通过回调的方式来通知 ZKFailoverController。HealthMonitor 主要检测 NameNode 的两类状态，分别是 HealthMonitor.State 和 HAServiceStatus。HealthMonitor.State 是通过 HAServiceProtocol RPC 接口的 monitorHealth 方法来获取的，反映了 NameNode 节点的健康状况，主要是磁盘存储资源是否充足。HealthMonitor.State 包括下面几种状态：INITIALIZING：HealthMonitor 在初始化过程中，还没有开始进行健康状况检测；SERVICE_HEALTHY：NameNode 状态正常；SERVICE_NOT_RESPONDING：调用 NameNode 的 monitorHealth 方法调用无响应或响应超时；SERVICE_UNHEALTHY：NameNode 还在运行，但是 monitorHealth 方法返回状态不正常，磁盘存储资源不足；HEALTH_MONITOR_FAILED：HealthMonitor 自己在运行过程中发生了异常，不能继续检测 NameNode 的健康状况，会导致 ZKFailoverController 进程退出；HealthMonitor.State 在状态检测之中起主要的作用，在 HealthMonitor.State 发生变化的时候，HealthMonitor 会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。而 HAServiceStatus 则是通过 HAServiceProtocol RPC 接口的 getServiceStatus 方法来获取的，主要反映的是 NameNode 的 HA 状态，包括：INITIALIZING：NameNode 在初始化过程中；ACTIVE：当前 NameNode 为主 NameNode；STANDBY：当前 NameNode 为备 NameNode；STOPPING：当前 NameNode 已停止；HAServiceStatus 在状态检测之中只是起辅助的作用，在 HAServiceStatus 发生变化时，HealthMonitor 也会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。ActiveStandbyElector 实现分析Namenode(包括 YARN ResourceManager) 的主备选举是通过 ActiveStandbyElector 来完成的，ActiveStandbyElector 主要是利用了 Zookeeper 的写一致性和临时节点机制，具体的主备选举实现如下：创建锁节点如果 HealthMonitor 检测到对应的 NameNode 的状态正常，那么表示这个 NameNode 有资格参加 Zookeeper 的主备选举。如果目前还没有进行过主备选举的话，那么相应的 ActiveStandbyElector 就会发起一次主备选举，尝试在 Zookeeper 上创建一个路径为/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 的临时节点 (${dfs.nameservices} 为 Hadoop 的配置参数 dfs.nameservices 的值，下同)，Zookeeper 的写一致性会保证最终只会有一个 ActiveStandbyElector 创建成功，那么创建成功的 ActiveStandbyElector 对应的 NameNode 就会成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Active 状态。而创建失败的 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Standby 状态。注册 Watcher 监听不管创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点是否成功，ActiveStandbyElector 随后都会向 Zookeeper 注册一个 Watcher 来监听这个节点的状态变化事件，ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件。自动触发主备选举如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。防止脑裂Zookeeper 在工程实践的过程中经常会发生的一个现象就是 Zookeeper 客户端“假死”，所谓的“假死”是指如果 Zookeeper 客户端机器负载过高或者正在进行 JVM Full GC，那么可能会导致 Zookeeper 客户端到 Zookeeper 服务端的心跳不能正常发出，一旦这个时间持续较长，超过了配置的 Zookeeper Session Timeout 参数的话，Zookeeper 服务端就会认为客户端的 session 已经过期从而将客户端的 Session 关闭。“假死”有可能引起分布式系统常说的双主或脑裂 (brain-split) 现象。具体到本文所述的 NameNode，假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，即使随后 NameNode1 对应的 ZKFailoverController 因为负载下降或者 Full GC 结束而恢复了正常，感知到自己和 Zookeeper 的 Session 已经关闭，但是由于网络的延迟以及 CPU 线程调度的不确定性，仍然有可能会在接下来的一段时间窗口内 NameNode1 认为自己还是处于 Active 状态。这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况对于 NameNode 这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。Zookeeper 社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息。Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候 (注意由于/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 是临时节点，也会随之删除)，会一起删除节点/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb。但是如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来。后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing，具体处理见后文 ZKFailoverController 部分所述。ZKFailoverController 实现分析ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调函数，ZKFailoverController 的处理逻辑主要靠 HealthMonitor 和 ActiveStandbyElector 的回调函数来驱动。对 HealthMonitor 状态变化的处理如前所述，HealthMonitor 会检测 NameNode 的两类状态，HealthMonitor.State 在状态检测之中起主要的作用，ZKFailoverController 注册到 HealthMonitor 上的处理 HealthMonitor.State 状态变化的回调函数主要关注 SERVICE_HEALTHY、SERVICE_NOT_RESPONDING 和 SERVICE_UNHEALTHY 这 3 种状态：如果检测到状态为 SERVICE_HEALTHY，表示当前的 NameNode 有资格参加 Zookeeper 的主备选举，如果目前还没有进行过主备选举的话，ZKFailoverController 会调用 ActiveStandbyElector 的 joinElection 方法发起一次主备选举。如果检测到状态为 SERVICE_NOT_RESPONDING 或者是 SERVICE_UNHEALTHY，就表示当前的 NameNode 出现问题了，ZKFailoverController 会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举，这样其它的 NameNode 就有机会成为主 NameNode。而 HAServiceStatus 在状态检测之中仅起辅助的作用，在 HAServiceStatus 发生变化时，ZKFailoverController 注册到 HealthMonitor 上的处理 HAServiceStatus 状态变化的回调函数会判断 NameNode 返回的 HAServiceStatus 和 ZKFailoverController 所期望的是否一致，如果不一致的话，ZKFailoverController 也会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举。对 ActiveStandbyElector 主备选举状态变化的处理在 ActiveStandbyElector 的主备选举状态发生变化时，会回调 ZKFailoverController 注册的回调函数来进行相应的处理：如果 ActiveStandbyElector 选主成功，那么 ActiveStandbyElector 对应的 NameNode 成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeActive 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToActive 方法，将 NameNode 转换为 Active 状态。如果 ActiveStandbyElector 选主失败，那么 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeStandby 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，将 NameNode 转换为 Standby 状态。如果 ActiveStandbyElector 选主成功之后，发现了上一个 Active NameNode 遗留下来的/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 节点 (见“ActiveStandbyElector 实现分析”一节“防止脑裂”部分所述)，那么 ActiveStandbyElector 会首先回调 ZKFailoverController 注册的 fenceOldActive 方法，尝试对旧的 Active NameNode 进行 fencing，在进行 fencing 的时候，会执行以下的操作：首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离；只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。NameNode 的共享存储实现过去几年中 Hadoop 社区涌现过很多的 NameNode 共享存储方案，比如 shared NAS+NFS、BookKeeper、BackupNode 和 QJM(Quorum Journal Manager) 等等。目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。为了理解 QJM 的设计和实现，首先要对 NameNode 的元数据存储结构有所了解。NameNode 的元数据存储概述一个典型的 NameNode 的元数据存储目录结构如图 3 所示 (图片来源于参考文献 [4])，这里主要关注其中的 EditLog 文件和 FSImage 文件：图 3 .NameNode 的元数据存储目录结构NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog 仅仅只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment。正在写入的 EditLog Segment 处于 in-progress 状态，其文件名形如 edits_inprogress_${start_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，例如上图中的 edits_inprogress_0000000000000000020。而已经写入完成的 EditLog Segment 处于 finalized 状态，其文件名形如 edits_${start_txid}-${end_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，${end_txid} 表示这个 segment 的结束事务 id，例如上图中的 edits_0000000000000000001-0000000000000000019。NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，FSImage 文件的文件名形如 fsimage_${end_txid}，其中${end_txid} 表示这个 fsimage 文件的结束事务 id，例如上图中的 fsimage_0000000000000000020。在 NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。基于 QJM 的共享存储系统的总体架构基于 QJM 的共享存储系统主要用于保存 EditLog，并不保存 FSImage 文件。FSImage 文件还是在 NameNode 的本地磁盘上。QJM 共享存储的基本思想来自于 Paxos 算法 (参见参考文献 [3])，采用多个称为 JournalNode 的节点组成的 JournalNode 集群来存储 EditLog。每个 JournalNode 保存同样的 EditLog 副本。每次 NameNode 写 EditLog 的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向 JournalNode 集群之中的每一个 JournalNode 发送写请求，只要大多数 (majority) 的 JournalNode 节点返回成功就认为向 JournalNode 集群写入 EditLog 成功。如果有 2N+1 台 JournalNode，那么根据大多数的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。基于 QJM 的共享存储系统的内部实现架构图如图 4 所示，主要包含下面几个主要的组件：图 4 . 基于 QJM 的共享存储系统的内部实现架构图FSEditLog：这个类封装了对 EditLog 的所有操作，是 NameNode 对 EditLog 的所有操作的入口。JournalSet： 这个类封装了对本地磁盘和 JournalNode 集群上的 EditLog 的操作，内部包含了两类 JournalManager，一类为 FileJournalManager，用于实现对本地磁盘上 EditLog 的操作。一类为 QuorumJournalManager，用于实现对 JournalNode 集群上共享目录的 EditLog 的操作。FSEditLog 只会调用 JournalSet 的相关方法，而不会直接使用 FileJournalManager 和 QuorumJournalManager。FileJournalManager：封装了对本地磁盘上的 EditLog 文件的操作，不仅 NameNode 在向本地磁盘上写入 EditLog 的时候使用 FileJournalManager，JournalNode 在向本地磁盘写入 EditLog 的时候也复用了 FileJournalManager 的代码和逻辑。QuorumJournalManager：封装了对 JournalNode 集群上的 EditLog 的操作，它会根据 JournalNode 集群的 URI 创建负责与 JournalNode 集群通信的类 AsyncLoggerSet， QuorumJournalManager 通过 AsyncLoggerSet 来实现对 JournalNode 集群上的 EditLog 的写操作，对于读操作，QuorumJournalManager 则是通过 Http 接口从 JournalNode 上的 JournalNodeHttpServer 读取 EditLog 的数据。AsyncLoggerSet：内部包含了与 JournalNode 集群进行通信的 AsyncLogger 列表，每一个 AsyncLogger 对应于一个 JournalNode 节点，另外 AsyncLoggerSet 也包含了用于等待大多数 JournalNode 返回结果的工具类方法给 QuorumJournalManager 使用。AsyncLogger：具体的实现类是 IPCLoggerChannel，IPCLoggerChannel 在执行方法调用的时候，会把调用提交到一个单线程的线程池之中，由线程池线程来负责向对应的 JournalNode 的 JournalNodeRpcServer 发送 RPC 请求。JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。下面对基于 QJM 的共享存储系统的两个关键性问题同步数据和恢复数据进行详细分析。基于 QJM 的共享存储系统的数据同步机制分析Active NameNode 和 StandbyNameNode 使用 JouranlNode 集群来进行数据同步的过程如图 5 所示，Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog：图 5 . 基于 QJM 的共享存储的数据同步机制Active NameNode 提交 EditLog 到 JournalNode 集群当处于 Active 状态的 NameNode 调用 FSEditLog 类的 logSync 方法来提交 EditLog 的时候，会通过 JouranlSet 同时向本地磁盘目录和 JournalNode 集群上的共享存储目录写入 EditLog。写入 JournalNode 集群是通过并行调用每一个 JournalNode 的 QJournalProtocol RPC 接口的 journal 方法实现的，如果对大多数 JournalNode 的 journal 方法调用成功，那么就认为提交 EditLog 成功，否则 NameNode 就会认为这次提交 EditLog 失败。提交 EditLog 失败会导致 Active NameNode 关闭 JournalSet 之后退出进程，留待处于 Standby 状态的 NameNode 接管之后进行数据恢复。从上面的叙述可以看出，Active NameNode 提交 EditLog 到 JournalNode 集群的过程实际上是同步阻塞的，但是并不需要所有的 JournalNode 都调用成功，只要大多数 JournalNode 调用成功就可以了。如果无法形成大多数，那么就认为提交 EditLog 失败，NameNode 停止服务退出进程。如果对应到分布式系统的 CAP 理论的话，虽然采用了 Paxos 的“大多数”思想对 C(consistency，一致性) 和 A(availability，可用性) 进行了折衷，但还是可以认为 NameNode 选择了 C 而放弃了 A，这也符合 NameNode 对数据一致性的要求。Standby NameNode 从 JournalNode 集群同步 EditLog当 NameNode 进入 Standby 状态之后，会启动一个 EditLogTailer 线程。这个线程会定期调用 EditLogTailer 类的 doTailEdits 方法从 JournalNode 集群上同步 EditLog，然后把同步的 EditLog 回放到内存之中的文件系统镜像上 (并不会同时把 EditLog 写入到本地磁盘上)。这里需要关注的是：从 JournalNode 集群上同步的 EditLog 都是处于 finalized 状态的 EditLog Segment。“NameNode 的元数据存储概述”一节说过 EditLog Segment 实际上有两种状态，处于 in-progress 状态的 Edit Log 当前正在被写入，被认为是处于不稳定的中间态，有可能会在后续的过程之中发生修改，比如被截断。Active NameNode 在完成一个 EditLog Segment 的写入之后，就会向 JournalNode 集群发送 finalizeLogSegment RPC 请求，将完成写入的 EditLog Segment finalized，然后开始下一个新的 EditLog Segment。一旦 finalizeLogSegment 方法在大多数的 JournalNode 上调用成功，表明这个 EditLog Segment 已经在大多数的 JournalNode 上达成一致。一个 EditLog Segment 处于 finalized 状态之后，可以保证它再也不会变化。从上面描述的过程可以看出，虽然 Active NameNode 向 JournalNode 集群提交 EditLog 是同步的，但 Standby NameNode 采用的是定时从 JournalNode 集群上同步 EditLog 的方式，那么 Standby NameNode 内存中文件系统镜像有很大的可能是落后于 Active NameNode 的，所以 Standby NameNode 在转换为 Active NameNode 的时候需要把落后的 EditLog 补上来。基于 QJM 的共享存储系统的数据恢复机制分析处于 Standby 状态的 NameNode 转换为 Active 状态的时候，有可能上一个 Active NameNode 发生了异常退出，那么 JournalNode 集群中各个 JournalNode 上的 EditLog 就可能会处于不一致的状态，所以首先要做的事情就是让 JournalNode 集群中各个节点上的 EditLog 恢复为一致。另外如前所述，当前处于 Standby 状态的 NameNode 的内存中的文件系统镜像有很大的可能是落后于旧的 Active NameNode 的，所以在 JournalNode 集群中各个节点上的 EditLog 达成一致之后，接下来要做的事情就是从 JournalNode 集群上补齐落后的 EditLog。只有在这两步完成之后，当前新的 Active NameNode 才能安全地对外提供服务。补齐落后的 EditLog 的过程复用了前面描述的 Standby NameNode 从 JournalNode 集群同步 EditLog 的逻辑和代码，最终调用 EditLogTailer 类的 doTailEdits 方法来完成 EditLog 的补齐。使 JournalNode 集群上的 EditLog 达成一致的过程是一致性算法 Paxos 的典型应用场景，QJM 对这部分的处理可以看做是 Single Instance Paxos(参见参考文献 [3]) 算法的一个实现，在达成一致的过程中，Active NameNode 和 JournalNode 集群之间的交互流程如图 6 所示，具体描述如下：图 6.Active NameNode 和 JournalNode 集群的交互流程图生成一个新的 EpochEpoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制，为什么需要 fencing 已经在前面“ActiveStandbyElector 实现分析”一节的“防止脑裂”部分进行了说明。产生新 Epoch 的流程与 Zookeeper 的 ZAB(Zookeeper Atomic Broadcast) 协议在进行数据恢复之前产生新 Epoch 的过程完全类似：Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功。在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。选择需要数据恢复的 EditLog Segment 的 id需要恢复的 Edit Log 只可能是各个 JournalNode 上的最后一个 Edit Log Segment，如前所述，JournalNode 在处理完 newEpoch RPC 请求之后，会向 NameNode 返回它自己的本地磁盘上最新的一个 EditLog Segment 的起始事务 id，这个起始事务 id 实际上也作为这个 EditLog Segment 的 id。NameNode 会在所有这些 id 之中选择一个最大的 id 作为要进行数据恢复的 EditLog Segment 的 id。向 JournalNode 集群发送 prepareRecovery RPC 请求NameNode 接下来向 JournalNode 集群发送 prepareRecovery RPC 请求，请求的参数就是选出的 EditLog Segment 的 id。JournalNode 收到请求后返回本地磁盘上这个 Segment 的起始事务 id、结束事务 id 和状态 (in-progress 或 finalized)。这一步对应于 Paxos 算法的 Phase 1a 和 Phase 1b(参见参考文献 [3]) 两步。Paxos 算法的 Phase1 是 prepare 阶段，这也与方法名 prepareRecovery 相对应。并且这里以前面产生的新的 Epoch 作为 Paxos 算法中的提案编号 (proposal number)。只要大多数的 JournalNode 的 prepareRecovery RPC 调用成功返回，NameNode 就认为成功。选择进行同步的基准数据源，向 JournalNode 集群发送 acceptRecovery RPC 请求 NameNode 根据 prepareRecovery 的返回结果，选择一个 JournalNode 上的 EditLog Segment 作为同步的基准数据源。选择基准数据源的原则大致是：在 in-progress 状态和 finalized 状态的 Segment 之间优先选择 finalized 状态的 Segment。如果都是 in-progress 状态的话，那么优先选择 Epoch 比较高的 Segment(也就是优先选择更新的)，如果 Epoch 也一样，那么优先选择包含的事务数更多的 Segment。在选定了同步的基准数据源之后，NameNode 向 JournalNode 集群发送 acceptRecovery RPC 请求，将选定的基准数据源作为参数。JournalNode 接收到 acceptRecovery RPC 请求之后，从基准数据源 JournalNode 的 JournalNodeHttpServer 上下载 EditLog Segment，将本地的 EditLog Segment 替换为下载的 EditLog Segment。这一步对应于 Paxos 算法的 Phase 2a 和 Phase 2b(参见参考文献 [3]) 两步。Paxos 算法的 Phase2 是 accept 阶段，这也与方法名 acceptRecovery 相对应。只要大多数 JournalNode 的 acceptRecovery RPC 调用成功返回，NameNode 就认为成功。向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成上一步执行完成之后，NameNode 确认大多数 JournalNode 上的 EditLog Segment 已经从基准数据源进行了同步。接下来，NameNode 向 JournalNode 集群发送 finalizeLogSegment RPC 请求，JournalNode 接收到请求之后，将对应的 EditLog Segment 从 in-progress 状态转换为 finalized 状态，实际上就是将文件名从 edits_inprogress_${startTxid} 重命名为 edits_${startTxid}-${endTxid}，见“NameNode 的元数据存储概述”一节的描述。只要大多数 JournalNode 的 finalizeLogSegment RPC 调用成功返回，NameNode 就认为成功。此时可以保证 JournalNode 集群的大多数节点上的 EditLog 已经处于一致的状态，这样 NameNode 才能安全地从 JournalNode 集群上补齐落后的 EditLog 数据。需要注意的是，尽管基于 QJM 的共享存储方案看起来理论完备，设计精巧，但是仍然无法保证数据的绝对强一致，下面选取参考文献 [2] 中的一个例子来说明：假设有 3 个 JournalNode：JN1、JN2 和 JN3，Active NameNode 发送了事务 id 为 151、152 和 153 的 3 个事务到 JournalNode 集群，这 3 个事务成功地写入了 JN2，但是在还没能写入 JN1 和 JN3 之前，Active NameNode 就宕机了。同时，JN3 在整个写入的过程中延迟较大，落后于 JN1 和 JN2。最终成功写入 JN1 的事务 id 为 150，成功写入 JN2 的事务 id 为 153，而写入到 JN3 的事务 id 仅为 125，如图 7 所示 (图片来源于参考文献 [2])。按照前面描述的只有成功地写入了大多数的 JournalNode 才认为写入成功的原则，显然事务 id 为 151、152 和 153 的这 3 个事务只能算作写入失败。在进行数据恢复的过程中，会发生下面两种情况：图 7.JournalNode 集群写入的事务 id 情况如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段收到了 JN2 的回复，那么肯定会以 JN2 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 153。从恢复的结果来看，实际上可以认为前面宕机的 Active NameNode 对事务 id 为 151、152 和 153 的这 3 个事务的写入成功了。但是如果从 NameNode 自身的角度来看，这显然就发生了数据不一致的情况。如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段没有收到 JN2 的回复，那么肯定会以 JN1 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 150。在这种情况下，如果从 NameNode 自身的角度来看的话，数据就是一致的了。事实上不光本文描述的基于 QJM 的共享存储方案无法保证数据的绝对一致，大家通常认为的一致性程度非常高的 Zookeeper 也会发生类似的情况，这也从侧面说明了要实现一个数据绝对一致的分布式存储系统的确非常困难。NameNode 在进行状态转换时对共享存储的处理下面对 NameNode 在进行状态转换的过程中对共享存储的处理进行描述，使得大家对基于 QJM 的共享存储方案有一个完整的了解，同时也作为本部分的总结。NameNode 初始化启动，进入 Standby 状态在 NameNode 以 HA 模式启动的时候，NameNode 会认为自己处于 Standby 模式，在 NameNode 的构造函数中会加载 FSImage 文件和 EditLog Segment 文件来恢复自己的内存文件系统镜像。在加载 EditLog Segment 的时候，调用 FSEditLog 类的 initSharedJournalsForRead 方法来创建只包含了在 JournalNode 集群上的共享目录的 JournalSet，也就是说，这个时候只会从 JournalNode 集群之中加载 EditLog，而不会加载本地磁盘上的 EditLog。另外值得注意的是，加载的 EditLog Segment 只是处于 finalized 状态的 EditLog Segment，而处于 in-progress 状态的 Segment 需要后续在切换为 Active 状态的时候，进行一次数据恢复过程，将 in-progress 状态的 Segment 转换为 finalized 状态的 Segment 之后再进行读取。加载完 FSImage 文件和共享目录上的 EditLog Segment 文件之后，NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式。如前所述，EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog。而 StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。NameNode 从 Standby 状态切换为 Active 状态当 NameNode 从 Standby 状态切换为 Active 状态的时候，首先需要做的就是停止它在 Standby 状态的时候启动的线程和相关的服务，包括上面提到的 EditLogTailer 线程和 StandbyCheckpointer 线程，然后关闭用于读取 JournalNode 集群的共享目录上的 EditLog 的 JournalSet，接下来会调用 FSEditLog 的 initJournalSetForWrite 方法重新打开 JournalSet。不同的是，这个 JournalSet 内部同时包含了本地磁盘目录和 JournalNode 集群上的共享目录。这些工作完成之后，就开始执行“基于 QJM 的共享存储系统的数据恢复机制分析”一节所描述的流程，调用 FSEditLog 类的 recoverUnclosedStreams 方法让 JournalNode 集群中各个节点上的 EditLog 达成一致。然后调用 EditLogTailer 类的 catchupDuringFailover 方法从 JournalNode 集群上补齐落后的 EditLog。最后打开一个新的 EditLog Segment 用于新写入数据，同时启动 Active NameNode 所需要的线程和服务。NameNode 从 Active 状态切换为 Standby 状态当 NameNode 从 Active 状态切换为 Standby 状态的时候，首先需要做的就是停止它在 Active 状态的时候启动的线程和服务，然后关闭用于读取本地磁盘目录和 JournalNode 集群上的共享目录的 EditLog 的 JournalSet。接下来会调用 FSEditLog 的 initSharedJournalsForRead 方法重新打开用于读取 JournalNode 集群上的共享目录的 JournalSet。这些工作完成之后，就会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，EditLogTailer 线程会定时从 JournalNode 集群上同步 Edit Log。NameNode 高可用运维中的注意事项本节结合笔者的实践，从初始化部署和日常运维两个方面介绍一些在 NameNode 高可用运维中的注意事项。初始化部署如果在开始部署 Hadoop 集群的时候就启用 NameNode 的高可用的话，那么相对会比较容易。但是如果在采用传统的单 NameNode 的架构运行了一段时间之后，升级为 NameNode 的高可用架构的话，就要特别注意在升级的时候需要按照以下的步骤进行操作：对 Zookeeper 进行初始化，创建 Zookeeper 上的/hadoop-ha/${dfs.nameservices} 节点。创建节点是为随后通过 Zookeeper 进行主备选举做好准备，在进行主备选举的时候会在这个节点下面创建子节点 (具体可参照“ActiveStandbyElector 实现分析”一节的叙述)。这一步通过在原有的 NameNode 上执行命令 hdfs zkfc -formatZK 来完成。启动所有的 JournalNode，这通过脚本命令 hadoop-daemon.sh start journalnode 来完成。对 JouranlNode 集群的共享存储目录进行格式化，并且将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件 (具体可参照“NameNode 的元数据存储概述”一节的叙述) 之后的 EditLog 拷贝到 JournalNode 集群上的共享目录之中，这通过在原有的 NameNode 上执行命令 hdfs namenode -initializeSharedEdits 来完成。启动原有的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。对新增的 NameNode 节点进行初始化，将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件拷贝到这个新增的 NameNode 的本地磁盘上，同时需要验证 JournalNode 集群的共享存储目录上已经具有了这个 FSImage 文件之后的 EditLog(已经在第 3 步完成了)。这一步通过在新增的 NameNode 上执行命令 hdfs namenode -bootstrapStandby 来完成。启动新增的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。在这两个 NameNode 上启动 zkfc(ZKFailoverController) 进程，谁通过 Zookeeper 选主成功，谁就是主 NameNode，另一个为备 NameNode。这通过脚本命令 hadoop-daemon.sh start zkfc 完成。日常维护笔者在日常的维护之中主要遇到过下面两种问题：Zookeeper 过于敏感：Hadoop 的配置项中 Zookeeper 的 session timeout 的配置参数 ha.zookeeper.session-timeout.ms 的默认值为 5000，也就是 5s，这个值比较小，会导致 Zookeeper 比较敏感，可以把这个值尽量设置得大一些，避免因为网络抖动等原因引起 NameNode 进行无谓的主备切换。单台 JouranlNode 故障时会导致主备无法切换：在理论上，如果有 3 台或者更多的 JournalNode，那么挂掉一台 JouranlNode 应该仍然可以进行正常的主备切换。但是笔者在某次 NameNode 重启的时候，正好赶上一台 JournalNode 挂掉宕机了，这个时候虽然某一台 NameNode 通过 Zookeeper 选主成功，但是这台被选为主的 NameNode 无法成功地从 Standby 状态切换为 Active 状态。事后追查原因发现，被选为主的 NameNode 卡在退出 Standby 状态的最后一步，这个时候它需要等待到 JournalNode 的请求全部完成之后才能退出。但是由于有一台 JouranlNode 宕机，到这台 JournalNode 的请求都积压在一起并且在不断地进行重试，同时在 Hadoop 的配置项中重试次数的默认值非常大，所以就会导致被选为主的 NameNode 无法及时退出 Standby 状态。这个问题主要是 Hadoop 内部的 RPC 通信框架的设计缺陷引起的，Hadoop HA 的源代码 IPCLoggerChannel 类中有关于这个问题的 TODO，但是截止到社区发布的 2.7.1 版本这个问题仍然存在。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"Hadoop 2.x - Yarn","slug":"Hadoop-2-x-Yarn","date":"2016-10-04T05:30:26.000Z","updated":"2020-04-04T17:25:59.731Z","comments":true,"path":"2016/10/04/Hadoop-2-x-Yarn/","link":"","permalink":"cpeixin.cn/2016/10/04/Hadoop-2-x-Yarn/","excerpt":"","text":"What - YarnYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。Why - Yarn随着互联网的高速发展，新的计算框架不断出现，如内存计算框架、流式计算框架、迭代计算资源框架、这几种框架通常都会被用到考虑到资源的利用率运维和数据共享等因素，企业通常希望将所有的计算框架部署到一个 公共集群中，让他们共享集群的计算资源，并对资源进行同意使用，同时又能采用简单的资源隔离方案，这样便催生了轻量弹性计算平台需求Yarn的设计是一个弹性计算平台，不仅仅支持Mapreduce计算框架而是朝着对多种计算框架进行统一管理方向发展。优点:资源利利用率高，按照框架角度进行资源划分，往往存在应用程序数据和计算资源需求的不均衡性，使得某段时间内计算资源紧张，而另外一种计算方式的资源空闲，共享集群模式则通过框架共享全部的计算资源，使得集群中的资源更加充分合理的利用。运维成本低，如果使用：”一个框架一个集群“的模式，运维人员需要独立管理多个框架，进而增加运维的难度，共享模式通常只需要少数管理员可以完成多个框架的管理。数据共享，随着数据量的增加，跨集群之间的数据不仅增加了硬件成本，而且耗费时间，共享集群模式可以共享框架和硬件资源，大大降低了数据移动带来的成本。How - Yarnyarn是内置在Hadoop平台中的，所以在已经搭建好的Hadoop集群中就可以直接使用。对于其他计算框架，在配置文档配置后即可使用，例如spark的任务提交方式在生产环境中，一定是要用yarn方式来提交，例如：spark-submit –master yarn –class com.xx.xx.classname spark_job_name.jarYarn - 组成YARN总体上仍然是master/slave结构，在整个资源管理框架中，resourcemanager为master，nodemanager是slave。Resourcemanager负责对各个nademanger上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。YARN的基本组成结构，YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。ResourceManager是Master上一个独立运行的进程，负责集群统一的资源管理、调度、分配等等；NodeManager是Slave上一个独立运行的进程，负责上报节点的状态；App Master和Container是运行在Slave上的组件，Container是yarn中分配资源的一个单位，包涵内存、CPU等等资源，yarn以Container为单位分配资源。Client向ResourceManager提交的每一个应用程序都必须有一个Application Master，它经过ResourceManager分配资源后，运行于某一个Slave节点的Container中，具体做事情的Task，同样也运行与某一个Slave节点的Container中。RM，NM，AM乃至普通的Container之间的通信，都是用RPC机制。YARN的架构设计使其越来越像是一个云操作系统，数据处理操作系统。Yarn - 架构ResourcemanagerRM是一个全局的资源管理器，集群只有一个，负责整个系统的资源管理和分配，包括处理客户端请求、启动/监控APP master、监控nodemanager、资源的分配与调度。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）。调度器（Scheduler）调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。应用程序管理器（Applications Manager，ASM）应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。ApplicationMaster（AM）** **管理YARN内运行的应用程序的每个实例。功能：数据切分为应用程序申请资源并进一步分配给内部任务。任务监控与容错负责协调来自resourcemanager的资源，并通过nodemanager监视容易的执行和资源使用情况。NodeManager（NM）Nodemanager整个集群有多个，负责每个节点上的资源和使用。功能：单个节点上的资源管理和任务。处理来自于resourcemanager的命令。处理来自域app master的命令。Nodemanager管理着抽象容器，这些抽象容器代表着一些特定程序使用针对每个节点的资源。Nodemanager定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态（cpu和内存等资源）ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。需要注意的是，Container不同于MRv1中的slot，它是一个动态资源划分单位，是根据应用程序的需求动态生成的。目前为止，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。功能：对task环境的抽象描述一系列信息任务运行资源的集合（cpu、内存、io等）任务运行环境Yarn - 资源管理资源调度和隔离是yarn作为一个资源管理系统，最重要且最基础的两个功能。资源调度由resourcemanager完成，而资源隔离由各个nodemanager实现。Resourcemanager将某个nodemanager上资源分配给任务（这就是所谓的“资源调度”）后，nodemanager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础和保证，这就是所谓的资源隔离。当谈及到资源时，我们通常指内存、cpu、io三种资源。Hadoop yarn目前为止仅支持cpu和内存两种资源管理和调度。内存资源多少决定任务的生死，如果内存不够，任务可能运行失败；相比之下，cpu资源则不同，它只会决定任务的快慢，不会对任务的生死产生影响。 #### Yarn - 队列调度策略Yarn的队列调度策略主要分三种：FIFO、Capacity调度、Fair调度。FIFO调度策略：为先进去的任务分配资源，后入的任务等待前面任务完成才能获得资源。(大任务可能导致后续任务饿死)Capacity调度策略：将集群资源分为一条条队列，每个队列包含一定百分比资源。每个队列中任务采取FIFO的调度策略。在某些队列资源宽裕的情况下，允许跨队列申请资源，同时允许抢占机制。当其他队列任务使用了当前队列任务资源时，当前队列任务在等待一定时间后，允许抢占该队列资源(将改队列内不属于其他队列任务的Container杀死)。Fair调度策略：n个任务情况下，每个任务占据1/n份额的资源。在某任务结束后，该任务资源会被其余资源瓜分，每个任务占据1/(n-1)份资源。与Capacity一样也将集群分为队列且允许抢占机制。不同的是队列内部的资源调度同时允许FIFO和Fair调度。Yarn - 内存管理yarn允许用户配置每个节点上可用的物理内存资源，注意，这里是“可用的”，因为一个节点上内存会被若干个服务共享，比如一部分给了yarn，一部分给了hdfs，一部分给了hbase等，yarn配置的只是自己可用的，配置参数如下：yarn.nodemanager.resource.memory-mb表示该节点上yarn可以使用的物理内存总量，默认是8192m，注意，如果你的节点内存资源不够8g，则需要调减这个值，yarn不会智能的探测节点物理内存总量。yarn.nodemanager.vmem-pmem-ratio任务使用1m物理内存最多可以使用虚拟内存量，默认是2.1yarn.nodemanager.pmem-check-enabled是否启用一个线程检查每个任务证使用的物理内存量，如果任务超出了分配值，则直接将其kill，默认是true。yarn.nodemanager.vmem-check-enabled是否启用一个线程检查每个任务证使用的虚拟内存量，如果任务超出了分配值，则直接将其kill，默认是true。yarn.scheduler.minimum-allocation-mb单个任务可以使用最小物理内存量，默认1024m，如果一个任务申请物理内存量少于该值，则该对应值改为这个数。yarn.scheduler.maximum-allocation-mb单个任务可以申请的最多的内存量，默认8192mYarn - cpu管理目前cpu被划分为虚拟cpu，这里的虚拟cpu是yarn自己引入的概念，初衷是考虑到不同节点cpu性能可能不同，每个cpu具有计算能力也是不一样的，比如，某个物理cpu计算能力可能是另外一个物理cpu的2倍，这时候，你可以通过为第一个物理cpu多配置几个虚拟cpu弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟cpu个数。在yarn中，cpu相关配置参数如下：yarn.nodemanager.resource.cpu-vcores表示该节点上yarn可使用的虚拟cpu个数，默认是8个，注意，目前推荐将该值为与物理cpu核数相同。如果你的节点cpu合数不够8个，则需要调减小这个值，而yarn不会智能的探测节点物理cpu总数。yarn.scheduler.minimum-allocation-vcores单个任务可申请最小cpu个数，默认1，如果一个任务申请的cpu个数少于该数，则该对应值被修改为这个数yarn.scheduler.maximum-allocation-vcores单个任务可以申请最多虚拟cpu个数，默认是32.","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"cpeixin.cn/tags/yarn/"}]},{"title":"Hadoop 2.x - MapReduce","slug":"Hadoop-2-x-MapReduce","date":"2016-10-02T06:30:21.000Z","updated":"2020-04-04T17:26:12.269Z","comments":true,"path":"2016/10/02/Hadoop-2-x-MapReduce/","link":"","permalink":"cpeixin.cn/2016/10/02/Hadoop-2-x-MapReduce/","excerpt":"","text":"MapReduce 工作原理What - MapReduceHadoop主要解决了两个问题，海量数据的存储和海量数据的计算。MapReduce就是Hadoop大数据框架下的分布式计算框架，是基于Hadoop数据分析中的核心计算框架。同时，Mapreduce是一种编程模型，是一种编程方法，抽象理论Why - MapReduce用简洁的方式，就能实现 TB，PB级别数据在百台，千台，万台服务器上的并行运算，程序人员并不需要关心如何处理并行计算、如何分发数据、如何处理错误，这些问题都已经由mapreduce框架来处理。How - MapReduce我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。MapReduce 优缺点优点Mapreduce易于编程它简单的实现一些接口，就可以完成一个分布式程序，这个程序可以分布到大量的廉价的pc机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特性使的Mapreduce编程变得非常流行。良好的扩展性项目当你的计算资源得不到满足的时候，你可以通过简单的通过增加机器来扩展它的计算能力高容错性Mapreduce的设计初衷就是使程序能够部署在廉价的pc机器上，这就要求它具有很高的容错性。比如一个机器挂了，它可以把上面的计算任务转移到另一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由hadoop内部完成的。适合PB级以上海量数据的离线处理**缺点不擅长实时计算Mapreduce无法做到像Mysql那样做到毫秒或者秒级的返回结果不擅长流式计算流式计算的输入数据是动态的，而Mapreduce的输入数据集是静态的，不能流态变化。这是Mapreduce自身的设计特点决定了数据源必须是静态的。不擅长DAG(有向图)计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个应用程序的输出，在这种情况下，Mapreduce并不是不能做，而是使用后每个Mapreduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常低下。MapReduce 工作流程MapReduce整体流程图input file的切分成小文件，默认情况下是按照hdfs block块大小一致为128M对文件进行切分。Map的数量：input file文件按照128Msplit后，有多少个数据块，就对应着有多少个Map。Reduce的数量： 与Map阶段定义的Partition数量一致。MapReduce shuffle阶段：下图先从整体来看，other maps、 other reduces的指向，可以看出，一共有4个Map 和 3个 reduces。那么现在再细分来看，下图表示的是一个 map task 和 reduce task 以及 map 和 reduce之间的shuffle执行流程map 阶段开始后，会经过用户自定义逻辑对数据进行处理。完成map端处理完的数据，会被写入到环形缓冲区（buffer in memory）。这里的环形缓冲区需要说明一下，环形缓冲区的底层实现为环形队列， 默认大小为100MB数据在写入环形缓冲区后，首先进行分区（partition）, 随后对每个partition内的数据进行sort操作，这里的排序方式是按照字典排序，采用快速排序算法进行排序。当map阶段数据不断的向环形缓冲区写的过程中，环形缓冲区有一个阈值，默认为80%，当数据量达到80%后，缓冲区的数据会溢写到磁盘上（merge on disk）溢写到磁盘上后，可以想像成磁盘上存放了很多内部已经排序好，并且带有分区信息的小文件，这时候要对属于同一分区的小文件进行merge，并同时进行排序，这里的排序算法选择的是归并排序此时，每个Map端，都已经准备好了每个partition的文件。上图中，绿色箭头的指向，各个Map端，带有partition信息的文件，fetch到对应的reduce端，可以看到，红色虚线箭头 other reduces对应的磁盘文件则指向其他的两个reduce端。fetch到同一reduce端的partition数据，则属于同一分区数据，这时还要进行一次merge合并排序操作，排序算法选择的是归并排序，来合并成大文件最后，reduce端的shuffle阶段完成，合并好的大文件由reduce函数进行处理，最后到output输出结果。超超超超超超超超详细的工作流程图","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"cpeixin.cn/tags/mapreduce/"}]},{"title":"Hadoop 1.x - HDFS","slug":"Hadoop-1-x-HDFS","date":"2016-10-01T07:36:21.000Z","updated":"2020-04-04T17:26:36.675Z","comments":true,"path":"2016/10/01/Hadoop-1-x-HDFS/","link":"","permalink":"cpeixin.cn/2016/10/01/Hadoop-1-x-HDFS/","excerpt":"","text":"介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。假设与目标硬件故障硬件故障是正常现象，而非例外。HDFS实例可能包含数百或数千个服务器计算机，每个服务器计算机都存储文件系统数据的一部分。存在大量组件并且每个组件的故障概率都很低的事实意味着HDFS的某些组件始终无法运行。因此，检测故障并快速，自动地从故障中恢复是HDFS的核心目标。流数据访问在HDFS上运行的应用程序需要对其数据集进行流式访问。它们不是通常在通用文件系统上运行的通用应用程序。HDFS设计用于批处理，而不是用户交互使用。重点在于数据访问的高吞吐量，而不是数据访问的低延迟。POSIX提出了许多针对HDFS的应用程序不需要的硬性要求。在一些关键领域中，POSIX语义已经被交易以提高数据吞吐率。大数据集在HDFS上运行的应用程序具有大量数据集。HDFS中的典型文件大小为GB到TB。因此，HDFS已调整为支持大文件。它应提供较高的聚合数据带宽，并可以扩展到单个群集中的数百个节点。它应该在单个实例中支持数千万个文件。简单一致性模型HDFS应用程序需要文件一次写入多次读取访问模型。一旦创建，写入和关闭文件，除了追加和截断外，无需更改。支持将内容追加到文件末尾，但不能在任意点更新。该假设简化了数据一致性问题并实现了高吞吐量数据访问。MapReduce应用程序或Web爬网程序应用程序非常适合此模型。移动计算比移动数据便宜如果应用程序所请求的计算在其所操作的数据附近执行，则效率会更高。当数据集的大小巨大时，尤其如此。这样可以最大程度地减少网络拥塞，并提高系统的整体吞吐量。假设通常是将计算迁移到更靠近数据的位置，而不是将数据移动到应用程序正在运行的位置。HDFS为应用程序提供了接口，使它们自己更靠近数据所在的位置。跨异构硬件和软件平台的可移植性HDFS的设计目的是可以轻松地从一个平台移植到另一个平台。这有助于将HDFS广泛用作大量应用程序的首选平台。HDFS 设计原理2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。架构的稳定性心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。HDFS 的特点高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。附：图解HDFS存储原理说明：以下图片引用自博客：翻译经典 HDFS 原理讲解漫画HDFS写数据原理HDFS读数据原理HDFS故障类型和其检测方法检测故障并快速，自动地从故障中恢复是HDFS的核心目标。第二部分：读写故障的处理第三部分：DataNode 故障处理副本布局策略：HDFS shell-ls查看目录hdfs dfs -ls /-mkdir创建目录hdfs dfs -mkdir -p /aaa/bbb/cc/dd-rm删除文件或文件夹hdfs dfs -rm -r /aaa/bbb/cc/dd-rmdir删除空目录hdfs dfs -rmdir /aaa/bbb/cc/dd-puthdfs dfs -put /opt/jdk-8u181-linux-x64.tar.gz /opt/-gethdfs dfs -get /aaa/jdk.tar.gz-cp从hdfs的一个路径拷贝hdfs的另一个路径hdfs dfs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2-count统计一个指定目录下的文件节点数量hdfs dfs -count /-df统计文件系统的可用空间信息hdfs dfs -df -h /-setrep设置hdfs中文件的副本数量hdfs dfs -setrep 3 /aaa/jdk.tar.gzdifference between hdfs dfs and hadoop fshadoop fs是一种更“通用”的命令，它使您可以与包括Hadoop在内的多个文件系统进行交互，而hdfs dfs该命令专用于HDFS。请注意，如果使用的文件系统是HDFS ，则hdfs dfs and hadoop fs命令成为同义词。实际上，如果您发出命令，它将告诉您该命令已被弃用，您应该改用hdfs dfs但是，当您调用这些命令时，实际文件将在hadoop安装目录的bin目录中执行。如果运行hdfs dfs命令，它将调用hadoop安装目录中bin目录中的hdfs文件，后跟第一个参数dfs 告诉hdfs命令我们要使用分布式文件系统（hdfs）而不是本地文件系统文件系统，这是默认选项。第二个命令hadoop fs。它将在hadoop安装目录的bin目录中调用hadoop文件。它将通过发送fs作为第一个参数来跟进该命令/文件。它会告诉“ HADOOP”，我们想要做的任何操作都应该在该特定群集（即HDFS）上由HADOOP安装管理的文件系统上完成。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"算法-插入排序","slug":"算法-插入排序","date":"2016-08-21T15:22:43.000Z","updated":"2020-04-04T15:08:37.181Z","comments":true,"path":"2016/08/21/算法-插入排序/","link":"","permalink":"cpeixin.cn/2016/08/21/%E7%AE%97%E6%B3%95-%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/","excerpt":"","text":"我们先来看一个问题。一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。那插入排序具体是如何借助上面的思想来实现排序的呢？首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。为什么说移动次数就等于逆序度呢？我拿刚才的例子画了一个图表，你一看就明白了。满有序度是 n*(n-1)/2=15，初始序列的有序度是 5，所以逆序度是 10。插入排序中，数据移动的个数总和也等于 10=3+3+4。插入排序的原理也很简单吧？我也将代码实现贴在这里，你可以结合着代码再看下。123456789101112def insertion_sort(a: List[int]): length = len(a) if length &lt;= 1: return for i in range(1, length): value = a[i] j = i - 1 while j &gt;= 0 and a[j] &gt; value: a[j + 1] = a[j] j -= 1 a[j + 1] = value现在，我们来看点稍微复杂的东西。我这里还是有三个问题要问你。第一，插入排序是原地排序算法吗？从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。第二，插入排序是稳定的排序算法吗？在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。第三，插入排序的时间复杂度是多少？如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n)。注意，这里是从尾到头遍历已经有序的数据。如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n2)。还记得我们在数组中插入一个数据的平均时间复杂度是多少吗？没错，是 O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n2)。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法-冒泡排序","slug":"算法-冒泡排序","date":"2016-08-20T15:22:43.000Z","updated":"2020-04-04T15:08:37.246Z","comments":true,"path":"2016/08/20/算法-冒泡排序/","link":"","permalink":"cpeixin.cn/2016/08/20/%E7%AE%97%E6%B3%95-%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法，可能就是排序。大部分编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序。排序非常重要，所以我会花多一点时间来详细讲一讲经典的排序算法。排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。我按照时间复杂度把它们分成了三类，分三节课来讲解。带着问题去学习，是最有效的学习方法。所以按照惯例，我还是先给你出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是 O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？你可以先思考一两分钟，带着这个问题，我们开始今天的内容！如何分析一个“排序算法”？学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：最好情况、最坏情况、平均情况时间复杂度我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。时间复杂度的系数、常数 、低阶我们知道，时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。比较次数和交换（或移动）次数这一节和下一节讲的都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。排序算法的内存消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。我们今天讲的三种排序算法，都是原地排序算法。排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。我通过一个例子来解释一下。比如我们有一组数据 2，9，3，4，8，3，按照大小排序之后就是 2，3，3，4，8，9。这组数据里有两个 3。经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。你可能要问了，两个 3 哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序。比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。冒泡排序（Bubble Sort）我们从冒泡排序开始，学习今天的三种排序算法。冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。我用一个例子，带你看下冒泡排序的整个过程。我们要对一组数据 4，5，6，3，2，1，从小到大进行排序。第一次冒泡操作的详细过程就是这样：可以看出，经过一次冒泡操作之后，6 这个元素已经存储在正确的位置上。要想完成所有数据的排序，我们只要进行 6 次这样的冒泡操作就行了。实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给 6 个元素排序，只需要 4 次冒泡操作就可以了。冒泡排序算法的原理比较容易理解，具体的代码我贴到下面，你可以结合着代码来看我前面讲的原理。12345678910111213def bubble_sort(a: List[int]): length = len(a) if length &lt;= 1: return for i in range(length): made_swap = False for j in range(length - i - 1): if a[j] &gt; a[j + 1]: a[j], a[j + 1] = a[j + 1], a[j] made_swap = True if not made_swap: break现在，结合刚才我分析排序算法的三个方面，我有三个问题要问你。第一，冒泡排序是原地排序算法吗？冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。第二，冒泡排序是稳定的排序算法吗？在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。第三，冒泡排序的时间复杂度是多少？最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n2)。最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。对于包含 n 个数据的数组，这 n 个数据就有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：有序元素对：a[i] &lt;= a[j], 如果i &lt; j。同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是 n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。逆序度的定义正好跟有序度相反（默认从小到大为有序），我想你应该已经想到了。关于逆序度，我就不举例子讲了。你可以对照我讲的有序度的例子自己看下。逆序元素对：a[i] &gt; a[j], 如果i &lt; j。关于这三个概念，我们还可以得到一个公式：逆序度 = 满有序度 - 有序度。我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是 4，5，6，3，2，1 ，其中，有序元素对有 (4，5) (4，6)(5，6)，所以有序度是 3。n=6，所以排序完成之后终态的满有序度为 n*(n-1)/2=15。冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n(n-1)/2–初始有序度。此例中就是 15–3=12，要进行 12 次交换操作。对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n(n-1)/2 次交换。最好情况下，初始状态的有序度是 n(n-1)/2，就不需要进行交换。我们可以取个中间值 n(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。换句话说，平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以平均情况下的时间复杂度就是 O(n2)。这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法-递归","slug":"算法-递归","date":"2016-08-19T15:22:43.000Z","updated":"2020-04-04T15:08:37.246Z","comments":true,"path":"2016/08/19/算法-递归/","link":"","permalink":"cpeixin.cn/2016/08/19/%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92/","excerpt":"","text":"推荐注册返佣金的这个功能我想你应该不陌生吧？现在很多 App 都有这个功能。这个功能中，用户 A 推荐用户 B 来注册，用户 B 又推荐了用户 C 来注册。我们可以说，用户 C 的“最终推荐人”为用户 A，用户 B 的“最终推荐人”也为用户 A，而用户 A 没有“最终推荐人”。一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中 actor_id 表示用户 id，referrer_id 表示推荐人 id。基于这个背景，我的问题是，给定一个用户 ID，如何查找这个用户的“最终推荐人？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！如何理解“递归”？从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。不过，别看我说了这么多，递归本身可是一点儿都不“高冷”，咱们生活中就有很多用到递归的例子。周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？别忘了你是程序员，这个可难不倒你，递归就开始排上用场了。于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：12345int f(int n) &#123; if (n == 1) return 1; return f(n-1) + 1;&#125;递归需要满足的三个条件刚刚这个例子是非常典型的递归，那究竟什么样的问题可以用递归来解决呢？我总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。一个问题的解可以分解为几个子问题的解何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。存在递归终止条件把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是 f(1)=1，这就是递归的终止条件。如何编写递归代码刚刚铺垫了这么多，现在我们来看，如何来写递归代码？我个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。你先记住这个理论。我举一个例子，带你一步一步实现一个递归代码，帮你理解。假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：1f(n) = f(n-1)+f(n-2)有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以 f(1)=1。这个递归终止条件足够吗？我们可以用 n=2，n=3 这样比较小的数试验一下。n=2 时，f(2)=f(1)+f(0)。如果递归终止条件只有一个 f(1)=1，那 f(2) 就无法求解了。所以除了 f(1)=1 这一个递归终止条件外，还要有 f(0)=1，表示走 0 个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。所以，我们可以把 f(2)=2 作为一种终止条件，表示走 2 个台阶，有两种走法，一步走完或者分两步来走。所以，递归终止条件就是 f(1)=1，f(2)=2。这个时候，你可以再拿 n=3，n=4 来验证一下，这个终止条件是否足够并且正确。我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的123456int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2);&#125;我总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。虽然我讲了这么多方法，但是作为初学者的你，现在是不是还是有种想不太清楚的感觉呢？实际上，我刚学递归的时候，也有这种感觉，这也是文章开头我说递归代码比较难理解的地方。刚讲的电影院的例子，我们的递归调用只有一个分支，也就是说“一个问题只需要分解为一个子问题”，我们很容易能够想清楚“递“和”归”的每一个步骤，所以写起来、理解起来都不难。但是，当我们面对的是一个问题要分解为多个子问题的情况，递归代码就没那么好理解了。像我刚刚讲的第二个例子，人脑几乎没办法把整个“递”和“归”的过程一步一步都想清楚。计算机擅长做重复的事情，所以递归正和它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。递归代码要警惕堆栈溢出在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果会非常严重。为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？我在“栈”那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。比如前面的讲到的电影院的例子，如果我们将系统栈或者 JVM 堆栈大小设置为 1KB，在求解 f(19999) 时便会出现如下堆栈报错：12Exception in thread \"main\" java.lang.StackOverflowError那么，如何避免出现堆栈溢出呢？我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了，直接返回报错。还是电影院那个例子，我们可以改造成下面这样子，就可以避免堆栈溢出了。不过，我写的代码是伪代码，为了代码简洁，有些边界条件没有考虑，比如 x&lt;=0。1234567891011// 全局变量，表示递归的深度。int depth = 0;int f(int n) &#123; ++depth； if (depth &gt; 1000) throw exception; if (n == 1) return 1; return f(n-1) + 1;&#125;但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。递归代码要警惕重复计算为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。解答开篇到此为止，递归相关的基础知识已经讲完了，咱们来看一下开篇的问题：如何找到“最终推荐人”？我的解决方案是这样的：123456long findRootReferrerId(long actorId) &#123; Long referrerId = select referrer_id from [table] where actor_id = actorId; if (referrerId == null) return actorId; return findRootReferrerId(referrerId);&#125;是不是非常简洁？用三行代码就能搞定了，不过在实际项目中，上面的代码并不能工作，为什么呢？这里面有两个问题。第一，如果递归很深，可能会有堆栈溢出的问题。第二，如果数据库里存在脏数据，我们还需要处理由此产生的无限递归问题。比如 demo 环境下数据库中，测试工程师为了方便测试，会人为地插入一些数据，就会出现脏数据。如果 A 的推荐人是 B，B 的推荐人是 C，C 的推荐人是 A，这样就会发生死循环。第一个问题，我前面已经解答过了，可以用限制递归深度来解决。第二个问题，也可以用限制递归深度来解决。不过，还有一个更高级的处理方法，就是自动检测 A-B-C-A 这种“环”的存在。如何来检测环的存在呢？这个我暂时不细说，你可以自己思考下，后面的章节我们还会讲。内容小结关于递归的知识，到这里就算全部讲完了。我来总结一下。递归是一种非常高效、简洁的编码技巧。只要是满足“三个条件”的问题就可以通过递归代码来解决。不过递归代码也比较难写、难理解。编写递归代码的关键就是不要把自己绕进去，正确姿势是写出递推公式，找出终止条件，然后再翻译成递归代码。递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"数据结构-链表反转","slug":"数据结构-链表反转","date":"2016-08-18T14:16:51.000Z","updated":"2020-04-04T17:34:24.764Z","comments":true,"path":"2016/08/18/数据结构-链表反转/","link":"","permalink":"cpeixin.cn/2016/08/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/","excerpt":"","text":"题目：输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL这道题目可以用迭代，递归两种方法来实现迭代假设存在链表 1 → 2 → 3 → 4 → 5 → Ø，我们想要把它改成 Ø ← 1 ← 2 ← 3 ← 4 ← 5。在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！根据题目总结一下核心的操作逻辑：从头节点 1 开始，1 指向 2，更改指向顺序，需要将 2 的指针指向 1 (此时，虽然 2 的指针方向已经指向 1，但是 1 的指针方向没有改变，依然指向 2，则是 2 → 1 → 2 → 1….. ), 同时也要存储 2 的下一个节点 3，以便接下来迭代操作 2 和 3 节点之间的指针反转，每两个相邻节点进行相同的操作。迭代最后，尾节点 5 无后续节点，则跳出迭代。在迭代完相邻节点的指针反转后，还需要做的两步原头节点指向 None，变成尾节点将原尾节点设置成新的头节点操作完成～～！！12345678910111213141516171819202122232425262728293031323334def reversed_self(self): \"\"\"翻转链表自身.\"\"\" if self.head is None or self.head.next_node is None: return pre = self.head node = self.head.next_node while node is not None: pre,node = self.__reversed_with_two_node(pre,node) \"\"\"循环到最后一位 node is None退出\"\"\" \"\"\"将原链表的头节点指向为下一节点 改为 指向为None\"\"\" self.head.next_node = None \"\"\"将头节点设置为原链表的尾节点,链表反转则成功。链表元素位置不变，但是元素之间的指向改变\"\"\" self.head = predef __reversed_with_two_node(self, pre, node): \"\"\"翻转相邻两个节点. 参数: pre:前一个节点 node:当前节点 返回: *******(pre,node):下一个相邻节点的元组 \"\"\" tmp = node.next_node node.next_node = pre pre = node node = tmp return pre, node调试图：123while node is not None: pre,node = self.__reversed_with_two_node(pre,node)核心步骤迭代完，链表中的节点指向已经反转上图。此时，节点之间指针反转完毕，虽然指针反转了，但是头节点还是 1 ， 因为 1 指向仍然为 2 还没有处理。并且 5 还不是头节点，对应的还没有执行12self.head.next_node = Noneself.head = pre操作完下面两步后，上图中，1 的下一个节点指针变成None， 头节点赋值为 512self.head.next_node = Noneself.head = pre","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-如何写好链表代码","slug":"数据结构-如何写好链表代码","date":"2016-08-17T14:15:49.000Z","updated":"2020-04-04T11:59:24.090Z","comments":true,"path":"2016/08/17/数据结构-如何写好链表代码/","link":"","permalink":"cpeixin.cn/2016/08/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E5%86%99%E5%A5%BD%E9%93%BE%E8%A1%A8%E4%BB%A3%E7%A0%81/","excerpt":"","text":"如何写好链表代码？理解指针或引用的含义什么是指针？指针是一个变量将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。警惕指针丢失和内存泄漏在插入和删除结点时，要注意先持有后面的结点再操作，否者一旦后面结点的前继指针被断开，就无法再访问，导致内存泄漏。插入结点时，一定要注意操作的顺序删除链表结点时，也一定要记得手动释放内存空间利用哨兵简化难度链表的插入、删除操作，需要对插入第一个结点和删除最后一个节点做特殊处理。利用哨兵对象可以不用边界判断，链表的哨兵对象是只存指针不存数据的头结点。重点留意边界条件处理操作链表时要考虑如果链表为空时，代码是否能正常工作？如果链表只包含一个结点时，代码是否能正常工作？如果链表只包含两个结点时，代码是否能正常工作？代码逻辑在处理头结点和尾结点的时候，是否能正常工作？学习数据结构和算法主要是掌握一系列思想，能在其它的编码中也养成考虑边界的习惯。举例画图，辅助思考对于比较复杂的操作，可以用纸笔画一画，释放脑容量来做逻辑处理（时间换空间思想），也便于完成后的检查。多写多练，没有捷径孰能生巧，不管是什么算法，只有经过反复的练习，才能信手拈来。哨兵对象思想，在 iOS AutoreleasePool 中有用到，在 AutoreleasePoolPush 时添加一个哨兵对象，Pop 时将到哨兵对象之间的所有 Autorelease 对象发送 release 消息。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-如何实现LRU缓存淘汰算法","slug":"数据结构-如何实现LRU缓存淘汰算法","date":"2016-08-16T07:30:21.000Z","updated":"2020-04-04T11:59:17.945Z","comments":true,"path":"2016/08/16/数据结构-如何实现LRU缓存淘汰算法/","link":"","permalink":"cpeixin.cn/2016/08/16/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","excerpt":"","text":"今天我们来聊聊“链表（Linked list）”这个数据结构。学习链表有什么用呢？为了回答这个问题，我们先来讨论一个经典的链表应用场景，那就是 LRU 缓存淘汰算法。缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）最少使用策略 LFU（Least Frequently Used）最近最少使用策略 LRU（Least Recently Used）这些策略你不用死记，我打个比方你很容易就明白了。假如说，你买了很多本技术书，但有一天你发现，这些书太多了，太占书房空间了，你要做个大扫除，扔掉一些书籍。那这个时候，你会选择扔掉哪些书呢？对应一下，你的选择标准是不是和上面的三种策略神似呢？好了，回到正题，我们今天的开篇问题就是：如何用链表来实现 LRU 缓存淘汰策略呢？ 带着这个问题，我们开始今天的内容吧！五花八门的链表结构相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常将会放到一块儿来比较。所以我们先来看，这两者有什么区别。我们先从底层的存储结构上来看一看。为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表双向链表循环链表我们首先来看最简单、最常用的单链表。我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针 next。从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。与数组一样，链表也支持数据的查找、插入和删除操作。我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。但是，有利就有弊。链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第 k 位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要 O(n) 的时间复杂度。好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，循环链表和双向链表。循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。单链表和循环链表是不是都不难？接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适合解决哪种问题呢？从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是 O(1) 了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。我们先来看删除操作。在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：删除结点中“值等于某个给定值”的结点；删除给定指针指向的结点。对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉 Java 语言，你肯定用过 LinkedHashMap 这个容器。如果你深入研究 LinkedHashMap 的实现原理，就会发现其中就用到了双向链表这种数据结构。实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。**当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。所以我总结一下，对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不用我多讲，你应该知道双向循环链表长什么样子了吧？你可以自己试着在纸上画一画。链表 VS 数组性能大比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。你可能会说，我们 Java 中的 ArrayList 容器，也可以支持动态扩容啊？我们上一节课讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。我举一个稍微极端的例子。如果我们用 ArrayList 存储了了 1GB 大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList 会申请一个 1.5GB 大小的存储空间，并且把原来那 1GB 的数据拷贝到新申请的空间上。听起来是不是就很耗时？除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。解答开篇好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。如果此数据没有在缓存链表中，又可以分为两种情况：如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。这样我们就用链表实现了一个 LRU 缓存，是不是很简单？现在我们来看下 m 缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？我把这个问题留给你思考。LRU最后回到前题，来实现LRU缓存机制设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。O(1) 时间复杂度内完成这两种操作。设计lru算法的思路如何表示最近访问的数据和最早访问的数据如何查找是否缓存了数据有缓存，如何处理数据没有缓存，如何处理缓存未满缓存已满实现LRU数据结构选型有序字典题目要求实现 LRU 缓存机制，需要在 O(1)O(1) 时间内完成如下操作：获取键 / 检查键是否存在设置键删除最先插入的键前两个操作可以用标准的哈希表在 O(1)O(1) 时间内完成。有一种叫做有序字典的数据结构，综合了哈希表和链表，在 Python 中为 OrderedDict，在 Java 中为 LinkedHashMap。哈希表 + 双向链表这个问题可以用哈希表，辅以双向链表记录键值对的信息。所以可以在 O(1) 时间内完成 put 和 get 操作，同时也支持 O(1) 删除第一个添加的节点。使用双向链表的一个好处是不需要额外信息删除一个节点，同时可以在常数时间内从头部或尾部插入删除节点。一个需要注意的是，在双向链表实现中，这里使用一个伪头部和伪尾部标记界限，这样在更新的时候就不需要检查是否是 null 节点。实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125class DbListNode(object): def __init__(self, x, y): \"\"\" 节点为哈希表+双向链表 :param x: :param y: \"\"\" self.key = x self.value = y self.next = None self.prev = Noneclass LRUCache(object): def __init__(self, capacity): \"\"\" 初始化一个空双向链表 :type capacity: int \"\"\" self.cap = capacity self.catche = &#123;&#125; self.top = DbListNode(None, -1) self.tail = DbListNode(None, -1) self.top.next = self.tail self.tail.prev = self.top def get(self, key): \"\"\" :type key: int :rtype: int \"\"\" \"\"\"判断节点是否存在\"\"\" if key in self.catche.keys(): cur = self.catche[key] \"\"\"首先跳出原来的位置\"\"\" cur.prev.next = cur.next cur.next.prev = cur.prev \"\"\"top,tail为哨兵节点\"\"\" top_node = self.top.next cur.next = top_node top_node.prev = cur self.top.next = cur cur.prev = self.top return cur.value return -1 def put(self, key, value): \"\"\" :type key: int :type value: int :rtype: None \"\"\" if key in self.catche.keys(): \"\"\"如果插入节点存在，则将插入节点调换为位，插入到哨兵节点后的头节点。此时不存在增删节点，所以不用判断链表长度\"\"\" cur = self.catche[key] \"\"\"首先跳出原来的位置\"\"\" cur.prev.next = cur.next cur.next.prev = cur.prev \"\"\"top,tail为哨兵节点\"\"\" top_node = self.top.next cur.next = top_node top_node.prev = cur self.top.next = cur cur.prev = self.top else: # 增加新结点至首部 cur = DbListNode(key, value) self.catche[key] = cur # 最近用过的置于链表首部 top_node = self.top.next self.top.next = cur cur.prev = self.top cur.next = top_node top_node.prev = cur \"\"\"判断长度删除尾节点\"\"\" if len(self.catche.keys()) &gt; self.cap: self.catche.pop(self.tail.prev.key) # 去掉原尾结点 self.tail.prev.prev.next = self.tail self.tail.prev = self.tail.prev.prev def __repr__(self): vals = [] p = self.top.next while p.next: vals.append(str(p.value)) p = p.next return '-&gt;'.join(vals)if __name__ == '__main__': cache = LRUCache(2) cache.put(1, 1) cache.put(2, 2) print(cache) cache.get(1) # 返回 1 print(cache) cache.put(3, 3) # 该操作会使得密钥 2 作废 print(cache) cache.get(2) # 返回 -1 (未找到) print(cache) cache.put(4, 4) # 该操作会使得密钥 1 作废 print(cache) cache.get(1) # 返回 -1 (未找到) cache.get(3) # 返回 3 print(cache) cache.get(4) # 返回 4 print(cache)内容小结今天我们讲了一种跟数组“相反”的数据结构，链表。它跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。课后思考如何判断一个字符串是否是回文字符串的问题，我想你应该听过，我们今天的题目就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？欢迎留言和我分享，我会第一时间给你反馈。Q&amp;A关于CPU缓存机制CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块(这个大小我不太确定。。)并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LRU淘汰算法","slug":"LRU淘汰算法","permalink":"cpeixin.cn/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"}]},{"title":"数据结构-栈","slug":"数据结构-栈","date":"2016-08-14T09:27:21.000Z","updated":"2020-04-04T17:30:57.853Z","comments":true,"path":"2016/08/14/数据结构-栈/","link":"","permalink":"cpeixin.cn/2016/08/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88/","excerpt":"","text":"浏览器的前进、后退功能，我想你肯定很熟悉吧？当你依次访问完一串页面 a-b-c 之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面 b 和 a。当你后退到页面 a，点击前进按钮，就可以重新查看页面 b 和 c。但是，如果你后退到页面 b 后，点击了新的页面 d，那就无法再通过前进、后退功能查看页面 c 了。假设你是 Chrome 浏览器的开发工程师，你会如何实现这个功能呢？这就要用到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。如何理解“栈”？关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为我觉得，相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表不就好了吗？为什么还要用这个“操作受限”的“栈”呢？事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。如何实现一个“栈”？从刚才栈的定义里，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。我这里实现一个基于数组的顺序栈。基于链表实现的链式栈的代码，基于数组12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class stack: def __init__(self, size): \"\"\" 栈结构 :param size: 栈大小 \"\"\" self.count = 0 self.size = size self.array = [] def push(self, value): \"\"\" 入栈 入栈判满 :param value: \"\"\" if self.count == self.size: return False self.array.append(value) self.count += 1 return True def pop(self): \"\"\" 出栈 出栈判空 :return: \"\"\" if self.count == 0: return False data = self.array[self.count - 1] self.count -= 1 return dataif __name__ == '__main__': st = stack(4) st.push(1) st.push(2) st.push(3) st.push(4) st.pop() st.pop() st.pop() data = st.pop() print(data) st.pop()基于链表12345678910111213141516171819202122232425262728293031323334353637383940414243from typing import Optionalclass Node: def __init__(self, data: int, next=None): self.data = data self.next = nextclass linkstack: def __init__(self): self.top: Node = None def push(self, value: int): new_node = Node(value) new_node.next = self.top self.top = new_node def pop(self)-&gt;Optional[int]: if self.top: data = self.top.data self.top = self.top.next return data def __repr__(self) -&gt; str: current = self._top nums = [] while current: nums.append(current._data) current = current._next return \" \".join(f\"&#123;num&#125;]\" for num in nums)if __name__ == \"__main__\": stack = linkstack() for i in range(9): stack.push(i) print(stack) for _ in range(3): stack.pop() print(stack)了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？不管是顺序栈还是链式栈，我们存储数据只需要一个大小为 n 的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。注意，这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。因为，这 n 个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。空间复杂度分析是不是很简单？时间复杂度也不难。不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 O(1)。支持动态扩容的顺序栈刚才那个基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储 next 指针，内存消耗相对较多。那我们如何基于数组实现一个可以支持动态扩容的栈呢？你还记得，我们在数组那一节，是如何来实现一个支持动态扩容的数组的吗？当数组空间不够时，我们就重新申请一块更大的内存，将原来数组中数据统统拷贝过去。这样就实现了一个支持动态扩容的数组。所以，如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。我画了一张图，你可以对照着理解一下。实际上，支持动态扩容的顺序栈，我们平时开发中并不常用到。我讲这一块的目的，主要还是希望带你练习一下前面讲的复杂度分析方法。所以这一小节的重点是复杂度分析。你不用死记硬背入栈、出栈的时间复杂度，你需要掌握的是分析方法。能够自己分析才算是真正掌握了。现在我就带你分析一下支持动态扩容的顺序栈的入栈、出栈操作的时间复杂度。对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。也就是说，对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。那平均情况下的时间复杂度又是多少呢？还记得我们在复杂度分析那一节中讲的摊还分析法吗？这个入栈操作的平均情况下的时间复杂度可以用摊还分析法来分析。我们也正好借此来实战一下摊还分析法。为了分析的方便，我们需要事先做一些假设和定义：栈空间不够时，我们重新申请一个是原来大小两倍的数组；为了简化分析，假设只有入栈操作没有出栈操作；定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。为了让你更加直观地理解这个过程，我画了一张图。你应该可以看出来，这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。通过这个例子的实战分析，也印证了前面讲到的，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。栈在函数调用中的应用前面我讲的都比较偏理论，我们现在来看下，栈在软件工程中的实际应用。栈作为一个比较基础的数据结构，应用场景还是蛮多的。其中，比较经典的一个应用场景就是函数调用栈。我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。为了让你更好地理解，我们一块来看下这段代码的执行过程。123456789101112131415int main() &#123; int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(\"%d\", res); reuturn 0;&#125;int add(int x, int y) &#123; int sum = 0; sum = x + y; return sum;&#125;从代码中我们可以看出，main() 函数调用了 add() 函数，获取计算结果，并且与临时变量 a 相加，最后打印 res 的值。为了让你清晰地看到这个过程对应的函数栈里出栈、入栈的操作，我画了一张图。图中显示的是，在执行到 add() 函数时，函数调用栈的情况。栈在表达式求值中的应用我们再来看栈的另一个常见的应用场景，编译器如何利用栈来实现表达式求值。为了方便解释，我将算术表达式简化为只包含加减乘除四则运算，比如：34+139+44-12/3。对于这个四则运算，我们人脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事儿。如果换作你，让你来实现这样一个表达式求值的功能，你会怎么做呢？实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。我将 3+58-6 这个表达式的计算过程画成了一张图，你可以结合图来理解我刚讲的计算过程。这样用两个栈来解决的思路是不是非常巧妙？你有没有想到呢？栈在括号匹配中的应用除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[] ()[{}]}或[{()}([])]等都为合法格式，而{[}()]或[({)]为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。解答开篇好了，我想现在你已经完全理解了栈的概念。我们再回来看看开篇的思考题，如何实现浏览器的前进、后退功能？其实，用两个栈就可以非常完美地解决这个问题。我们使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。比如你顺序查看了 a，b，c 三个页面，我们就依次把 a，b，c 压入栈，这个时候，两个栈的数据就是当你通过浏览器的后退按钮，从页面 c 后退到页面 a 之后，我们就依次把 c 和 b 从栈 X 中弹出，并且依次放入到栈 Y。这个时候，两个栈的数据就是这个样子：这个时候你又想看页面 b，于是你又点击前进按钮回到 b 页面，我们就把 b 再从栈 Y 中出栈，放入栈 X 中。此时两个栈的数据是这个样子：这个时候，你通过页面 b 又跳转到新的页面 d 了，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。此时两个栈的数据这个样子：内容小结我们来回顾一下今天讲的内容。栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。除此之外，我们还讲了一种支持动态扩容的顺序栈，你需要重点掌握它的均摊时间复杂度分析方法。课后思考为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。我们都知道，JVM 内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"栈","slug":"栈","permalink":"cpeixin.cn/tags/%E6%A0%88/"}]},{"title":"数据结构-队列","slug":"数据结构-队列","date":"2016-08-13T10:30:21.000Z","updated":"2020-04-04T17:32:07.465Z","comments":true,"path":"2016/08/13/数据结构-队列/","link":"","permalink":"cpeixin.cn/2016/08/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97/","excerpt":"","text":"我们知道，CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学的内容，队列（queue）。如何理解“队列”队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的 “队列”。我们知道，栈只支持两个基本操作： 入栈 push()和出栈 pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。所以，队列跟栈一样，也是一种操作受限的线性表数据结构。队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。顺序队列和链式队列我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在队头删除元素，那究竟该如何实现一个队列呢？跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。顺序队列数组实现**我们先来思考一下用数组实现队列，对于栈来说，我们只需要一个栈顶指针就可以了。但是队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾。你可以结合下面这幅图来理解。当 a、b、c、d 依次入队之后，队列中的 head 指针指向下标为 0 的位置，tail 指针指向下标为 4 的位置。当我们调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置。你肯定已经发现了，在固定长度的数组中，随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。再向后加，就会产生数组越界的错误。可实际上，我们的队列在下标为0和1的地方还是空闲的。此时又不应该扩充数组，我们把这种现象叫做“假溢出”。否则会造成数组越界而遭致程序出错。这个问题该如何解决呢？你是否还记得，在数组那一节，我们也遇到过类似的问题，就是数组的删除操作会导致数组中的数据不连续。你还记得我们当时是怎么解决的吗？对，用数据搬移！但是，每次进行出队操作都相当于删除数组下标为 0 的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的 O(1) 变为 O(n)。能不能优化一下呢？实际上，我们在出队时可以不用搬移数据。如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作。借助这个思想，出队函数 dequeue() 保持不变，我们稍加改造一下入队函数 enqueue() 的实现，就可以轻松解决刚才的问题了。下面是具体的代码：我们先来看下基于数组，需要进行数据迁移的实现方法。12345678910111213141516171819202122232425262728293031from typing import Optionalclass ArrayQueue: def __init__(self, capacity: int): self._items = [] self._capacity = capacity self._head = 0 self._tail = 0 def enqueue(self, item: str) -&gt; bool: if self._tail == self._capacity: if self._head == 0: return False else: for i in range(0, self._tail - self._head): self._items[i] = self._items[i + self._head] self._tail = self._tail - self._head self._head = 0 self._items.insert(self._tail, item) self._tail += 1 return True def dequeue(self) -&gt; Optional[str]: if self._head != self._tail: item = self._items[self._head] self._head += 1 return item else: return None从代码中我们看到，当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。链式队列链表实现接下来，我们再来看下基于链表的队列实现方法。基于链表的实现，我们同样需要两个指针：head 指针和 tail 指针。它们分别指向链表的第一个结点和最后一个结点。如图所示，入队时，tail-&gt;next= new_node, tail = tail-&gt;next；出队时，head = head-&gt;next。123456789101112131415161718192021222324252627282930313233343536373839404142class Node: def __init__(self, data, next_node=None): self.data = data self.next = next_nodeclass LinkedQueue: def __init__(self): self.head = None self.tail = None def enqueue(self, data): new_node = Node(data) if self.tail: self.tail.next = new_node else: self.head = new_node self.tail = new_node def dequeue(self): if self.head is None: return False self.head = self.head.next \"\"\"移除头节点后，如果头节点为空，则尾节点也\"\"\" if not self.head: self.tail = Noneif __name__ == '__main__': linked_queue = LinkedQueue() linked_queue.enqueue(1) linked_queue.enqueue(2) linked_queue.enqueue(3) linked_queue.dequeue() linked_queue.dequeue() linked_queue.dequeue() print(linked_queue.tail)循环队列循环队列动态效果如下图：我们刚才用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？我们来看看循环队列的解决思路。循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。现在我们把首尾相连，扳成了一个环, 可以想像像钟表一样，enqueue和dequeue节点时，tail, head指针都是逆时针运动。我们可以看到，图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子：通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有 bug 的循环队列的实现代码，我个人觉得，最关键的是，确定好队空和队满的判定条件。在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。那针对循环队列，如何判断队空和队满呢？队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。我画了一张队列满的图，你可以看一下，试着总结一下规律。就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。多画几张队满的图，你就会发现，当队满时，(tail+1)%n=head。你有没有发现，当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。12345678910111213141516171819202122232425262728293031323334class circular_queue(): def __init__(self, size): self.array = [] self.head = 0 self.tail = 0 self.size = size def enqueue(self, value): \"\"\"入队判满\"\"\" if (self.tail + 1) % self.size == self.head: return False self.array.append(value) \"\"\"入队，尾节点下标变化\"\"\" self.tail = (self.tail + 1) % self.size return True def dequeue(self): \"\"\"出队判空\"\"\" if self.head != self.tail: item = self.array[self.head] \"\"\"出队，头节点下标变化\"\"\" self.head = (self.head + 1) % self.size return itemif __name__ == '__main__': q = circular_queue(5) for i in range(5): q.enqueue(i) q.dequeue() q.dequeue() q.enqueue(1) print(q)队列满的表达式这里讲一下，这个表达式是怎么来的。在一般情况下，我们可以看出来，当队列满时，tail+1=head。但是，有个特殊情况，就是tail=n-1，而head=0时，这时候，tail+1=n，而head=0，所以用(tail+1)%n == n%n == 0。而且，tail+1最大的情况就是 n ，不会大于 n，这样，tail+1 除了最大情况，不然怎么余 n 都是 tail+1 本身，也就是 head。这样，表达式就出现了。阻塞队列和并发队列前面讲的内容理论比较多，看起来很难跟实际的项目开发扯上关系。确实，队列这种数据结构很基础，平时的业务开发不大可能从零实现一个队列，甚至都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。你应该已经发现了，上述的定义就是一个“生产者 - 消费者模型”！是的，我们可以使用阻塞队列，轻松实现一个“生产者 - 消费者模型”！这种基于阻塞队列实现的“生产者 - 消费者模型”，可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。而且不仅如此，基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。前面我们讲了阻塞队列，在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲 Disruptor 的时候，我会再详细讲并发队列的应用。队列的知识就讲完了，我们现在回过来看下开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢？基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。除了前面讲到队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。内容小结今天我们讲了一种跟栈很相似的数据结构，队列。关于队列，你能掌握下面的内容，这节就没问题了。队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想解决数据搬移的问题，我们就需要像环一样的循环队列。循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。除此之外，我们还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全队列应用在现实生活中Queue的应用也很广泛，最广泛的就是排队了，”先来后到” First come first service ，以及Queue这个单词就有排队的意思。还有，比如我们的播放器上的播放列表，我们的数据流对象，异步的数据传输结构(文件IO，管道通讯，套接字等)还有一些解决对共享资源的冲突访问，比如打印机的打印队列等。消息队列等。交通状况模拟，呼叫中心用户等待的时间的模拟等等。最后，关于队列，入队要判满，出队要判空**","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"队列","slug":"队列","permalink":"cpeixin.cn/tags/%E9%98%9F%E5%88%97/"}]},{"title":"数据结构与算法-线性表","slug":"数据结构与算法-线性表","date":"2016-08-12T02:39:21.000Z","updated":"2020-04-04T17:29:23.790Z","comments":true,"path":"2016/08/12/数据结构与算法-线性表/","link":"","permalink":"cpeixin.cn/2016/08/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E7%BA%BF%E6%80%A7%E8%A1%A8/","excerpt":"","text":"线性表及其逻辑结构线性表是最简单也是最常用的一种数据结构。英文字母表（A、B、…、Z）是一个线性表，表中每个英文字母是一个数据元素；成绩单是一个线性表，表中每一行是一个数据元素，每个数据元素又由学号、姓名、成绩等数据项组成。线性表的定义线性表是具有相同特性的数据元素的一个有限序列。线性表一般表示为：1 L = (a1, a2, …, ai,ai+1 ,…, an)线性表中元素在位置上是有序的，这种位置上有序性就是一种线性关系，用二元组表示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101L = (D, R) D = &#123;ai| 1≤i≤n, n≥0&#125; R = &#123;r&#125; r = &#123;&lt;ai, ai+1&gt; | 1≤i≤n-1&#125;``` ### 线性表的抽象数据类型描述 将线性表数据结构抽象成为一种数据类型，这个数据类型中包含数据元素、元素之间的关系、操作元素的基本算法。对于基本数据类型（int、float、boolean等等）java已经帮我们实现了用于操作他们的基本运算，我们需要基于这些基本运算，为我们封装的自定义数据类型提供操作它们的算法。比如数组就是一种被抽象出来的线性表数据类型，数组自带很多基本方法用于操作数据元素。 Java中的List我们经常会使用到，但是很少关注其内部实现，List是一个接口，里面定义了一些抽象的方法，其目的就是对线性表的抽象，其中的方法就是线性表的一些常用基本运算。 而对于线性表的不同**存储结构**其实现方式就有所不同了，比如**ArrayList**是对线性表顺序存储结构的实现，**LinkedList**是线性表链式存储结构的实现等。存储结构没有确定我们就不知道数据怎么存储，但是对于线性表这种逻辑结构中数据的基本操作我们可以预知，无非就是获取长度、获取指定位置的数据、插入数据、删除数据等等操作，可以参考List。 对于本系列文章，只是对数据结构和一些常用算法学习，接下来的代码将选择性实现并分析算法。对线性表的抽象数据类型描述如下：```javapublic interface IList&lt;T&gt; &#123; /** * 判断线性表是否为空 * @return */ boolean isEmpty(); /** * 获取长度 * @return */ int length(); /** * 将结点添加到指定序列的位置 * @param index * @param data * @return */ boolean add(int index, T data); /** * 将指定的元素追加到列表的末尾 * @param data * @return */ boolean add(T data); /** * 根据index移除元素 * @param index * @return */ T remove(int index); /** * 移除值为data的第一个结点 * @param data * @return */ boolean remove(T data); /** * 移除所有值为data的结点 * @param data * @return */ boolean removeAll(T data); /** * 清空表 */ void clear(); /** * 设置指定序列元素的值 * @param index * @param data * @return */ T set(int index, T data); /** * 是否包含值为data的结点 * @param data * @return */ boolean contains(T data); /** * 根据值查询索引 * @param data * @return */ int indexOf(T data); /** * 根据data值查询最后一次出现在表中的索引 * @param data * @return */ int lastIndexOf(T data); /** * 获取指定序列的元素 * @param index * @return */ T get(int index); /** * 输出格式 * @return */ String toString();&#125;线性表的顺序存储结构顺序表把线性表中的所有元素按照其逻辑顺序依次存储在计算机存储器中指定存储位置开始的一块连续的存储空间中。在Java中创建一个数组对象就是分配了一块可供用户使用的连续的存储空间，该存储空间的起始位置就是由数组名表示的地址常量。线性表的顺序存储结构是利用数组来实现的。在Java中，我们通常利用下面的方式来使用数组：123int[] array = new int[]&#123;1,2,3&#125;; //创建一个数组Array.getInt(array, 0); //获取数组中序列为0的元素Array.set(array, 0, 1); //设置序列为0的元素值为1Array这种方式创建的数组是固定长度的，其容量无法修改，当array被创建出来的时候，系统只为其分配3个存储空间，所以我们无法对其进行添加和删除操作。Array这个类里面提供了很多方法用于操作数组，这些方法都是静态的，所以Array是一个用于操作数组的工具类，这个类提供的方法只有两种：get和set，所以只能获取和设置数组中的元素，然后对于这两种操作，我们通常使用array[i]、array[i] = 0的简化方式，所以Array这个类用的比较少。另外一种数组ArrayList，其内部维护了一个数组，所以本质上也是数组，其操作都是对数组的操作，与上述数组不同的是，ArrayList是一种可变长度的数组。既然数组创建时就已经分配了存储空间，为什么ArrayList是长度可变的呢？长度可变意味着可以从数组中添加、删除元素，向ArrayList中添加数据时，实际上是创建了一个新的数组，将原数组中元素一个个复制到新数组后，将新元素添加进来。如果ArrayList仅仅做了这么简单的操作，那他就不应该出现了。ArrayList中的数组长度是大于等于其元素个数的，当执行add()操作时首先会检查数组长度是否够用，只有当数组长度不够用时才会创建新的数组，由于创建新数组意味着老数据的搬迁，所以这个机制也算是利用空间换取时间上的效率。但是如果添加操作并不是尾部添加，而是头部或者中间位置插入，也避免不了元素位置移动。顺序表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229public class LinearArray&lt;T&gt; implements IList&lt;T&gt;&#123; private Object[] datas; /** * 通过给定的数组 建立顺序表 * @param objs * @return */ public static &lt;T&gt; LinearArray&lt;T&gt; createArray(T[] objs)&#123; LinearArray&lt;T&gt; array = new LinearArray(); array.datas = new Object[objs.length]; for(int i = 0; i&lt;objs.length; i++) array.datas[i] = objs[i]; return array; &#125; private LinearArray()&#123; &#125; @Override public boolean isEmpty() &#123; return datas.length == 0; &#125; @Override public int length() &#123; return datas.length; &#125; /** * 获取指定位置的元素 * 分析：时间复杂度O(1) * 从顺序表中检索值是简单高效的，因为顺序表内部采用数组作为容器，数组可直接通过索引值访问元素 */ @Override public T get(int index) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); return (T) datas[index]; &#125; /** * 为指定索引的结点设置值 * 分析：时间复杂度O(1) */ @Override public T set(int index, T data) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; datas[index] = data; return oldValue; &#125; /** * 判断是否包含某值只需要判断该值有没有出现过 * 分析：时间复杂度O(n) */ @Override public boolean contains(T data) &#123; return indexOf(data) &gt;= 0; &#125; /** * 获取某值第一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int indexOf(T data) &#123; if (data == null) &#123; for (int i = 0; i &lt; datas.length; i++) if (datas[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; datas.length; i++) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 获取某值最后一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int lastIndexOf(T data) &#123; if (data == null) &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (datas[i]==null) return i; &#125; else &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 指定位置插入元素 * 分析：时间复杂度O(n) * 在数组中插入元素时，需要创建一个比原数组容量大1的新数组， * 将原数组中(0,index-1)位置的元素拷贝到新数组，指定新数组index位置元素值为新值， * 继续将原数组(index, length-1)的元素拷贝到新数组 * @param index * @param data * @return */ @Override public boolean add(int index, T data) &#123; if (index &gt; datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); destination[index] = data; System.arraycopy(datas, index, destination, index + 1, datas.length - index); datas = destination; return true; &#125; /** * 在顺序表末尾处插入元素 * 分析：时间复杂度O(n) * 同上面一样，也需要创建新数组 * @param data * @return */ @Override public boolean add(T data) &#123; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, datas.length); destination[datas.length] = data; datas = destination; return true; &#125; /** * 有序表添加元素 * @param data * @return */ public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true; &#125; /** * 移除指定索引的元素 * 分析：时间复杂度O(n) * 此处由于数组元素数量-1，所以需要创建新数组。 * ArrayList由于是动态数组（list.size()≠data.length），所以只需要将删除的元素之后的前移一位 * @param index * @return */ @Override public T remove(int index) &#123; if (index &gt;= datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; fastRemove(index); return oldValue; &#125; /** * 删除指定值的第一个元素 * @param data * @return */ @Override public boolean remove(T data) &#123; if (data == null) &#123; for (int index = 0; index &lt; datas.length; index++) if (datas[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; datas.length; index++) if (data.equals(datas[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /** * 移除指定序列的元素 * @param index */ private void fastRemove(int index) &#123; Object destination[] = new Object[datas.length - 1]; System.arraycopy(datas, 0, destination, 0, index); System.arraycopy(datas, index+1, destination, index, datas.length - index-1); datas = destination; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; @Override public void clear() &#123; datas = new Object[]&#123;&#125;; &#125; @Override public String toString() &#123; if(isEmpty()) return \"\"; String str = \"[\"; for(int i = 0; i&lt;datas.length; i++)&#123; str += (datas[i]+\", \"); &#125; str = str.substring(0, str.lastIndexOf(\", \")); return str+\"]\"; &#125;&#125;算法分析：插入元素：删除元素：**线性表的链式存储结构顺序表必须占用一整块事先分配大小固定的存储空间，这样不便于存储空间的管理。为此提出了可以实现存储空间动态管理的链式存储方式–链表。链表在链式存储中，每个存储结点不仅包含元素本身的信息（数据域），还包含元素之间逻辑关系的信息，即一个结点中包含有直接后继结点的地址信息，这称为指针域。这样可以通过一个结点的指针域方便的找到后继结点的位置。由于顺序表中每个元素至多只有一个直接前驱元素和一个直接后继元素。当采用链式存储时，一种最简单也最常用的方法是：在每个结点中除包含数据域外，只设置一个指针域用以指向其直接后继结点，这种构成的链接表称为线性单向链接表，简称单链表。另一种方法是，在每个结点中除包含数值域外，设置两个指针域，分别用以指向直接前驱结点和直接后继结点，这样构成的链接表称为线性双向链接表，简称双链表。单链表当访问一个结点后，只能接着访问它的直接后继结点，而无法访问他的直接前驱结点。双链表则既可以依次向后访问每个结点，也可以依次向前访问每个结点。单链表结点元素类型定义：1234public class LNode &#123; protected LNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;双链表结点元素类型定义：12345public class DNode &#123; protected DNode prior; //指针域，指向直接前驱结点 protected DNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;在顺序表中，逻辑上相邻的元素，对应的存储位置也相邻，所以进行插入或删除操作时，通常需要平均移动半个表的元素，这是相当费时的操作。在链表中，每个结点存储位置可以任意安排，不必要求相邻，插入或删除操作只需要修改相关结点的指针域即可，方便省时。对于单链表，如果要在结点p之前插入一个新结点，由于通过p并不能找到其前驱结点，我们需要从链表表头遍历至p的前驱结点然后进行插入操作，这样时间复杂度就是O(n)，而顺序表插入删除结点时间复杂度也是O(n)，那为什么说链表插入删除操作更加高效呢？因为单链表插入删除操作所消耗的时间主要在于查找前驱结点，这个查找工作的时间复杂度为O(n)，而真正超如删除时间为O(1)还有顺序表需要移动结点，移动结点通常比单纯的查找更加费时，链表不需要连续的空间，不需要扩容创建新表，所以同样时间复杂度O(n)，链表更适合插入和删除操作。对于遍历查找前驱结点的问题，在双链表中就能很好的解决，双链表在已知某结点的插入和删除操作时间复杂度是O(1)。由于链表的每个结点带有指针域，从存储密度来讲，这是不经济的。所谓存储密度是指结点数据本身所占存储量和整改结点结构所占存储量之比。3.2 单链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534public class LinkList&lt;T&gt; implements IList&lt;T&gt;&#123; public LNode&lt;T&gt; head; //单链表开始结点 /** * 1.1 创建单链表（头插法：倒序） * 解：遍历数组，创建新结点，新结点的指针域指向头结点，让新结点作为头结点 * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListF(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; for (T obj : array) &#123; LNode&lt;T&gt; node = new LNode(); node.data = obj; node.next = llist.head; llist.head = node; &#125; &#125; return llist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 解： * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListR(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; llist.head = new LNode(); llist.head.data = array[0]; LNode&lt;T&gt; temp = llist.head; for(int i = 1; i &lt; array.length; i++)&#123; LNode node = new LNode(); node.data = array[i]; temp.next = node; temp = node; &#125; &#125; return llist; &#125; /** * 判断单链表是否为空表 * 时间复杂度O(1) * @return */ @Override public boolean isEmpty() &#123; return head==null; &#125; /** * 4 获取单链表长度 * 时间复杂度O(n) * @return */ @Override public int length() &#123; if(head==null) return 0; int l = 1; LNode node = head; while(node.next!=null) &#123; l++; node = node.next; &#125; return l; &#125; @Override public void clear() &#123; head = null; &#125; @Override public T set(int index, T data) &#123; return null; &#125; @Override public boolean contains(T data) &#123; return false; &#125; @Override public T get(int index) &#123; return getNode(index).data; &#125; /** * 6.1 获取指定索引的结点 * 时间复杂度O(n) * @param index * @return */ public LNode&lt;T&gt; getNode(int index)&#123; LNode node = head; int j = 0; while(j &lt; index &amp;&amp; node!=null)&#123; j++; node = node.next; &#125; return node; &#125; /** * 6.2 获取指定数据值结点的索引 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @return */ @Override public int indexOf(T data) &#123; if(head==null) return -1; //没有此结点 LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) return j; j++; node = node.next; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; if(head==null) return -1; int index = -1; LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) &#123; index = j; &#125; j++; node = node.next; &#125; return index; &#125; /** * 6.3 单链表中的倒数第k个结点（k &gt; 0） * 解：先找到顺数第k个结点，然后使用前后指针移动到结尾即可 * 时间复杂度O(n) 空间复杂度O(1) * @param k * @return */ public LNode&lt;T&gt; getReNode(int k)&#123; if(head==null) return null; int len = length(); if(k &gt; len) return null; LNode target = head; LNode next = head; for(int i=0;i &lt; k;i++) next = next.next; while(next!=null)&#123; target = target.next; next = next.next; &#125; return target; &#125; /** * 6.4 查找单链表的中间结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode getMiddleNode()&#123; if(head == null|| head.next == null) return head; LNode target = head; LNode temp = head; while(temp != null &amp;&amp; temp.next != null)&#123; target = target.next; temp = temp.next.next; &#125; return target; &#125; /** * 2.1 将单链表合并为一个单链表 * 解：遍历第一个表，用其尾结点指向第二个表头结点 * 时间复杂度O(n) * @return */ public static LNode mergeList(LNode head1, LNode head2)&#123; if(head1==null) return head2; if(head2==null) return head1; LNode loop = head1; while(loop.next!=null) //找到list1尾结点 loop = loop.next; loop.next = head2; //将list1尾结点指向list2头结点 return head1; &#125; /** * 2.1 通过递归，合并两个有序的单链表head1和head2 * * 解：两个指针分别指向两个头结点，比较两个结点大小， * 小的结点指向下一次比较结果（两者中较小），最终返回第一次递归的最小结点 * @param head1 * @param head2 * @return */ public static LNode mergeSortedListRec(LNode head1, LNode head2)&#123; if(head1==null)return head2; if(head2==null)return head1; if (((int)head1.data)&gt;((int)head2.data)) &#123; head2.next = mergeSortedListRec(head2.next, head1); return head2; &#125; else &#123; head1.next = mergeSortedListRec(head1.next, head2); return head1; &#125; &#125; /** * 3.1 循环的方式将单链表反转 * 时间复杂度O(n) 空间复杂度O(1) */ public void reverseListByLoop() &#123; if (head == null || head.next == null) return; LNode pre = null; LNode nex = null; while (head != null) &#123; nex = head.next; head.next = pre; pre = head; head = nex; &#125; head = pre; &#125; /** * 3.2 递归的方式将单链表反转,返回反转后的链表头结点 * 时间复杂度O(n) 空间复杂度O(n) */ public LNode reverseListByRec(LNode head) &#123; if(head==null||head.next==null) return head; LNode reHead = reverseListByRec(head.next); head.next.next = head; head.next = null; return reHead; &#125; /** * 5.1 获取单链表字符串表示 * 时间复杂度O(n) */ @Override public String toString() &#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; return buffer.toString(); &#125; public static String display(LNode head)&#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(\" -&gt; \"+node.data); node = node.next; &#125; return buffer.toString(); &#125; /** * 5.2 用栈的方式获取单链表从尾到头倒叙字符串表示 * 解：由于栈具有先进后出的特性，现将表中的元素放入栈中，然后取出就倒序了 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public String displayReverseStack()&#123; if(head == null) return \"\"; Stack &lt;LNode&gt; stack = new Stack &lt; &gt;(); //堆栈 先进先出 LNode head = this.head; while(head!=null)&#123; stack.push(head); head=head.next; &#125; StringBuffer buffer = new StringBuffer(); while(!stack.isEmpty())&#123; //pop()移除堆栈顶部的对象，并将该对象作为该函数的值返回。 buffer.append(\" -&gt; \"+stack.pop().data); &#125; return buffer.toString(); &#125; /** * 5.3 用递归的方式获取单链表从尾到头倒叙字符串表示 * @return */ public void displayReverseRec(StringBuffer buffer, LNode head)&#123; if(head==null) return; displayReverseRec(buffer, head.next); buffer.append(\" -&gt; \"); buffer.append(head.data); &#125; /** * 7.1 插入结点 * 解：先找到第i-1个结点，让创建的新结点的指针域指向第i-1结点指针域指向的结点， * 然后将i-1结点的指针域指向新结点 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @param index */ @Override public boolean add(int index, T data) &#123; if(index==0)&#123; //插入为头结点 LNode temp = new LNode(); temp.next = head; return true; &#125; int j = 0; LNode node = head; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return false; LNode temp = new LNode(); //创建新结点 temp.data = data; temp.next = node.next; //新结点插入到Index-1结点之后 node.next = temp; return true; &#125; @Override public boolean add(T data) &#123; LNode node = head; while(node!=null &amp;&amp; node.next!=null) //找到尾结点 node = node.next; LNode temp = new LNode(); //创建新结点 temp.data = data; node.next = temp; return false; &#125; @Override public T remove(int index) &#123; LNode&lt;T&gt; node = deleteNode(index); return node==null?null:node.data; &#125; /** * 7.2 删除结点 * 解：让被删除的结点前一个结点的指针域指向后一个结点指针域 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode deleteNode(int index)&#123; LNode node = head; if(index==0)&#123; //删除头结点 if(node==null) return null; head = node.next; return node; &#125; //非头结点 int j = 0; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return null; LNode delete = node.next; if(delete==null) return null; //不存在第index个结点 node.next = delete.next; return delete; &#125; @Override public boolean remove(T data) &#123; return false; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; /** * 7.3 给出一单链表头指针head和一节点指针delete，要求O(1)时间复杂度删除节点delete * 解：将delete节点value值与它下个节点的值互换的方法， * 但是如果delete是最后一个节点，需要特殊处理，但是总得复杂度还是O(1) * @return */ public static void deleteNode(LNode head, LNode delete)&#123; if(delete==null) return; //首先处理delete节点为最后一个节点的情况 if(delete.next==null)&#123; if(head==delete) //只有一个结点 head = null; else&#123; //删除尾结点 LNode temp = head; while(temp.next!=delete) temp = temp.next; temp.next=null; &#125; &#125; else&#123; delete.data = delete.next.data; delete.next = delete.next.next; &#125; return; &#125; /** * 8.1 判断一个单链表中是否有环 * 解：使用快慢指针方法，如果存在环，两个指针必定指向同一结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public static boolean hasCycle(LNode head)&#123; LNode p1 = head; LNode p2 = head; while(p1!=null &amp;&amp; p2!=null)&#123; p1 = p1.next; //一次跳一步 p2 = p2.next.next; //一次跳两步 if(p2 == p1) return true; &#125; return false; &#125; /** * 8.2、已知一个单链表中存在环，求进入环中的第一个节点 * 利用hashmap，不要用ArrayList，因为判断ArrayList是否包含某个元素的效率不高 * @param head * @return */ public static LNode getFirstNodeInCycleHashMap(LNode head)&#123; LNode target = null; HashMap&lt;LNode,Boolean &gt; map=new HashMap&lt; &gt;(); while(head != null)&#123; if(map.containsKey(head)) &#123; target = head; break; &#125; else &#123; map.put(head, true); head = head.next; &#125; &#125; return target; &#125; /** * 8.3、已知一个单链表中存在环，求进入环中的第一个节点,不用hashmap * 用快慢指针，与判断一个单链表中是否有环一样，找到快慢指针第一次相交的节点， * 此时这个节点距离环开始节点的长度和链表头距离环开始的节点的长度相等 * 参考 https://www.cnblogs.com/fankongkong/p/7007869.html * @param head * @return */ public static LNode getFirstNodeInCycle(LNode head)&#123; LNode fast = head; LNode slow = head; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; if(slow == fast) break; &#125; if(fast == null||fast.next == null) return null;//判断是否包含环 //相遇节点距离环开始节点的长度和链表投距离环开始的节点的长度相等 slow=head; while(slow!=fast)&#123; slow=slow.next; fast=fast.next; &#125;//同步走 return slow; &#125; /** * 9、判断两个单链表是否相交,如果相交返回第一个节点，否则返回null * ①、暴力遍历两个表，是否有相同的结点(时间复杂度O(n²)) * ②、第一个表的尾结点指向第二个表头结点，然后判断第二个表是否存在环，但不容易找出交点（时间复杂度O(n)） * ③、两个链表相交，必然会经过同一个结点，这个结点的后继结点也是相同的（链表结点只有一个指针域，后继结点只能有一个）， * 所以他们的尾结点必然相同。两个链表相交，只能是前面的结点不同，所以，砍掉较长链表的差值后同步遍历，判断结点是否相同，相同的就是交点了。 * 时间复杂度（时间复杂度O(n)） * @param list1 * @param list2 * @return 交点 */ public static LNode isIntersect(LinkList list1, LinkList list2)&#123; LNode head1 = list1.head; LNode head2 = list2.head; if(head1==null || head2==null)return null; int len1 = list1.length(); int len2 = list2.length(); //砍掉较长链表的差值 if(len1 &gt;= len2)&#123; for(int i=0;i &lt; len1-len2;i++)&#123; head1=head1.next; &#125; &#125;else&#123; for(int i=0;i &lt; len2-len1;i++)&#123; head2=head2.next; &#125; &#125; //同步遍历 while(head1 != null&amp;&amp;head2 != null)&#123; if(head1 == head2) return head1; head1=head1.next; head2=head2.next; &#125; return null; &#125;&#125;算法分析判断一个单链表中是否有环我们可以通过HashMap判断，遍历结点，将结点值放入HashMap，如果某一刻发现当前结点在map中已经存在，则存在环，并且此结点正是环的入口，此算法见8.2方法。但是有一种问法是不通过任何其他数据结构怎么判断单链表是否存在环。这样我们可利用的就只有单链表本身，一种解法是通过快慢指针，遍历链表，一个指针跳一步（慢指针步长为1），另一个指针跳两步（快指针步长为2），如果存在环，这两个指针必将在某一刻指向同一结点，假设此时慢指针跳了n步，则快指针跳的步数为n/2步：判断两个单链表是否相交由于单链表的特性（只有一个指针域），如果两个表相交，那必定是Y形相交，不会是X形相交，如图所示。两个单链表后面的结点相同，不同的部分只有前面，砍掉较长的链表的前面部分，然后两个链表同步遍历，必将指向同一个结点，这个结点就是交点：双链表双链表中每个结点有两个指针域，一个指向其直接后继结点，一个指向其直接前驱结点。建立双链表也有两种方法，头插法和尾插法，这与创建单链表过程相似。在双链表中，有些算法如求长度、取元素值、查找元素等算法与单链表中相应算法是相同的。但是在单链表中，进行结点插入和删除时涉及前后结点的一个指针域的变化，而双链表中结点的插入和删除操作涉及前后结点的两个指针域的变化。java中LinkedList正是对双链表的实现，算法可参考此类。双链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327public class DLinkList&lt;T&gt; implements IList&lt;T&gt;&#123; transient DNode&lt;T&gt; first; //双链表开始结点 transient DNode&lt;T&gt; last; //双链表末端结点 private int size; //结点数 /** * 创建单链表（头插法：倒序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListF(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; dlist.size = array.length; for (T obj : array) &#123; DNode&lt;T&gt; node = new DNode(); node.data = obj; node.next = dlist.first; if(dlist.first!=null) dlist.first.prior = node; //相比单链表多了此步 else dlist.last = node; dlist.first = node; &#125; &#125; return dlist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListR(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; dlist.size = array.length; dlist.first = new DNode&lt;T&gt;(); dlist.first.data = array[0]; dlist.last = dlist.first; for(int i = 1; i &lt; array.length; i++)&#123; DNode&lt;T&gt; node = new DNode(); node.data = array[i]; dlist.last.next = node; node.prior = dlist.last; //相比单链表多了此步 dlist.last = node; &#125; &#125; return dlist; &#125; @Override public boolean isEmpty() &#123; return size==0; &#125; @Override public int length() &#123; return size; &#125; /**2 添加结点*/ @Override public boolean add(int index, T data) &#123; if(index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; newNode = new DNode(); newNode.data = data; if (index == size) &#123; //在末尾添加结点不需要遍历 final DNode&lt;T&gt; l = last; if (l == null) //空表 first = newNode; else &#123; l.next = newNode; newNode.prior = l; &#125; last = newNode; size++; &#125; else &#123; //其他位置添加结点需要遍历找到index位置的结点 DNode&lt;T&gt; indexNode = getNode(index); DNode&lt;T&gt; pred = indexNode.prior; newNode.prior = pred; newNode.next = indexNode; indexNode.prior = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; &#125; return false; &#125; @Override public boolean add(T data) &#123; return add(size, data); &#125; /**3 删除结点*/ @Override public T remove(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return unlink(getNode(index)); &#125; @Override public boolean remove(T data) &#123; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); return true; &#125; &#125; &#125; return false; &#125; @Override public boolean removeAll(T data) &#123; boolean result = false; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); result = true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); result = true; &#125; &#125; &#125; return result; &#125; /** * 将指定的结点解除链接 * @param x * @return */ private T unlink(DNode&lt;T&gt; x) &#123; // assert x != null; final T element = x.data; final DNode&lt;T&gt; next = x.next; final DNode&lt;T&gt; prev = x.prior; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prior = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prior = prev; x.next = null; &#125; x.data = null; size--; return element; &#125; /** * 清空 */ @Override public void clear() &#123; for (DNode&lt;T&gt; x = first; x != null; ) &#123; DNode&lt;T&gt; next = x.next; x.data = null; x.next = null; x.prior = null; x = next; &#125; first = last = null; size = 0; &#125; /** * 设置结点值 * @param index * @param data * @return */ @Override public T set(int index, T data) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; x = getNode(index); T oldVal = x.data; x.data = data; return oldVal; &#125; /** * 判断是否存在结点值 * @param data * @return */ @Override public boolean contains(T data) &#123; return indexOf(data) != -1; &#125; /** * 检索结点值 * @param data * @return */ @Override public int indexOf(T data) &#123; int index = 0; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) return index; index++; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) return index; index++; &#125; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; int index = size; if (data == null) &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (x.data == null) return index; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (data.equals(x.data)) return index; &#125; &#125; return -1; &#125; @Override public T get(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return getNode(index).data; &#125; /** * 获取指定索引的结点 * 解：由于双链表能双向检索，判断index离开始结点近还是终端结点近，从近的一段开始遍历 * 时间复杂度O(n) * @param index * @return */ private DNode&lt;T&gt; getNode(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123; DNode&lt;T&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; DNode&lt;T&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prior; return x; &#125; &#125; /** * 倒序 * 遍历每个结点，让node.next = node.prior; node.prior = (node.next此值需要体现保存); */ public void reverse()&#123; last = first; //反转后终端结点=开始结点 DNode now = first; DNode next; while(now!=null)&#123; next = now.next; //保存当前结点的后继结点 now.next = now.prior; now.prior = next; first = now; now = next; &#125; &#125; @Override public String toString() &#123; if(size == 0) return \"\"; DNode node = first; StringBuffer buffer = new StringBuffer(); buffer.append(\" \"); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; buffer.append(\"next\\npre\"); node = last; int start = buffer.length(); LogUtil.i(getClass().getSimpleName(), \"buffer长度：\"+buffer.length()); while(node != null)&#123; buffer.insert(start ,\" &lt;- \"+node.data); node = node.prior; &#125; return buffer.toString(); &#125;&#125;算法分析双链表与单链表不同之处在于，双链表能从两端依次访问各个结点。单链表相对于顺序表优点是插入、删除数据更方便，但是访问结点需要遍历，时间复杂度为O(n)；双链表就是在单链表基础上做了一个优化，使得访问结点更加便捷（从两端），这样从近的一端出发时间复杂度变为O(n/2)，虽然不是指数阶的区别，但也算是优化。双链表在插入、删除结点时逻辑比单链表稍麻烦：循环链表循环链表是另一种形式的链式存储结构，它的特点是表中尾结点的指针域不再是空，而是指向头结点，整个链表形成一个环。由此，从表中任意一结点出发均可找到链表中其他结点。如图所示为带头结点的循环单链表和循环双链表：有序表所谓有序表，是指所有结点元素值以递增或递减方式排列的线性表，并规定有序表中不存在元素值相同的结点。有序表可以采用顺序表和链表进行存储，若以顺序表存储有序表，其算法除了add(T data)以外，其他均与前面说的顺序表对应的运算相同。有序表的add(T data)操作不是插入到末尾，而需要遍历比较大小后插入相应位置。123456789101112131415public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true;&#125;最后 想强调的是 由于顺序表结构的底层实现借助的就是数组，因此对于初学者来说，可以把顺序表完全等价为数组，但实则不是这样。数据结构是研究数据存储方式的一门学科，它囊括的都是各种存储结构，而数组只是各种编程语言中的基本数据类型，并不属于数据结构的范畴。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"线性表","slug":"线性表","permalink":"cpeixin.cn/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/"}]},{"title":"数据结构-数组","slug":"数据结构-数组","date":"2016-08-11T07:30:21.000Z","updated":"2020-04-04T17:33:56.188Z","comments":true,"path":"2016/08/11/数据结构-数组/","link":"","permalink":"cpeixin.cn/2016/08/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%95%B0%E7%BB%84/","excerpt":"","text":"提到数组，我想你肯定不陌生，甚至还会自信地说，它很简单啊。是的，在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。在大部分编程语言中，数组都是从 0 开始编号的，但你是否下意识地想过，为什么数组要从 0 开始编号，而不是从 1 开始呢？ 从 1 开始不是更符合人类的思维习惯吗？Tips: 在C，Java中都有明显的数组实现，但是在Python中有list、tuple、set、dict, 并没有显示的数组关键字，“数组”实际上确实存在于python中。当人们谈论数组时，Python至少有三件事要谈论：在Python list有许多在其他语言，如C或Java中 数组的行为。但是，它不需要预先分配内存，分配数组长度，它带有许多便捷的方法。此外，它的基础数据结构通常是一个数组。请记住，语言只是语法-它可以通过不同的方式实现。CPython中Python列表的基础数据结构是C数组，在Jython中是ArrayList（来源：Python列表的基础数据结构是什么？）列表对象被实现为数组。它们针对快速的固定长度操作进行了优化，并且会为pop（0）和insert（0，v）操作产生O（n）内存移动成本，这些操作会同时更改基础数据表示的大小和位置。所述的Python阵列模块。可以在这里找到：8.6。array-有效的数字数组。阵列模块用于某些相当特殊的情况，当您有大量的一种类型的数据时，它们可以提供更好的内存性能（不一定是速度）。我在Chris Riederer的回答“ Python中的列表和数组之间有什么区别？”的答案中写了更多内容。最后，还有NumPy 数组。这是一个非常快速，非常有用的数据结构，可让您快速进行许多数值计算。在此处了解有关NumPy的更多信息：NumPy-Numpy这个定义里有几个关键词，理解了这几个关键词，我想你就能彻底掌握数组的概念了。下面就从我的角度分别给你“点拨”一下。第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10]来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：12a[i]_address = base_address + i * data_type_size其中 data_type_size 表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节。这个公式非常简单，我就不多做解释了。这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。Tips: 针对上面提到的 数组存储的必须是相同的数据类型，在这里我自己的理解是，对于数组内的元素，进行访问的过程中，实际上是根据每个元素的内存地址来进行访问的。元素的内存地址不是随意分配的，而是通过一个公式计算而来的，每个元素的内存地址合成一段连续的内存地址区间段。如上图，不同类型的元素所占字节不同，如果数组中存储不同数据类型的数据，那么就不能统一的按照 内存地址计算公式来计算。低效的“插入”和“删除”前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。现在我们就来详细说一下，究竟为什么会导致低效？又有哪些改进方法呢？我们先来看插入操作。假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？你可以自己先试着分析一下。如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。为了更好地理解，我们举一个例子。假设数组 a[10]中存储了如下 5 个元素：a，b，c，d，e。我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2]赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到，我会在排序那一节具体来讲，这里就说到这儿。我们再来看删除操作。跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？我们继续来看例子。数组 a[10]中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。如果你了解 JVM，你会发现，这不就是 JVM 标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。容器能否完全替代数组？这个问题，我在刚学编程语言之处，是根本没有想过的，在我眼里，程序语言中集合就是数据结构中的数组，没有任何差别，实际情况中，针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL 中的 vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？这里我拿 Java 语言来举例。如果你是 Java 工程师，几乎天天都在用 ArrayList，对它应该非常熟悉。那它与数组相比，到底有哪些优势呢？我个人觉得，ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。如果使用 ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList 已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。比如我们要从数据库中取出 10000 条数据放入 ArrayList。我们看下面这几行代码，你会发现，相比之下，事先指定数据大小可以省掉很多次内存申请和数据搬移操作。作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适些，我总结了几点自己的经验。Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 Object[][] array；而用容器的话则需要这样定义：ArrayListarray。我总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。解答开篇现在我们来思考开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：a[k]_address = base_address + k * type_size但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：a[k]_address = base_address + (k-1)*type_size对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。不过我认为，上面解释得再多其实都算不上压倒性的证明，说数组起始编号非 0 开始不可。所以我觉得最主要的原因可能是历史原因。C 语言设计者用 0 开始计数数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。实际上，很多语言中数组也并不是从 0 开始计数的，比如 Matlab。甚至还有一些语言支持负数下标，比如 Python。内容小结我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数组","slug":"数组","permalink":"cpeixin.cn/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"数据结构与算法-复盘","slug":"数据结构与算法-复盘","date":"2016-08-10T07:30:21.000Z","updated":"2020-04-04T17:30:49.884Z","comments":true,"path":"2016/08/10/数据结构与算法-复盘/","link":"","permalink":"cpeixin.cn/2016/08/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E5%A4%8D%E7%9B%98/","excerpt":"","text":"对于软件工程专业的同学来说，数据结构与算法这门课程是必修课程，课程时间应该是大学二年级，在学习完了一门编程语言后，进行展开学习的。我是在学习完C语言后，进行数据结构与算法这门课程的。个人感觉这门课程 veryyyyyyyyy 枯燥的。老师在课堂上也没有做相关知识点的扩展，再加上那时候我们对于知识的渴望度也没有那么高 哈哈哈哈，单纯的就是为了考试而学习，也没有利用网络资源来对这门课程进行进一步的拓展。等到工作之后，才慢慢发现，这门课程就是在大学时候埋的雷啊 XD但是毕竟还是学过的，时间还不晚，知识学到了，就是自己的，定期对自己复盘，重视自己，查缺补漏，就是最棒的！学前三问：WHAT ？ HOW ？ WHY？在之后写博客的时候，我也会遵循着 WWH 法则作为提纲。简单明了，直入主题。WHAT - 数据结构 算法数据结构简单直白的理解 ： 数据结构就是指一组数据的存储结构。百度百科：数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率。数据结构往往同高效的检索算法和索引技术有关算法简单直白的理解 ： 算法就是操作数据的一组方法。百度百科：算法（Algorithm）是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制。也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。HOW - 数据结构 算法有的人喜欢看书稳扎稳打有的人习惯看视频生动一点的获取知识也有人倾向于直接进入主题，找教程，找博文攻破单个知识点我这里只是说一下我的方法首先就是这篇博文，先搞清楚WWH然后呢，对数据结构和算法整个知识体系列一个大纲或者思维导图接下来就进入学习的过程了，我个人是比较喜欢看视 频和读好的博客的，每一个点学完后啊，先找习题，完了打开IDEA ，写一些Demo去理解，然后呢，带着学过的算法和数据结构啊，进入实战场，看看之前自己写过的项目，可不可以重新装修一下。最后一点，找一些成熟项目，读源码，读源码，读源码！！！WHY - 数据结构 算法这一栏位的内容，我找了很久，也想了很久怎样的去写，后来在搜寻的时候，发现很多博主和学习论坛都引用了知乎上涛吴的观点（涛吴，知乎上最受欢迎程序员前十名👍🏻），写的确实好，好东西就要分享出来😂如果说 Java 是自动档轿车，C 就是手动档吉普。数据结构呢？是变速箱的工作原理。你完全可以不知道变速箱怎样工作，就把自动档的车子从 A 开到 B，而且未必就比懂得的人慢。写程序这件事，和开车一样，经验可以起到很大作用，但如果你不知道底层是怎么工作的，就永远只能开车，既不会修车，也不能造车。如果你对这两件事都不感兴趣也就罢了，数据结构懂得用就好。但若你此生在编程领域还有点更高的追求，数据结构是绕不开的课题。Java 替你做了太多事情，那么多动不动还支持范型的容器类，加上垃圾收集，会让你觉得编程很容易。但你有没有想过，那些容器类是怎么来的，以及它存在的意义是什么？最粗浅的，比如 ArrayList 这个类，你想过它的存在是多么大的福利吗——一个可以随机访问、自动增加容量的数组，这种东西 C 是没有的，要自己实现。但是，具体怎么实现呢？如果你对这种问题感兴趣，那数据结构是一定要看的。甚至，面向对象编程范式本身，就是个数据结构问题：怎么才能把数据和操作数据的方法封装到一起，来造出 class / prototype 这种东西？此外，很重要的一点是，数据结构也是通向各种实用算法的基石，所以学习数据结构都是提升内力的事情。原文链接学 Java 有必要看数据结构的书吗？如果是，那么哪本书比较好？ - 涛吴的回答 - 知乎学数据结构和算法可以应用在哪些场景？在大学的时候，学习数据结构与算法，学的时候云里雾里的，搞不清楚这些数据结构和算法应该用在哪里？能干些什么？还没有一个长长的SQL带来的成就感强烈。以为这门课程只是应试教育中的必须进行的一环，现在想起来真是单纯的孩子啊 哈哈😄下面我引用InfoQ上的一篇文章，看完之后，有种醍醐灌顶的感觉，我觉得我明天应该会去翻遍所有的源码去看，有点迫不及待了……实际项目中的常见算法其次，以下场景中使用了丰富的数据结构来实现，可以择一来深入研读Linux 内核SQL引擎网络拓扑结构集群节点各种协议学数据结构和算法的精髓是什么？想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。时间复杂度和空间复杂度这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招怎样去选择合适的数据结构首先，选择数据结构的时候，第一点就要知道，数据怎么去组织，和数据规模是有关的。不一样规模的数据，难度也就不一样。解决问题的效率，是与数据组织方式相关的。首先上来我先Po出一张图，来应对第一步大类结构的选择查询操作更多的程序中，你应该用顺序表修改操作更多的程序中，你要使用链表单向链 双向链表 循环链表。栈，涉及后入先出的问题，例如函数递归就是个栈模型、Android的屏幕跳转就用到栈，很多类似的东西，你就会第一时间想到：我会用这东西来去写算法实现这个功能。队列，先入先出要排队的问题，你就要用到队列，例如多个网络下载任务，我该怎么去调度它们去获得网络资源呢？再例如操作系统的进程（or线程）调度，我该怎么去分配资源（像 CPU）给多个任务呢？肯定不能全部一起拥有的，资源只有一个，那就要排队！那么怎么排队呢？用普通的队列？但是对于那些优先级高的线程怎么办？这时，你就会想到了优先队列，优 先队列怎么实现？用堆，然后你就有疑问了，堆是啥玩意？怎样评测算法的好坏时间复杂度空间复杂度怎样将数据结构和算法应用到实际之中？写一些程序，尤其是比较底层的程序。数据结构如红黑树,后缀树, 算法如快速傅里叶变换,网络流等… 平时工作本来就很难碰上这些东西(如果确实从事尖端研究除外)，个人经常碰到的也无非就是些搜索, 优化剪枝…留下一个问题去给大家思考：如果让你实现qq那种分组的好友列表，支持各种qq里边的好友操作，你会怎么做？Sooooooooooo 学习数据结构与算法是提升自身内力，内力提升了，学习其他技术和框架，就会简单容易一些🐨","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[]},{"title":"小谈多线程","slug":"小谈多线程","date":"2016-08-09T07:30:21.000Z","updated":"2020-04-04T11:10:51.174Z","comments":true,"path":"2016/08/09/小谈多线程/","link":"","permalink":"cpeixin.cn/2016/08/09/%E5%B0%8F%E8%B0%88%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"多线程简介Java 给多线程编程提供了内置的支持。 一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。多线程是多任务的一种特别的形式，但多线程使用了更小的资源开销。这里定义和线程相关的另一个术语 - 进程：一个进程包括由操作系统分配的内存空间，包含一个或多个线程。一个线程不能独立的存在，它必须是进程的一部分。一个进程一直运行，直到所有的非守护线程都结束运行后才能结束。多线程能满足程序员编写高效率的程序来达到充分利用 CPU 的目的。多线程状态VM启动时会有一个由主方法Main所定义的主线程，在主线程中可以通过Thread创建其它线程。Thread对象的方法run()称为线程体。通过调用Thread类的start()方法来启动一个线程。通俗的说就是在run()方法中定义要做什么事情，start()方法用来发出命令可以开始做了，但这不代表JVM会立即运行 run()方法中的内容，而只是让他具备运行的资格，具体什么时侯开始真正运行run()方法，需要看JVM的调度。在Java当中，线程通常都有五种状态，新建、就绪、运行、阻塞和死亡。新建状态（New）：新创建了一个线程对象。就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：（一）、等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。（二）、同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。（三）、其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。在这里我要说明一下start()和run()之间的关系，因为在多线程编程的过程中并没有去调用run()方法，但是run()又是主要的执行体，首先Po上来start()，run()的源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Causes this thread to begin execution; the Java Virtual Machine * calls the &lt;code&gt;run&lt;/code&gt; method of this thread. * &lt;p&gt; * The result is that two threads are running concurrently: the * current thread (which returns from the call to the * &lt;code&gt;start&lt;/code&gt; method) and the other thread (which executes its * &lt;code&gt;run&lt;/code&gt; method). * &lt;p&gt; * It is never legal to start a thread more than once. * In particular, a thread may not be restarted once it has completed * execution. * * @exception IllegalThreadStateException if the thread was already * started. * @see #run() * @see #stop() */ public synchronized void start() &#123; /** * This method is not invoked for the main method thread or \"system\" * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * A zero status value corresponds to state \"NEW\". */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group's list of threads * and the group's unstarted count can be decremented. */ group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; &#125; private native void start0(); /** * If this thread was constructed using a separate * &lt;code&gt;Runnable&lt;/code&gt; run object, then that * &lt;code&gt;Runnable&lt;/code&gt; object's &lt;code&gt;run&lt;/code&gt; method is called; * otherwise, this method does nothing and returns. * &lt;p&gt; * Subclasses of &lt;code&gt;Thread&lt;/code&gt; should override this method. * * @see #start() * @see #stop() * @see #Thread(ThreadGroup, Runnable, String) */ @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125;根据Java API ： Causes this thread to begin execution; the Java Virtual Machine calls the run method of this thread.start()方法会使得该线程开始执行；java虚拟机会自动去调用该线程的run()方法。因此，t.start()会导致run()方法被调用，run()方法中的内容称为线程体，它就是这个线程需要执行的工作。在start方法里调用了一次start0方法，这个方法是一个只声明未定义的方法，并且使用了native关键字进行定义native指的是调用本机的原生系统函数。所以，调用start方法，会告诉JVM去分配本机系统的资源，才能实现多线程。而如果使用run()来启动线程，就不是异步执行了，而是同步执行，不会达到使用线程的意义。用start()来启动线程，实现了真正意义上的启动线程，此时会出现异步执行的效果，即在线程的创建和启动中所述的随机性。多线程创建Java 提供了三种创建线程的方法实现 Runnable 接口我们经常使用的构造方法，threadOb为创建的实例对象Thread(Runnable threadOb,String threadName);12345678910111213141516171819202122232425262728293031323334353637383940414243class RunnableDemo implements Runnable &#123; private Thread t; private String threadName; RunnableDemo( String name) &#123; threadName = name; System.out.println(\"Creating \" + threadName ); &#125; public void run() &#123; System.out.println(\"Running \" + threadName ); try &#123; for(int i = 4; i &gt; 0; i--) &#123; System.out.println(\"Thread: \" + threadName + \", \" + i); // 让线程睡眠一会 Thread.sleep(50); &#125; &#125;catch (InterruptedException e) &#123; System.out.println(\"Thread \" + threadName + \" interrupted.\"); &#125; System.out.println(\"Thread \" + threadName + \" exiting.\"); &#125; public void start () &#123; System.out.println(\"Starting \" + threadName ); if (t == null) &#123; t = new Thread (this, threadName); t.start (); &#125; &#125;&#125; public class TestThread &#123; public static void main(String args[]) &#123; RunnableDemo R1 = new RunnableDemo( \"Thread-1\"); R1.start(); RunnableDemo R2 = new RunnableDemo( \"Thread-2\"); R2.start(); &#125; &#125;继承 Thread 类本身创建一个线程的第二种方法是创建一个新的类，该类继承 Thread 类，然后创建一个该类的实例。继承类必须重写 run() 方法，该方法是新线程的入口点。它也必须调用 start() 方法才能执行。该方法尽管被列为一种多线程实现方式，但是本质上也是实现了 Runnable 接口的一个实例。123456789101112131415161718192021222324252627282930313233343536373839404142class ThreadDemo extends Thread &#123; private Thread t; private String threadName; ThreadDemo( String name) &#123; threadName = name; System.out.println(\"Creating \" + threadName ); &#125; public void run() &#123; System.out.println(\"Running \" + threadName ); try &#123; for(int i = 4; i &gt; 0; i--) &#123; System.out.println(\"Thread: \" + threadName + \", \" + i); // 让线程睡眠一会 Thread.sleep(50); &#125; &#125;catch (InterruptedException e) &#123; System.out.println(\"Thread \" + threadName + \" interrupted.\"); &#125; System.out.println(\"Thread \" + threadName + \" exiting.\"); &#125; public void start () &#123; System.out.println(\"Starting \" + threadName ); if (t == null) &#123; t = new Thread (this, threadName); t.start (); &#125; &#125;&#125; public class TestThread &#123; public static void main(String args[]) &#123; ThreadDemo T1 = new ThreadDemo( \"Thread-1\"); T1.start(); ThreadDemo T2 = new ThreadDemo( \"Thread-2\"); T2.start(); &#125; &#125;Callable和Future创建线程创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值。123456789101112131415161718192021222324252627282930313233343536public class CallableThreadTest implements Callable&lt;Integer&gt; &#123; public static void main(String[] args) &#123; CallableThreadTest ctt = new CallableThreadTest(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt); for(int i = 0;i &lt; 100;i++) &#123; System.out.println(Thread.currentThread().getName()+\" 的循环变量i的值\"+i); if(i==20) &#123; new Thread(ft,\"有返回值的线程\").start(); &#125; &#125; try &#123; System.out.println(\"子线程的返回值：\"+ft.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Integer call() throws Exception &#123; int i = 0; for(;i&lt;100;i++) &#123; System.out.println(Thread.currentThread().getName()+\" \"+i); &#125; return i; &#125; &#125;创建线程方法之间的区别实现Runnable和实现Callable接口的方式基本相同，不过是后者执行call()方法有返回值，后者线程执行体run()方法无返回值，因此可以把这两种方式归为一种这种方式与继承Thread类的方法之间的差别如下：1、线程只是实现Runnable或实现Callable接口，还可以继承其他类。2、这种方式下，多个线程可以共享一个target对象，非常适合多线程处理同一份资源的情形。3、但是编程稍微复杂，如果需要访问当前线程，必须调用Thread.currentThread()方法。4、继承Thread类的线程类不能再继承其他父类（Java单继承决定）。注：一般推荐采用实现接口的方式来创建多线程示例Account类123456789101112131415161718192021222324252627public class Acount &#123; private String UserName; private float Money; public Acount(String userName, float money) &#123; super(); UserName = userName; Money = money; &#125; public String getUserName() &#123; return UserName; &#125; public void setUserName(String userName) &#123; UserName = userName; &#125; public float getMoney() &#123; return Money; &#125; public void setMoney(float money) &#123; Money = money; &#125;&#125;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class BankOperator implements Runnable &#123; @Override public void run() &#123; //锁定对象 synchronized (a) &#123; withdraw(5); deposit(50); System.out.println(\"brent账户余额： \"+a.getMoney()); &#125; &#125; static Acount a; public void deposit(float money) &#123; a.setMoney(a.getMoney() + money); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \":\" + \"存款后 \" + a.getMoney()); &#125; public void withdraw(float money) &#123; a.setMoney(a.getMoney() - money); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \":\" + \"取款后 \" + a.getMoney()); &#125; BankOperator(Acount a) &#123; super(); this.a = a; &#125; public static void main(String args[]) &#123; Acount a = new Acount(\"brent\", 100); BankOperator bko = new BankOperator(a); Thread t1 = new Thread(bko, \"customer1\"); Thread t2 = new Thread(bko, \"customer2\"); Thread t3 = new Thread(bko, \"customer3\"); Thread t4 = new Thread(bko, \"customer4\"); t1.start(); t2.start(); t3.start(); t4.start(); &#125;&#125;","categories":[],"tags":[]},{"title":"进程和线程基本概念","slug":"进程和线程基本概念","date":"2016-08-07T07:30:21.000Z","updated":"2020-04-04T17:55:34.701Z","comments":true,"path":"2016/08/07/进程和线程基本概念/","link":"","permalink":"cpeixin.cn/2016/08/07/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"前言进程（process）和线程（thread）是操作系统的基本概念，也是平常编程的过程中，我们经常遇到和听到的名词由于上大学的时候，老师在讲解有关线程、进程课程时，我应该在玩《神庙逃亡》，所以这一方面一直不是很扎实就在最近，我读到了国外的一篇文章和阮一峰博士的文章，这两篇文章利用‘车间’和‘工人’的关系来类比，浅显易懂，分享给大家概念计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。进程假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工背后的含义就是，单个CPU一次只能运行一个任务进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。线程一个车间里，可以有很多工人。他们协同完成一个任务。线程就好比车间里的工人。背后的含义就是，一个进程可以包括多个线程线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。共享内存车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。背后的含义就是，这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存互斥锁可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。“互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域信号量还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。优先级如果卫生间目前已锁定并且有许多人正在等待使用它怎么办？显然，所有的人都坐在外面，等待浴室里的任何人出去。真正的问题是，“当门解锁时会发生什么？谁下次去？“你会认为允许等待时间最长的人接下来是“公平的”。或者，让最老的人走向下一步可能是“公平的”。或者最高。或者最重要的。有很多方法可以确定什么是“公平的”。我们通过两个因素来解决这个问题：优先级和等待时间。假设两个人同时出现在（锁定的）浴室门口。其中一个人有一个紧迫的截止日期（他们已经迟到了会议），而另一个则没有。让紧迫的截止日期的人下一步是不是有意义？嗯，当然会。唯一的问题是你如何决定谁更“重要”。 这可以通过分配优先级来完成（让我们使用像Neutrino这样的数字 - 一个是最低的可用优先级，255是此版本中最高的优先级）。房屋内有紧迫期限的人将获得更高的优先权，而那些没有最后期限的人将被赋予较低的优先权。线程也一样。线程从其父线程继承其调度算法，但可以调用 pthread_setschedparam（） 来更改其调度策略和优先级（如果它有权执行此操作）。如果有多个线程在等待，并且互斥锁被解锁，我们会将互斥锁提供给具有最高优先级的等待线程。但是，假设两个人都有同样的优先权。那你现在怎么办？那么，在这种情况下，允许等待时间最长的人下一步是“公平的”。这不仅是“公平的”，而且也是Neutrino内核的作用。在一堆线程等待的情况下，我们主要通过优先级，其次 是等待的长度。互斥锁肯定不是我们遇到的唯一同步对象。我们来看看其他一些。操作系统的设计，因此可以归结为三点：（1）以多进程形式，允许多个任务同时运行；（2）以多线程形式，允许单个任务分成不同的部分运行；（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。进程和线程简单而基本靠谱的定义如下：进程：程序的一次执行线程：CPU的基本调度单位","categories":[],"tags":[]}],"categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"},{"name":"开发工具","slug":"开发工具","permalink":"cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"},{"name":"架构","slug":"架构","permalink":"cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"},{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"},{"name":"工具","slug":"工具","permalink":"cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"},{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"},{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"},{"name":"IDEA","slug":"IDEA","permalink":"cpeixin.cn/tags/IDEA/"},{"name":"GPT-2","slug":"GPT-2","permalink":"cpeixin.cn/tags/GPT-2/"},{"name":"kali","slug":"kali","permalink":"cpeixin.cn/tags/kali/"},{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"},{"name":"爬虫","slug":"爬虫","permalink":"cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"shadowsock","slug":"shadowsock","permalink":"cpeixin.cn/tags/shadowsock/"},{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"},{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"},{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"},{"name":"K-Means","slug":"K-Means","permalink":"cpeixin.cn/tags/K-Means/"},{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"},{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"},{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"},{"name":"sklearn","slug":"sklearn","permalink":"cpeixin.cn/tags/sklearn/"},{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"数据清洗","slug":"数据清洗","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"},{"name":"数据采集","slug":"数据采集","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"},{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"},{"name":"词向量","slug":"词向量","permalink":"cpeixin.cn/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"},{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"},{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"},{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"},{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"},{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"},{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"},{"name":"yarn","slug":"yarn","permalink":"cpeixin.cn/tags/yarn/"},{"name":"mapreduce","slug":"mapreduce","permalink":"cpeixin.cn/tags/mapreduce/"},{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"},{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"},{"name":"LRU淘汰算法","slug":"LRU淘汰算法","permalink":"cpeixin.cn/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"},{"name":"栈","slug":"栈","permalink":"cpeixin.cn/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"cpeixin.cn/tags/%E9%98%9F%E5%88%97/"},{"name":"线性表","slug":"线性表","permalink":"cpeixin.cn/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/"},{"name":"数组","slug":"数组","permalink":"cpeixin.cn/tags/%E6%95%B0%E7%BB%84/"}]}