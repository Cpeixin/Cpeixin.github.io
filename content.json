{"meta":{"title":"布兰特 | 不忘初心","subtitle":"人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华","description":"","author":"Brent","url":"cpeixin.cn","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-04-04T08:29:08.762Z","updated":"2020-04-03T14:56:33.596Z","comments":false,"path":"/404.html","permalink":"cpeixin.cn/404.html","excerpt":"","text":""},{"title":"关于","date":"2020-04-04T14:37:07.111Z","updated":"2020-04-04T14:37:07.111Z","comments":false,"path":"about/index.html","permalink":"cpeixin.cn/about/index.html","excerpt":"","text":""},{"title":"书单","date":"2020-04-04T08:29:08.739Z","updated":"2020-04-03T14:56:33.596Z","comments":false,"path":"books/index.html","permalink":"cpeixin.cn/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-04-04T08:29:08.728Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"categories/index.html","permalink":"cpeixin.cn/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-04-04T08:29:08.717Z","updated":"2020-04-03T14:56:33.597Z","comments":true,"path":"links/index.html","permalink":"cpeixin.cn/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-04-04T08:29:08.705Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"repository/index.html","permalink":"cpeixin.cn/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-04-04T08:29:08.693Z","updated":"2020-04-03T14:56:33.597Z","comments":false,"path":"tags/index.html","permalink":"cpeixin.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"关于Hive中的NULL","slug":"关于Hive中的NULL","date":"2020-11-18T16:35:14.000Z","updated":"2020-11-18T16:37:34.829Z","comments":true,"path":"2020/11/19/关于Hive中的NULL/","link":"","permalink":"cpeixin.cn/2020/11/19/%E5%85%B3%E4%BA%8EHive%E4%B8%AD%E7%9A%84NULL/","excerpt":"","text":"最近公司的小伙伴基本上都在用Hive来查询数据，毕竟对于后端程序员来讲更为熟悉的还是像MySQL，Oracle这样的数据库，虽然Hive中的SQL标准和上面两者很相似了，但是还有些差别的，这也造成了同事们的水土不服，其中就有关于Hive判空的操作，那这里就记录一下Hive的Nullhive的使用中对null、‘’（空字符串）进行判断识别不同数据类型对空值的存储规则int与string类型数据存储，null默认存储为 \\N；string类型的数据如果为””，存储则是””；另外往int类型的字段插入数据“”时，结果还是\\N。不同数据类型，空值的查询对于int可以使用is null来判断空；而对于string类型，条件is null 查出来的是\\N的数据；而条件 =’’，查询出来的是””的数据。下面举例来说明：上图 t_user_details，brent用户的register_date字段为NULL值，那这时候，我们应该怎样去根据register_date字段值去查询brent这个用户呢？直接看结果：对于上面的查询，register_date = ‘’ 是不能成功过滤的。根据上面所说的，也就是 对于string类型，条件is null 查出来的是\\N的数据；而条件 =’’，查询出来的是””的数据所以对于 cloumn = ‘’ 的查询，是针对这个cloumn字段，写入值的时候，是 ‘’ 空子串，所以此时，is null 的条件则不生效。现在我们去HDFS底层存储中去求证一下有图有真相，hive 中null实际在HDFS中默认存储为 \\N可以使用serialization.null.format来指定Hive中保存和标识NULL，可以设置为默认的\\N，也可以为NULL或’’eg : ALTER TABLE b SET SERDEPROPERTIES (‘serialization.null.format’=’’);如果表中存在大量的NULL值，则在Hive的数据文件中会产生大量的\\N数据，浪费存储空间，那我们可以将serialization.null.format设置为’’那么对于上游系统写入的数据不清楚的情况下，我们怎么去方便的对空值进行判断呢？给一个万金油的写法：1select * from t_user_details where register_date is null or register_date &lt;&gt; &#39;&#39;;","categories":[{"name":"categories","slug":"categories","permalink":"cpeixin.cn/categories/categories/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"DMP系统设计的选型考量","slug":"DMP系统设计的选型考量","date":"2020-10-27T16:14:18.000Z","updated":"2020-10-27T16:18:20.600Z","comments":true,"path":"2020/10/28/DMP系统设计的选型考量/","link":"","permalink":"cpeixin.cn/2020/10/28/DMP%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%80%89%E5%9E%8B%E8%80%83%E9%87%8F/","excerpt":"","text":"DMP：数据管理平台我们先来看一下什么是 DMP 系统。DMP 系统的全称叫作数据管理平台（Data Management Platform），目前广泛应用在互联网的广告定向（Ad Targeting）、个性化推荐（Recommendation）这些领域。通常来说，DMP 系统会通过处理海量的互联网访问数据以及机器学习算法，给一个用户标注上各种各样的标签。然后，在我们做个性化推荐和广告投放的时候，再利用这些这些标签，去做实际的广告排序、推荐等工作。无论是 Google 的搜索广告、淘宝里千人千面的商品信息，还是抖音里面的信息流推荐，背后都会有一个 DMP 系统。那么，一个 DMP 系统应该怎么搭建呢？对于外部使用 DMP 的系统或者用户来说，可以简单地把 DMP 看成是一个键 - 值对（Key-Value）数据库。我们的广告系统或者推荐系统，可以通过一个客户端输入用户的唯一标识（ID），然后拿到这个用户的各种信息。这些信息中，有些是用户的人口属性信息（Demographic），比如性别、年龄；有些是非常具体的行为（Behavior），比如用户最近看过的商品是什么，用户的手机型号是什么；有一些是我们通过算法系统计算出来的兴趣（Interests），比如用户喜欢健身、听音乐；还有一些则是完全通过机器学习算法得出的用户向量，给后面的推荐算法或者广告算法作为数据输入。基于此，对于这个 KV 数据库，我们的期望也很清楚，那就是：低响应时间（Low Response Time）、高可用性（High Availability）、高并发（High Concurrency）、海量数据（Big Data），同时我们需要付得起对应的成本（Affordable Cost）。如果用数字来衡量这些指标，那么我们的期望就会具体化成下面这样。低响应时间：一般的广告系统留给整个广告投放决策的时间也就是 10ms 左右，所以对于访问 DMP 获取用户数据，预期的响应时间都在 1ms 之内。高可用性：DMP 常常用在广告系统里面。DMP 系统出问题，往往就意味着我们整个的广告收入在不可用的时间就没了，所以我们对于可用性的追求可谓是没有上限的。Google 2018 年的广告收入是 1160 亿美元，折合到每一分钟的收入是 22 万美元。即使我们做到 99.99% 的可用性，也意味着每个月我们都会损失 100 万美元。高并发：还是以广告系统为例，如果每天我们需要响应 100 亿次的广告请求，那么我们每秒的并发请求数就在 100 亿 / (86400) ~= 12K 次左右，所以我们的 DMP 需要支持高并发。数据量：如果我们的产品针对中国市场，那么我们需要有 10 亿个 Key，对应的假设每个用户有 500 个标签，标签有对应的分数。标签和分数都用一个 4 字节（Bytes）的整数来表示，那么一共我们需要 10 亿 x 500 x (4 + 4) Bytes = 4 TB 的数据了。低成本：我们还是从广告系统的角度来考虑。广告系统的收入通常用 CPM（Cost Per Mille），也就是千次曝光来统计。如果千次曝光的利润是 0.10，那么每天100亿次的曝光就是100万美元的利润。这个利润听起来非常高了。但是反过来算一下，你会发现，DMP每1000次的请求的成本不能超过0.10。最好只有 $0.01，甚至更低，我们才能尽可能多赚到一点广告利润。这五个因素一结合，听起来是不是就不那么简单了？不过，更复杂的还在后面呢。虽然从外部看起来，DMP 特别简单，就是一个 KV 数据库，但是生成这个数据库需要做的事情更多。我们下面一起来看一看。为了能够生成这个 KV 数据库，我们需要有一个在客户端或者 Web 端的数据采集模块，不断采集用户的行为，向后端的服务器发送数据。服务器端接收到数据，就要把这份数据放到一个数据管道（Data Pipeline）里面。数据管道的下游，需要实际将数据落地到数据仓库（Data Warehouse），把所有的这些数据结构化地存储起来。后续，我们就可以通过程序去分析这部分日志，生成报表或者或者利用数据运行各种机器学习算法。除了这个数据仓库之外，我们还会有一个实时数据处理模块（Realtime Data Processing），也放在数据管道的下游。它同样会读取数据管道里面的数据，去进行各种实时计算，然后把需要的结果写入到 DMP 的 KV 数据库里面去。MongoDB 真的万能吗？面对这里的 KV 数据库、数据管道以及数据仓库，这三个不同的数据存储的需求，最合理的技术方案是什么呢？你可以先自己思考一下，我这里先卖个关子。我共事过的不少不错的 Web 程序员，面对这个问题的时候，常常会说：“这有什么难的，用 MongoDB 就好了呀！”如果你也选择了 MongoDB，那最终的结果一定是一场灾难。我为什么这么说呢？MongoDB 的设计听起来特别厉害，不需要预先数据 Schema，访问速度很快，还能够无限水平扩展。作为 KV 数据库，我们可以把 MongoDB 当作 DMP 里面的 KV 数据库；除此之外，MongoDB 还能水平扩展、跑 MQL，我们可以把它当作数据仓库来用。至于数据管道，只要我们能够不断往 MongoDB 里面，插入新的数据就好了。从运维的角度来说，我们只需要维护一种数据库，技术栈也变得简单了。看起来，MongoDB 这个选择真是相当完美！但是，作为一个老程序员，第一次听到 MongoDB 这样“万能”的解决方案，我的第一反应是，“天底下哪有这样的好事”。所有的软件系统，都有它的适用场景，想通过一种解决方案适用三个差异非常大的应用场景，显然既不合理，又不现实。接下来，我们就来仔细看一下，这个“不合理”“不现实”在什么地方。上面我们已经讲过 DMP 的 KV 数据库期望的应用场景和性能要求了，这里我们就来看一下数据管道和数据仓库的性能取舍。对于数据管道来说，我们需要的是高吞吐量，它的并发量虽然和 KV 数据库差不多，但是在响应时间上，要求就没有那么严格了，1-2 秒甚至再多几秒的延时都是可以接受的。而且，和 KV 数据库不太一样，数据管道的数据读写都是顺序读写，没有大量的随机读写的需求。数据仓库就更不一样了，数据仓库的数据读取的量要比管道大得多。管道的数据读取就是我们当时写入的数据，一天有 10TB 日志数据，管道只会写入 10TB。下游的数据仓库存放数据和实时数据模块读取的数据，再加上个 2 倍的 10TB，也就是 20TB 也就够了。但是，数据仓库的数据分析任务要读取的数据量就大多了。一方面，我们可能要分析一周、一个月乃至一个季度的数据。这一次分析要读取的数据可不是 10TB，而是 100TB 乃至 1PB。我们一天在数据仓库上跑的分析任务也不是 1 个，而是成千上万个，所以数据的读取量是巨大的。另一方面，我们存储在数据仓库里面的数据，也不像数据管道一样，存放几个小时、最多一天的数据，而是往往要存上 3 个月甚至是 1 年的数据。所以，我们需要的是 1PB 乃至 5PB 这样的存储空间。我把 KV 数据库、数据管道和数据仓库的应用场景，总结成了一个表格，放在这里。你可以对照着看一下，想想为什么 MongoDB 在这三个应用场景都不合适。在 KV 数据库的场景下，需要支持高并发。那么 MongoDB 需要把更多的数据放在内存里面，但是这样我们的存储成本就会特别高了。在数据管道的场景下，我们需要的是大量的顺序读写，而 MongoDB 则是一个文档数据库系统，并没有为顺序写入和吞吐量做过优化，看起来也不太适用。而在数据仓库的场景下，主要的数据读取时顺序读取，并且需要海量的存储。MongoDB 这样的文档式数据库也没有为海量的顺序读做过优化，仍然不是一个最佳的解决方案。而且文档数据库里总是会有很多冗余的字段的元数据，还会浪费更多的存储空间。那我们该选择什么样的解决方案呢？拿着我们的应用场景去找方案，其实并不难找。对于 KV 数据库，最佳的选择方案自然是使用 SSD 硬盘，选择 AeroSpike 这样的 KV 数据库。高并发的随机访问并不适合 HDD 的机械硬盘，而 400TB 的数据，如果用内存的话，成本又会显得太高。对于数据管道，最佳选择自然是 Kafka。因为我们追求的是吞吐率，采用了 Zero-Copy 和 DMA 机制的 Kafka 最大化了作为数据管道的吞吐率。而且，数据管道的读写都是顺序读写，所以我们也不需要对随机读写提供支持，用上 HDD 硬盘就好了。到了数据仓库，存放的数据量更大了。在硬件层面使用 HDD 硬盘成了一个必选项。否则，我们的存储成本就会差上 10 倍。这么大量的数据，在存储上我们需要定义清楚 Schema，使得每个字段都不需要额外存储元数据，能够通过 Avro/Thrift/ProtoBuffer 这样的二进制序列化的方存储下来，或者干脆直接使用 Hive 这样明确了字段定义的数据仓库产品。很明显，MongoDB 那样不限制 Schema 的数据结构，在这个情况下并不好用。总结好了，相信到这里，你应该对怎么从最基本的原理出发，来选择技术栈有些感觉了。你应该更多地从底层的存储系统的特性和原理去考虑问题。一旦能够从这个角度去考虑问题，那么你对各类新的技术项目和产品的公关稿，自然会有一定的免疫力了，而不会轻易根据商业公司的宣传来做技术选型了。因为低延时、高并发、写少读多的 DMP 的 KV 数据库，最适合用 SSD 硬盘，并且采用专门的 KV 数据库是最合适的。可以选择AeroSpike，也可以用开源的 Cassandra 来提供服务。对于数据管道，因为主要是顺序读和顺序写，所以我们不一定要选用 SSD 硬盘，而可以用 HDD 硬盘。不过，对于最大化吞吐量的需求，使用 zero-copy 和 DMA 是必不可少的，所以现在的数据管道的标准解决方案就是 Kafka 了。对于数据仓库，我们通常是一次写入、多次读取。并且，由于存储的数据量很大，我们还要考虑成本问题。于是，一方面，我们会用 HDD 硬盘而不是 SSD 硬盘；另一方面，我们往往会预先给数据规定好 Schema，使得单条数据的序列化，不需要像存 JSON 或者 MongoDB 的 BSON 那样，存储冗余的字段名称这样的元数据。所以，最常用的解决方案是，用 Hadoop 这样的集群，采用 Hive 这样的数据仓库系统，或者采用 Avro/Thrift/ProtoBuffer 这样的二进制序列化方案。在大型的 DMP 系统设计当中，我们需要根据各个应用场景面临的实际情况，选择不同的硬件和软件的组合，来作为整个系统中的不同组件。思考这一讲里，我们讲到了数据管道通常所使用的开源系统 Kafka，并且选择了使用机械硬盘。在 Kafka 的使用上，我们有没有必要使用 SSD 硬盘呢？如果用了 SSD 硬盘，又会带来哪些好处和坏处呢？从应用场景来看，SSD的确还没有太大必要。从数据量的角度，搞很多块HDD已经可以满足目前的高并发的需求了，kafka频繁擦写 ssd的寿命是一个问题，从成本上和使用寿命也比SSD更划算了。但是SSD和几年前比已经便宜了很多了，而且在PCI-E接口普及的情况下，顺序读写速度比起HDD也能拉开差距了，所以逐步我们也看到业界开始直接用SSD来部署Kafka也变得比较常见了。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"DMP","slug":"DMP","permalink":"cpeixin.cn/tags/DMP/"}]},{"title":"HBase中bloomfilter源码实现","slug":"HBase中bloomfilter源码实现","date":"2020-10-03T08:10:56.000Z","updated":"2020-10-07T08:11:45.114Z","comments":true,"path":"2020/10/03/HBase中bloomfilter源码实现/","link":"","permalink":"cpeixin.cn/2020/10/03/HBase%E4%B8%ADbloomfilter%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Bloom filter最优的大小计算Bloom过滤器对插入到其中的元素的数量非常敏感。对于HBase来说，条目的数量取决于存储在列中的数据的大小。当前默认区域大小为256MB，因此条目计数~=256MB/（列的平均值大小）。尽管有这个经验法则，但是由于压缩，我们并没有有效的方法来计算压缩后的条目计数。因此，通常使用动态bloom过滤器来添加额外的空间，而不是允许错误率增长。Bloom filter最优的大小计算公示为：bloom size m = -(n * ln(err) / (ln(2)^2) ~= n * ln(err) / ln(0.6185)m表示Bloom filter中的位数（bitSize）n表示插入bloomfilter中的元素数（maxKeys）k表示使用的哈希函数数（nbHash）e表示Bloom所需的误报率（err）但且仅当k=m/n ln（2）时，误报概率最小。Hbase中Bloom filter的设置是在创建列族时通过setBloomFilterType方法设定，Hbase支持ROW、ROWCOL、ROWPREFIX_FIXED_LENGTH三种类型的Bloom filter，创建列族时默认设置为ROW，对所有插入数据的rowkey写入到Bloom filter中。**HBase中布隆过滤器的实现Bloom filter接口实现Hbase的布隆过滤器由父接口BloomFilterBase类定义，包含2个子继承接口BloomFilter和BloomFilterWriter。BloomFilter负责读取、判断，BloomFilterWriter负责将数据写入布隆过滤器。read类BloomFilter负责数据的读取判断其中定义了三个方法123456&#x2F;&#x2F;检查所定义的keyCell是否包含boolean contains(Cell keyCell, ByteBuff bloom, BloomType type);boolean contains(byte[] buf, int offset, int length, ByteBuff bloom); &#x2F;&#x2F;是否允许Bloom filter自动load数据，默认实现为trueboolean supportsAutoLoading();BloomFilter最终的实现类是CompoundBloomFilter类，CompoundBloomFilter的核心方法是contains方法。12345678910111213141516171819202122232425262728public boolean contains(Cell keyCell, ByteBuff bloom, BloomType type) &#123; &#x2F;&#x2F;如果根索引不包含keyCell，返回false，根索引在Hfile创建时构建，不是对所有rowkey int block &#x3D; index.rootBlockContainingKey(keyCell); if (block &lt; 0) &#123; return false; &#x2F;&#x2F; This key is not in the file. &#125; boolean result; &#x2F;&#x2F;获得Bloom的Block HFileBlock bloomBlock &#x3D; getBloomBlock(block); try &#123; ByteBuff bloomBuf &#x3D; bloomBlock.getBufferReadOnly(); &#x2F;&#x2F;通过BloomFilterUtil的contains方法判断 result &#x3D; BloomFilterUtil.contains(keyCell, bloomBuf, bloomBlock.headerSize(), bloomBlock.getUncompressedSizeWithoutHeader(), hash, hashCount, type); &#125; finally &#123; &#x2F;&#x2F; After the use return back the block if it was served from a cache. reader.returnBlock(bloomBlock); &#125; if (numPositivesPerChunk !&#x3D; null &amp;&amp; result) &#123; &#x2F;&#x2F; Update statistics. Only used in unit tests. ++numPositivesPerChunk[block]; &#125; return result;&#125;BloomFilterUtil.contains方法中，通过不同的 BloomType，构建不同的BloomHashKey，然后读取bloomBuf中的bitvals，计算cell对应类型的HashKey，判断在bitvals中是否为1.12345678public static boolean contains(Cell cell, ByteBuff bloomBuf, int bloomOffset, int bloomSize, Hash hash, int hashCount, BloomType type) &#123; HashKey&lt;Cell&gt; hashKey &#x3D; type &#x3D;&#x3D; BloomType.ROWCOL ? new RowColBloomHashKey(cell) : new RowBloomHashKey(cell);&#x2F;&#x2F;最终的判断方法，实现还是比较简单 return contains(bloomBuf, bloomOffset, bloomSize, hash, hashCount, hashKey);&#125;write类BloomFilterWriter类定义了一些写入方法123456789101112 &#x2F;&#x2F;在数据写入磁盘之前，压缩Bloom filter void compactBloom(); &#x2F;&#x2F;获得一个meta data 的Writer，写入Bloom TYpe 、数据大小等Writable getMetaWriter(); &#x2F;&#x2F;获取一个 Bloom bits 的WriterWritable getDataWriter(); &#x2F;&#x2F; previous cell writtenCell getPrevCell();BloomFilterWriter的最终实现类为CompoundBloomFilterWriter。子类CompoundBloomFilterWriter的核心方法是append方法，负责将添加的数据写入到BloomFilter123456789101112131415161718192021222324252627282930@Overridepublic void append(Cell cell) throws IOException &#123; if (cell &#x3D;&#x3D; null) throw new NullPointerException(); enqueueReadyChunk(false); if (chunk &#x3D;&#x3D; null) &#123; if (firstKeyInChunk !&#x3D; null) &#123; throw new IllegalStateException(&quot;First key in chunk already set: &quot; + Bytes.toStringBinary(firstKeyInChunk)); &#125; &#x2F;&#x2F;第一添加时需要allocateNewChunk，Chunk动态添加，完成hash等 &#x2F;&#x2F; This will be done only once per chunk if (bloomType &#x3D;&#x3D; BloomType.ROWCOL) &#123; firstKeyInChunk &#x3D; PrivateCellUtil .getCellKeySerializedAsKeyValueKey(PrivateCellUtil.createFirstOnRowCol(cell)); &#125; else &#123; firstKeyInChunk &#x3D; CellUtil.copyRow(cell); &#125; allocateNewChunk(); &#125; &#x2F;&#x2F;Chunk 实现具体的hash计算，和bit置位 chunk.add(cell); this.prevCell &#x3D; cell; ++totalKeyCount;&#125;BloomFilterChunk继承自BloomFilterBase，实现了BloomFilter的读写，其中add方法实现写入12345678910111213141516171819202122 public void add(Cell cell) &#123; &#x2F;* * For faster hashing, use combinatorial generation * http:&#x2F;&#x2F;www.eecs.harvard.edu&#x2F;~kirsch&#x2F;pubs&#x2F;bbbf&#x2F;esa06.pdf *&#x2F; int hash1; int hash2; HashKey&lt;Cell&gt; hashKey; &#x2F;&#x2F; 计算2次hash 写入位 if (this.bloomType &#x3D;&#x3D; BloomType.ROWCOL) &#123; hashKey &#x3D; new RowColBloomHashKey(cell); hash1 &#x3D; this.hash.hash(hashKey, 0); hash2 &#x3D; this.hash.hash(hashKey, hash1); &#125; else &#123; hashKey &#x3D; new RowBloomHashKey(cell); hash1 &#x3D; this.hash.hash(hashKey, 0); hash2 &#x3D; this.hash.hash(hashKey, hash1); &#125; setHashLoc(hash1, hash2); &#125;get方法完成数据查询，和之前BloomFilterUtil.contains方法一致12345678910static boolean get(int pos, ByteBuffer bloomBuf, int bloomOffset) &#123; &#x2F;&#x2F;实现位查找 int bytePos &#x3D; pos &gt;&gt; 3; &#x2F;&#x2F;pos &#x2F; 8 int bitPos &#x3D; pos &amp; 0x7; &#x2F;&#x2F;pos % 8 &#x2F;&#x2F; TODO access this via Util API which can do Unsafe access if possible(?) byte curByte &#x3D; bloomBuf.get(bloomOffset + bytePos); curByte &amp;&#x3D; BloomFilterUtil.bitvals[bitPos]; return (curByte !&#x3D; 0);&#125;布隆过滤器的使用对于之前创建的布隆过滤器的使用，hbase中体现在两个地方，一个是构建scannner时，判断scanner的是否包含所需要的数据列或者列族在构建StoreFileScanner时，会通过shouldUseScanner方法判断，时都用到当前Scanner，其中用到了reader.passesBloomFilter的方法。123456789101112public boolean shouldUseScanner(Scan scan, HStore store, long oldestUnexpiredTS) &#123; &#x2F;&#x2F; if the file has no entries, no need to validate or create a scanner. byte[] cf &#x3D; store.getColumnFamilyDescriptor().getName(); TimeRange timeRange &#x3D; scan.getColumnFamilyTimeRange().get(cf); if (timeRange &#x3D;&#x3D; null) &#123; timeRange &#x3D; scan.getTimeRange(); &#125; &#x2F;&#x2F;从时间范围、startkey和endkey范围、bloomfilter判断 return reader.passesTimerangeFilter(timeRange, oldestUnexpiredTS) &amp;&amp; reader .passesKeyRangeFilter(scan) &amp;&amp; reader.passesBloomFilter(scan, scan.getFamilyMap().get(cf));&#125;StoreFileReader在创建StoreFileScanner的时候创建，主要用来读取hfile文件。 passesBloomFilter方法当前Hfile的bloomFilter的类型，构建具体的bloomFilter。bloomFilter的类型是创建表时，列族中定义的。1234567891011121314151617181920212223242526272829boolean passesBloomFilter(Scan scan, final SortedSet&lt;byte[]&gt; columns) &#123; byte[] row &#x3D; scan.getStartRow(); switch (this.bloomFilterType) &#123; case ROW: if (!scan.isGetScan()) &#123; return true; &#125; return passesGeneralRowBloomFilter(row, 0, row.length); case ROWCOL: if (!scan.isGetScan()) &#123; return true; &#125; if (columns !&#x3D; null &amp;&amp; columns.size() &#x3D;&#x3D; 1) &#123; byte[] column &#x3D; columns.first(); &#x2F;&#x2F; create the required fake key Cell kvKey &#x3D; PrivateCellUtil.createFirstOnRow(row, HConstants.EMPTY_BYTE_ARRAY, column); return passesGeneralRowColBloomFilter(kvKey); &#125; &#x2F;&#x2F; For multi-column queries the Bloom filter is checked from the &#x2F;&#x2F; seekExact operation. return true; case ROWPREFIX_FIXED_LENGTH: return passesGeneralRowPrefixBloomFilter(scan); default: return true; &#125;&#125;passesGeneralRowBloomFilter方法中this.generalBloomFilter是创建reader时构建的BloomFilter。1234567891011121314151617private boolean passesGeneralRowBloomFilter(byte[] row, int rowOffset, int rowLen) &#123; BloomFilter bloomFilter &#x3D; this.generalBloomFilter; if (bloomFilter &#x3D;&#x3D; null) &#123; return true; &#125; &#x2F;&#x2F; Used in ROW bloom byte[] key &#x3D; null; if (rowOffset !&#x3D; 0 || rowLen !&#x3D; row.length) &#123; throw new AssertionError( &quot;For row-only Bloom filters the row must occupy the whole array&quot;); &#125; key &#x3D; row; &#x2F;&#x2F;判断row是否在本hfile中 return checkGeneralBloomFilter(key, null, bloomFilter);&#125;checkGeneralBloomFilter方法中调用contains完成最终的判断。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"}]},{"title":"布隆过滤器在HBase中的应用","slug":"布隆过滤器在HBase中的应用","date":"2020-09-30T07:18:58.000Z","updated":"2020-10-07T12:38:25.330Z","comments":true,"path":"2020/09/30/布隆过滤器在HBase中的应用/","link":"","permalink":"cpeixin.cn/2020/09/30/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%9C%A8HBase%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","excerpt":"","text":"布隆过滤器在HBase中的应用块索引机制在讨论布隆过滤器在HBase中的应用之前，先介绍一下HBase的块索引机制。块索引是HBase固有的一个特性，因为HBase的底层数据是存储在HFile中的，而每个HFile中存储的是有序的&lt;key, value&gt;键值对，HFile文件内部由连续的块组成，每个块中存储的第一行数据的行键组成了这个文件的块索引，这些块索引信息存储在文件尾部。当HBase打开一个HFile时，块索引信息会优先加载到内存；HBase首先在内存的块索引中进行二分查找，确定可能包含给定键的块，然后读取磁盘块找到实际想要的键。注意这里的块不是HDFS的块，HBase块的默认大小是64KB。可以根据需要配置不同的大小，对于顺序访问较多的表，建议使用较大的块；随机访问较多的表，建议使用较小的块。但实际应用中，仅仅只有块索引满足不了需求，这是因为，块索引能帮助我们更快地在一个文件中找到想要的数据，但是我们可能依然需要扫描很多文件。而布隆过滤器就是为解决这个问题而生。因为布隆过滤器的作用是，用户可以立即判断一个文件是否包含特定的行键，从而帮我们过滤掉一些不需要扫描的文件。如下图所示，块索引显示每个文件中都可能包含对应的行键，而布隆过滤器能帮我们跳过一些明显不包含对应行键的文件。如上图所示，布隆过滤器不能明确指出哪一个文件一定包含所查找的行键，布隆过滤器的结果有误差存在。当布隆过滤器判断文件中不包含对应的行时，这个答案是绝对正确的；但是，当布隆过滤器判断得到文件中包含对应行时，这个答案却有可能是错的。也就是说，HBase还是有可能加载了不必要的块。尽管如此，布隆过滤器还是可以帮助我们跳过一些明显不需要扫描的文件。另外，错误率可以通过调整布隆过滤器所占空间的大小来调整，通常设置错误率为1%。需要注意的是，使用布隆过滤器，并不一定能立即提升个别的get操作性能，因为同一时间可能有多个客户端向HBase发送请求，当负载过大时，HBase的性能受限于读磁盘的效率。但是，使用了布隆过滤器之后，可以减少不必要的块加载，从而可以提高整个集群的吞吐率。并且，因为HBase加载的块数量少了，缓存波动也降低了，进而提高了读缓存的命中率。然而，相比与布隆过滤器的优点，它的缺点显得如此微不足道，就是需要占用少量的存储空间。在使用布隆过滤器时，需要注意两个问题什么时候应该使用布隆过滤器根据上面的描述，布隆过滤器的主要作用，是帮助HBase跳过那些显然不包括所查找数据的底层文件。那么，当所查找的数据均匀分布在所有文件中（当用户定期更新所有行时，就可能导致这种情况），布隆过滤器的作用就微乎其微，反而浪费了存储空间。相反，如果我们查找的数据只包含在少部分的文件中，就应该果断使用布隆过滤器。选择行级还是行加列级布隆过滤器布隆过滤器是hbase中的高级功能，它能够减少特定访问模式（get/scan）下的查询时间。不过由于这种模式增加了内存和存储的负担，所以被默认为关闭状态。hbase支持如下类型的布隆过滤器：1、NONE 不使用布隆过滤器2、ROW 行键使用布隆过滤器3、ROWCOL 列键使用布隆过滤器其中ROWCOL是粒度更细的模式。很显然，行加列级因为粒度更细，占用的存储空间也就越多。因此，如果用户总是读取整行的数据，行级布隆过滤器就够用了。在可以选择的情形下，尽可能使用行级布隆过滤器，因为它在额外的空间开销和利用过滤存储文件提升性能之间取得了更好的平衡。例如：ROW 使用场景，假设有2个Hfile文件hf1和hf2， hf1包含kv1（r1 cf:q1 v）、kv2（r2 cf:q1 v） hf2包含kv3（r3 cf:q1 v）、kv4（r4 cf:q1 v） 如果设置了CF属性中的bloomfilter（布隆过滤器）为ROW，那么get(r1)时就会过滤hf2，get(r3)就会过滤hf1 。ROWCOL使用场景，假设有2个Hfile文件hf1和hf2， hf1包含kv1（r1 cf:q1 v）、kv2（r2 cf:q1 v） hf2包含kv3（r1 cf:q2 v）、kv4（r2 cf:q2 v） 如果设置了CF属性中的bloomfilter为ROW，无论get(r1,q1)还是get(r1,q2)，都会读取hf1+hf2；而如果设置了CF属性中的bloomfilter为ROWCOL，那么get(r1,q1)就会过滤hf2，get(r1,q2)就会过滤hf1。注意：ROW和ROWCOL只是名字上有联系，但是ROWCOL并不是ROW的扩展，也不能取代ROW布隆过滤器的存储在哪对于hbase而言，当我们选择采用布隆过滤器之后，HBase会在生成StoreFile（HFile）时包含一份布隆过滤器结构的数据，称其为MetaBlock；MetaBlock与DataBlock（真实的KeyValue数据）一起由LRUBlockCache维护。所以，开启bloomfilter会有一定的存储及内存cache开销。但是在大多数情况下，这些负担相对于布隆过滤器带来的好处是可以接受的。采用布隆过滤器后，hbase如何get数据在读取数据时，hbase会首先在布隆过滤器中查询，根据布隆过滤器的结果，再在MemStore中查询，最后再在对应的HFile中查询。采用布隆过滤器后，hbase如何Scan数据get操作会enable bloomfilter帮助剔除掉不会用到的Storefile,在scan初始化时（get会包装为scan）对于每个storefile会做shouldSeek的检查，如果返回false，则表明该storefile里没有要找的内容，直接跳过1234if (memOnly &#x3D;&#x3D; false &amp;&amp; ((StoreFileScanner) kvs).shouldSeek(scan, columns)) &#123; scanners.add(kvs); &#125;shouldSeek方法：如果是scan直接返回true表明不能跳过，然后根据bloomfilter类型检查。1234567891011121314151617181920if (!scan.isGetScan()) &#123; return true; &#125; byte[] row &#x3D; scan.getStartRow(); switch (this.bloomFilterType) &#123; case ROW: return passesBloomFilter(row, 0, row.length, null, 0, 0); case ROWCOL: if (columns !&#x3D; null &amp;&amp; columns.size() &#x3D;&#x3D; 1) &#123; byte[] column &#x3D; columns.first(); return passesBloomFilter(row, 0, row.length, column, 0, column.length); &#125; &#x2F;&#x2F; For multi-column queries the Bloom filter is checked from the &#x2F;&#x2F; seekExact operation. return true; default: return true; &#125;指明qualified的scan在配了rowcol的情况下会剔除不会用掉的StoreFile。对指明了qualify的scan或者get进行检查：seekExactly123456789&#x2F;&#x2F; Seek all scanners to the start of the Row (or if the exact matching row &#x2F;&#x2F; key does not exist, then to the start of the next matching Row). if (matcher.isExactColumnQuery()) &#123; for (KeyValueScanner scanner : scanners) scanner.seekExactly(matcher.getStartKey(), false); &#125; else &#123; for (KeyValueScanner scanner : scanners) scanner.seek(matcher.getStartKey()); &#125;如果bloomfilter没命中，则创建一个很大的假的keyvalue，表明该storefile不需要实际的scan12345678910111213141516171819202122public boolean seekExactly(KeyValue kv, boolean forward) throws IOException &#123; if (reader.getBloomFilterType() !&#x3D; StoreFile.BloomType.ROWCOL || kv.getRowLength() &#x3D;&#x3D; 0 || kv.getQualifierLength() &#x3D;&#x3D; 0) &#123; return forward ? reseek(kv) : seek(kv); &#125; boolean isInBloom &#x3D; reader.passesBloomFilter(kv.getBuffer(), kv.getRowOffset(), kv.getRowLength(), kv.getBuffer(), kv.getQualifierOffset(), kv.getQualifierLength()); if (isInBloom) &#123; &#x2F;&#x2F; This row&#x2F;column might be in this store file. Do a normal seek. return forward ? reseek(kv) : seek(kv); &#125; &#x2F;&#x2F; Create a fake key&#x2F;value, so that this scanner only bubbles up to the top &#x2F;&#x2F; of the KeyValueHeap in StoreScanner after we scanned this row&#x2F;column in &#x2F;&#x2F; all other store files. The query matcher will then just skip this fake &#x2F;&#x2F; key&#x2F;value and the store scanner will progress to the next column. cur &#x3D; kv.createLastOnRowCol(); return true; &#125;这边为什么是rowcol才能剔除storefile纳，很简单，scan是一个范围，如果是row的bloomfilter不命中只能说明该rowkey不在此storefile中，但next rowkey可能在。而rowcol的bloomfilter就不一样了，如果rowcol的bloomfilter没有命中表明该qualifiy不在这个storefile中，因此这次scan就不需要scan此storefile了！1.任何类型的get（基于rowkey和基于row+col）bloomfilter都能生效，关键是get的类型要匹配bloomfilter的类型2.基于row的scan是没办法优化的3.row+col+qualify的scan可以去掉不存在此qualify的storefile，也算是不错的优化了，而且指明qualify也能减少流量，因此scan尽量指明qualify。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"}]},{"title":"探索B+树和LSM-Tree","slug":"探索B-树和LSM-Tree","date":"2020-09-27T03:07:25.000Z","updated":"2020-10-07T03:08:50.702Z","comments":true,"path":"2020/09/27/探索B-树和LSM-Tree/","link":"","permalink":"cpeixin.cn/2020/09/27/%E6%8E%A2%E7%B4%A2B-%E6%A0%91%E5%92%8CLSM-Tree/","excerpt":"","text":"今天来了解下关系型数据库MySQL和NoSQL存储引擎HBase的底层存储机制。对于一个数据库的性能来说，其数据的组织方式至关重要。众所周知，数据库的数据大多存储在磁盘上，而磁盘的访问相对内存的访问来说是一项很耗时的操作，对比如下。因此，提高数据库数据的查找速度的关键点之一便是尽量减少磁盘的访问次数。为了加速数据库数据的访问，大多传统的关系型数据库都会使用特殊的数据结构来帮助查找数据，这种数据结构叫作索引（ Index）。对于传统的关系型数据库，考虑到经常需要范围查找某一批数据，因此其索引一般不使用 Hash算法，而使用树（ Tree）结构。然而，树结构的种类很多，却不一定都适合用于做数据库索引。二叉查找树与平衡二叉树最常见的树结构是二叉查找树（ Binary Search Tree），它就是一棵二叉有序树：保证左子树上所有节点的值都小于根节点的值，而右子树上所有节点的值都大于根节点的值。其优点在于实现简单，并且树在平衡的状态下查找效率能达到 O(log n)；缺点是在极端非平衡情况下查找效率会退化到 O(n)，因此很难保证索引的效率。针对上述二叉查找树的缺点，人们很自然就想到是否能用平衡二叉树（ Balanced Binary Tree）来解决这个问题。但是平衡二叉树依然有个比较大的问题：它的树高为 log n——对于索引树来说，树的高度越高，意味着查找所要花费的访问次数越多，查询效率越低。况且，主存从磁盘读数据一般以页为单位，因此每次访问磁盘都会读取多个扇区的数据（比如 4KB大小的数据），远大于单个二叉树节点的值（字节级别），这也是造成二叉树相对索引树效率低下的原因。正因如此，人们就想到了通过增加每个树节点的度来提高访问效率，而 B+树（B+-tree）便受到了更多的关注。B+树在传统的关系型数据库里， B+树（ B+-tree）及其衍生树是被用得比较多的索引树。B+树的主要特点如下。每个树节点只存放键值，不存放数值，而由叶子节点存放数值。这样会使树节点的度比较大，而树的高度就比较低，从而有利于提高查询效率。叶子节点存放数值，并按照值大小顺序排序，且带指向相邻节点的指针，以便高效地进行区间数据查询；并且所有叶子节点与根节点的距离相同，因此任何查询的效率都很相似。与二叉树不同， B+树的数据更新操作不从根节点开始，而从叶子节点开始，并且在更新过程中树能以比较小的代价实现自平衡。正是由于 B+树的上述优点，它成了传统关系型数据库的宠儿。当然，它也并非无懈可击，我们总在讨论B+树的有点，下面来说说缺点，它的主要缺点在于随着数据插入的不断发生，叶子节点会慢慢分裂——这可能会导致逻辑上原本连续的数据实际上存放在不同的物理磁盘块位置上，在做范围查询的时候会导致较高的磁盘 IO，以致严重影响到性能。日志结构合并树众所周知，数据库的数据大多存储在磁盘上，而无论是传统的机械硬盘（ HardDiskDrive, HDD）还是固态硬盘（ Solid State Drive, SSD），对磁盘数据的顺序读写速度都远高于随机读写。然而，基于 B+树的索引结构是违背上述磁盘基本特点的——它会需要较多的磁盘随机读写，于是， 1992年，名为日志结构（ Log-Structured）的新型索引结构方法便应运而生。日志结构方法的主要思想是将磁盘看作一个大的日志，每次都将新的数据及其索引结构添加到日志的最末端，以实现对磁盘的顺序操作，从而提高索引性能。不过，日志结构方法也有明显的缺点，随机读取数据时效率很低。1996年，一篇名为 Thelog-structured merge-tree（LSM-tree）的论文创造性地提出了日志结构合并树（ Log-Structured Merge-Tree）的概念，该方法既吸收了日志结构方法的优点，又通过将数据文件预排序克服了日志结构方法随机读性能较差的问题。尽管当时 LSM-tree新颖且优势鲜明，但它真正声名鹊起却是在 10年之后的 2006年，那年谷歌的一篇使用了 LSM-tree技术的论文 Bigtable: A Distributed Storage System for Structured Data横空出世，在分布式数据处理领域掀起了一阵旋风，随后两个声名赫赫的大数据开源组件（ 2007年的 HBase与 2008年的 Cassandra，目前两者同为 Apache顶级项目）直接在其思想基础上破茧而出，彻底改变了大数据基础组件的格局，同时也极大地推广了 LSM-tree技术。LSM-tree最大的特点是同时使用了两部分类树的数据结构来存储数据，并同时提供查询。其中一部分数据结构（ C0树）存在于内存缓存（通常叫作 memtable）中，负责接受新的数据插入更新以及读请求，并直接在内存中对数据进行排序；另一部分数据结构（ C1树）存在于硬盘上 (这部分通常叫作 sstable)，它们是由存在于内存缓存中的 C0树冲写到磁盘而成的，主要负责提供读操作，特点是有序且不可被更改。LSM-tree的另一大特点是除了使用两部分类树的数据结构外，还会使用日志文件（通常叫作 commit log）来为数据恢复做保障。这三类数据结构的协作顺序一般是：所有的新插入与更新操作都首先被记录到 commit log中——该操作叫作 WAL（Write Ahead Log），然后再写到 memtable，最后当达到一定条件时数据会从 memtable冲写到 sstable，并抛弃相关的 log数据；memtable与 sstable可同时供查询；当 memtable出问题时，可从 commit log与 sstable中将 memtable的数据恢复。我们可以参考 HBase的架构来体会其架构中基于 LSM-tree的部分特点。按照 WAL的原则，数据首先会写到 HBase的 HLog(相当于 commit log)里，然后再写到 MemStore（相当于 memtable）里，最后会冲写到磁盘 StoreFile（相当于 sstable）中。这样 HBase的 HRegionServer就通过 LSM-tree实现了数据文件的生成。HBase LSM-tree架构示意图如下图。LSM-tree的这种结构非常有利于数据的快速写入（理论上可以接近磁盘顺序写速度），但是不利于读——因为理论上读的时候可能需要同时从 memtable和所有硬盘上的 sstable中查询数据，这样显然会对性能造成较大的影响。为了解决这个问题， LSM-tree采取了以下主要的相关措施。定期将硬盘上小的 sstable合并（通常叫作 Merge或 Compaction操作）成大的 sstable，以减少 sstable的数量。而且，平时的数据更新删除操作并不会更新原有的数据文件，只会将更新删除操作加到当前的数据文件末端，只有在 sstable合并的时候才会真正将重复的操作或更新去重、合并。对每个 sstable使用布隆过滤器（ Bloom Filter），以加速对数据在该 sstable的存在性进行判定，从而减少数据的总查询时间。总结LSM树和B+树的差异主要在于读性能和写性能进行权衡，在牺牲的同时寻找其余补救方案。B+树存储引擎，不仅支持单条记录的增、删、读、改操作，还支持顺序扫描（B+树的叶子节点之间的指针），对应的存储系统就是关系数据库。但随着写入操作增多，为了维护B+树结构，节点分裂，读磁盘的随机读写概率会变大，性能会逐渐减弱。LSM树（Log-Structured MergeTree）存储引擎和B+树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"}]},{"title":"Kafka 压缩算法","slug":"Kafka-压缩算法","date":"2020-09-13T13:58:54.000Z","updated":"2020-09-13T14:00:26.778Z","comments":true,"path":"2020/09/13/Kafka-压缩算法/","link":"","permalink":"cpeixin.cn/2020/09/13/Kafka-%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95/","excerpt":"","text":"压缩（compression），我相信你一定不会感到陌生。它秉承了用时间去换空间的经典 trade-off 思想，具体来说就是用 CPU 时间去换磁盘空间或网络 I/O 传输量，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I/O 传输。在 Kafka 中，压缩也是用来做这件事的。怎么压缩？Kafka 是如何压缩消息的呢？要弄清楚这个问题，就要从 Kafka 的消息格式说起了。目前 Kafka 共有两大类消息格式，社区分别称之为 V1 版本和 V2 版本。V2 版本是 Kafka 0.11.0.0 中正式引入的。不论是哪个版本，Kafka 的消息层次都分为两层：消息集合（message set）以及消息（message）。一个消息集合中包含若干条日志项（record item），而日志项才是真正封装消息的地方。Kafka 底层的消息日志由一系列消息集合日志项组成。Kafka 通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行写入操作。那么社区引入 V2 版本的目的是什么呢？V2 版本主要是针对 V1 版本的一些弊端做了修正，和我们今天讨论的主题相关的修正有哪些呢？先介绍一个，就是把消息的公共部分抽取出来放到外层消息集合里面，这样就不用每条消息都保存这些信息了。我来举个例子。原来在 V1 版本中，每条消息都需要执行 CRC 校验，但有些情况下消息的 CRC 值是会发生变化的。比如在 Broker 端可能会对消息时间戳字段进行更新，那么重新计算之后的 CRC 值也会相应更新；再比如 Broker 端在执行消息格式转换时（主要是为了兼容老版本客户端程序），也会带来 CRC 值的变化。鉴于这些情况，再对每条消息都执行 CRC 校验就有点没必要了，不仅浪费空间还耽误 CPU 时间，因此在 V2 版本中，消息的 CRC 校验工作就被移到了消息集合这一层。校验的目的是防止因为网络传输出现问题导致broker端接收了受损的消息，所以应该放在作为server broker端进行，而不是在作为client端的producer。V2 版本还有一个和压缩息息相关的改进，就是保存压缩消息的方法发生了变化。之前 V1 版本中保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；而 V2 版本的做法是对整个消息集合进行压缩。显然后者应该比前者有更好的压缩效果。我对两个版本分别做了一个简单的测试，结果显示，在相同条件下，不论是否启用压缩，V2 版本都比 V1 版本节省磁盘空间。当启用压缩时，这种节省空间的效果更加明显，就像下面这两张图展示的那样：何时压缩？在 Kafka 中，压缩可能发生在两个地方：生产者端和 Broker 端。生产者程序中配置 compression.type 参数即表示启用指定类型的压缩算法。比如下面这段程序代码展示了如何构建一个开启 GZIP 的 Producer 对象：12345678910Properties props &#x3D; new Properties();props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);props.put(&quot;acks&quot;, &quot;all&quot;);props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);&#x2F;&#x2F; 开启GZIP压缩props.put(&quot;compression.type&quot;, &quot;gzip&quot;);Producer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;&gt;(props);这里比较关键的代码行是 props.put(“compression.type”, “gzip”)，它表明该 Producer 的压缩算法使用的是 GZIP。这样 Producer 启动后生产的每个消息集合都是经 GZIP 压缩过的，故而能很好地节省网络传输带宽以及 Kafka Broker 端的磁盘占用。在生产者端启用压缩是很自然的想法，那为什么我说在 Broker 端也可能进行压缩呢？其实大部分情况下 Broker 从 Producer 端接收到消息后仅仅是原封不动地保存而不会对其进行任何修改，但这里的“大部分情况”也是要满足一定条件的。有两种例外情况就可能让 Broker 重新压缩消息。情况一：Broker 端指定了和 Producer 端不同的压缩算法。先看一个例子。想象这样一个对话。Producer 说：“我要使用 GZIP 进行压缩。”Broker 说：“不好意思，我这边接收的消息必须使用 Snappy 算法进行压缩。”你看，这种情况下 Broker 接收到 GZIP 压缩消息后，只能解压缩然后使用 Snappy 重新压缩一遍。如果你翻开 Kafka 官网，你会发现 Broker 端也有一个参数叫 compression.type，和上面那个例子中的同名。但是这个参数的默认值是 producer，这表示 Broker 端会“尊重”Producer 端使用的压缩算法。可一旦你在 Broker 端设置了不同的 compression.type 值，就一定要小心了，因为可能会发生预料之外的压缩 / 解压缩操作，通常表现为 Broker 端 CPU 使用率飙升。情况二：Broker 端发生了消息格式转换。所谓的消息格式转换主要是为了兼容老版本的消费者程序。还记得之前说过的 V1、V2 版本吧？在一个生产环境中，Kafka 集群中同时保存多种版本的消息格式非常常见。为了兼容老版本的格式，Broker 端会对新版本消息执行向老版本格式的转换。这个过程中会涉及消息的解压缩和重新压缩。一般情况下这种消息格式转换对性能是有很大影响的，除了这里的压缩之外，它还让 Kafka 丧失了引以为豪的 Zero Copy 特性。所谓“Zero Copy”就是“零拷贝”，说的是当数据在磁盘和网络进行传输时避免昂贵的内核态数据拷贝，从而实现快速的数据传输。因此如果 Kafka 享受不到这个特性的话，性能必然有所损失，所以尽量保证消息格式的统一吧，这样不仅可以避免不必要的解压缩 / 重新压缩，对提升其他方面的性能也大有裨益。如果有兴趣你可以深入地了解下 Zero Copy 的原理。何时解压缩？有压缩必有解压缩！通常来说解压缩发生在消费者程序中，也就是说 Producer 发送压缩消息到 Broker 后，Broker 照单全收并原样保存起来。当 Consumer 程序请求这部分消息时，Broker 依然原样发送出去，当消息到达 Consumer 端后，由 Consumer 自行解压缩还原成之前的消息。那么现在问题来了，Consumer 怎么知道这些消息是用何种压缩算法压缩的呢？其实答案就在消息中。Kafka 会将启用了哪种压缩算法封装进消息集合中，这样当 Consumer 读取到消息集合时，它自然就知道了这些消息使用的是哪种压缩算法。如果用一句话总结一下压缩和解压缩，那么我希望你记住这句话：Producer 端压缩、Broker 端保持、Consumer 端解压缩。除了在 Consumer 端解压缩，Broker 端也会进行解压缩。注意了，这和前面提到消息格式转换时发生的解压缩是不同的场景。每个压缩过的消息集合在** Broker 端写入时都要发生解压缩操作，目的就是为了对消息执行各种验证。**我们必须承认这种解压缩对 Broker 端性能是有一定影响的，特别是对 CPU 的使用率而言。事实上，最近国内京东的小伙伴们刚刚向社区提出了一个 bugfix，建议去掉因为做消息校验而引入的解压缩。据他们称，去掉了解压缩之后，Broker 端的 CPU 使用率至少降低了 50%。不过有些遗憾的是，目前社区并未采纳这个建议，原因就是这种消息校验是非常重要的，不可盲目去之。毕竟先把事情做对是最重要的，在做对的基础上，再考虑把事情做好做快。针对这个使用场景，你也可以思考一下，是否有一个两全其美的方案，既能避免消息解压缩也能对消息执行校验。各种压缩算法对比那么我们来谈谈压缩算法。这可是重头戏！之前说了这么多，我们还是要比较一下各个压缩算法的优劣，这样我们才能有针对性地配置适合我们业务的压缩策略。在 Kafka 2.1.0 版本之前，Kafka 支持 3 种压缩算法：GZIP、Snappy 和 LZ4。从 2.1.0 开始，Kafka 正式支持 Zstandard 算法（简写为 zstd）。它是 Facebook 开源的一个压缩算法，能够提供超高的压缩比（compression ratio）。对了，看一个压缩算法的优劣，有两个重要的指标：一个指标是压缩比，原先占 100 份空间的东西经压缩之后变成了占 20 份空间，那么压缩比就是 5，显然压缩比越高越好；另一个指标就是压缩 / 解压缩吞吐量，比如每秒能压缩或解压缩多少 MB 的数据。同样地，吞吐量也是越高越好。下面这张表是 Facebook Zstandard 官网提供的一份压缩算法 benchmark 比较结果：从表中我们可以发现 zstd 算法有着最高的压缩比，而在吞吐量上的表现只能说中规中矩。反观 LZ4 算法，它在吞吐量方面则是毫无疑问的执牛耳者。当然对于表格中数据的权威性我不做过多解读，只想用它来说明一下当前各种压缩算法的大致表现。在实际使用中，GZIP、Snappy、LZ4 甚至是 zstd 的表现各有千秋。但对于 Kafka 而言，它们的性能测试结果却出奇得一致，即在吞吐量方面：LZ4 &gt; Snappy &gt; zstd 和 GZIP；而在压缩比方面，zstd &gt; LZ4 &gt; GZIP &gt; Snappy。具体到物理资源，使用 Snappy 算法占用的网络带宽最多，zstd 最少，这是合理的，毕竟 zstd 就是要提供超高的压缩比；在 CPU 使用率方面，各个算法表现得差不多，只是在压缩时 Snappy 算法使用的 CPU 较多一些，而在解压缩时 GZIP 算法则可能使用更多的 CPU。最佳实践了解了这些算法对比，我们就能根据自身的实际情况有针对性地启用合适的压缩算法。首先来说压缩。何时启用压缩是比较合适的时机呢？你现在已经知道 Producer 端完成的压缩，那么启用压缩的一个条件就是 Producer 程序运行机器上的 CPU 资源要很充足。如果 Producer 运行机器本身 CPU 已经消耗殆尽了，那么启用消息压缩无疑是雪上加霜，只会适得其反。除了 CPU 资源充足这一条件，如果你的环境中带宽资源有限，那么我也建议你开启压缩。事实上我见过的很多 Kafka 生产环境都遭遇过带宽被打满的情况。这年头，带宽可是比 CPU 和内存还要珍贵的稀缺资源，毕竟万兆网络还不是普通公司的标配，因此千兆网络中 Kafka 集群带宽资源耗尽这件事情就特别容易出现。如果你的客户端机器 CPU 资源有很多富余，我强烈建议你开启 zstd 压缩，这样能极大地节省网络资源消耗。其次说说解压缩。其实也没什么可说的。一旦启用压缩，解压缩是不可避免的事情。这里只想强调一点：我们对不可抗拒的解压缩无能为力，但至少能规避掉那些意料之外的解压缩。就像我前面说的，因为要兼容老版本而引入的解压缩操作就属于这类。有条件的话尽量保证不要出现消息格式转换的情况。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"序列化在Spark中节约空间了么？","slug":"序列化在Spark中节约空间了么？","date":"2020-09-03T15:58:14.000Z","updated":"2020-09-13T16:00:09.807Z","comments":true,"path":"2020/09/03/序列化在Spark中节约空间了么？/","link":"","permalink":"cpeixin.cn/2020/09/03/%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8Spark%E4%B8%AD%E8%8A%82%E7%BA%A6%E7%A9%BA%E9%97%B4%E4%BA%86%E4%B9%88%EF%BC%9F/","excerpt":"","text":"序列化在Spark中的用处在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。Java序列化与Kryo序列化对比Java序列化：采用objectOutputStream对象进行序列化，任何的类实现一个 java.io.Serializable接口都可以进行序列化，尽管Java序列化操作十分灵活，但是却十分缓慢，会产生更大的序列化类Kryo序列化：kryo序列化更快且比Java占用空间更小。但是并不支持所有序列化类，需要事先在应用程序中注册类kryo启动方式在conf中配置：1&quot;spark.serializer&#x3D;&quot;org.apache.spark.serializer.KryoSerializer&quot;调优spark.kryoserializer.buffer 这个是core中kryo缓存的大小，每个core一个，默认64Kspark.kryoserializer.buffer.max 这个是缓存的最大大小，默认为64M，序列化的类最大不可超过2G测试普通不序列化的MEMORY_ONLY12345678val persons&#x3D;new ArrayBuffer[Person]for(i&lt;-1 to 1000000)&#123;persons+&#x3D;(Person(&quot;name&quot;+i,10+i,&quot;male&quot;,&quot;haerbin&quot;))&#125;val personrdd&#x3D;sc.parallelize(persons)personrdd.persist(StorageLevel.MEMORY_ONLY)personrdd.count()结果：95.3M使用Java序列化的MEMORY_ONLY_SER12345678val persons&#x3D;new ArrayBuffer[Person]for(i&lt;-1 to 1000000)&#123;persons+&#x3D;(Person(&quot;name&quot;+i,10+i,&quot;male&quot;,&quot;haerbin&quot;))&#125;val personrdd&#x3D;sc.parallelize(persons)personrdd.persist(StorageLevel.MEMORY_ONLY_SER)personrdd.count()结果：39.8Mkryo的未注册类的MEMORY_ONLY_SER12345678val persons&#x3D;new ArrayBuffer[Person]for(i&lt;-1 to 1000000)&#123;persons+&#x3D;(Person(&quot;name&quot;+i,10+i,&quot;male&quot;,&quot;haerbin&quot;))&#125;val personrdd&#x3D;sc.parallelize(persons)personrdd.persist(StorageLevel.MEMORY_ONLY_SER)personrdd.count()结果：119.1M，之所以会发生这种情况，是因为没有注册类，所以kryo就将所有的类进行操作，所有导致占用很大内存使用kryo注册类，然后打包到集群运行（这种方式命令行看不到效果）12345678910111213sc.getConf.registerKryoClasses(Array(classOf[Person]))val persons&#x3D;new ArrayBuffer[Person]for(i&lt;-1 to 1000000)&#123;persons+&#x3D;(Person(&quot;name&quot;+i,10+i,&quot;male&quot;,&quot;haerbin&quot;))&#125;val personrdd&#x3D;sc.parallelize(persons)personrdd.persist(StorageLevel.MEMORY_ONLY_SER)使用submit提交jar包--class sparkcore.SerializerApp \\--name SerializerApp \\--master yarn \\ &#x2F;home&#x2F;hadoop&#x2F;lib2&#x2F;scala6-1.0.jar结果 ：27.5M，结果显而易见，kryo效果更好序列化时间实体类1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.io.Serializable;import java.util.Map;public class Simple implements Serializable&#123; private static final long serialVersionUID &#x3D; -4914434736682797743L; private String name; private int age; private Map&lt;String,Integer&gt; map; public Simple()&#123; &#125; public Simple(String name,int age,Map&lt;String,Integer&gt; map)&#123; this.name &#x3D; name; this.age &#x3D; age; this.map &#x3D; map; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age &#x3D; age; &#125; public Map&lt;String, Integer&gt; getMap() &#123; return map; &#125; public void setMap(Map&lt;String, Integer&gt; map) &#123; this.map &#x3D; map; &#125; &#125;java原生序列化 OriginalSerializable.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.util.HashMap;import java.util.Map;import bhz.entity.Simple;public class OriginalSerializable &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; long start &#x3D; System.currentTimeMillis(); setSerializableObject(); System.out.println(&quot;java原生序列化时间:&quot; + (System.currentTimeMillis() - start) + &quot; ms&quot; ); start &#x3D; System.currentTimeMillis(); getSerializableObject(); System.out.println(&quot;java原生反序列化时间:&quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;); &#125; public static void setSerializableObject() throws IOException&#123; FileOutputStream fo &#x3D; new FileOutputStream(&quot;D:&#x2F;file2.bin&quot;); ObjectOutputStream so &#x3D; new ObjectOutputStream(fo); for (int i &#x3D; 0; i &lt; 100000; i++) &#123; Map&lt;String,Integer&gt; map &#x3D; new HashMap&lt;String, Integer&gt;(2); map.put(&quot;zhang0&quot;, i); map.put(&quot;zhang1&quot;, i); so.writeObject(new Simple(&quot;zhang&quot;+i,(i+1),map)); &#125; so.flush(); so.close(); &#125; public static void getSerializableObject()&#123; FileInputStream fi; try &#123; fi &#x3D; new FileInputStream(&quot;D:&#x2F;file2.bin&quot;); ObjectInputStream si &#x3D; new ObjectInputStream(fi); Simple simple &#x3D;null; while((simple&#x3D;(Simple)si.readObject()) !&#x3D; null)&#123; &#x2F;&#x2F;System.out.println(simple.getAge() + &quot; &quot; + simple.getName()); &#125; fi.close(); si.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; &#x2F;&#x2F;e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; &#125;kyro序列化 KyroSerializable.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.util.HashMap;import java.util.Map;import org.objenesis.strategy.StdInstantiatorStrategy;import bhz.entity.Simple;import com.esotericsoftware.kryo.Kryo;import com.esotericsoftware.kryo.KryoException;import com.esotericsoftware.kryo.io.Input;import com.esotericsoftware.kryo.io.Output;public class KyroSerializable &#123; public static void main(String[] args) throws IOException &#123; long start &#x3D; System.currentTimeMillis(); setSerializableObject(); System.out.println(&quot;Kryo 序列化时间:&quot; + (System.currentTimeMillis() - start) + &quot; ms&quot; ); start &#x3D; System.currentTimeMillis(); getSerializableObject(); System.out.println(&quot;Kryo 反序列化时间:&quot; + (System.currentTimeMillis() - start) + &quot; ms&quot;); &#125; public static void setSerializableObject() throws FileNotFoundException&#123; Kryo kryo &#x3D; new Kryo(); kryo.setReferences(false); kryo.setRegistrationRequired(false); kryo.setInstantiatorStrategy(new StdInstantiatorStrategy()); kryo.register(Simple.class); Output output &#x3D; new Output(new FileOutputStream(&quot;D:&#x2F;file1.bin&quot;)); for (int i &#x3D; 0; i &lt; 100000; i++) &#123; Map&lt;String,Integer&gt; map &#x3D; new HashMap&lt;String, Integer&gt;(2); map.put(&quot;zhang0&quot;, i); map.put(&quot;zhang1&quot;, i); kryo.writeObject(output, new Simple(&quot;zhang&quot;+i,(i+1),map)); &#125; output.flush(); output.close(); &#125; public static void getSerializableObject()&#123; Kryo kryo &#x3D; new Kryo(); kryo.setReferences(false); kryo.setRegistrationRequired(false); kryo.setInstantiatorStrategy(new StdInstantiatorStrategy()); Input input; try &#123; input &#x3D; new Input(new FileInputStream(&quot;D:&#x2F;file1.bin&quot;)); Simple simple &#x3D;null; while((simple&#x3D;kryo.readObject(input, Simple.class)) !&#x3D; null)&#123; &#x2F;&#x2F;System.out.println(simple.getAge() + &quot; &quot; + simple.getName() + &quot; &quot; + simple.getMap().toString()); &#125; input.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch(KryoException e)&#123; &#125; &#125; &#125;**java原生序列化时间:8281 msjava原生反序列化时间:5899 msKryo 序列化时间:630 msKryo 反序列化时间:15 ms经过对比，可以发现kryo是java原生序列化性能十几倍官方也推荐尽量使用Kryo的序列化库（版本2）。官文介绍，Kryo序列化机制比Java序列化机制性能提高10倍左右，Spark之所以没有默认使用Kryo作为序列化类库，是因为它不支持所有对象的序列化，同时Kryo需要用户在使用前注册需要序列化的类型，不够方便。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Redis底层数据结构","slug":"Redis底层数据结构","date":"2020-08-30T06:47:56.000Z","updated":"2020-09-07T03:30:07.709Z","comments":true,"path":"2020/08/30/Redis底层数据结构/","link":"","permalink":"cpeixin.cn/2020/08/30/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"经典数据库 Redis 中的常用数据类型，底层都是用哪种数据结构实现的？Redis 数据库介绍Redis 是一种键值（Key-Value）数据库。相对于关系型数据库（比如 MySQL），Redis 也被叫作非关系型数据库。像 MySQL 这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过 SQL 语句，来实现非常复杂的查询需求。而 Redis 中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让 Redis 的读写效率非常高。除此之外，Redis 主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。这一点，我们后面会介绍。Redis 中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。“字符串（string）”这种数据类型非常简单，对应到数据结构里，就是字符串。你应该非常熟悉，这里我就不多介绍了。我们着重看下，其他四种比较复杂点的数据类型，看看它们底层都依赖了哪些数据结构。列表（list）我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表（ziplist），另一种是双向循环链表。当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件：列表中保存的单个数据（有可能是字符串类型的）小于 64 字节；列表中数据个数少于 512 个。关于压缩列表，我这里稍微解释一下。它并不是基础数据结构，而是 Redis 自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据，压缩列表不支持随机访问，Redis一般都是通过key获取整个value的值，也就是整个压缩列表的数据，并不需要随机访问。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，你可以看我下面画的这幅图。现在，我们来看看，压缩列表中的“压缩”两个字该如何理解？听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小（假设是 20 个字节）。那当我们存储小于 20 个字节长度的字符串的时候，便会浪费部分存储空间。听起来有点儿拗口，我画个图解释一下。压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。在链表里，我们已经讲过双向循环链表这种数据结构了。这里我们着重看一下 Redis 中双向链表的编码实现方式。Redis 的这种双向链表的实现方式，非常值得借鉴。它额外定义一个 list 结构体，来组织链表的首、尾指针，还有长度等信息。这样，在使用的时候就会非常方便。123456789101112131415// 以下是C语言代码，因为Redis是用C语言实现的。typedef struct listnode &#123; struct listNode *prev; struct listNode *next; void *value;&#125; listNode;typedef struct list &#123; listNode *head; listNode *tail; unsigned long len; // ....省略其他定义&#125; list;字典（hash）字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。同样，只有当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型。具体需要满足两个条件：字典中保存的键和值的大小都要小于 64 字节；字典中键值对的个数要小于 512 个。当不能同时满足上面两个条件的时候，Redis 就使用散列表来实现字典类型。Redis 使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis 使用链表法来解决。除此之外，Redis 还支持散列表的动态扩容、缩容。当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于 1 的时候，Redis 会触发扩容，将散列表扩大为原来大小的 2 倍左右（具体值需要计算才能得到，如果感兴趣，你可以去阅读源码）。当数据动态减少之后，为了节省内存，当装载因子小于 0.1 的时候，Redis 就会触发缩容，缩小为字典中数据个数的大约 2 倍大小（这个值也是计算得到的，如果感兴趣，你也可以去阅读源码）。我们前面讲过，扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis 使用我们在散列表（中）讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。集合（set）集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。当要存储的数据，同时满足下面这样两个条件的时候，Redis 就采用有序数组，来实现集合这种数据类型。存储的数据都是整数；存储的数据元素个数不超过 512 个。当不能同时满足这两个条件的时候，Redis 就使用散列表来存储集合中的数据。有序集合（sortedset）有序集合这种数据类型，我们在跳表里已经详细讲过了。它用来存储一组数据，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。实际上，跟 Redis 的其他数据类型一样，有序集合也并不仅仅只有跳表这一种实现方式。当数据量比较小的时候，Redis 会用压缩列表来实现有序集合。具体点说就是，使用压缩列表来实现有序集合的前提，有这样两个：所有数据的大小都要小于 64 字节；元素个数要小于 128 个。数据结构持久化尽管 Redis 经常会被用作内存数据库，但是，它也支持数据落盘，也就是将内存中的数据存储到硬盘中。这样，当机器断电的时候，存储在 Redis 中的数据也不会丢失。在机器重新启动之后，Redis 只需要再将存储在硬盘中的数据，重新读取到内存，就可以继续工作了。刚刚我们讲到，Redis 的数据格式由“键”和“值”两部分组成。而“值”又支持很多数据类型，比如字符串、列表、字典、集合、有序集合。像字典、集合等类型，底层用到了散列表，散列表中有指针的概念，而指针指向的是内存中的存储地址。 那 Redis 是如何将这样一个跟具体内存地址有关的数据结构存储到磁盘中的呢？实际上，Redis 遇到的这个问题并不特殊，很多场景中都会遇到。我们把它叫作数据结构的持久化问题，或者对象的持久化问题。这里的“持久化”，你可以笼统地理解为“存储到磁盘”。如何将数据结构持久化到硬盘？我们主要有两种解决思路。第一种是清除原有的存储结构，只将数据存储到磁盘中。当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，Redis 采用的就是这种持久化思路。不过，这种方式也有一定的弊端。那就是数据从硬盘还原到内存的过程，会耗用比较多的时间。比如，我们现在要将散列表中的数据存储到磁盘。当我们从磁盘中，取出数据重新构建散列表的时候，需要重新计算每个数据的哈希值。如果磁盘中存储的是几 GB 的数据，那重构数据结构的耗时就不可忽视了。第二种方式是保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号等信息，都保存在磁盘中。有了这些信息，我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。总结今天，我们学习了 Redis 中常用数据类型底层依赖的数据结构，总结一下大概有这五种：压缩列表（可以看作一种特殊的数组）有序数组链表散列表跳表Q&amp;AQ:为什么redis没有使用B+树而选择跳表？A:跳表更灵活 更容易实现","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"}]},{"title":"分布式一致性算法 - Paxos（2）","slug":"分布式一致性算法-Paxos（2）","date":"2020-08-30T05:10:05.000Z","updated":"2020-08-30T05:13:21.224Z","comments":true,"path":"2020/08/30/分布式一致性算法-Paxos（2）/","link":"","permalink":"cpeixin.cn/2020/08/30/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95-Paxos%EF%BC%882%EF%BC%89/","excerpt":"","text":"Paxos是共识算法，不是一致性协议Basic Paxos 只能就单个值（Value）达成共识，一旦遇到为一系列的值实现共识的时候，它就不管用了。虽然兰伯特提到可以通过多次执行 Basic Paxos 实例（比如每接收到一个值时，就执行一次 Basic Paxos 算法）实现一系列值的共识。但是，很多同学读完论文后，应该还是两眼摸黑，虽然每个英文单词都能读懂，但还是不理解兰伯特提到的 Multi-Paxos，为什么 Multi-Paxos 这么难理解呢？在我看来，兰伯特并没有把 Multi-Paxos 讲清楚，只是介绍了大概的思想，缺少算法过程的细节和编程所必须的细节（比如缺少选举领导者的细节）。这也就导致每个人实现的 Multi-Paxos 都不一样。不过从本质上看，大家都是在兰伯特提到的 Multi-Paxos 思想上补充细节，设计自己的 Multi-Paxos 算法，然后实现它（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）。所以在这里，我补充一下：兰伯特提到的 Multi-Paxos 是一种思想，不是算法。而 Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）。 这一点尤其需要你注意。为了帮你掌握 Multi-Paxos 思想，我会先带你了解，对于 Multi-Paxos 兰伯特是如何思考的，也就是说，如何解决 Basic Paxos 的痛点问题；然后我再以 Chubby 的 Multi-Paxos 实现为例，具体讲解一下。为啥选它呢？因为 Chubby 的 Multi-Paxos 实现，代表了 Multi-Paxos 思想在生产环境中的真正落地，它将一种思想变成了代码实现。贴一段6年前的代码🐂兰伯特关于 Multi-Paxos 的思考熟悉 Basic Paxos 的同学可能还记得，Basic Paxos 是通过二阶段提交来达成共识的。在第一阶段，也就是准备阶段，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段（也就是接受阶段）：而如果我们直接通过多次执行 Basic Paxos 实例，来实现一系列值的共识，就会存在这样几个问题：如果多个提议者同时提交提案，可能出现因为提案编号冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。你要知道，分布式系统的运行是建立在 RPC 通讯的基础之上的，因此，延迟一直是分布式系统的痛点，是需要我们在开发分布式系统时认真考虑和优化的。那么如何解决上面的 2 个问题呢？可以通过引入领导者和优化 Basic Paxos 执行来解决，咱们首先聊一聊领导者。领导者（Leader）我们可以通过引入领导者节点，也就是说，领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了：在这里，我补充一点：在论文中，兰伯特没有说如何选举领导者，需要我们在实现 Multi-Paxos 算法的时候自己实现。 比如在 Chubby 中，主节点（也就是领导者节点）是通过执行 Basic Paxos 算法，进行投票选举产生的。那么，如何解决第二个问题，也就是如何优化 Basic Paxos 执行呢？优化 Basic Paxos 执行我们可以采用“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，优化 Basic Paxos 执行。也就是说，领导者节点上，序列中的命令是最新的，不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段：你看，和重复执行 Basic Paxos 相比，Multi-Paxos 引入领导者节点之后，因为只有领导者节点一个提议者，只有它说了算，所以就不存在提案冲突。另外，当主节点处于稳定状态时，就省掉准备阶段，直接进入接受阶段，所以在很大程度上减少了往返的消息数，提升了性能，降低了延迟。讲到这儿，你可能会问了：在实际系统中，该如何实现 Multi-Paxos 呢？接下来，我以 Chubby 的 Multi-Paxos 实现为例，具体讲解一下。Chubby 的 Multi-Paxos 实现既然兰伯特只是大概的介绍了 Multi-Paxos 思想，那么 Chubby 是如何补充细节，实现 Multi-Paxos 算法的呢？首先，它通过引入主节点，实现了兰伯特提到的领导者（Leader）节点的特性。也就是说，主节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了。另外，在 Chubby 中，主节点是通过执行 Basic Paxos 算法，进行投票选举产生的，并且在运行过程中，主节点会通过不断续租的方式来延长租期（Lease）。比如在实际场景中，几天内都是同一个节点作为主节点。如果主节点故障了，那么其他的节点又会投票选举出新的主节点，也就是说主节点是一直存在的，而且是唯一的。其次，在 Chubby 中实现了兰伯特提到的，“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制。最后，在 Chubby 中，实现了成员变更（Group membership），以此保证节点变更的时候集群的平稳运行。最后，我想补充一点：在 Chubby 中，为了实现了强一致性，读操作也只能在主节点上执行。 也就是说，只要数据写入成功，之后所有的客户端读到的数据都是一致的。具体的过程，就是下面的样子。所有的读请求和写请求都由主节点来处理。当主节点从客户端接收到写请求后，作为提议者，执行 Basic Paxos 实例，将数据发送给所有的节点，并且在大多数的服务器接受了这个写请求之后，再响应给客户端成功：当主节点接收到读请求后，处理就比较简单了，主节点只需要查询本地数据，然后返回给客户端就可以了：Chubby 的 Multi-Paxos 实现，尽管是一个闭源的实现，但这是 Multi-Paxos 思想在实际场景中的真正落地，Chubby 团队不仅编程实现了理论，还探索了如何补充细节。其中的思考和设计非常具有参考价值，不仅能帮助我们理解 Multi-Paxos 思想，还能帮助我们理解其他的 Multi-Paxos 算法（比如 Raft 算法）。内容小结重点如下：兰伯特提到的 Multi-Paxos 是一种思想，不是算法，而且还缺少算法过程的细节和编程所必须的细节，比如如何选举领导者等，这也就导致了每个人实现的 Multi-Paxos 都不一样。而 Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列数据的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）。Chubby 实现了主节点（也就是兰伯特提到的领导者），也实现了兰伯特提到的 “当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段” 这个优化机制，省掉 Basic Paxos 的准备阶段，提升了数据的提交效率，但是所有写请求都在主节点处理，限制了集群处理写请求的并发能力，约等于单机。因为在 Chubby 的 Multi-Paxos 实现中，也约定了“大多数原则”，也就是说，只要大多数节点正常运行时，集群就能正常工作，所以 Chubby 能容错（n - 1）/2 个节点的故障。本质上而言，“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，是通过减少非必须的协商步骤来提升性能的。这种方法非常常用，也很有效。比如，Google 设计的 QUIC 协议，是通过减少 TCP、TLS 的协商步骤，优化 HTTPS 性能。我希望你能掌握这种性能优化思路，后续在需要时，可以通过减少非必须的步骤，优化系统性能。最后，我想说的是，我个人比较喜欢 Paxos 算法（兰伯特的 Basic Paxos 和 Multi-Paxos），虽然 Multi-Paxos 缺失算法细节，但这反而给我们提供了思考空间，让我们可以反复思考和考据缺失的细节，比如在 Multi-Paxos 中到底需不需要选举领导者，再比如如何实现提案编号等等。","categories":[{"name":"分布式","slug":"分布式","permalink":"cpeixin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"paxos","slug":"paxos","permalink":"cpeixin.cn/tags/paxos/"}]},{"title":"分布式一致性算法 - Paxos（1）","slug":"分布式一致性算法-Paxos（1）","date":"2020-08-28T16:54:44.000Z","updated":"2020-08-30T06:03:04.075Z","comments":true,"path":"2020/08/29/分布式一致性算法-Paxos（1）/","link":"","permalink":"cpeixin.cn/2020/08/29/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95-Paxos%EF%BC%881%EF%BC%89/","excerpt":"","text":"生有涯知无涯，只有掌握了最核心的“道”，才能以不变应万变。Paxos 算法提到分布式算法，就不得不提 Paxos 算法，在过去几十年里，它基本上是分布式共识的代名词，因为当前最常用的一批共识算法都是基于它改进的。比如，Fast Paxos 算法、Cheap Paxos 算法、Raft 算法等等。而很多同学都会在准确和系统理解 Paxos 算法上踩坑，比如，只知道它可以用来达成共识，但不知道它是如何达成共识的。这其实侧面说明了 Paxos 算法有一定的难度，可分布式算法本身就很复杂，Paxos 算法自然也不会例外，当然了，除了这一点，还跟兰伯特有关。兰伯特提出的 Paxos 算法包含 2 个部分：一个是 Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；另一个是 Multi-Paxos 思想，描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。可因为兰伯特提到的 Multi-Paxos 思想，缺少代码实现的必要细节（比如怎么选举领导者），这里举一段十年前的代码🐂，接下来分别以 Basic Paxos 和 Multi-Paxos 为核心，带你了解 Basic Paxos 如何达成共识，以及针对 Basic Paxos 的局限性 Multi-Paxos 又是如何改进的。今天咱们先来聊聊 Basic Paxos。在我看来，Basic Paxos 是 Multi-Paxos 思想的核心，说白了，Multi-Paxos 就是多执行几次 Basic Paxos。所以掌握它之后，你能更好地理解后几讲基于 Multi-Paxos 思想的共识算法（比如 Raft 算法），还能掌握分布式共识算法的最核心内容，当现在的算法不能满足业务需求，进行权衡折中，设计自己的算法。假设我们要实现一个分布式集群，这个集群是由节点 A、B、C 组成，提供只读 KV 存储服务。你应该知道，创建只读变量的时候，必须要对它进行赋值，而且这个值后续没办法修改。因此一个节点创建只读变量后就不能再修改它了，所以所有节点必须要先对只读变量的值达成共识，然后所有节点再一起创建这个只读变量。那么，当有多个客户端（比如客户端 1、2）访问这个系统，试图创建同一个只读变量（比如 X），客户端 1 试图创建值为 3 的 X，客户端 2 试图创建值为 7 的 X，这样要如何达成共识，实现各节点上 X 值的一致呢？带着这个问题，我们进入今天的学习。在一些经典的算法中，你会看到一些既形象又独有的概念（比如二阶段提交协议中的协调者），Basic Paxos 算法也不例外。为了帮助人们更好地理解 Basic Paxos 算法，兰伯特在讲解时，也使用了一些独有而且比较重要的概念，提案、准备（Prepare）请求、接受（Accept）请求、角色等等，其中最重要的就是“角色”。因为角色是对 Basic Paxos 中最核心的三个功能的抽象，比如，由接受者（Acceptor）对提议的值进行投票，并存储接受的值。你需要了解的三种角色在 Basic Paxos 中，有提议者（Proposer）、接受者（Acceptor）、学习者（Learner）三种角色，他们之间的关系如下：看着是不是有些复杂，其实并不难理解：提议者（Proposer）：提议一个值，用于投票表决。为了方便演示，你可以把图 1 中的客户端 1 和 2 看作是提议者。但在绝大多数场景中，集群中收到客户端请求的节点，才是提议者（图 1 这个架构，是为了方便演示算法原理）。这样做的好处是，对业务代码没有入侵性，也就是说，我们不需要在业务代码中实现算法逻辑，就可以像使用数据库一样访问后端的数据。接受者（Acceptor）：对每个提议的值进行投票，并存储接受的值，比如 A、B、C 三个节点。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受和存储数据。讲到这儿，你可能会有疑惑：前面不是说接收客户端请求的节点是提议者吗？这里怎么又是接受者呢？这是因为一个节点（或进程）可以身兼多个角色。想象一下，一个 3 节点的集群，1 个节点收到了请求，那么该节点将作为提议者发起二阶段提交，然后这个节点和另外 2 个节点一起作为接受者进行共识协商，就像下图的样子：学习者（Learner）：被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，被动地接受数据，容灾备份。其实，这三种角色，在本质上代表的是三种功能：提议者代表的是接入和协调功能，收到客户端请求后，发起二阶段提交，进行共识协商；接受者代表投票协商和存储数据，对提议的值进行投票，并接受达成共识的值，存储保存；学习者代表存储数据，不参与共识协商，只接受达成共识的值，存储保存。因为一个完整的算法过程是由这三种角色对应的功能组成的，所以理解这三种角色，是你理解 Basic Paxos 如何就提议的值达成共识的基础。如何达成共识？想象这样一个场景，现在疫情这么严重，每个村的路都封得差不多了，就你的村委会不作为，迟迟没有什么防疫的措施。你决定给村委会提交个提案，提一些防疫的建议，除了建议之外，为了和其他村民的提案做区分，你的提案还得包含一个提案编号，来起到唯一标识的作用。与你的做法类似，在 Basic Paxos 中，兰伯特也使用提案代表一个提议。不过在提案中，除了提案编号，还包含了提议值。为了方便演示，我使用[n, v]表示一个提案，其中 n 为提案编号，v 为提议值。我想强调一下，整个共识协商是分 2 个阶段进行的。那么具体要如何协商呢？我们假设客户端 1 的提案编号为 1，客户端 2 的提案编号为 5，并假设节点 A、B 先收到来自客户端 1 的准备请求，节点 C 先收到来自客户端 2 的准备请求。准备（Prepare）阶段先来看第一个阶段，首先客户端 1、2 作为提议者，分别向所有接受者发送包含提案编号的准备请求：你要注意，在准备请求中是不需要指定提议的值的，只需要携带提案编号就可以了，这是很多同学容易产生误解的地方。接着，当节点 A、B 收到提案编号为 1 的准备请求，节点 C 收到提案编号为 5 的准备请求后，将进行这样的处理：由于之前没有通过任何提案，所以节点 A、B 将返回一个 “尚无提案”的响应。也就是说节点 A 和 B 在告诉提议者，我之前没有通过任何提案呢，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案。节点 C 也是如此，它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案。另外，当节点 A、B 收到提案编号为 5 的准备请求，和节点 C 收到提案编号为 1 的准备请求的时候，将进行这样的处理过程：当节点 A、B 收到提案编号为 5 的准备请求的时候，因为提案编号 5 大于它们之前响应的准备请求的提案编号 1，而且两个节点都没有通过任何提案，所以它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案。当节点 C 收到提案编号为 1 的准备请求的时候，由于提案编号 1 小于它之前响应的准备请求的提案编号 5，所以丢弃该准备请求，不做响应。接受（Accept）阶段第二个阶段也就是接受阶段，首先客户端 1、2 在收到大多数节点的准备响应之后，会分别发送接受请求：当客户端 1 收到大多数的接受者（节点 A、B）的准备响应后，根据响应中提案编号最大的提案的值，设置接受请求中的值。因为该值在来自节点 A、B 的准备响应中都为空（也就是图 5 中的“尚无提案”），所以就把自己的提议值 3 作为提案的值，发送接受请求[1, 3]。当客户端 2 收到大多数的接受者的准备响应后（节点 A、B 和节点 C），根据响应中提案编号最大的提案的值，来设置接受请求中的值。因为该值在来自节点 A、B、C 的准备响应中都为空（也就是图 5 和图 6 中的“尚无提案”），所以就把自己的提议值 7 作为提案的值，发送接受请求[5, 7]。当三个节点收到 2 个客户端的接受请求时，会进行这样的处理：当节点 A、B、C 收到接受请求[1, 3]的时候，由于提案的提案编号 1 小于三个节点承诺能通过的提案的最小提案编号 5，所以提案[1, 3]将被拒绝。当节点 A、B、C 收到接受请求[5, 7]的时候，由于提案的提案编号 5 不小于三个节点承诺能通过的提案的最小提案编号 5，所以就通过提案[5, 7]，也就是接受了值 7，三个节点就 X 值为 7 达成了共识。讲到这儿我想补充一下，如果集群中有学习者，当接受者通过了一个提案时，就通知给所有的学习者。当学习者发现大多数的接受者都通过了某个提案，那么它也通过该提案，接受该提案的值。通过上面的演示过程，你可以看到，最终各节点就 X 的值达成了共识。那么在这里我还想强调一下，Basic Paxos 的容错能力，源自“大多数”的约定，你可以这么理解：当少于一半的节点出现故障的时候，共识协商仍然在正常工作。内容小结本节课我主要带你了解了 Basic Paxos 的原理和一些特点，我希望你明确这样几个重点。你可以看到，Basic Paxos 是通过二阶段提交的方式来达成共识的。二阶段提交是达成共识的常用方式，如果你需要设计新的共识算法的时候，也可以考虑这个方式。除了共识，Basic Paxos 还实现了容错，在少于一半的节点出现故障时，集群也能工作。它不像分布式事务算法那样，必须要所有节点都同意后才提交操作，因为“所有节点都同意”这个原则，在出现节点故障的时候会导致整个集群不可用。也就是说，“大多数节点都同意”的原则，赋予了 Basic Paxos 容错的能力，让它能够容忍少于一半的节点的故障。本质上而言，提案编号的大小代表着优先级，你可以这么理解，根据提案编号的大小，接受者保证三个承诺，具体来说：如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者将承诺不响应这个准备请求；如果接受请求中的提案的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者将承诺不通过这个提案；如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中，包含已经通过的最大编号的提案信息。Basic Paxos 算法解决的问题是一个分布式系统如何就某个值X（决议）达成一致。一个典型的场景是，在一个分布式数据库存储中，如果各节点的初始状态一致，每个节点执行相同的操作序列（例如上面是每个节点都要执行SET(X, 3)操作和SET(X, 7)操作，其中SET(X, 3)指令是客户端1向每个节点发出，SET(X, 7)指令是客户端2向每个节点发出），那么他们最后肯定能达到一个一致的状态。例如A、B、C三个节点执行的指令按照顺序都是 SET(X,3)–&gt;SET(X, 7)的话，则A、B、C三个节点最终X的值都是7.但是网络的不可靠性导致实际上最后每个节点执行的指令顺序并不一样，如果A节点、B节点执行顺序是SET(X,3)–&gt;SET(X,7)，则A、B节点上最后的值就是X=7，而C节点上可能因为网络原因执行的顺序是 SET(X,7)–&gt;SET(X,3), 结果最后C节点上的X值就是3了. 这种情况可能导致客户端发送相同执行指令，但是最终节点上的值不完全相同. 我们当然不希望看到这种情况的发生. 于是 Basic paxos算法横空出世.basic paxos算法需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。","categories":[{"name":"分布式","slug":"分布式","permalink":"cpeixin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"paxos","slug":"paxos","permalink":"cpeixin.cn/tags/paxos/"}]},{"title":"Flink从源码看状态定时清理","slug":"Flink从源码看状态定时清理","date":"2020-08-14T07:19:54.000Z","updated":"2020-10-09T07:21:30.324Z","comments":true,"path":"2020/08/14/Flink从源码看状态定时清理/","link":"","permalink":"cpeixin.cn/2020/08/14/Flink%E4%BB%8E%E6%BA%90%E7%A0%81%E7%9C%8B%E7%8A%B6%E6%80%81%E5%AE%9A%E6%97%B6%E6%B8%85%E7%90%86/","excerpt":"","text":"状态是Flink中的一个重要特征，不论是存储在内存中，还是在rockdb中，但是我们有时候需要对状态进行清查，不需要保存太久的状态，否则这个状态会太大，这个时候就会用到Flink的清除。为什么需要清理状态其一是Flink中的状态是有时效性的，也就是在一定的实际内，是有效的，一旦过了某个时间点，它就没有什么价值了，其二是需要控制flink的状态大小，能够有效的管理不断增长的state的规模大小。从flink的1.6中，就引入了关于状态时效性方面的特性，在1.8中，引入了基于TTL的对过去state的清理，让我们可以通过程序的 方式，对state进行清理，否则还的依赖其他额外的操作，来对state清理，这样会容易导致出错，并且不容易控制。Apache Flink的1.6.0版本引入了State TTL功能。它使流处理应用程序的开发人员配置过期时间，并在定义时间超时（Time to Live）之后进行清理。在Flink 1.8.0中，该功能得到了扩展，包括对RocksDB和堆状态后端（FSStateBackend和MemoryStateBackend）的历史数据进行持续清理，从而实现旧条目的连续清理过程（根据TTL设置）。Flink程序中的状态，是通过状态描述符来定义的，所以我们定义ttl，是通过flink程序中的StateTtlConfiguration对象，传递给状态描述符，来实现状态的清理。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class StateDemo &#123; public static void main(String[] args) throws Exception &#123; LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); // this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env) env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 10L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(0) .flatMap(new MyFlatMapFunction()) .print(); // the printed output will be (1,4) and (1,5) env.execute(); &#125;&#125;class MyFlatMapFunction extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; &#123; private static final long serialVersionUID = 1808329479322205953L; /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; // 状态过期清除 // flink 的状态清理是惰性策略，也就是我们访问的状态，可能已经过期了，但是还没有删除状态数据，我们可以配置 // 是否返回过期状态的数据，不论是否返回过期数据，数据被访问后会立即清除过期状态。并且截止1.8.0 的版本 // 状态的清除针对的是process time ，还不支持event time，可能在后期的版本中会支持。 // flink的内部，状态ttl 功能是通过上次相关状态访问的附加时间戳和实际状态值来实现的，这样的方案会增加存储 // 上的开销，但是会允许flink程序在查询数据，cp的时候访问数据的过期状态 StateTtlConfig ttlConfig = StateTtlConfig.newBuilder(Time.days(1)) //它是生存时间值 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) //状态可见性配置是否在读取访问时返回过期值// .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .cleanupFullSnapshot() // 在快照的时候进行删除 .build(); @Override public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; // access the state value Tuple2&lt;Long, Long&gt; currentSum = sum.value(); // update the count currentSum.f0 += 1; // add the second field of the input value currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 &gt;= 2) &#123; out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); &#125; &#125; @Override public void open(Configuration config) &#123; ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( \"average\", // the state name TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123; &#125;), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set //设置stage过期时间 descriptor.enableTimeToLive(ttlConfig); sum = getRuntimeContext().getState(descriptor); &#125;&#125;上面是定义了StateTtlConfig ，用于描述状态清除的配置信息，这个类是状态清除的核心配置类。Flink提供了多个选项来配置TTL功能的行为。状态的时间什么被修改？默认情况下，是当我们的数据状态修改会更新数据的ttl时间，当然我们也可以在读取数据时候对它进行更新，这样做会出现额外写入操作来更新时间戳操作。过去的状态数据是否可以访问？state ttl采用惰性策略来清理过期状态。这可能导致我们的应用程序会去尝试读取已过期但处于尚未删除状态的数据。我们可以观察此类读取请求是否返回了过期状态。无论哪种情况，数据被访问后会立即清除过期状态注意**使用Flink 1.8.0，用户只能根据处理时间（Processing Time）定义状态TTL。未来的Apache Flink版本中计划支持事件时间（Event Time）Flink内部，状态TTL功能是通过存储上次相关状态访问的附加时间戳以及实际状态值来实现的。虽然这种方法增加了一些存储开销，但它允许Flink程序在查询数据、checkpointing，数据恢复的时候访问数据的过期状态。如何避免读取过期数据？在读取操作中访问状态对象时，Flink将检查其时间戳并清除状态是否已过期（取决于配置的状态可见性，是否返回过期状态）。由于这种延迟删除的特性，永远不会再次访问的过期状态数据将永远占用存储空间，除非被垃圾回收。那么如何在没有应用程序逻辑明确的处理它的情况下删除过期的状态呢？通常，我们可以配置不同的策略进行后台删除。完整快照自动删除过期状态当获取检查点或保存点的完整快照时，Flink 1.6.0已经支持自动删除过期状态。大家注意，过期状态删除不适用于增量检查点。必须明确启用完全快照的状态删除1234567891011StateTtlConfig ttlConfig &#x3D; StateTtlConfig.newBuilder(Time.days(1)) &#x2F;&#x2F;它是生存时间值 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .cleanupFullSnapshot() &#x2F;&#x2F; 在快照的时候进行删除 .build();&#x2F;** Cleanup expired state in full snapshot on checkpoint. *&#x2F; @Nonnull public Builder cleanupFullSnapshot() &#123; cleanupStrategies.activate(CleanupStrategies.Strategies.FULL_STATE_SCAN_SNAPSHOT); return this; &#125;堆状态后端的增量清理**此方法特定于堆状态后端（FSStateBackend和MemoryStateBackend）。它的实现方法是存储后端在所有状态条目上维护一个惰性全局迭代器。某些事件（例如状态访问）会触发增量清理。每次触发增量清理时，迭代器都会向前迭代删除已遍历的过期数据。以下代码示例演示如何启用增量清理12345StateTtlConfig ttlConfig &#x3D; StateTtlConfig.newBuilder(Time.days(1)) &#x2F;&#x2F;它是生存时间值 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .cleanupIncrementally(15,false)&#x2F;&#x2F; .build();如果启用，则每次进行状态访问都会触发清理步骤。对于每个清理步骤，都会检查一定数量的数据是否过期。参数说明：第一个参数是检查每个清理步骤的状态条目数。第二个参数是一个标志，用于数据处理后触发清理步骤，此外对于每次状态访问同样有效。注意**第一个是增量清理所花费的时间增加了数据处理延迟。第二个应该可以忽略不计，但仍然值得一提：如果没有状态访问或没有数据处理记录，则不会删除过期状态RocksDB后台压缩可以过滤掉过期状态如果你的Flink应用程序使用RocksDB作为状态后端存储，则可以启用另一个基于Flink特定压缩过滤器的清理策略。RocksDB定期运行异步压缩以合并状态更新并减少存储。Flink压缩过滤器使用TTL检查状态条目的到期时间戳，并丢弃所有过期值。激活此功能的第一步是通过设置以下Flink配置选项来配置RocksDB状态后端1state.backend.rocksdb.ttl.compaction.filter.enabled配置RocksDB状态后端后，将为状态启用压缩清理策略，如以下代码示例所示12345StateTtlConfig ttlConfig &#x3D; StateTtlConfig.newBuilder(Time.days(1)) &#x2F;&#x2F;它是生存时间值 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .cleanupInRocksdbCompactFilter() &#x2F;&#x2F; 基于rocksdb的定期压缩合并进行清理 .build();使用定时器删除（Timers）手动清除状态的另一种方法是基于Flink定时器。这是社区目前正在评估未来版本的想法。通过这种方法，为每个状态访问注册清理定时器。这种方法更容易预测，因为状态一旦到期就会被删除。但是，这种方法代价很大，因为定时器消耗存储资源，并且会频繁读取状态信息。StateTtlConfig这个类用于配置state TTL 的逻辑UpdateType 这个枚举类表示此选项值配置何时更新上次访问时间戳记，以延长状态TTL12345public enum UpdateType &#123; Disabled,&#x2F;&#x2F;禁用过期，状态不过期 OnCreateAndWrite,&#x2F;&#x2F; 创建并且写时候 OnReadAndWrite&#x2F;&#x2F; 读然后写 &#125;StateVisibility 这个枚举类表示是否可以返回过期的用户值1234public enum StateVisibility &#123; ReturnExpiredIfNotCleanedUp,&#x2F;&#x2F; 如果还没有清除，那么返回 NeverReturnExpired&#x2F;&#x2F; 不返回过期的值 &#125;CleanupStrategies 这个抽象类是定义TTL 清除策略的类，里面有个枚举类Strategies ,用于定义清除策略12345enum Strategies &#123; FULL_STATE_SCAN_SNAPSHOT,&#x2F;&#x2F; 快照扫描时候 INCREMENTAL_CLEANUP,&#x2F;&#x2F;增量清理 ROCKSDB_COMPACTION_FILTER&#x2F;&#x2F; rocksdb 压缩过滤 &#125;CleanupStrategy接口这是一个空接口，用于定义各个清除策略的实现类，有三个实现类，分别是：EmptyCleanupStrategy：这个类没有任何内容，是个空类IncrementalCleanupStrategy：增量清理策略的配置实现类RocksdbCompactFilterCleanupStrategy：基于rocksdb 压缩过滤策略的配置实现类StateTtlConfig 的核心属性：12345private UpdateType updateType &#x3D; OnCreateAndWrite;private StateVisibility stateVisibility &#x3D; NeverReturnExpired;private TimeCharacteristic timeCharacteristic &#x3D; ProcessingTime;private Time ttl;private CleanupStrategies cleanupStrategies &#x3D; new CleanupStrategies();说明：StateTtlConfig 这个类主要有上面5个属性updateType ：代表什么时候更新上次的时间戳stateVisibility ：表示状态过期后是否给用户返回相关值timeCharacteristic ：表示时间特征，现在只支持process time上的状态ttl定义ttl：表示时间频率cleanupStrategies ：代表清除策略,有三种，EmptyCleanupStrategy，IncrementalCleanupStrategy和RocksdbCompactFilterCleanupStrategy","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 故障恢复和重启策略","slug":"Flink-故障恢复和重启策略","date":"2020-08-08T14:02:55.000Z","updated":"2020-10-08T14:04:30.988Z","comments":true,"path":"2020/08/08/Flink-故障恢复和重启策略/","link":"","permalink":"cpeixin.cn/2020/08/08/Flink-%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%E5%92%8C%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/","excerpt":"","text":"自动故障恢复是 Flink 提供的一个强大的功能，在实际运行环境中，我们会遇到各种各样的问题从而导致应用挂掉，比如我们经常遇到的非法数据、网络抖动等。Flink 提供了强大的可配置故障恢复和重启策略来进行自动恢复。故障恢复Flink 的配置文件，其中有一个参数 jobmanager.execution.failover-strategy: region。Flink 支持了不同级别的故障恢复策略，jobmanager.execution.failover-strategy 的可配置项有两种：full 和 region。当我们配置的故障恢复策略为 full 时，集群中的 Task 发生故障，那么该任务的所有 Task 都会发生重启。而在实际生产环境中，我们的大作业可能有几百个 Task，出现一次异常如果进行整个任务重启，那么经常会导致长时间任务不能正常工作，导致数据延迟。根据图论知识，如果我们的ExecutionGraph是一个非连通图（即可以划分为多个独立的依赖pipeline），那么当某个Task失败时，就可以只回溯到该Task所在的连通分量的Source，并重启该连通分量涉及到的所有Task，而其他Task不受影响，如下图所示。此时一个连通分量就是一个Region。这个思路很容易理解，但是对于ExecutionGraph本身就是连通图的情况就不高效了，因为还是要重启所有Task，如下图所示。所以Flink对这种情况又做了一个优化：在发生一对多依赖的Task后面缓存计算出来的中间结果（intermediate result）。当下游的Task失败重启时，就可以不必回溯到Source，而是回溯到中间结果就行了，重启的Task数进一步减少。此时从中间结果缓存起计的所有下游Task形成一个Region。用语言描述可能有些不直观，一张图就能说明白了。注意B1、B2后面的黑框框当然，如果是靠近Source一端的Task出了问题，或者中间结果缓存失效，这种方法就行不通了，老老实实从Source重启吧。所以集群中某一个或几个 Task 发生了故障，只需要重启有问题的一部分即可，这就是 Flink 基于 Region 的局部重启策略。在这个策略下，Flink 会把我们的任务分成不同的 Region，当某一个 Task 发生故障时，Flink 会计算需要故障恢复的最小 Region。Flink 在判断需要重启的 Region 时，采用了以下的判断逻辑：发生错误的 Task 所在的 Region 需要重启；如果当前 Region 的依赖数据出现损坏或者部分丢失，那么生产数据的 Region 也需要重启；为了保证数据一致性，当前 Region 的下游 Region 也需要重启。Job重启策略的相关源码Task重启策略的相关源码重启策略Flink 提供了多种类型和级别的重启策略，常用的重启策略包括：固定延迟重启策略模式失败率重启策略模式无重启策略模式Flink 在判断使用的哪种重启策略时做了默认约定，如果用户配置了 checkpoint，但没有设置重启策略，那么会按照固定延迟重启策略模式进行重启；如果用户没有配置 checkpoint，那么默认不会重启。下面我们分别对这三种模式进行详细讲解。**无重启策略模式在这种情况下，如果我们的作业发生错误，任务会直接退出。我们可以在 flink-conf.yaml 中配置：1restart-strategy: none也可以在程序中使用代码指定：12final ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart());**固定延迟重启策略模式固定延迟重启策略会通过在 flink-conf.yaml 中设置如下配置参数，来启用此策略：1restart-strategy: fixed-delay固定延迟重启策略模式需要指定两个参数，首先 Flink 会根据用户配置的重试次数进行重试，每次重试之间根据配置的时间间隔进行重试，如下表所示：举个例子，假如我们需要任务重试 3 次，每次重试间隔 5 秒，那么需要进行一下配置：12restart-strategy.fixed-delay.attempts: 3restart-strategy.fixed-delay.delay: 5 s当前我们也可以在代码中进行设置：1234env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, &#x2F;&#x2F; 重启次数 Time.of(5, TimeUnit.SECONDS) &#x2F;&#x2F; 时间间隔));失败率重启策略模式首先我们在 flink-conf.yaml 中指定如下配置：1restart-strategy: failure-rate这种重启模式需要指定三个参数，如下表所示。失败率重启策略在 Job 失败后会重启，但是超过失败率后，Job 会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。这种策略的配置理解较为困难，我们举个例子，假如 5 分钟内若失败了 3 次，则认为该任务失败，每次失败的重试间隔为 5 秒。那么我们的配置应该是：123restart-strategy.failure-rate.max-failures-per-interval: 3restart-strategy.failure-rate.failure-rate-interval: 5 minrestart-strategy.failure-rate.delay: 5 s当然，也可以在代码中直接指定：12345env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, &#x2F;&#x2F; 每个时间间隔的最大故障次数 Time.of(5, TimeUnit.MINUTES), &#x2F;&#x2F; 测量故障率的时间间隔 Time.of(5, TimeUnit.SECONDS) &#x2F;&#x2F; 每次任务失败时间间隔));最后，需要注意的是，在实际生产环境中由于每个任务的负载和资源消耗不一样，我们推荐在代码中指定每个任务的重试机制和重启策略。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Redis为什么高效","slug":"Redis为什么高效","date":"2020-08-01T08:08:08.000Z","updated":"2020-08-30T08:11:27.667Z","comments":true,"path":"2020/08/01/Redis为什么高效/","link":"","permalink":"cpeixin.cn/2020/08/01/Redis%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E6%95%88/","excerpt":"","text":"为什么高效？redis是非关系型内存数据库 数据存储于内存中，内存读取速度非常快，如果只是简单的 key-value，内存不是瓶颈。一般情况下，hash 查找可以达到每秒数百万次的数量级。采用单线程，避免了不必要的上下文切换和竞争条件内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间因为Redis的操作都非常快速——它的数据全部在内存里，完全不需要访问磁盘。至于并发，Redis 使用多路 I/O 复用技术，本身的并发效率不成问题。当然，单个 Redis 进程没办法使用多核（任一时刻只能跑在一个 CPU 核心上），但是它本来就不是非常计算密集型的服务。如果单核性能不够用，可以多开几个进程。Redis采用了单线程的模型，保证了每个操作的原子性，也减少了线程的上下文切换和竞争。另外，数据结构也帮了不少忙，Redis全程使用hash结构，读取速度快，还有一些特殊的数据结构，对数据存储进行了优化，如压缩表，对短数据进行压缩存储，再如，跳表，使用有序的数据结构加快读取的速度。还有一点，Redis采用自己实现的事件分离器，效率比较高，内部采用非阻塞的执行方式，吞吐能力比较大。Q&amp;AQ: 为什么redis需要把所有数据放到内存中？A: Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。Q:使用Redis有哪些好处？A: **(1) 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)(2) 支持丰富数据类型，支持string，list，set，sorted set，hash(3) 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行(4) 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除Q: 为什么redis使用单线程？A: 官方表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，多线程会更复杂，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了注：这里我们一直在强调的单线程，只是在处理我们的网络请求的时候只有一个线程来处理，一个正式的Redis Server运行的时候肯定是不止一个线程的，这里需要大家明确的注意一下！例如Redis进行持久化的时候会以子进程或者子线程的方式执行（具体是子线程还是子进程待读者深入研究）**","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"}]},{"title":"Flink 分布式缓存-demo","slug":"Flink-分布式缓存-demo","date":"2020-07-30T13:08:09.000Z","updated":"2020-10-08T13:30:38.459Z","comments":true,"path":"2020/07/30/Flink-分布式缓存-demo/","link":"","permalink":"cpeixin.cn/2020/07/30/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98-demo/","excerpt":"","text":"分布式缓存Flink提供了一个分布式缓存，类似于hadoop，可以使用户在并行函数中很方便的读取本地文件，并把它放在taskmanager节点中，防止task重复拉取。此缓存的工作机制如下：程序注册一个文件或者目录(本地或者远程文件系统，例如hdfs或者s3)，通过ExecutionEnvironment注册缓存文件并为它起一个名称。当程序执行，Flink自动将文件或者目录复制到所有taskmanager节点的本地文件系统，仅会执行一次。**用户可以通过这个指定的名称查找文件或者目录，然后从taskmanager节点的本地文件系统访问它。Broadcast 广播变量一句话解释，可以理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的任务在节点上都能够获取到，这个数据在每个节点上只会存在一份。如果不使用broadcast，则在每个节点中的每个任务中都需要拷贝一份dataset数据集，比较浪费内存(也就是一个节点中可能会存在多份dataset数据)。注意：1：广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大，避免发生OOM。因为广播出去的数据，会常驻内存，除非程序执行结束。2：广播变量在初始化广播出去以后不支持修改，这样才能保证每个节点的数据都是一致的。个人建议：如果数据集在几十兆或者百兆的时候，可以选择进行广播，如果数据集的大小上G的话，就不建议进行广播了。区别1.广播变量是基于内存的,是将变量分发到各个worker节点的内存上（避免多次复制，节省内存）2.分布式缓存是基于磁盘的,将文件copy到各个节点上,当函数运行时可以在本地文件系统检索该文件（避免多次复制，提高执行效率）Code接下来我们来看一下简单的使用方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import org.apache.commons.io.FileUtils;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.functions.RichMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.operators.DataSource;import org.apache.flink.configuration.Configuration;import java.io.File;import java.util.ArrayList;import java.util.List;/** * 分布式缓存 * 第一步：首先需要在 env 环境中注册一个文件，该文件可以来源于本地，也可以来源于 HDFS ，并且为该文件取一个名字。 * 第二步：在使用分布式缓存时，可根据注册的名字直接获取。 */public class DistributeCache &#123; public static void main(String[] args) throws Exception &#123; final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.registerCachedFile(\"/Users/cpeixin/cache/distributedcache.txt\", \"distributedCache\"); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试 DataSource&lt;String&gt; data = env.fromElements(\"Linea\", \"Lineb\", \"Linec\", \"Lined\"); // RichFuction除了提供原来MapFuction的方法之外，还提供open, close, getRuntimeContext 和setRuntimeContext方法， // 这些功能可用于参数化函数（传递参数），创建和完成本地状态，访问广播变量以及访问运行时信息以及有关迭代中的信息。 DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用该缓存文件 File myFile = getRuntimeContext().getDistributedCache().getFile(\"distributedCache\"); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; dataList.add(line); System.err.println(\"分布式缓存为:\" + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println(\"使用datalist：\" + dataList + \"-------\" +value); //业务逻辑 return dataList +\"：\" + value; &#125; &#125;); result.printToErr(); &#125;&#125;下面给出一个更加贴合生产场景下的需求实现，在流计算中使用DistributedCache来完成我们的计算，整个过程其实有些像与维度数据的join1234567 1. Prepare resources on hdfs[robin@node01 ~]$ hdfs dfs -mkdir /flink/cache[robin@node01 ~]$ vi gender.txt 1, male 2, female[robin@node01 ~]$ hdfs dfs -put gender.txt /flink/cache 2. Turn on the source (socket)代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package distributeCacheimport java.io.Fileimport org.apache.flink.api.common.functions.RichMapFunctionimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.api.scala._import org.apache.flink.configuration.Configurationimport scala.collection.mutableimport scala.io.&#123;BufferedSource, Source&#125;object DistrubutedCacheTest &#123; def main(args: Array[String]): Unit = &#123; //1. Environment val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //2. Read the resources on hdfs and set them in the distributed cache env.registerCachedFile(\"hdfs://node01:9000/flink/cache/gender.txt\",\"hdfsGenderInfo\") //3. Read the student information sent by the socket in real time, calculate it, and output the result //(101,\"jackson\",1,\"Shanghai\") env.socketTextStream(\"node01\",8888) .filter((_: String).trim.nonEmpty) .map(new RichMapFunction[String,(Int,String,Char,String)] &#123; //Used to store student information read from the distributed cache val map:mutable.Map[Int,Char]= mutable.HashMap() var bs: BufferedSource = _ override def open(parameters: Configuration): Unit = &#123; //1. Read the data stored in the distributed cache var file:File = getRuntimeContext.getDistributedCache.getFile(\"hdfsGenderInfo\") //2. Encapsulate the read information into a map instance for storage bs = Source.fromFile(file) val lst: List[String] = bs.getLines().toList for(perLine &lt;-lst)&#123; val arr: Array[String] = perLine.split(\",\") val genderFlg: Int = arr(0).trim.toInt val genderName: Char = arr(1).trim.toCharArray()(0) map.put(genderFlg,genderName) &#125; &#125; override def map(perStudentInfo: String): (Int, String, Char, String) = &#123; //Get student details val arr: Array[String] = perStudentInfo.split(\",\") val id: Int = arr(0).trim.toInt val name: String = arr(1).trim val genderFlg: Int = arr(2).trim.toInt val address: String = arr(3).trim //According to the data in the distributed cache stored in the container Map, replace the gender identifier in the student information with the real gender var genderName: Char = map.getOrElse(genderFlg, 'x') (id, name, genderName, address) &#125; override def close(): Unit = &#123; if(bs != null)&#123; bs.close() &#125; &#125; &#125;).print(\"The information completed by the student is -&gt;\") //4. Start env.execute(this.getClass.getSimpleName) &#125;&#125;结果","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"HBase RowKey设计","slug":"HBase-RowKey设计","date":"2020-07-21T01:42:33.000Z","updated":"2020-07-21T01:45:26.624Z","comments":true,"path":"2020/07/21/HBase-RowKey设计/","link":"","permalink":"cpeixin.cn/2020/07/21/HBase-RowKey%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"RowKey的作用RowKey在查询中的作用HBase中RowKey可以唯一标识一行记录，在HBase中检索数据有以下三种方式：通过 get 方式，指定 RowKey 获取唯一一条记录通过 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配全表扫描，即直接扫描整张表中所有行记录当大量请求访问HBase集群的一个或少数几个节点，造成少数RegionServer的读写请求过多、负载过大，而其他RegionServer负载却很小，这样就造成热点现象。大量访问会使热点Region所在的主机负载过大，引起性能下降，甚至导致Region不可用。所以我们在向HBase中插入数据的时候，应尽量均衡地把记录分散到不同的Region里去，平衡每个Region的压力。下面根据一个例子分别介绍下根据RowKey进行查询的时候支持的情况。如果我们RowKey设计为uid+phone+name，那么这种设计可以很好的支持一下的场景:1uid&#x3D;873969725 AND phone&#x3D;18900000000 AND name&#x3D;zhangsanuid&#x3D; 873969725 AND phone&#x3D;18900000000uid&#x3D; 873969725 AND phone&#x3D;189?uid&#x3D; 873969725复制代码难以支持的场景：1phone&#x3D;18900000000 AND name &#x3D; zhangsanphone&#x3D;18900000000 name&#x3D;zhangsan复制代码从上面的例子中可以看出，在进行查询的时候，根据RowKey从前向后匹配，所以我们在设计RowKey的时候选择好字段之后，还应该结合我们的实际的高频的查询场景来组合选择的字段，越高频的查询字段排列越靠左。RowKey在Region中的作用在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有StartRowKey和StopRowKey，这是表示 Region 存储的 RowKey 的范围，HBase 表的数据时按照 RowKey 来分散到不同的 Region，要想将数据记录均衡的分散到不同的Region中去，因此需要 RowKey 满足这种散列的特点。此外，在数据读写过程中也是与RowKey 密切相关，RowKey在读写过程中的作用：读写数据时通过 RowKey 找到对应的 Region；MemStore 中的数据是按照 RowKey 的字典序排序；HFile 中的数据是按照 RowKey 的字典序排序。RowKey的设计在HBase中RowKey在数据检索和数据存储方面都有重要的作用，一个好的RowKey设计会影响到数据在HBase中的分布，还会影响我们查询效率，所以一个好的RowKey的设计方案是多么重要。首先我们先来了解下RowKey的设计原则。RowKey设计原则长度原则RowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节，原因如下：数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。唯一原则必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。排序原则HBase的RowKey是按照ASCII有序排序的，因此我们在设计RowKey的时候要充分利用这点。散列原则**设计的RowKey应均匀的分布在各个HBase节点上。RowKey字段选择RowKey字段的选择，遵循的最基本原则是唯一性，RowKey必须能够唯一的识别一行数据。无论应用的负载特点是什么样，RowKey字段都应该参考最高频的查询场景。数据库通常都是以如何高效的读取和消费数据为目的，而不是数据存储本身。然后，结合具体的负载特点，再对选取的RowKey字段值进行改造，组合字段场景下需要重点考虑字段的顺序。避免数据热点的方法在对HBase的读写过程中，如何避免热点现象呢？主要有以下几种方法：Reversing如果经初步设计出的RowKey在数据分布上不均匀，但RowKey尾部的数据却呈现出了良好的随机性，此时，可以考虑将RowKey的信息翻转，或者直接将尾部的bytes提前到RowKey的开头。Reversing可以有效的使RowKey随机分布，但是牺牲了RowKey的有序性。缺点：利于Get操作，但不利于Scan操作，因为数据在原RowKey上的自然顺序已经被打乱。SaltingSalting（加盐）的原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之间的RowKey的开头不同。随机数能保障数据在所有Regions间的负载均衡。缺点：因为添加的是随机数，基于原RowKey查询时无法知道随机数是什么，那样在查询的时候就需要去各个可能的Regions中查找，Salting对于读取是利空的。并且加盐这种方式增加了读写时的吞吐量。Hashing**基于 RowKey 的完整或部分数据进行 Hash，而后将Hashing后的值完整替换或部分替换原RowKey的前缀部分。这里说的 hash 包含 MD5、sha1、sha256 或 sha512 等算法。缺点：与 Reversing 类似，Hashing 也不利于 Scan，因为打乱了原RowKey的自然顺序。RowKey设计案例剖析1. 查询某用户在某应用中的操作记录reverse(userid) + appid + timestamp2. 查询某用户在某应用中的操作记录（优先展现最近的数据）reverse(userid) + appid + (Long.Max_Value - timestamp)3. 查询某用户在某段时间内所有应用的操作记录reverse(userid) + timestamp + appid4. 查询某用户的基本信息reverse(userid)5. 查询某eventid记录信息salt + eventid + timestamp如果 userid是按数字递增的，并且长度不一，可以先预估 userid 最大长度，然后将userid进行翻转，再在翻转之后的字符串后面补0（至最大长度）；如果长度固定，直接进行翻转即可（如手机号码）。在第5个例子中，加盐的目的是为了增加查询的并发性，加入Slat的范围是0~n，可以将数据分为n个split同时做scan操作，有利于提高查询效率。总结在HBase的使用过程，设计RowKey是一个很重要的一个环节。我们在进行RowKey设计的时候可参照如下步骤：结合业务场景特点，选择合适的字段来做为RowKey，并且按照查询频次来放置字段顺序通过设计的RowKey能尽可能的将数据打散到整个集群中，均衡负载，避免热点问题设计的RowKey应尽量简短","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"}]},{"title":"数据仓库超详细案例","slug":"数据仓库超详细案例","date":"2020-07-07T09:17:57.000Z","updated":"2020-08-03T09:19:05.745Z","comments":true,"path":"2020/07/07/数据仓库超详细案例/","link":"","permalink":"cpeixin.cn/2020/07/07/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%B6%85%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B/","excerpt":"","text":"以下这篇博客转载自数据仓库案例，巨详细👍👍👍👍👍离线数据仓库数据仓库（Data WareHouse）是为企业所有决策制定过程，提供所有系统数据支持的战略集合通过对数据仓库中数据的分析，可以帮助企业，改进业务流程、控制、成本、提高产品质量等数据仓库，并不是数据最终目的地，而是为数据最终的目的地做好准备：清洗、转义、分类、重组、合并、拆分、统计等等1 项目简介1.1 项目需求用户行为数据采集平台搭建业务数据采集平台搭建数据仓库维度建模分析：用户、流量、会员、商品、销售、地区、活动等主题采用即席查询工具，随时进行指标分析对集群性能进行监控，发生异常需要报警元数据管理质量监控1.2 技术选型数据采集功能如何技术选型采集框架名称主要功能Sqoop大数据平台和关系型数据库的导入导出Datax大数据平台和关系型数据库的导入导出flume擅长日志数据的采集和解析logstash擅长日志数据的采集和解析maxwell常用作实时解析mysql的binlog数据canal常用作实时解析mysql的binlog数据waterDrop数据导入导出工具消息中间件的技术选型开源MQ概述RabbitMQLShift 用Erlang实现，支持多协议，broker架构，重量级ZeroMQAMQP最初设计者iMatix公司实现，轻量消息内核，无broker设计。C++实现KafkaLinkedIn用Scala语言实现，支持hadoop数据并行加载ActiveMQApach的一种JMS具体实现，支持代理和p2p部署。支持多协议。Java实现RedisKey-value NoSQL数据库，有MQ的功能MemcacheQ国人利用memcache缓冲队列协议开发的消息队列,C/C++实现数据永久存储技术框架选型框架名称主要用途HDFS分布式文件存储系统HbaseKey，value对的nosql数据库KuduCloudera公司开源提供的类似于Hbase的数据存储Hive基于MR的数据仓库工具数据离线计算框架技术选型(hive引擎)框架名称基本介绍MapReduce最早期的分布式文件计算系统Spark基于spark，一站式解决批流处理问题Flink基于flink，一站式解决批流处理问题分析数据库选型对比项目DruidKylinPrestoImpalaES亚秒级响应√√×××百亿数据集√√√√√SQL支持√√√√√(需插件)离线√√√√√实时√√×××精确去重×√√√×多表Join×√√√×JDBC for BI×√√√×其他选型任务调度：DolphinScheduler集群监控：CM+CDH元数据管理：AtlasBI工具：Zeppelin、Superset1.3 架构1.4 集群资源规划如何确认集群规模（假设每台服务器8T磁盘，128G内存）每天日活跃用户100万，每人一天平均100条：100万 * 100条 = 1亿条每条日志1K左右，每天1一条：1亿 / 1024 /1024 = 约100G半年内不扩容服务器来算：100G * 180天 = 约18T保存3个副本：18T * 3 = 54T预留20% ~ 30%BUF：54T / 0.7 = 77T总结：约10台服务器由于资源有限，采用3台进行制作服务名称子服务服务器 cdh01.cm服务器 cdh02.cm服务器 cdh03.cmHDFSNameNodeDataNodeSecondaryNameNode√√√√√YarnNodeManagerResourcemanager√√√√ZookeeperZookeeper Server√√√FlumeFlumeFlume（消费 Kafka）√√√KafkaKafka√√√HiveHive√MySQLMySQL√SqoopSqoop√PrestoCoordinatorWorker√√√DolphinSchedulerDolphinScheduler√DruidDruid√√√KylinKylin√HbaseHMasterHRegionServer√√√√SupersetSuperset√AtlasAtlas√SolrSolr√2 数据生成模块此模块主要针对于用户行为数据的采集，为什么要进行用户行为数据的采集呢？因为对于企业来说，用户就是钱，需要将用户的习惯等数据进行采集，以便在大数据衍生产品如用户画像标签系统进行分析，那么一般情况下用户的信息都是离线分析的，后期我们可以将分析结果存入ES等倒排索引生态中，在使用实时计算的方式匹配用户习惯，进行定制化推荐，更进一步的深度学习，对相似用户进行推荐。2.1 埋点数据基本格式公共字段：基本所有安卓手机都包含的字段业务字段：埋点上报的字段，有具体的业务类型1234567891011121314151617181920212223242526272829303132333435&#123;&quot;ap&quot;:&quot;xxxxx&quot;,&#x2F;&#x2F;项目数据来源 app pc&quot;cm&quot;: &#123; &#x2F;&#x2F;公共字段 &quot;mid&quot;: &quot;&quot;, &#x2F;&#x2F; (String) 设备唯一标识 &quot;uid&quot;: &quot;&quot;, &#x2F;&#x2F; (String) 用户标识 &quot;vc&quot;: &quot;1&quot;, &#x2F;&#x2F; (String) versionCode，程序版本号 &quot;vn&quot;: &quot;1.0&quot;, &#x2F;&#x2F; (String) versionName，程序版本名 &quot;l&quot;: &quot;zh&quot;, &#x2F;&#x2F; (String) language 系统语言 &quot;sr&quot;: &quot;&quot;, &#x2F;&#x2F; (String) 渠道号，应用从哪个渠道来的。 &quot;os&quot;: &quot;7.1.1&quot;, &#x2F;&#x2F; (String) Android 系统版本 &quot;ar&quot;: &quot;CN&quot;, &#x2F;&#x2F; (String) area 区域 &quot;md&quot;: &quot;BBB100-1&quot;, &#x2F;&#x2F; (String) model 手机型号 &quot;ba&quot;: &quot;blackberry&quot;, &#x2F;&#x2F; (String) brand 手机品牌 &quot;sv&quot;: &quot;V2.2.1&quot;, &#x2F;&#x2F; (String) sdkVersion &quot;g&quot;: &quot;&quot;, &#x2F;&#x2F; (String) gmail &quot;hw&quot;: &quot;1620x1080&quot;, &#x2F;&#x2F; (String) heightXwidth，屏幕宽高 &quot;t&quot;: &quot;1506047606608&quot;, &#x2F;&#x2F; (String) 客户端日志产生时的时间 &quot;nw&quot;: &quot;WIFI&quot;, &#x2F;&#x2F; (String) 网络模式 &quot;ln&quot;: 0, &#x2F;&#x2F; (double) lng 经度 &quot;la&quot;: 0 &#x2F;&#x2F; (double) lat 纬度&#125;,&quot;et&quot;: [ &#x2F;&#x2F;事件 &#123; &quot;ett&quot;: &quot;1506047605364&quot;, &#x2F;&#x2F;客户端事件产生时间 &quot;en&quot;: &quot;display&quot;, &#x2F;&#x2F;事件名称 &quot;kv&quot;: &#123; &#x2F;&#x2F;事件结果，以 key-value 形式自行定义 &quot;goodsid&quot;: &quot;236&quot;, &quot;action&quot;: &quot;1&quot;, &quot;extend1&quot;: &quot;1&quot;, &quot;place&quot;: &quot;2&quot;, &quot;category&quot;: &quot;75&quot; &#125; &#125;]&#125;示例日志（服务器时间戳 | 日志），时间戳可以有效判定网络服务的通信时长：12345678910111213141516171819202122232425262728293031323334353637383940411540934156385| &#123; &quot;ap&quot;: &quot;gmall&quot;, &#x2F;&#x2F;数仓库名 &quot;cm&quot;: &#123; &quot;uid&quot;: &quot;1234&quot;, &quot;vc&quot;: &quot;2&quot;, &quot;vn&quot;: &quot;1.0&quot;, &quot;la&quot;: &quot;EN&quot;, &quot;sr&quot;: &quot;&quot;, &quot;os&quot;: &quot;7.1.1&quot;, &quot;ar&quot;: &quot;CN&quot;, &quot;md&quot;: &quot;BBB100-1&quot;, &quot;ba&quot;: &quot;blackberry&quot;, &quot;sv&quot;: &quot;V2.2.1&quot;, &quot;g&quot;: &quot;abc@gmail.com&quot;, &quot;hw&quot;: &quot;1620x1080&quot;, &quot;t&quot;: &quot;1506047606608&quot;, &quot;nw&quot;: &quot;WIFI&quot;, &quot;ln&quot;: 0, &quot;la&quot;: 0&#125;,&quot;et&quot;: [ &#123; &quot;ett&quot;: &quot;1506047605364&quot;, &#x2F;&#x2F;客户端事件产生时间 &quot;en&quot;: &quot;display&quot;, &#x2F;&#x2F;事件名称 &quot;kv&quot;: &#123; &#x2F;&#x2F;事件结果，以 key-value 形式自行定义 &quot;goodsid&quot;: &quot;236&quot;, &quot;action&quot;: &quot;1&quot;, &quot;extend1&quot;: &quot;1&quot;, &quot;place&quot;: &quot;2&quot;, &quot;category&quot;: &quot;75&quot; &#125; &#125;,&#123; &quot;ett&quot;: &quot;1552352626835&quot;, &quot;en&quot;: &quot;active_background&quot;, &quot;kv&quot;: &#123; &quot;active_source&quot;: &quot;1&quot; &#125; &#125; ]&#125;&#125;2.2 埋点事件日志数据2.2.1 商品列表页事件名称：loading标签含义action动作：开始加载=1，加载成功=2，加载失败=3loading_time加载时长：计算下拉开始到接口返回数据的时间，（开始加载报 0，加载成 功或加载失败才上报时间）loading_way加载类型：1-读取缓存，2-从接口拉新数据 （加载成功才上报加载类型）extend1扩展字段 Extend1extend2扩展字段 Extend2type加载类型：自动加载=1，用户下拽加载=2，底部加载=3（底部条触发点击底部提示条/点击返回顶部加载）type1加载失败码：把加载失败状态码报回来（报空为加载成功，没有失败）2.2.2 商品点击事件标签：display标签含义action动作：曝光商品=1，点击商品=2goodsid商品 ID（服务端下发的 ID）place顺序（第几条商品，第一条为 0，第二条为 1，如此类推）extend1曝光类型：1 - 首次曝光 2-重复曝光category分类 ID（服务端定义的分类 ID）2.2.3 商品详情页事件标签：newsdetail标签含义entry页面入口来源：应用首页=1、push=2、详情页相关推荐=3action动作：开始加载=1，加载成功=2（pv），加载失败=3, 退出页面=4goodsid商品 ID（服务端下发的 ID）show_style商品样式：0、无图、1、一张大图、2、两张图、3、三张小图、4、一张小图、 5、一张大图两张小图news_staytime页面停留时长：从商品开始加载时开始计算，到用户关闭页面所用的时间。 若中途用跳转到其它页面了，则暂停计时，待回到详情页时恢复计时。或中 途划出的时间超过 10 分钟，则本次计时作废，不上报本次数据。如未加载成 功退出，则报空。loading_time加载时长：计算页面开始加载到接口返回数据的时间 （开始加载报 0，加载 成功或加载失败才上报时间）type1加载失败码：把加载失败状态码报回来（报空为加载成功，没有失败）category分类 ID（服务端定义的分类 ID）2.2.4 广告事件名称：ad标签含义entry入口：商品列表页=1 应用首页=2 商品详情页=3action动作： 广告展示=1 广告点击=2contentTypeType: 1 商品 2 营销活动displayMills展示时长 毫秒数itemId商品 idactivityId营销活动 id2.2.5 消息通知事件标签：notification标签含义action动作：通知产生=1，通知弹出=2，通知点击=3，常驻通知展示（不重复上 报，一天之内只报一次）=4type通知 id：预警通知=1，天气预报（早=2，晚=3），常驻=4ap_time客户端弹出时间content备用字段2.2.6 用户后台活跃事件标签: active_background标签含义active_source1=upgrade,2=download(下载),3=plugin_upgrade2.2.7 评论描述：评论表（comment）序号字段名称字段描述字段类型长度允许空缺省值1comment_id评论表int10,02userid用户 idint10,0√03p_comment_id父级评论 id(为 0 则是一级评论,不 为 0 则是回复)int10,0√4content评论内容string1000√5addtime创建时间string√6other_id评论的相关 idint10,0√7praise_count点赞数量int10,0√08reply_count回复数量int10,0√02.2.8 收藏描述：收藏（favorites）序号字段名称字段描述字段类型长度允许空缺省值1id主键int10,02course_id商品 idint10,0√03userid用户 IDint10,0√04add_time创建时间string√2.2.9 点赞描述：所有的点赞表（praise）序号字段名称字段描述字段类型长度允许空缺省值1id主键 idint10,02userid用户 idint10,0√3target_id点赞的对象 idint10,0√4type创建点赞类型：1问答点赞 2问答评论点赞3文章点赞数 4评论点赞int10,0√5add_time添加时间string√2.2.10 错误日志errorBrief错误摘要errorBrief错误详情2.3 埋点启动日志数据1234567891011121314151617181920212223242526&#123; &quot;action&quot;:&quot;1&quot;, &quot;ar&quot;:&quot;MX&quot;, &quot;ba&quot;:&quot;HTC&quot;, &quot;detail&quot;:&quot;&quot;, &quot;en&quot;:&quot;start&quot;, &quot;entry&quot;:&quot;2&quot;, &quot;extend1&quot;:&quot;&quot;, &quot;g&quot;:&quot;43R2SEQX@gmail.com&quot;, &quot;hw&quot;:&quot;640*960&quot;, &quot;l&quot;:&quot;en&quot;, &quot;la&quot;:&quot;20.4&quot;, &quot;ln&quot;:&quot;-99.3&quot;, &quot;loading_time&quot;:&quot;2&quot;, &quot;md&quot;:&quot;HTC-2&quot;, &quot;mid&quot;:&quot;995&quot;, &quot;nw&quot;:&quot;4G&quot;, &quot;open_ad_type&quot;:&quot;2&quot;, &quot;os&quot;:&quot;8.1.2&quot;, &quot;sr&quot;:&quot;B&quot;, &quot;sv&quot;:&quot;V2.0.6&quot;, &quot;t&quot;:&quot;1561472502444&quot;, &quot;uid&quot;:&quot;995&quot;, &quot;vc&quot;:&quot;10&quot;, &quot;vn&quot;:&quot;1.3.4&quot;&#125;事件标签: start标签含义entry入 口 ： push=1 ， widget=2 ， icon=3 ， notification=4, lockscreen_widget =5open_ad_type开屏广告类型: 开屏原生广告=1, 开屏插屏广告=2action状态：成功=1 失败=2loading_time加载时长：计算下拉开始到接口返回数据的时间，（开始加载报 0，加载成 功或加载失败才上报时间）detail失败码（没有则上报空）extend1失败的 message（没有则上报空）en日志类型 start2.4 数据生成脚本如下案例中将省略图中红框中的日志生成过程，直接使用Java程序构建logFile文件。2.4.1 数据生成格式启动日志{“action”:”1”,”ar”:”MX”,”ba”:”Sumsung”,”detail”:”201”,”en”:”start”,”entry”:”4”,”extend1”:””,”g”:”69021X1Q@gmail.com“,”hw”:”1080*1920”,”l”:”pt”,”la”:”-11.0”,”ln”:”-70.0”,”loading_time”:”9”,”md”:”sumsung-5”,”mid”:”244”,”nw”:”3G”,”open_ad_type”:”1”,”os”:”8.2.3”,”sr”:”D”,”sv”:”V2.1.3”,”t”:”1589612165914”,”uid”:”244”,”vc”:”16”,”vn”:”1.2.1”}事件日志(由于转换问题，图中没有 “时间戳|”)1589695383284|{“cm”:{“ln”:”-79.4”,”sv”:”V2.5.3”,”os”:”8.0.6”,”g”:”81614U54@gmail.com“,”mid”:”245”,”nw”:”WIFI”,”l”:”pt”,”vc”:”6”,”hw”:”1080*1920”,”ar”:”MX”,”uid”:”245”,”t”:”1589627025851”,”la”:”-39.6”,”md”:”HTC-7”,”vn”:”1.3.5”,”ba”:”HTC”,”sr”:”N”},”ap”:”app”,”et”:[{“ett”:”1589650631883”,”en”:”display”,”kv”:{“goodsid”:”53”,”action”:”2”,”extend1”:”2”,”place”:”3”,”category”:”50”}},{“ett”:”1589690866312”,”en”:”newsdetail”,”kv”:{“entry”:”3”,”goodsid”:”54”,”news_staytime”:”1”,”loading_time”:”6”,”action”:”4”,”showtype”:”0”,”category”:”78”,”type1”:””}},{“ett”:”1589641734037”,”en”:”loading”,”kv”:{“extend2”:””,”loading_time”:”0”,”action”:”1”,”extend1”:””,”type”:”2”,”type1”:”201”,”loading_way”:”2”}},{“ett”:”1589687684878”,”en”:”ad”,”kv”:{“activityId”:”1”,”displayMills”:”92030”,”entry”:”3”,”action”:”5”,”contentType”:”0”}},{“ett”:”1589632980772”,”en”:”active_background”,”kv”:{“active_source”:”1”}},{“ett”:”1589682030324”,”en”:”error”,”kv”:{“errorDetail”:”java.lang.NullPointerException\\n at cn.lift.appIn.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)\\n at cn.lift.dfdf.web.AbstractBaseController.validInbound”,”errorBrief”:”at cn.lift.dfdf.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)”}},{“ett”:”1589675065650”,”en”:”comment”,”kv”:{“p_comment_id”:2,”addtime”:”1589624299628”,”praise_count”:509,”other_id”:6,”comment_id”:7,”reply_count”:35,”userid”:3,”content”:”关色芦候佰间纶珊斑禁尹赞涤仇彭企呵姜毅”}},{“ett”:”1589631359459”,”en”:”favorites”,”kv”:{“course_id”:7,”id”:0,”add_time”:”1589681240066”,”userid”:7}},{“ett”:”1589616574187”,”en”:”praise”,”kv”:{“target_id”:1,”id”:7,”type”:3,”add_time”:”1589642497314”,”userid”:8}}]}2.4.2 创建maven工程data-producer：pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;!--版本号统一--&gt; &lt;properties&gt; &lt;slf4j.version&gt;1.7.20&lt;&#x2F;slf4j.version&gt; &lt;logback.version&gt;1.0.7&lt;&#x2F;logback.version&gt; &lt;&#x2F;properties&gt; &lt;dependencies&gt; &lt;!--阿里巴巴开源 json 解析框架--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;1.2.51&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!--日志生成框架--&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;&#x2F;groupId&gt; &lt;artifactId&gt;logback-core&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;&#x2F;groupId&gt; &lt;artifactId&gt;logback-classic&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;&#x2F;groupId&gt; &lt;artifactId&gt;lombok&lt;&#x2F;artifactId&gt; &lt;version&gt;1.18.10&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;2.3.2&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;&#x2F;source&gt; &lt;target&gt;1.8&lt;&#x2F;target&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt; &lt;&#x2F;descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!--主类名--&gt; &lt;mainClass&gt;com.heaton.bigdata.datawarehouse.app.App&lt;&#x2F;mainClass&gt; &lt;&#x2F;manifest&gt; &lt;&#x2F;archive&gt; &lt;&#x2F;configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;&#x2F;id&gt; &lt;phase&gt;package&lt;&#x2F;phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;data-producer：logback.xml12345678910111213141516171819202122232425262728293031323334&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;configuration debug&#x3D;&quot;false&quot;&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径 --&gt; &lt;property name&#x3D;&quot;LOG_HOME&quot; value&#x3D;&quot;&#x2F;root&#x2F;logs&#x2F;&quot;&#x2F;&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name&#x3D;&quot;STDOUT&quot; class&#x3D;&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder class&#x3D;&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!--格式化输出：%d 表示日期，%thread 表示线程名，%-5level：级别从左显示 5 个字符宽度%msg： 日志消息，%n 是换行符 --&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;encoder&gt; &lt;&#x2F;appender&gt; &lt;!-- 按照每天生成日志文件。存储事件日志 --&gt; &lt;appender name&#x3D;&quot;FILE&quot; class&#x3D;&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!-- &lt;File&gt;$&#123;LOG_HOME&#125;&#x2F;app.log&lt;&#x2F;File&gt;设置日志不超过$&#123;log.max.size&#125;时的保存路径，注意， 如果是 web 项目会保存到 Tomcat 的 bin 目录 下 --&gt; &lt;rollingPolicy class&#x3D;&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件输出的文件名 --&gt; &lt;FileNamePattern&gt;$&#123;LOG_HOME&#125;&#x2F;app-%d&#123;yyyy-MM-dd&#125;.log&lt;&#x2F;FileNamePattern&gt; &lt;!--日志文件保留天数 --&gt; &lt;MaxHistory&gt;30&lt;&#x2F;MaxHistory&gt; &lt;&#x2F;rollingPolicy&gt; &lt;encoder class&#x3D;&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;%msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;encoder&gt; &lt;!--日志文件最大的大小 --&gt; &lt;triggeringPolicy class&#x3D;&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;10MB&lt;&#x2F;MaxFileSize&gt; &lt;&#x2F;triggeringPolicy&gt; &lt;&#x2F;appender&gt; &lt;!--异步打印日志--&gt; &lt;appender name&#x3D;&quot;ASYNC_FILE&quot; class&#x3D;&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;!-- 不丢失日志.默认的,如果队列的 80%已满,则会丢弃 TRACT、DEBUG、INFO 级别的日志 --&gt; &lt;discardingThreshold&gt;0&lt;&#x2F;discardingThreshold&gt; &lt;!-- 更改默认的队列的深度,该值会影响性能.默认值为 256 --&gt; &lt;queueSize&gt;512&lt;&#x2F;queueSize&gt; &lt;!-- 添加附加的 appender,最多只能添加一个 --&gt; &lt;appender-ref ref&#x3D;&quot;FILE&quot;&#x2F;&gt; &lt;&#x2F;appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;STDOUT&quot;&#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;ASYNC_FILE&quot;&#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;error&quot;&#x2F;&gt; &lt;&#x2F;root&gt;&lt;&#x2F;configuration&gt;data-flume：pom.xml12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;2.3.2&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;&#x2F;source&gt; &lt;target&gt;1.8&lt;&#x2F;target&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;hive-function：pom.xml12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt; &lt;version&gt;2.1.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;2.3.2&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;&#x2F;source&gt; &lt;target&gt;1.8&lt;&#x2F;target&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt; &lt;&#x2F;build&gt;2.4.3 各事件beandata-producer工程2.4.3.1 公共日志类123456789101112131415161718192021222324252627import lombok.Data;&#x2F;** * @author Heaton* @email 70416450@qq.com* @date 2020&#x2F;4&#x2F;25 14:54 * @describe 公共日志类*&#x2F;@Datapublic class AppBase &#123; private String mid; &#x2F;&#x2F; (String) 设备唯一 private String uid; &#x2F;&#x2F; (String) 用户 uid private String vc; &#x2F;&#x2F; (String) versionCode，程序版本号 private String vn; &#x2F;&#x2F; (String) versionName，程序版本名 private String l; &#x2F;&#x2F; (String) 系统语言 private String sr; &#x2F;&#x2F; (String) 渠道号，应用从哪个渠道来的。 private String os; &#x2F;&#x2F; (String) Android 系统版本 private String ar; &#x2F;&#x2F; (String) 区域 private String md; &#x2F;&#x2F; (String) 手机型号 private String ba; &#x2F;&#x2F; (String) 手机品牌 private String sv; &#x2F;&#x2F; (String) sdkVersion private String g; &#x2F;&#x2F; (String) gmail private String hw; &#x2F;&#x2F; (String) heightXwidth，屏幕宽高 private String t; &#x2F;&#x2F; (String) 客户端日志产生时的时间 private String nw; &#x2F;&#x2F; (String) 网络模式 private String ln; &#x2F;&#x2F; (double) lng 经度 private String la; &#x2F;&#x2F; (double) lat 纬度&#125;2.4.3.2 启动日志类1234567891011121314151617import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 启动日志类 *&#x2F;@Datapublic class AppStart extends AppBase &#123; private String entry;&#x2F;&#x2F;入口： push&#x3D;1，widget&#x3D;2，icon&#x3D;3，notification&#x3D;4, lockscreen_widget private String open_ad_type;&#x2F;&#x2F;开屏广告类型: 开屏原生广告&#x3D;1, 开屏插屏广告&#x3D;2 private String action;&#x2F;&#x2F;状态：成功&#x3D;1 失败&#x3D;2 private String loading_time;&#x2F;&#x2F;加载时长：计算下拉开始到接口返回数据的时间，（开始加载报 0，加载成功或加载失败才上报时间） private String detail;&#x2F;&#x2F;失败码（没有则上报空） private String extend1;&#x2F;&#x2F;失败的 message（没有则上报空） private String en;&#x2F;&#x2F;启动日志类型标记&#125;2.4.3.3 错误日志类123456789101112import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 错误日志类 *&#x2F;@Datapublic class AppErrorLog &#123; private String errorBrief; &#x2F;&#x2F;错误摘要 private String errorDetail; &#x2F;&#x2F;错误详情&#125;2.4.3.4 商品点击日志类123456789101112131415import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 商品点击日志类 *&#x2F;@Datapublic class AppDisplay &#123; private String action;&#x2F;&#x2F;动作：曝光商品&#x3D;1，点击商品&#x3D;2 private String goodsid;&#x2F;&#x2F;商品 ID（服务端下发的 ID） private String place;&#x2F;&#x2F;顺序（第几条商品，第一条为 0，第二条为 1，如此类推） private String extend1;&#x2F;&#x2F;曝光类型：1 - 首次曝光 2-重复曝光（没有使用） private String category;&#x2F;&#x2F;分类 ID（服务端定义的分类 ID）&#125;2.4.3.5 商品详情类123456789101112131415161718import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 商品详情类 *&#x2F;@Datapublic class AppNewsDetail &#123; private String entry;&#x2F;&#x2F;页面入口来源：应用首页&#x3D;1、push&#x3D;2、详情页相关推荐 private String action;&#x2F;&#x2F;动作：开始加载&#x3D;1，加载成功&#x3D;2（pv），加载失败&#x3D;3, 退出页面&#x3D;4 private String goodsid;&#x2F;&#x2F;商品 ID（服务端下发的 ID） private String showtype;&#x2F;&#x2F;商品样式：0、无图 1、一张大图 2、两张图 3、三张小图 4、一张小 图 5、一张大图两张小图 来源于详情页相关推荐的商品，上报样式都为 0（因为都是左文右图） private String news_staytime;&#x2F;&#x2F;页面停留时长：从商品开始加载时开始计算，到用户关闭页面 所用的时间。若中途用跳转到其它页面了，则暂停计时，待回到详情页时恢复计时。或中途划出的时间超 过 10 分钟，则本次计时作废，不上报本次数据。如未加载成功退出，则报空。 private String loading_time;&#x2F;&#x2F;加载时长：计算页面开始加载到接口返回数据的时间 （开始加 载报 0，加载成功或加载失败才上报时间） private String type1;&#x2F;&#x2F;加载失败码：把加载失败状态码报回来（报空为加载成功，没有失败） private String category;&#x2F;&#x2F;分类 ID（服务端定义的分类 ID）&#125;2.4.3.6 商品列表类1234567891011121314151617import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 商品列表类 *&#x2F;@Datapublic class AppLoading &#123; private String action;&#x2F;&#x2F;动作：开始加载&#x3D;1，加载成功&#x3D;2，加载失败 private String loading_time;&#x2F;&#x2F;加载时长：计算下拉开始到接口返回数据的时间，（开始加载报 0， 加载成功或加载失败才上报时间） private String loading_way;&#x2F;&#x2F;加载类型：1-读取缓存，2-从接口拉新数据 （加载成功才上报加 载类型） private String extend1;&#x2F;&#x2F;扩展字段 Extend1 private String extend2;&#x2F;&#x2F;扩展字段 Extend2 private String type;&#x2F;&#x2F;加载类型：自动加载&#x3D;1，用户下拽加载&#x3D;2，底部加载&#x3D;3（底部条触发点击底 部提示条&#x2F;点击返回顶部加载） private String type1;&#x2F;&#x2F;加载失败码：把加载失败状态码报回来（报空为加载成功，没有失败）&#125;2.4.3.7 广告类12345678910111213141516import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 广告类 *&#x2F;@Datapublic class AppAd &#123; private String entry;&#x2F;&#x2F;入口：商品列表页&#x3D;1 应用首页&#x3D;2 商品详情页&#x3D;3 private String action;&#x2F;&#x2F;动作： 广告展示&#x3D;1 广告点击&#x3D;2 private String contentType;&#x2F;&#x2F;Type: 1 商品 2 营销活动 private String displayMills;&#x2F;&#x2F;展示时长 毫秒数 private String itemId; &#x2F;&#x2F;商品id private String activityId; &#x2F;&#x2F;营销活动id&#125;2.4.3.8 消息通知日志类1234567891011121314import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 消息通知日志类 *&#x2F;@Datapublic class AppNotification &#123; private String action;&#x2F;&#x2F;动作：通知产生&#x3D;1，通知弹出&#x3D;2，通知点击&#x3D;3，常驻通知展示（不重复上 报，一天之内只报一次） private String type;&#x2F;&#x2F;通知 id：预警通知&#x3D;1，天气预报（早&#x3D;2，晚&#x3D;3），常驻&#x3D;4 private String ap_time;&#x2F;&#x2F;客户端弹出时间 private String content;&#x2F;&#x2F;备用字段&#125;2.4.3.9 用户后台活跃类1234567891011import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 用户后台活跃类 *&#x2F;@Datapublic class AppActive &#123; private String active_source;&#x2F;&#x2F;1&#x3D;upgrade,2&#x3D;download(下载),3&#x3D;plugin_upgrade&#125;2.4.3.10 用户评论类123456789101112131415161718import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 用户评论类 *&#x2F;@Datapublic class AppComment &#123; private int comment_id;&#x2F;&#x2F;评论表 private int userid;&#x2F;&#x2F;用户 id private int p_comment_id;&#x2F;&#x2F;父级评论 id(为 0 则是一级评论,不为 0 则是回复) private String content;&#x2F;&#x2F;评论内容 private String addtime;&#x2F;&#x2F;创建时间 private int other_id;&#x2F;&#x2F;评论的相关 id private int praise_count;&#x2F;&#x2F;点赞数量 private int reply_count;&#x2F;&#x2F;回复数量&#125;2.4.3.11 用户收藏类1234567891011121314import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 用户收藏类 *&#x2F;@Datapublic class AppFavorites &#123; private int id;&#x2F;&#x2F;主键 private int course_id;&#x2F;&#x2F;商品 id private int userid;&#x2F;&#x2F;用户 ID private String add_time;&#x2F;&#x2F;创建时间&#125;2.4.3.12 用户点赞类123456789101112131415import lombok.Data;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 用户点赞类 *&#x2F;@Datapublic class AppPraise &#123; private int id; &#x2F;&#x2F;主键 id private int userid;&#x2F;&#x2F;用户 id private int target_id;&#x2F;&#x2F;点赞的对象 id private int type;&#x2F;&#x2F;点赞类型 1 问答点赞 2 问答评论点赞 3 文章点赞数 4 评论点赞 private String add_time;&#x2F;&#x2F;添加时间&#125;2.4.4 启动类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.UnsupportedEncodingException;import java.util.Random;&#x2F;** * @author Heaton * @email 70416450@qq.com * @date 2020&#x2F;4&#x2F;25 14:54 * @describe 启动类 *&#x2F;public class App &#123; private final static Logger logger &#x3D; LoggerFactory.getLogger(App.class); private static Random rand &#x3D; new Random(); &#x2F;&#x2F; 设备id private static int s_mid &#x3D; 0; &#x2F;&#x2F; 用户id private static int s_uid &#x3D; 0; &#x2F;&#x2F; 商品id private static int s_goodsid &#x3D; 0; public static void main(String[] args) &#123; &#x2F;&#x2F; 参数一：控制发送每条的延时时间，默认是0 Long delay &#x3D; args.length &gt; 0 ? Long.parseLong(args[0]) : 0L; &#x2F;&#x2F; 参数二：循环遍历次数 int loop_len &#x3D; args.length &gt; 1 ? Integer.parseInt(args[1]) : 1000; &#x2F;&#x2F; 生成数据 generateLog(delay, loop_len); &#125; private static void generateLog(Long delay, int loop_len) &#123; for (int i &#x3D; 0; i &lt; loop_len; i++) &#123; int flag &#x3D; rand.nextInt(2); switch (flag) &#123; case (0): &#x2F;&#x2F;应用启动 AppStart appStart &#x3D; generateStart(); String jsonString &#x3D; JSON.toJSONString(appStart); &#x2F;&#x2F;控制台打印 logger.info(jsonString); break; case (1): JSONObject json &#x3D; new JSONObject(); json.put(&quot;ap&quot;, &quot;app&quot;); json.put(&quot;cm&quot;, generateComFields()); JSONArray eventsArray &#x3D; new JSONArray(); &#x2F;&#x2F; 事件日志 &#x2F;&#x2F; 商品点击，展示 if (rand.nextBoolean()) &#123; eventsArray.add(generateDisplay()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 商品详情页 if (rand.nextBoolean()) &#123; eventsArray.add(generateNewsDetail()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 商品列表页 if (rand.nextBoolean()) &#123; eventsArray.add(generateNewList()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 广告 if (rand.nextBoolean()) &#123; eventsArray.add(generateAd()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 消息通知 if (rand.nextBoolean()) &#123; eventsArray.add(generateNotification()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 用户后台活跃 if (rand.nextBoolean()) &#123; eventsArray.add(generateBackground()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F;故障日志 if (rand.nextBoolean()) &#123; eventsArray.add(generateError()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 用户评论 if (rand.nextBoolean()) &#123; eventsArray.add(generateComment()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 用户收藏 if (rand.nextBoolean()) &#123; eventsArray.add(generateFavorites()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F; 用户点赞 if (rand.nextBoolean()) &#123; eventsArray.add(generatePraise()); json.put(&quot;et&quot;, eventsArray); &#125; &#x2F;&#x2F;时间 long millis &#x3D; System.currentTimeMillis(); &#x2F;&#x2F;控制台打印 logger.info(millis + &quot;|&quot; + json.toJSONString()); break; &#125; &#x2F;&#x2F; 延迟 try &#123; Thread.sleep(delay); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#x2F;** * 公共字段设置 *&#x2F; private static JSONObject generateComFields() &#123; AppBase appBase &#x3D; new AppBase(); &#x2F;&#x2F;设备id appBase.setMid(s_mid + &quot;&quot;); s_mid++; &#x2F;&#x2F; 用户id appBase.setUid(s_uid + &quot;&quot;); s_uid++; &#x2F;&#x2F; 程序版本号 5,6等 appBase.setVc(&quot;&quot; + rand.nextInt(20)); &#x2F;&#x2F;程序版本名 v1.1.1 appBase.setVn(&quot;1.&quot; + rand.nextInt(4) + &quot;.&quot; + rand.nextInt(10)); &#x2F;&#x2F; 安卓系统版本 appBase.setOs(&quot;8.&quot; + rand.nextInt(3) + &quot;.&quot; + rand.nextInt(10)); &#x2F;&#x2F; 语言 es,en,pt int flag &#x3D; rand.nextInt(3); switch (flag) &#123; case (0): appBase.setL(&quot;es&quot;); break; case (1): appBase.setL(&quot;en&quot;); break; case (2): appBase.setL(&quot;pt&quot;); break; &#125; &#x2F;&#x2F; 渠道号 从哪个渠道来的 appBase.setSr(getRandomChar(1)); &#x2F;&#x2F; 区域 flag &#x3D; rand.nextInt(2); switch (flag) &#123; case 0: appBase.setAr(&quot;BR&quot;); case 1: appBase.setAr(&quot;MX&quot;); &#125; &#x2F;&#x2F; 手机品牌 ba ,手机型号 md，就取2位数字了 flag &#x3D; rand.nextInt(3); switch (flag) &#123; case 0: appBase.setBa(&quot;Sumsung&quot;); appBase.setMd(&quot;sumsung-&quot; + rand.nextInt(20)); break; case 1: appBase.setBa(&quot;Huawei&quot;); appBase.setMd(&quot;Huawei-&quot; + rand.nextInt(20)); break; case 2: appBase.setBa(&quot;HTC&quot;); appBase.setMd(&quot;HTC-&quot; + rand.nextInt(20)); break; &#125; &#x2F;&#x2F; 嵌入sdk的版本 appBase.setSv(&quot;V2.&quot; + rand.nextInt(10) + &quot;.&quot; + rand.nextInt(10)); &#x2F;&#x2F; gmail appBase.setG(getRandomCharAndNumr(8) + &quot;@gmail.com&quot;); &#x2F;&#x2F; 屏幕宽高 hw flag &#x3D; rand.nextInt(4); switch (flag) &#123; case 0: appBase.setHw(&quot;640*960&quot;); break; case 1: appBase.setHw(&quot;640*1136&quot;); break; case 2: appBase.setHw(&quot;750*1134&quot;); break; case 3: appBase.setHw(&quot;1080*1920&quot;); break; &#125; &#x2F;&#x2F; 客户端产生日志时间 long millis &#x3D; System.currentTimeMillis(); appBase.setT(&quot;&quot; + (millis - rand.nextInt(99999999))); &#x2F;&#x2F; 手机网络模式 3G,4G,WIFI flag &#x3D; rand.nextInt(3); switch (flag) &#123; case 0: appBase.setNw(&quot;3G&quot;); break; case 1: appBase.setNw(&quot;4G&quot;); break; case 2: appBase.setNw(&quot;WIFI&quot;); break; &#125; &#x2F;&#x2F; 拉丁美洲 西经34°46′至西经117°09；北纬32°42′至南纬53°54′ &#x2F;&#x2F; 经度 appBase.setLn((-34 - rand.nextInt(83) - rand.nextInt(60) &#x2F; 10.0) + &quot;&quot;); &#x2F;&#x2F; 纬度 appBase.setLa((32 - rand.nextInt(85) - rand.nextInt(60) &#x2F; 10.0) + &quot;&quot;); return (JSONObject) JSON.toJSON(appBase); &#125; &#x2F;** * 商品展示事件 *&#x2F; private static JSONObject generateDisplay() &#123; AppDisplay appDisplay &#x3D; new AppDisplay(); boolean boolFlag &#x3D; rand.nextInt(10) &lt; 7; &#x2F;&#x2F; 动作：曝光商品&#x3D;1，点击商品&#x3D;2， if (boolFlag) &#123; appDisplay.setAction(&quot;1&quot;); &#125; else &#123; appDisplay.setAction(&quot;2&quot;); &#125; &#x2F;&#x2F; 商品id String goodsId &#x3D; s_goodsid + &quot;&quot;; s_goodsid++; appDisplay.setGoodsid(goodsId); &#x2F;&#x2F; 顺序 设置成6条吧 int flag &#x3D; rand.nextInt(6); appDisplay.setPlace(&quot;&quot; + flag); &#x2F;&#x2F; 曝光类型 flag &#x3D; 1 + rand.nextInt(2); appDisplay.setExtend1(&quot;&quot; + flag); &#x2F;&#x2F; 分类 flag &#x3D; 1 + rand.nextInt(100); appDisplay.setCategory(&quot;&quot; + flag); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(appDisplay); return packEventJson(&quot;display&quot;, jsonObject); &#125; &#x2F;** * 商品详情页 *&#x2F; private static JSONObject generateNewsDetail() &#123; AppNewsDetail appNewsDetail &#x3D; new AppNewsDetail(); &#x2F;&#x2F; 页面入口来源 int flag &#x3D; 1 + rand.nextInt(3); appNewsDetail.setEntry(flag + &quot;&quot;); &#x2F;&#x2F; 动作 appNewsDetail.setAction(&quot;&quot; + (rand.nextInt(4) + 1)); &#x2F;&#x2F; 商品id appNewsDetail.setGoodsid(s_goodsid + &quot;&quot;); &#x2F;&#x2F; 商品来源类型 flag &#x3D; 1 + rand.nextInt(3); appNewsDetail.setShowtype(flag + &quot;&quot;); &#x2F;&#x2F; 商品样式 flag &#x3D; rand.nextInt(6); appNewsDetail.setShowtype(&quot;&quot; + flag); &#x2F;&#x2F; 页面停留时长 flag &#x3D; rand.nextInt(10) * rand.nextInt(7); appNewsDetail.setNews_staytime(flag + &quot;&quot;); &#x2F;&#x2F; 加载时长 flag &#x3D; rand.nextInt(10) * rand.nextInt(7); appNewsDetail.setLoading_time(flag + &quot;&quot;); &#x2F;&#x2F; 加载失败码 flag &#x3D; rand.nextInt(10); switch (flag) &#123; case 1: appNewsDetail.setType1(&quot;102&quot;); break; case 2: appNewsDetail.setType1(&quot;201&quot;); break; case 3: appNewsDetail.setType1(&quot;325&quot;); break; case 4: appNewsDetail.setType1(&quot;433&quot;); break; case 5: appNewsDetail.setType1(&quot;542&quot;); break; default: appNewsDetail.setType1(&quot;&quot;); break; &#125; &#x2F;&#x2F; 分类 flag &#x3D; 1 + rand.nextInt(100); appNewsDetail.setCategory(&quot;&quot; + flag); JSONObject eventJson &#x3D; (JSONObject) JSON.toJSON(appNewsDetail); return packEventJson(&quot;newsdetail&quot;, eventJson); &#125; &#x2F;** * 商品列表 *&#x2F; private static JSONObject generateNewList() &#123; AppLoading appLoading &#x3D; new AppLoading(); &#x2F;&#x2F; 动作 int flag &#x3D; rand.nextInt(3) + 1; appLoading.setAction(flag + &quot;&quot;); &#x2F;&#x2F; 加载时长 flag &#x3D; rand.nextInt(10) * rand.nextInt(7); appLoading.setLoading_time(flag + &quot;&quot;); &#x2F;&#x2F; 失败码 flag &#x3D; rand.nextInt(10); switch (flag) &#123; case 1: appLoading.setType1(&quot;102&quot;); break; case 2: appLoading.setType1(&quot;201&quot;); break; case 3: appLoading.setType1(&quot;325&quot;); break; case 4: appLoading.setType1(&quot;433&quot;); break; case 5: appLoading.setType1(&quot;542&quot;); break; default: appLoading.setType1(&quot;&quot;); break; &#125; &#x2F;&#x2F; 页面 加载类型 flag &#x3D; 1 + rand.nextInt(2); appLoading.setLoading_way(&quot;&quot; + flag); &#x2F;&#x2F; 扩展字段1 appLoading.setExtend1(&quot;&quot;); &#x2F;&#x2F; 扩展字段2 appLoading.setExtend2(&quot;&quot;); &#x2F;&#x2F; 用户加载类型 flag &#x3D; 1 + rand.nextInt(3); appLoading.setType(&quot;&quot; + flag); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(appLoading); return packEventJson(&quot;loading&quot;, jsonObject); &#125; &#x2F;** * 广告相关字段 *&#x2F; private static JSONObject generateAd() &#123; AppAd appAd &#x3D; new AppAd(); &#x2F;&#x2F; 入口 int flag &#x3D; rand.nextInt(3) + 1; appAd.setEntry(flag + &quot;&quot;); &#x2F;&#x2F; 动作 flag &#x3D; rand.nextInt(5) + 1; appAd.setAction(flag + &quot;&quot;); &#x2F;&#x2F; 内容类型类型 flag &#x3D; rand.nextInt(6) + 1; appAd.setContentType(flag + &quot;&quot;); &#x2F;&#x2F; 展示样式 flag &#x3D; rand.nextInt(120000) + 1000; appAd.setDisplayMills(flag + &quot;&quot;); flag &#x3D; rand.nextInt(1); if (flag &#x3D;&#x3D; 1) &#123; appAd.setContentType(flag + &quot;&quot;); flag &#x3D; rand.nextInt(6); appAd.setItemId(flag + &quot;&quot;); &#125; else &#123; appAd.setContentType(flag + &quot;&quot;); flag &#x3D; rand.nextInt(1) + 1; appAd.setActivityId(flag + &quot;&quot;); &#125; JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(appAd); return packEventJson(&quot;ad&quot;, jsonObject); &#125; &#x2F;** * 启动日志 *&#x2F; private static AppStart generateStart() &#123; AppStart appStart &#x3D; new AppStart(); &#x2F;&#x2F;设备id appStart.setMid(s_mid + &quot;&quot;); s_mid++; &#x2F;&#x2F; 用户id appStart.setUid(s_uid + &quot;&quot;); s_uid++; &#x2F;&#x2F; 程序版本号 5,6等 appStart.setVc(&quot;&quot; + rand.nextInt(20)); &#x2F;&#x2F;程序版本名 v1.1.1 appStart.setVn(&quot;1.&quot; + rand.nextInt(4) + &quot;.&quot; + rand.nextInt(10)); &#x2F;&#x2F; 安卓系统版本 appStart.setOs(&quot;8.&quot; + rand.nextInt(3) + &quot;.&quot; + rand.nextInt(10)); &#x2F;&#x2F;设置日志类型 appStart.setEn(&quot;start&quot;); &#x2F;&#x2F; 语言 es,en,pt int flag &#x3D; rand.nextInt(3); switch (flag) &#123; case (0): appStart.setL(&quot;es&quot;); break; case (1): appStart.setL(&quot;en&quot;); break; case (2): appStart.setL(&quot;pt&quot;); break; &#125; &#x2F;&#x2F; 渠道号 从哪个渠道来的 appStart.setSr(getRandomChar(1)); &#x2F;&#x2F; 区域 flag &#x3D; rand.nextInt(2); switch (flag) &#123; case 0: appStart.setAr(&quot;BR&quot;); case 1: appStart.setAr(&quot;MX&quot;); &#125; &#x2F;&#x2F; 手机品牌 ba ,手机型号 md，就取2位数字了 flag &#x3D; rand.nextInt(3); switch (flag) &#123; case 0: appStart.setBa(&quot;Sumsung&quot;); appStart.setMd(&quot;sumsung-&quot; + rand.nextInt(20)); break; case 1: appStart.setBa(&quot;Huawei&quot;); appStart.setMd(&quot;Huawei-&quot; + rand.nextInt(20)); break; case 2: appStart.setBa(&quot;HTC&quot;); appStart.setMd(&quot;HTC-&quot; + rand.nextInt(20)); break; &#125; &#x2F;&#x2F; 嵌入sdk的版本 appStart.setSv(&quot;V2.&quot; + rand.nextInt(10) + &quot;.&quot; + rand.nextInt(10)); &#x2F;&#x2F; gmail appStart.setG(getRandomCharAndNumr(8) + &quot;@gmail.com&quot;); &#x2F;&#x2F; 屏幕宽高 hw flag &#x3D; rand.nextInt(4); switch (flag) &#123; case 0: appStart.setHw(&quot;640*960&quot;); break; case 1: appStart.setHw(&quot;640*1136&quot;); break; case 2: appStart.setHw(&quot;750*1134&quot;); break; case 3: appStart.setHw(&quot;1080*1920&quot;); break; &#125; &#x2F;&#x2F; 客户端产生日志时间 long millis &#x3D; System.currentTimeMillis(); appStart.setT(&quot;&quot; + (millis - rand.nextInt(99999999))); &#x2F;&#x2F; 手机网络模式 3G,4G,WIFI flag &#x3D; rand.nextInt(3); switch (flag) &#123; case 0: appStart.setNw(&quot;3G&quot;); break; case 1: appStart.setNw(&quot;4G&quot;); break; case 2: appStart.setNw(&quot;WIFI&quot;); break; &#125; &#x2F;&#x2F; 拉丁美洲 西经34°46′至西经117°09；北纬32°42′至南纬53°54′ &#x2F;&#x2F; 经度 appStart.setLn((-34 - rand.nextInt(83) - rand.nextInt(60) &#x2F; 10.0) + &quot;&quot;); &#x2F;&#x2F; 纬度 appStart.setLa((32 - rand.nextInt(85) - rand.nextInt(60) &#x2F; 10.0) + &quot;&quot;); &#x2F;&#x2F; 入口 flag &#x3D; rand.nextInt(5) + 1; appStart.setEntry(flag + &quot;&quot;); &#x2F;&#x2F; 开屏广告类型 flag &#x3D; rand.nextInt(2) + 1; appStart.setOpen_ad_type(flag + &quot;&quot;); &#x2F;&#x2F; 状态 flag &#x3D; rand.nextInt(10) &gt; 8 ? 2 : 1; appStart.setAction(flag + &quot;&quot;); &#x2F;&#x2F; 加载时长 appStart.setLoading_time(rand.nextInt(20) + &quot;&quot;); &#x2F;&#x2F; 失败码 flag &#x3D; rand.nextInt(10); switch (flag) &#123; case 1: appStart.setDetail(&quot;102&quot;); break; case 2: appStart.setDetail(&quot;201&quot;); break; case 3: appStart.setDetail(&quot;325&quot;); break; case 4: appStart.setDetail(&quot;433&quot;); break; case 5: appStart.setDetail(&quot;542&quot;); break; default: appStart.setDetail(&quot;&quot;); break; &#125; &#x2F;&#x2F; 扩展字段 appStart.setExtend1(&quot;&quot;); return appStart; &#125; &#x2F;** * 消息通知 *&#x2F; private static JSONObject generateNotification() &#123; AppNotification appNotification &#x3D; new AppNotification(); int flag &#x3D; rand.nextInt(4) + 1; &#x2F;&#x2F; 动作 appNotification.setAction(flag + &quot;&quot;); &#x2F;&#x2F; 通知id flag &#x3D; rand.nextInt(4) + 1; appNotification.setType(flag + &quot;&quot;); &#x2F;&#x2F; 客户端弹时间 appNotification.setAp_time((System.currentTimeMillis() - rand.nextInt(99999999)) + &quot;&quot;); &#x2F;&#x2F; 备用字段 appNotification.setContent(&quot;&quot;); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(appNotification); return packEventJson(&quot;notification&quot;, jsonObject); &#125; &#x2F;** * 后台活跃 *&#x2F; private static JSONObject generateBackground() &#123; AppActive appActive_background &#x3D; new AppActive(); &#x2F;&#x2F; 启动源 int flag &#x3D; rand.nextInt(3) + 1; appActive_background.setActive_source(flag + &quot;&quot;); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(appActive_background); return packEventJson(&quot;active_background&quot;, jsonObject); &#125; &#x2F;** * 错误日志数据 *&#x2F; private static JSONObject generateError() &#123; AppErrorLog appErrorLog &#x3D; new AppErrorLog(); String[] errorBriefs &#x3D; &#123;&quot;at cn.lift.dfdf.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)&quot;, &quot;at cn.lift.appIn.control.CommandUtil.getInfo(CommandUtil.java:67)&quot;&#125;; &#x2F;&#x2F;错误摘要 String[] errorDetails &#x3D; &#123;&quot;java.lang.NullPointerException\\\\n &quot; + &quot;at cn.lift.appIn.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)\\\\n &quot; + &quot;at cn.lift.dfdf.web.AbstractBaseController.validInbound&quot;, &quot;at cn.lift.dfdfdf.control.CommandUtil.getInfo(CommandUtil.java:67)\\\\n &quot; + &quot;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\\\n&quot; + &quot; at java.lang.reflect.Method.invoke(Method.java:606)\\\\n&quot;&#125;; &#x2F;&#x2F;错误详情 &#x2F;&#x2F;错误摘要 appErrorLog.setErrorBrief(errorBriefs[rand.nextInt(errorBriefs.length)]); &#x2F;&#x2F;错误详情 appErrorLog.setErrorDetail(errorDetails[rand.nextInt(errorDetails.length)]); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(appErrorLog); return packEventJson(&quot;error&quot;, jsonObject); &#125; &#x2F;** * 为各个事件类型的公共字段（时间、事件类型、Json数据）拼接 *&#x2F; private static JSONObject packEventJson(String eventName, JSONObject jsonObject) &#123; JSONObject eventJson &#x3D; new JSONObject(); eventJson.put(&quot;ett&quot;, (System.currentTimeMillis() - rand.nextInt(99999999)) + &quot;&quot;); eventJson.put(&quot;en&quot;, eventName); eventJson.put(&quot;kv&quot;, jsonObject); return eventJson; &#125; &#x2F;** * 获取随机字母组合 * * @param length 字符串长度 *&#x2F; private static String getRandomChar(Integer length) &#123; StringBuilder str &#x3D; new StringBuilder(); Random random &#x3D; new Random(); for (int i &#x3D; 0; i &lt; length; i++) &#123; &#x2F;&#x2F; 字符串 str.append((char) (65 + random.nextInt(26)));&#x2F;&#x2F; 取得大写字母 &#125; return str.toString(); &#125; &#x2F;** * 获取随机字母数字组合 * * @param length 字符串长度 *&#x2F; private static String getRandomCharAndNumr(Integer length) &#123; StringBuilder str &#x3D; new StringBuilder(); Random random &#x3D; new Random(); for (int i &#x3D; 0; i &lt; length; i++) &#123; boolean b &#x3D; random.nextBoolean(); if (b) &#123; &#x2F;&#x2F; 字符串 &#x2F;&#x2F; int choice &#x3D; random.nextBoolean() ? 65 : 97; 取得65大写字母还是97小写字母 str.append((char) (65 + random.nextInt(26)));&#x2F;&#x2F; 取得大写字母 &#125; else &#123; &#x2F;&#x2F; 数字 str.append(String.valueOf(random.nextInt(10))); &#125; &#125; return str.toString(); &#125; &#x2F;** * 收藏 *&#x2F; private static JSONObject generateFavorites() &#123; AppFavorites favorites &#x3D; new AppFavorites(); favorites.setCourse_id(rand.nextInt(10)); favorites.setUserid(rand.nextInt(10)); favorites.setAdd_time((System.currentTimeMillis() - rand.nextInt(99999999)) + &quot;&quot;); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(favorites); return packEventJson(&quot;favorites&quot;, jsonObject); &#125; &#x2F;** * 点赞 *&#x2F; private static JSONObject generatePraise() &#123; AppPraise praise &#x3D; new AppPraise(); praise.setId(rand.nextInt(10)); praise.setUserid(rand.nextInt(10)); praise.setTarget_id(rand.nextInt(10)); praise.setType(rand.nextInt(4) + 1); praise.setAdd_time((System.currentTimeMillis() - rand.nextInt(99999999)) + &quot;&quot;); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(praise); return packEventJson(&quot;praise&quot;, jsonObject); &#125; &#x2F;** * 评论 *&#x2F; private static JSONObject generateComment() &#123; AppComment comment &#x3D; new AppComment(); comment.setComment_id(rand.nextInt(10)); comment.setUserid(rand.nextInt(10)); comment.setP_comment_id(rand.nextInt(5)); comment.setContent(getCONTENT()); comment.setAddtime((System.currentTimeMillis() - rand.nextInt(99999999)) + &quot;&quot;); comment.setOther_id(rand.nextInt(10)); comment.setPraise_count(rand.nextInt(1000)); comment.setReply_count(rand.nextInt(200)); JSONObject jsonObject &#x3D; (JSONObject) JSON.toJSON(comment); return packEventJson(&quot;comment&quot;, jsonObject); &#125; &#x2F;** * 生成单个汉字 *&#x2F; private static char getRandomChar() &#123; String str &#x3D; &quot;&quot;; int hightPos; &#x2F;&#x2F; int lowPos; Random random &#x3D; new Random(); &#x2F;&#x2F;随机生成汉子的两个字节 hightPos &#x3D; (176 + Math.abs(random.nextInt(39))); lowPos &#x3D; (161 + Math.abs(random.nextInt(93))); byte[] b &#x3D; new byte[2]; b[0] &#x3D; (Integer.valueOf(hightPos)).byteValue(); b[1] &#x3D; (Integer.valueOf(lowPos)).byteValue(); try &#123; str &#x3D; new String(b, &quot;GBK&quot;); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); System.out.println(&quot;错误&quot;); &#125; return str.charAt(0); &#125; &#x2F;** * 拼接成多个汉字 *&#x2F; private static String getCONTENT() &#123; StringBuilder str &#x3D; new StringBuilder(); for (int i &#x3D; 0; i &lt; rand.nextInt(100); i++) &#123; str.append(getRandomChar()); &#125; return str.toString(); &#125;&#125;2.4.5 启动测试注意，需要将日志模拟放到2台服务器上，模拟日志每一条中即包括公共日志，又包含事件日志，需要flume拦截器进行日志分发，当然也需要两个flume-ng来做这个事情打包上传2台服务器节点，生产数据为后面的测试做准备，这里为用户目录test文件夹下通过参数控制生成消息速度及产量(如下 2秒一条，打印1000条)1234#控制时间及条数nohup java -jar data-producer-1.0-SNAPSHOT-jar-with-dependencies.jar 2000 1000 &amp;#监控日志tail -F &#x2F;root&#x2F;logs&#x2F;*.log通过www.json.cn查看数据格式3 创建KafKa-Topic创建启动日志主题：topic_start创建事件日志主题：topic_event4 Flume准备共分为2组flume第一组：将服务器日志收集，并使用Kafka-Channels将数据发往Kafka不同的Topic，其中使用拦截器进行公共日志和事件日志的分发，第二组：收集Kafka数据，使用Flie-Channels缓存数据，最终发往Hdfs保存4.1 Flume：File-&gt;Kafka配置编写vim /root/test/file-flume-kafka.conf1234567891011121314151617181920212223242526272829#1 定义组件a1.sources&#x3D;r1a1.channels&#x3D;c1 c2# 2 source配置 type类型 positionFile记录日志读取位置 filegroups读取哪些目录 app.+为读取什么开头 channels发往哪里a1.sources.r1.type &#x3D; TAILDIRa1.sources.r1.positionFile &#x3D; &#x2F;root&#x2F;test&#x2F;flume&#x2F;log_position.jsona1.sources.r1.filegroups &#x3D; f1a1.sources.r1.filegroups.f1 &#x3D; &#x2F;root&#x2F;logs&#x2F;app.+a1.sources.r1.fileHeader &#x3D; truea1.sources.r1.channels &#x3D; c1 c2#3 拦截器 这里2个为自定义的拦截器 multiplexing为类型区分选择器 header头用于区分类型 mapping匹配头a1.sources.r1.interceptors &#x3D; i1 i2a1.sources.r1.interceptors.i1.type &#x3D; com.heaton.bigdata.flume.LogETLInterceptor$Buildera1.sources.r1.interceptors.i2.type &#x3D; com.heaton.bigdata.flume.LogTypeInterceptor$Buildera1.sources.r1.selector.type &#x3D; multiplexinga1.sources.r1.selector.header &#x3D; topica1.sources.r1.selector.mapping.topic_start &#x3D; c1a1.sources.r1.selector.mapping.topic_event &#x3D; c2#4 channel配置 kafkaChannela1.channels.c1.type &#x3D; org.apache.flume.channel.kafka.KafkaChannela1.channels.c1.kafka.bootstrap.servers &#x3D; cdh01.cm:9092,cdh02.cm:9092,cdh03.cm:9092a1.channels.c1.kafka.topic &#x3D; topic_starta1.channels.c1.parseAsFlumeEvent &#x3D; falsea1.channels.c1.kafka.consumer.group.id &#x3D; flume-consumera1.channels.c2.type &#x3D;org.apache.flume.channel.kafka.KafkaChannela1.channels.c2.kafka.bootstrap.servers &#x3D; cdh01.cm:9092,cdh02.cm:9092,cdh03.cm:9092a1.channels.c2.kafka.topic &#x3D; topic_eventa1.channels.c2.parseAsFlumeEvent &#x3D; falsea1.channels.c2.kafka.consumer.group.id &#x3D; flume-consumer在生产日志的2台服务器节点上创建flume配置文件。LogETLInterceptor，LogTypeInterceptor为自定义拦截4.2 自定义拦截器data-flume工程LogUtils123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import org.apache.commons.lang.math.NumberUtils;public class LogUtils &#123; public static boolean validateEvent(String log) &#123; &#x2F;** 服务器时间 | json 1588319303710|&#123; &quot;cm&quot;:&#123; &quot;ln&quot;:&quot;-51.5&quot;,&quot;sv&quot;:&quot;V2.0.7&quot;,&quot;os&quot;:&quot;8.0.8&quot;,&quot;g&quot;:&quot;L1470998@gmail.com&quot;,&quot;mid&quot;:&quot;13&quot;, &quot;nw&quot;:&quot;4G&quot;,&quot;l&quot;:&quot;en&quot;,&quot;vc&quot;:&quot;7&quot;,&quot;hw&quot;:&quot;640*960&quot;,&quot;ar&quot;:&quot;MX&quot;,&quot;uid&quot;:&quot;13&quot;,&quot;t&quot;:&quot;1588291826938&quot;, &quot;la&quot;:&quot;-38.2&quot;,&quot;md&quot;:&quot;Huawei-14&quot;,&quot;vn&quot;:&quot;1.3.6&quot;,&quot;ba&quot;:&quot;Huawei&quot;,&quot;sr&quot;:&quot;Y&quot; &#125;, &quot;ap&quot;:&quot;app&quot;, &quot;et&quot;:[&#123; &quot;ett&quot;:&quot;1588228193191&quot;,&quot;en&quot;:&quot;ad&quot;,&quot;kv&quot;:&#123;&quot;activityId&quot;:&quot;1&quot;,&quot;displayMills&quot;:&quot;113201&quot;,&quot;entry&quot;:&quot;3&quot;,&quot;action&quot;:&quot;5&quot;,&quot;contentType&quot;:&quot;0&quot;&#125; &#125;,&#123; &quot;ett&quot;:&quot;1588300304713&quot;,&quot;en&quot;:&quot;notification&quot;,&quot;kv&quot;:&#123;&quot;ap_time&quot;:&quot;1588277440794&quot;,&quot;action&quot;:&quot;2&quot;,&quot;type&quot;:&quot;3&quot;,&quot;content&quot;:&quot;&quot;&#125; &#125;,&#123; &quot;ett&quot;:&quot;1588249203743&quot;,&quot;en&quot;:&quot;active_background&quot;,&quot;kv&quot;:&#123;&quot;active_source&quot;:&quot;3&quot;&#125; &#125;,&#123; &quot;ett&quot;:&quot;1588254200122&quot;,&quot;en&quot;:&quot;favorites&quot;,&quot;kv&quot;:&#123;&quot;course_id&quot;:5,&quot;id&quot;:0,&quot;add_time&quot;:&quot;1588264138625&quot;,&quot;userid&quot;:0&#125; &#125;,&#123; &quot;ett&quot;:&quot;1588281152824&quot;,&quot;en&quot;:&quot;praise&quot;,&quot;kv&quot;:&#123;&quot;target_id&quot;:4,&quot;id&quot;:3,&quot;type&quot;:3,&quot;add_time&quot;:&quot;1588307696417&quot;,&quot;userid&quot;:8&#125; &#125;] &#125; *&#x2F; &#x2F;&#x2F; 1 切割 String[] logContents &#x3D; log.split(&quot;\\\\|&quot;); &#x2F;&#x2F; 2 校验 if (logContents.length !&#x3D; 2) &#123; return false; &#125; &#x2F;&#x2F;3 校验服务器时间 if (logContents[0].length() !&#x3D; 13 || !NumberUtils.isDigits(logContents[0])) &#123; return false; &#125; &#x2F;&#x2F; 4 校验 json if (!logContents[1].trim().startsWith(&quot;&#123;&quot;) || !logContents[1].trim().endsWith(&quot;&#125;&quot;)) &#123; return false; &#125; return true; &#125; public static boolean validateStart(String log) &#123; &#x2F;** &#123; &quot;action&quot;:&quot;1&quot;,&quot;ar&quot;:&quot;MX&quot;,&quot;ba&quot;:&quot;HTC&quot;,&quot;detail&quot;:&quot;201&quot;,&quot;en&quot;:&quot;start&quot;,&quot;entry&quot;:&quot;4&quot;,&quot;extend1&quot;:&quot;&quot;, &quot;g&quot;:&quot;4Z174142@gmail.com&quot;,&quot;hw&quot;:&quot;750*1134&quot;,&quot;l&quot;:&quot;pt&quot;,&quot;la&quot;:&quot;-29.7&quot;,&quot;ln&quot;:&quot;-48.1&quot;,&quot;loading_time&quot;:&quot;0&quot;, &quot;md&quot;:&quot;HTC-18&quot;,&quot;mid&quot;:&quot;14&quot;,&quot;nw&quot;:&quot;3G&quot;,&quot;open_ad_type&quot;:&quot;2&quot;,&quot;os&quot;:&quot;8.0.8&quot;,&quot;sr&quot;:&quot;D&quot;,&quot;sv&quot;:&quot;V2.8.2&quot;, &quot;t&quot;:&quot;1588251833523&quot;,&quot;uid&quot;:&quot;14&quot;,&quot;vc&quot;:&quot;15&quot;,&quot;vn&quot;:&quot;1.2.9&quot; &#125; *&#x2F; if (log &#x3D;&#x3D; null) &#123; return false; &#125; &#x2F;&#x2F; 校验 json if (!log.trim().startsWith(&quot;&#123;&quot;) || !log.trim().endsWith(&quot;&#125;&quot;)) &#123; return false; &#125; return true; &#125;&#125;LogETLInterceptor123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.interceptor.Interceptor;import java.nio.charset.Charset;import java.util.ArrayList;import java.util.List;public class LogETLInterceptor implements Interceptor &#123; @Override public void initialize() &#123; &#x2F;&#x2F;初始化 &#125; @Override public Event intercept(Event event) &#123; &#x2F;&#x2F; 1 获取数据 byte[] body &#x3D; event.getBody(); String log &#x3D; new String(body, Charset.forName(&quot;UTF-8&quot;)); &#x2F;&#x2F; 2 判断数据类型并向 Header 中赋值 if (log.contains(&quot;start&quot;)) &#123; if (LogUtils.validateStart(log)) &#123; return event; &#125; &#125; else &#123; if (LogUtils.validateEvent(log)) &#123; return event; &#125; &#125; &#x2F;&#x2F; 3 返回校验结果 return null; &#125; @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; ArrayList&lt;Event&gt; interceptors &#x3D; new ArrayList&lt;&gt;(); for (Event event : events) &#123; Event intercept1 &#x3D; intercept(event); if (intercept1 !&#x3D; null) &#123; interceptors.add(intercept1); &#125; &#125; return interceptors; &#125; @Override public void close() &#123; &#x2F;&#x2F;关闭 &#125; public static class Builder implements Interceptor.Builder &#123; @Override public Interceptor build() &#123; return new LogETLInterceptor(); &#125; @Override public void configure(Context context) &#123; &#125; &#125;&#125;LogTypeInterceptor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.interceptor.Interceptor;import java.nio.charset.Charset;import java.util.ArrayList;import java.util.List;import java.util.Map;public class LogTypeInterceptor implements Interceptor &#123; @Override public void initialize() &#123; &#125; @Override public Event intercept(Event event) &#123; &#x2F;&#x2F; 区分日志类型： body header &#x2F;&#x2F; 1 获取 body 数据 byte[] body &#x3D; event.getBody(); String log &#x3D; new String(body, Charset.forName(&quot;UTF-8&quot;)); &#x2F;&#x2F; 2 获取 header Map&lt;String, String&gt; headers &#x3D; event.getHeaders(); &#x2F;&#x2F; 3 判断数据类型并向 Header 中赋值 if (log.contains(&quot;start&quot;)) &#123; headers.put(&quot;topic&quot;, &quot;topic_start&quot;); &#125; else &#123; headers.put(&quot;topic&quot;, &quot;topic_event&quot;); &#125; return event; &#125; @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; ArrayList&lt;Event&gt; interceptors &#x3D; new ArrayList&lt;&gt;(); for (Event event : events) &#123; Event intercept1 &#x3D; intercept(event); interceptors.add(intercept1); &#125; return interceptors; &#125; @Override public void close() &#123; &#125; public static class Builder implements Interceptor.Builder &#123; @Override public Interceptor build() &#123; return new LogTypeInterceptor(); &#125; @Override public void configure(Context context) &#123; &#125; &#125;&#125;将项目打包放入Flume/lib目录下(所有节点)：CDH路径参考：/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/flume-ng/lib4.3 Flume启停脚本vim /root/log-kafka-flume.sh1234567891011121314151617#! &#x2F;bin&#x2F;bashcase $1 in&quot;start&quot;)&#123; for i in cdh02.cm cdh03.cm do echo &quot; --------启动 $i 消费 flume-------&quot; ssh $i &quot;nohup flume-ng agent --conf-file &#x2F;root&#x2F;test&#x2F;file-flume-kafka.conf --name a1 -Dflume.root.logger&#x3D;INFO,LOGFILE &gt;&#x2F;root&#x2F;test&#x2F;file-flume-kafka.log 2&gt;&amp;1 &amp;&quot; done&#125;;;&quot;stop&quot;)&#123; for i in cdh02.cm cdh03.cm do echo &quot; --------停止 $i 消费 flume-------&quot; ssh $i &quot;ps -ef | grep file-flume-kafka | grep -v grep |awk &#39;&#123;print \\$2&#125;&#39; | xargs kill&quot; done&#125;;;esac4.4 Flume：Kafka-&gt;HDFS配置编写在第三台服务上准备vim /root/test/kafka-flume-hdfs.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364## 组件 a1.sources&#x3D;r1 r2a1.channels&#x3D;c1 c2 a1.sinks&#x3D;k1 k2 ## Kafka-source1a1.sources.r1.type &#x3D; org.apache.flume.source.kafka.KafkaSourcea1.sources.r1.batchSize &#x3D; 5000a1.sources.r1.batchDurationMillis &#x3D; 2000a1.sources.r1.kafka.bootstrap.servers&#x3D; cdh01.cm:9092,cdh02.cm:9092,cdh03.cm:9092a1.sources.r1.kafka.topics &#x3D; topic_start## Kafka- source2a1.sources.r2.type &#x3D; org.apache.flume.source.kafka.KafkaSourcea1.sources.r2.batchSize &#x3D; 5000a1.sources.r2.batchDurationMillis &#x3D; 2000a1.sources.r2.kafka.bootstrap.servers &#x3D; cdh01.cm:9092,cdh02.cm:9092,cdh03.cm:9092a1.sources.r2.kafka.topics &#x3D; topic_event ## channel1a1.channels.c1.type &#x3D; file##索引文件路径a1.channels.c1.checkpointDir&#x3D;&#x2F;root&#x2F;test&#x2F;flume&#x2F;checkpoint&#x2F;behavior1##持久化路径a1.channels.c1.dataDirs &#x3D; &#x2F;root&#x2F;test&#x2F;flume&#x2F;data&#x2F;behavior1&#x2F;a1.channels.c1.maxFileSize &#x3D; 2146435071a1.channels.c1.capacity &#x3D; 1000000a1.channels.c1.keep-alive &#x3D; 6## channel2a1.channels.c2.type &#x3D; file##索引文件路径a1.channels.c1.checkpointDir&#x3D;&#x2F;root&#x2F;test&#x2F;flume&#x2F;checkpoint&#x2F;behavior2##持久化路径a1.channels.c1.dataDirs &#x3D; &#x2F;root&#x2F;test&#x2F;flume&#x2F;data&#x2F;behavior2&#x2F;a1.channels.c2.maxFileSize &#x3D; 2146435071a1.channels.c2.capacity &#x3D; 1000000a1.channels.c2.keep-alive &#x3D; 6 ## HDFS-sink1a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path&#x3D;&#x2F;origin_data&#x2F;gmall&#x2F;log&#x2F;topic_start&#x2F;%Y-%m-%da1.sinks.k1.hdfs.filePrefix &#x3D; logstart-## HDFS-sink2 a1.sinks.k2.type &#x3D; hdfsa1.sinks.k2.hdfs.path &#x3D; &#x2F;origin_data&#x2F;gmall&#x2F;log&#x2F;topic_event&#x2F;%Y-%m-%da1.sinks.k2.hdfs.filePrefix &#x3D; logevent- ## 不要产生大量小文件a1.sinks.k1.hdfs.rollInterval &#x3D; 10a1.sinks.k1.hdfs.rollSize &#x3D; 134217728a1.sinks.k1.hdfs.rollCount &#x3D; 0a1.sinks.k2.hdfs.rollInterval &#x3D; 50a1.sinks.k2.hdfs.rollSize &#x3D; 134217728a1.sinks.k2.hdfs.rollCount &#x3D; 0## 控制输出文件是原生文件。a1.sinks.k1.hdfs.fileType &#x3D; CompressedStreama1.sinks.k2.hdfs.fileType &#x3D; CompressedStreama1.sinks.k1.hdfs.codeC &#x3D; snappya1.sinks.k2.hdfs.codeC &#x3D; snappy ## 组件拼装a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel&#x3D; c1a1.sources.r2.channels &#x3D; c2a1.sinks.k2.channel&#x3D; c24.5 Flume启停脚本在第三台服务上准备vim /root/test/kafka-hdfs-flume.sh1234567891011121314151617#! &#x2F;bin&#x2F;bashcase $1 in&quot;start&quot;)&#123; for i in cdh01.cm do echo &quot; --------启动 $i 消费 flume-------&quot; ssh $i &quot;nohup flume-ng agent --conf-file &#x2F;root&#x2F;test&#x2F;kafka-flume-hdfs.conf --name a1 -Dflume.root.logger&#x3D;INFO,LOGFILE &gt;&#x2F;root&#x2F;test&#x2F;kafka-flume-hdfs.log 2&gt;&amp;1 &amp;&quot; done&#125;;;&quot;stop&quot;)&#123; for i in cdh01.cm do echo &quot; --------停止 $i 消费 flume-------&quot; ssh $i &quot;ps -ef | grep kafka-flume-hdfs | grep -v grep |awk &#39;&#123;print \\$2&#125;&#39; | xargs kill&quot; done&#125;;;esac5 业务数据此模块后主要针对于企业报表决策，为数据分析提供数据支持，解决大数据量下，无法快速产出报表，及一些即席业务需求的快速展示提供数据支撑。划分企业离线与实时业务，用离线的方式直观的管理数据呈现，为实时方案奠定良好基础。5.1 电商业务流程5.2 SKU-SPUSKU（Stock Keeping Unit）：库存量基本单位，现在已经被引申为产品统一编号的简称， 每种产品均对应有唯一的 SKU 号。SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的 标准化信息集合。总结：黑鲨3 手机就是 SPU。一台铠甲灰、256G 内存的就是 SKU。5.3 业务表结构5.3.1 订单表（order_info）5.3.2 订单详情表（order_detail）5.3.3 SKU 商品表（sku_info）5.3.4 用户表（user_info）5.3.5 商品一级分类表（base_category1）5.3.6 商品二级分类表（base_category2）5.3.7 商品三级分类表（base_category3）5.3.8 支付流水表（payment_info）5.3.9 省份表（base_province）5.3.10 地区表（base_region）5.3.11 品牌表（base_trademark）5.3.12 订单状态表（order_status_log）5.3.13 SPU 商品表（spu_info）5.3.14 商品评论表（comment_info）5.3.15 退单表（order_refund_info）5.3.16 加入购物车表（cart_info）5.3.17 商品收藏表（favor_info）5.3.18 优惠券领用表（coupon_use）5.3.19 优惠券表（coupon_info）5.3.20 活动表（activity_info）5.3.21 活动订单关联表（activity_order）5.3.22 优惠规则表（activity_rule）5.3.23 编码字典表（base_dic）5.3.24 活动参与商品表（activity_sku）5.4 时间表结构5.4.1 时间表（date_info）5.4.2 假期表（holiday_info）5.4.3 假期年表（holiday_year）6 同步策略及数仓分层数据同步策略的类型包括：全量表、增量表、新增及变化表全量表：每天一个分区，存储完整的数据。增量表：每天新增数据放在一个分区，存储新增加的数据。新增及变化表：每天新增和变化的数据放在一个分区，存储新增加的数据和变化的数据。特殊表：没有分区，只需要存储一次。6.1 全量策略每日全量，每天存储一份完整数据，作为一个分区。适合场景：表数据量不大，且有新增或修改业务的场景例如：品牌表、编码表、商品分类表、优惠规则表、活动表、商品表、加购表、收藏表、SKU/SPU表6.2 增量策略每日增量，每天储存一份增量数据，作为一个分区适合场景：表数据量大，且只会有新增数据的场景。例如：退单表、订单状态表、支付流水表、订单详情表、活动与订单关联表、商品评论表6.3 新增及变化策略每日新增及变化，储存创建时间和操作时间都是今天的数据，作为一个分区适合场景：表数据量大，既会有新增，又会有修改。例如：用户表、订单表、优惠卷领用表。6.4 特殊策略某些特殊的维度表，可不必遵循上述同步策略，在数仓中只做一次同步，数据不变化不更新适合场景：表数据几乎不会变化1.客观世界维度：没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一 份固定值2.日期维度：日期维度可以一次性导入一年或若干年的数据。3.地区维度：省份表、地区表6.5 分析业务表同步策略考虑到特殊表可能会缓慢变化，比如打仗占地盘，地区表可能就会发生变化，故也选择分区全量同步策略。6.6 数仓分层为什么分层：简单化：把复杂的任务分解为多层来完成，每层处理各自的任务，方便定位问题。减少重复开发：规范数据分层，通过中间层数据，能够极大的减少重复计算，增加结果复用性。隔离数据：不论是数据异常还是数据敏感性，使真实数据和统计数据解耦。一般在DWD层进行维度建模ODS层：原始数据层，存放原始数据DWD层：对ODS层数据进行清洗（去空、脏数据，转换类型等），维度退化，脱敏(保护隐私)DWS层：以DWD为基础，按天进行汇总DWT层：以DWS为基础，按主题进行汇总ADS层：为各种数据分析报表提供数据7 Sqoop同步数据Sqoop注意点：Hive 中的 Null 在底层是以“\\N”来存储，而 MySQL 中的 Null 在底层就是 Null，为了 保证数据两端的一致性。在导出数据时采用 –input-null-string 和 –input-null-non-string导入数据时采用 –null-string 和 –null-non-string本例思路为：sqoop抽取mysql数据上传至Hdfs上，存储为parquet文件，在建立hive-ods表，使用对应数据。使用DolphinScheduler调度执行脚本。Sqoop采集Mysql和Hive数据格式mysql字段类型hive:ods字段类型hive:dwd-ads字段类型tinyinttinyinttinyintintintintbigintbigintbigintvarcharstringstringdatetimebigintstringbitbooleanintdoubledoubledoubledecimaldecimaldecimal8 ods层构建8.1 ods建表hive创建ods数据库，使用DolphinScheduler创建数据源，在创建DAG时需要选择hive库。顺便将dwd，dws，dwt，ads一起创建了base_dic123456789101112131415drop table if exists ods.mall__base_dicCREATE EXTERNAL TABLE &#96;ods.mall__base_dic&#96;( &#96;dic_code&#96; string COMMENT &#39;编号&#39;, &#96;dic_name&#96; string COMMENT &#39;编码名称&#39;, &#96;parent_code&#96; string COMMENT &#39;父编号&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建日期&#39;, &#96;operate_time&#96; bigint COMMENT &#39;修改日期&#39; ) COMMENT &#39;编码字典表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_dic&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)base_trademark123456789101112drop table if exists ods.mall__base_trademarkCREATE EXTERNAL TABLE &#96;ods.mall__base_trademark&#96;( &#96;tm_id&#96; string COMMENT &#39;品牌id&#39;, &#96;tm_name&#96; string COMMENT &#39;品牌名称&#39; ) COMMENT &#39;品牌表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_trademark&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)base_category312345678910111213drop table if exists ods.mall__base_category3CREATE EXTERNAL TABLE &#96;ods.mall__base_category3&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;name&#96; string COMMENT &#39;三级分类名称&#39;, &#96;category2_id&#96; bigint COMMENT &#39;二级分类编号&#39; ) COMMENT &#39;三级分类表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_category3&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)base_category212345678910111213drop table if exists ods.mall__base_category2CREATE EXTERNAL TABLE &#96;ods.mall__base_category2&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;name&#96; string COMMENT &#39;二级分类名称&#39;, &#96;category1_id&#96; bigint COMMENT &#39;一级分类编号&#39; ) COMMENT &#39;二级分类表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_category2&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)base_category1123456789101112drop table if exists ods.mall__base_category1CREATE EXTERNAL TABLE &#96;ods.mall__base_category1&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;name&#96; string COMMENT &#39;分类名称&#39; ) COMMENT &#39;一级分类表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_category1&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)activity_rule1234567891011121314151617drop table if exists ods.mall__activity_ruleCREATE EXTERNAL TABLE &#96;ods.mall__activity_rule&#96;( &#96;id&#96; int COMMENT &#39;编号&#39;, &#96;activity_id&#96; int COMMENT &#39;类型&#39;, &#96;condition_amount&#96; decimal(16,2) COMMENT &#39;满减金额&#39;, &#96;condition_num&#96; bigint COMMENT &#39;满减件数&#39;, &#96;benefit_amount&#96; decimal(16,2) COMMENT &#39;优惠金额&#39;, &#96;benefit_discount&#96; bigint COMMENT &#39;优惠折扣&#39;, &#96;benefit_level&#96; bigint COMMENT &#39;优惠级别&#39; ) COMMENT &#39;优惠规则&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;activity_rule&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)activity_info12345678910111213141516drop table if exists ods.mall__activity_infoCREATE EXTERNAL TABLE &#96;ods.mall__activity_info&#96;( &#96;id&#96; bigint COMMENT &#39;活动id&#39;, &#96;activity_name&#96; string COMMENT &#39;活动名称&#39;, &#96;activity_type&#96; string COMMENT &#39;活动类型&#39;, &#96;start_time&#96; bigint COMMENT &#39;开始时间&#39;, &#96;end_time&#96; bigint COMMENT &#39;结束时间&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39; ) COMMENT &#39;活动表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;activity_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)activity_sku1234567891011121314drop table if exists ods.mall__activity_skuCREATE EXTERNAL TABLE &#96;ods.mall__activity_sku&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;activity_id&#96; bigint COMMENT &#39;活动id&#39;, &#96;sku_id&#96; bigint COMMENT &#39;sku_id&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39; ) COMMENT &#39;活动参与商品&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;activity_sku&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)cart_info1234567891011121314151617181920drop table if exists ods.mall__cart_infoCREATE EXTERNAL TABLE &#96;ods.mall__cart_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户id&#39;, &#96;sku_id&#96; bigint COMMENT &#39;sku_id&#39;, &#96;cart_price&#96; decimal(10,2) COMMENT &#39;放入购物车时价格&#39;, &#96;sku_num&#96; bigint COMMENT &#39;数量&#39;, &#96;sku_name&#96; string COMMENT &#39;sku名称&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39;, &#96;operate_time&#96; bigint COMMENT &#39;修改时间&#39;, &#96;is_ordered&#96; bigint COMMENT &#39;是否已经下单&#39;, &#96;order_time&#96; bigint COMMENT &#39;下单时间&#39; ) COMMENT &#39;购物车表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;cart_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)favor_info1234567891011121314151617drop table if exists ods.mall__favor_infoCREATE EXTERNAL TABLE &#96;ods.mall__favor_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户id&#39;, &#96;sku_id&#96; bigint COMMENT &#39;sku_id&#39;, &#96;spu_id&#96; bigint COMMENT &#39;商品id&#39;, &#96;is_cancel&#96; string COMMENT &#39;是否已取消 0 正常 1 已取消&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39;, &#96;cancel_time&#96; bigint COMMENT &#39;修改时间&#39; ) COMMENT &#39;商品收藏表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;favor_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)coupon_info1234567891011121314151617181920212223242526drop table if exists ods.mall__coupon_infoCREATE EXTERNAL TABLE &#96;ods.mall__coupon_info&#96;( &#96;id&#96; bigint COMMENT &#39;购物券编号&#39;, &#96;coupon_name&#96; string COMMENT &#39;购物券名称&#39;, &#96;coupon_type&#96; string COMMENT &#39;购物券类型 1 现金券 2 折扣券 3 满减券 4 满件打折券&#39;, &#96;condition_amount&#96; decimal(10,2) COMMENT &#39;满额数&#39;, &#96;condition_num&#96; bigint COMMENT &#39;满件数&#39;, &#96;activity_id&#96; bigint COMMENT &#39;活动编号&#39;, &#96;benefit_amount&#96; decimal(16,2) COMMENT &#39;减金额&#39;, &#96;benefit_discount&#96; bigint COMMENT &#39;折扣&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39;, &#96;range_type&#96; string COMMENT &#39;范围类型 1、商品 2、品类 3、品牌&#39;, &#96;spu_id&#96; bigint COMMENT &#39;商品id&#39;, &#96;tm_id&#96; bigint COMMENT &#39;品牌id&#39;, &#96;category3_id&#96; bigint COMMENT &#39;品类id&#39;, &#96;limit_num&#96; int COMMENT &#39;最多领用次数&#39;, &#96;operate_time&#96; bigint COMMENT &#39;修改时间&#39;, &#96;expire_time&#96; bigint COMMENT &#39;过期时间&#39; ) COMMENT &#39;优惠券表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;coupon_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)sku_info12345678910111213141516171819drop table if exists ods.mall__sku_infoCREATE EXTERNAL TABLE &#96;ods.mall__sku_info&#96;( &#96;id&#96; bigint COMMENT &#39;skuid&#39;, &#96;spu_id&#96; bigint COMMENT &#39;spuid&#39;, &#96;price&#96; decimal(10,0) COMMENT &#39;价格&#39;, &#96;sku_name&#96; string COMMENT &#39;sku名称&#39;, &#96;sku_desc&#96; string COMMENT &#39;商品规格描述&#39;, &#96;weight&#96; decimal(10,2) COMMENT &#39;重量&#39;, &#96;tm_id&#96; bigint COMMENT &#39;品牌&#39;, &#96;category3_id&#96; bigint COMMENT &#39;三级分类id&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39; ) COMMENT &#39;库存单元表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;sku_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)spu_info1234567891011121314drop table if exists ods.mall__spu_infoCREATE EXTERNAL TABLE &#96;ods.mall__spu_info&#96;( &#96;id&#96; bigint COMMENT &#39;商品id&#39;, &#96;spu_name&#96; string COMMENT &#39;商品名称&#39;, &#96;category3_id&#96; bigint COMMENT &#39;三级分类id&#39;, &#96;tm_id&#96; bigint COMMENT &#39;品牌id&#39; ) COMMENT &#39;商品表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;spu_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)base_province123456789101112131415drop table if exists ods.mall__base_provinceCREATE EXTERNAL TABLE &#96;ods.mall__base_province&#96;( &#96;id&#96; bigint COMMENT &#39;id&#39;, &#96;name&#96; string COMMENT &#39;省名称&#39;, &#96;region_id&#96; string COMMENT &#39;大区id&#39;, &#96;area_code&#96; string COMMENT &#39;行政区位码&#39;, &#96;iso_code&#96; string COMMENT &#39;国际编码&#39; ) COMMENT &#39;省份表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_province&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)base_region123456789101112drop table if exists ods.mall__base_regionCREATE EXTERNAL TABLE &#96;ods.mall__base_region&#96;( &#96;id&#96; string COMMENT &#39;大区id&#39;, &#96;region_name&#96; string COMMENT &#39;大区名称&#39; ) COMMENT &#39;地区表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;base_region&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)refund_info12345678910111213141516171819drop table if exists ods.mall__order_refund_infoCREATE EXTERNAL TABLE &#96;ods.mall__order_refund_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户id&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;sku_id&#96; bigint COMMENT &#39;skuid&#39;, &#96;refund_type&#96; string COMMENT &#39;退款类型&#39;, &#96;refund_num&#96; bigint COMMENT &#39;退货件数&#39;, &#96;refund_amount&#96; decimal(16,2) COMMENT &#39;退款金额&#39;, &#96;refund_reason_type&#96; string COMMENT &#39;原因类型&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39; ) COMMENT &#39;退单表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;order_refund_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)order_status_log1234567891011121314drop table if exists ods.mall__order_status_logCREATE EXTERNAL TABLE &#96;ods.mall__order_status_log&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;order_status&#96; string COMMENT &#39;订单状态&#39;, &#96;operate_time&#96; bigint COMMENT &#39;操作时间&#39; ) COMMENT &#39;订单状态表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;order_status_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)payment_info12345678910111213141516171819drop table if exists ods.mall__payment_infoCREATE EXTERNAL TABLE &#96;ods.mall__payment_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;out_trade_no&#96; string COMMENT &#39;对外业务编号&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户编号&#39;, &#96;alipay_trade_no&#96; string COMMENT &#39;支付宝交易流水编号&#39;, &#96;total_amount&#96; decimal(16,2) COMMENT &#39;支付金额&#39;, &#96;subject&#96; string COMMENT &#39;交易内容&#39;, &#96;payment_type&#96; string COMMENT &#39;支付方式&#39;, &#96;payment_time&#96; bigint COMMENT &#39;支付时间&#39; ) COMMENT &#39;支付流水表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;payment_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)order_detail123456789101112131415161718drop table if exists ods.mall__order_detailCREATE EXTERNAL TABLE &#96;ods.mall__order_detail&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户id&#39;, &#96;sku_id&#96; bigint COMMENT &#39;sku_id&#39;, &#96;sku_name&#96; string COMMENT &#39;sku名称&#39;, &#96;order_price&#96; decimal(10,2) COMMENT &#39;购买价格(下单时sku价格）&#39;, &#96;sku_num&#96; string COMMENT &#39;购买个数&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39; ) COMMENT &#39;订单明细表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;order_detail&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)activity_order1234567891011121314drop table if exists ods.mall__activity_orderCREATE EXTERNAL TABLE &#96;ods.mall__activity_order&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;activity_id&#96; bigint COMMENT &#39;活动id&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;create_time&#96; bigint COMMENT &#39;发生日期&#39; ) COMMENT &#39;活动与订单关联表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;activity_order&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)comment_info123456789101112131415161718drop table if exists ods.mall__comment_infoCREATE EXTERNAL TABLE &#96;ods.mall__comment_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户名称&#39;, &#96;sku_id&#96; bigint COMMENT &#39;skuid&#39;, &#96;spu_id&#96; bigint COMMENT &#39;商品id&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;appraise&#96; string COMMENT &#39;评价 1 好评 2 中评 3 差评&#39;, &#96;comment_txt&#96; string COMMENT &#39;评价内容&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39; ) COMMENT &#39;商品评论表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;comment_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)coupon_use123456789101112131415161718drop table if exists ods.mall__coupon_useCREATE EXTERNAL TABLE &#96;ods.mall__coupon_use&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;coupon_id&#96; bigint COMMENT &#39;购物券ID&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户ID&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单ID&#39;, &#96;coupon_status&#96; string COMMENT &#39;购物券状态&#39;, &#96;get_time&#96; bigint COMMENT &#39;领券时间&#39;, &#96;using_time&#96; bigint COMMENT &#39;使用时间&#39;, &#96;used_time&#96; bigint COMMENT &#39;过期时间&#39; ) COMMENT &#39;优惠券领用表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;coupon_use&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)user_info123456789101112131415161718drop table if exists ods.mall__user_infoCREATE EXTERNAL TABLE &#96;ods.mall__user_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;name&#96; string COMMENT &#39;用户姓名&#39;, &#96;email&#96; string COMMENT &#39;邮箱&#39;, &#96;user_level&#96; string COMMENT &#39;用户级别&#39;, &#96;birthday&#96; bigint COMMENT &#39;用户生日&#39;, &#96;gender&#96; string COMMENT &#39;性别 M男,F女&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39;, &#96;operate_time&#96; bigint COMMENT &#39;修改时间&#39; ) COMMENT &#39;用户表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;user_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)order_info123456789101112131415161718192021drop table if exists ods.mall__order_infoCREATE EXTERNAL TABLE &#96;ods.mall__order_info&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;final_total_amount&#96; decimal(16,2) COMMENT &#39;总金额&#39;, &#96;order_status&#96; string COMMENT &#39;订单状态&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户id&#39;, &#96;out_trade_no&#96; string COMMENT &#39;订单交易编号（第三方支付用)&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39;, &#96;operate_time&#96; bigint COMMENT &#39;操作时间&#39;, &#96;province_id&#96; int COMMENT &#39;地区&#39;, &#96;benefit_reduce_amount&#96; decimal(16,2) COMMENT &#39;优惠金额&#39;, &#96;original_total_amount&#96; decimal(16,2) COMMENT &#39;原价金额&#39;, &#96;feight_fee&#96; decimal(16,2) COMMENT &#39;运费&#39; ) COMMENT &#39;订单表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;order_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)start_log此为埋点启动日志表123456789drop table if exists ods.mall__start_logCREATE EXTERNAL TABLE &#96;ods.mall__start_log&#96;( &#96;line&#96; string COMMENT &#39;启动日志&#39; ) COMMENT &#39;启动日志表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;location &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;start_log&#x2F;&#39;event_log此为埋点事件日志表123456789drop table if exists ods.mall__event_logCREATE EXTERNAL TABLE &#96;ods.mall__event_log&#96;( &#96;line&#96; string COMMENT &#39;事件日志&#39; ) COMMENT &#39;事件日志表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;location &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;event_log&#x2F;&#39;date_info此为时间表12345678910111213141516171819drop table if exists ods.mall__date_infoCREATE EXTERNAL TABLE &#96;ods.mall__date_info&#96;(&#96;date_id&#96; int COMMENT &#39;日&#39;,&#96;week_id&#96; int COMMENT &#39;周&#39;,&#96;week_day&#96; int COMMENT &#39;周的第几天&#39;,&#96;day&#96; int COMMENT &#39;每月的第几天&#39;,&#96;month&#96; int COMMENT &#39;第几月&#39;,&#96;quarter&#96; int COMMENT &#39;第几季度&#39;,&#96;year&#96; int COMMENT &#39;年&#39;,&#96;is_workday&#96; int COMMENT &#39;是否是周末&#39;,&#96;holiday_id&#96; int COMMENT &#39;是否是节假日&#39; ) COMMENT &#39;时间维度表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ods&#x2F;mall&#x2F;date_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)8.2 mysql数据抽取sqoop抽取脚本基础12345678910111213141516171819202122232425262728#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;mysql_db_name&#x3D;$&#123;db_name&#125;mysql_db_addr&#x3D;$&#123;db_addr&#125;mysql_db_user&#x3D;$&#123;db_user&#125;mysql_db_password&#x3D;$&#123;db_password&#125;# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fiecho &quot;日期:&quot;$db_dateecho &quot;mysql库名:&quot;$mysql_db_nameimport_data() &#123;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;sqoop import \\--connect jdbc:mysql:&#x2F;&#x2F;$mysql_db_addr:3306&#x2F;$mysql_db_name?tinyInt1isBit&#x3D;false \\--username $mysql_db_user \\--password $mysql_db_password \\--target-dir &#x2F;origin_data&#x2F;$mysql_db_name&#x2F;$1&#x2F;$db_date \\--delete-target-dir \\--num-mappers 1 \\--null-string &#39;&#39; \\--null-non-string &#39;\\\\n&#39; \\--fields-terminated-by &quot;\\t&quot; \\--query &quot;$2&quot;&#39; and $CONDITIONS;&#39; \\--as-parquetfile &#125;DolphinScheduler全局参数date不传为昨天db_name数据库名字db_addr数据库IP地址db_user数据库用户db_password数据库密码元数据中数据开始日期为2020-03-15如下导入数据代码片段，拼接上述的基础片段执行全量表代码片段123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114import_data &quot;base_dic&quot; &quot;selectdic_code,dic_name,parent_code,create_time,operate_timefrom base_dicwhere 1&#x3D;1&quot;import_data &quot;base_trademark&quot; &quot;selecttm_id,tm_namefrom base_trademarkwhere 1&#x3D;1&quot;import_data &quot;base_category3&quot; &quot;selectid,name,category2_idfrom base_category3 where 1&#x3D;1&quot;import_data &quot;base_category2&quot; &quot;selectid,name,category1_idfrom base_category2 where 1&#x3D;1&quot;import_data &quot;base_category1&quot; &quot;selectid,namefrom base_category1 where 1&#x3D;1&quot;import_data &quot;activity_rule&quot; &quot;selectid,activity_id,condition_amount,condition_num,benefit_amount,benefit_discount,benefit_levelfrom activity_rulewhere 1&#x3D;1&quot;import_data &quot;activity_info&quot; &quot;selectid,activity_name,activity_type,start_time,end_time,create_timefrom activity_infowhere 1&#x3D;1&quot;import_data &quot;activity_sku&quot; &quot;selectid,activity_id,sku_id,create_timeFROMactivity_skuwhere 1&#x3D;1&quot;import_data &quot;cart_info&quot; &quot;selectid,user_id,sku_id,cart_price,sku_num,sku_name,create_time,operate_time,is_ordered,order_timefrom cart_infowhere 1&#x3D;1&quot;import_data &quot;favor_info&quot; &quot;selectid,user_id,sku_id,spu_id,is_cancel,create_time,cancel_timefrom favor_infowhere 1&#x3D;1&quot;import_data &quot;coupon_info&quot; &quot;selectid,coupon_name,coupon_type,condition_amount,condition_num,activity_id,benefit_amount,benefit_discount,create_time,range_type,spu_id,tm_id,category3_id,limit_num,operate_time,expire_timefrom coupon_infowhere 1&#x3D;1&quot;import_data &quot;sku_info&quot; &quot;selectid,spu_id,price,sku_name,sku_desc,weight,tm_id,category3_id,create_timefrom sku_info where 1&#x3D;1&quot;import_data &quot;spu_info&quot; &quot;selectid,spu_name,category3_id,tm_idfrom spu_infowhere 1&#x3D;1&quot;特殊表代码片段12345678910111213141516171819202122232425import_data &quot;base_province&quot; &quot;selectid,name,region_id,area_code,iso_codefrom base_provincewhere 1&#x3D;1&quot;import_data &quot;base_region&quot; &quot;selectid,region_namefrom base_regionwhere 1&#x3D;1&quot;import_data &quot;date_info&quot; &quot;selectdate_id,week_id,week_day,day,month,quarter,year,is_workday,holiday_idfrom date_infowhere 1&#x3D;1&quot;增量表代码片段12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import_data &quot;order_refund_info&quot; &quot;selectid,user_id,order_id,sku_id,refund_type,refund_num,refund_amount,refund_reason_type,create_timefrom order_refund_infowheredate_format(create_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;&quot;import_data &quot;order_status_log&quot; &quot;selectid,order_id,order_status,operate_timefrom order_status_logwheredate_format(operate_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;&quot;import_data &quot;payment_info&quot; &quot;selectid,out_trade_no,order_id,user_id,alipay_trade_no,total_amount,subject,payment_type,payment_timefrom payment_infowhereDATE_FORMAT(payment_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;&quot;import_data &quot;order_detail&quot; &quot;selectod.id,od.order_id,oi.user_id,od.sku_id,od.sku_name,od.order_price,od.sku_num,od.create_timefrom order_detail odjoin order_info oion od.order_id&#x3D;oi.idwhereDATE_FORMAT(od.create_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;&quot;import_data &quot;activity_order&quot; &quot;selectid,activity_id,order_id,create_timefrom activity_orderwheredate_format(create_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;&quot;import_data &quot;comment_info&quot; &quot;selectid,user_id,sku_id,spu_id,order_id,appraise,comment_txt,create_timefrom comment_infowhere date_format(create_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;&quot;增量及变化表代码片段12345678910111213141516171819202122232425262728293031323334353637383940import_data &quot;coupon_use&quot; &quot;selectid,coupon_id,user_id,order_id,coupon_status,get_time,using_time,used_timefrom coupon_usewhere (date_format(get_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;or date_format(using_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;or date_format(used_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;)&quot;import_data &quot;user_info&quot; &quot;selectid,name,birthday,gender,email,user_level,create_time,operate_timefrom user_infowhere (DATE_FORMAT(create_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;or DATE_FORMAT(operate_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;)&quot;import_data &quot;order_info&quot; &quot;selectid,final_total_amount,order_status,user_id,out_trade_no,create_time,operate_time,province_id,benefit_reduce_amount,original_total_amount,feight_feefrom order_infowhere (date_format(create_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;or date_format(operate_time,&#39;%Y-%m-%d&#39;)&#x3D;&#39;$db_date&#39;)&quot;8.3 ods层数据加载脚本修改$table_name即可注意2张埋点日志表的数据导出目录1234567891011121314151617#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;odstable_name&#x3D;base_dichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; load data inpath &#39;&#x2F;origin_data&#x2F;$APP1&#x2F;$table_name&#x2F;$db_date&#39; OVERWRITE into table $hive_table_name partition(dt&#x3D;&#39;$db_date&#39;);&quot;$hive -e &quot;$sql&quot;9 dwd层构建9.1 dwd层构建（启动-事件日志）9.1.1 启动日志表建表123456789101112131415161718192021222324252627282930313233drop table if exists dwd.mall__start_logCREATE EXTERNAL TABLE &#96;dwd.mall__start_log&#96;( &#96;mid_id&#96; string COMMENT &#39;设备唯一标识&#39;, &#96;user_id&#96; string COMMENT &#39;用户标识&#39;, &#96;version_code&#96; string COMMENT &#39;程序版本号&#39;, &#96;version_name&#96; string COMMENT &#39;程序版本名&#39;, &#96;lang&#96; string COMMENT &#39;系统语言&#39;, &#96;source&#96; string COMMENT &#39;渠道号&#39;, &#96;os&#96; string COMMENT &#39;系统版本&#39;, &#96;area&#96; string COMMENT &#39;区域&#39;, &#96;model&#96; string COMMENT &#39;手机型号&#39;, &#96;brand&#96; string COMMENT &#39;手机品牌&#39;, &#96;sdk_version&#96; string COMMENT &#39;sdkVersion&#39;, &#96;gmail&#96; string COMMENT &#39;gmail&#39;, &#96;height_width&#96; string COMMENT &#39;屏幕宽高&#39;, &#96;app_time&#96; string COMMENT &#39;客户端日志产生时的时间&#39;, &#96;network&#96; string COMMENT &#39;网络模式&#39;, &#96;lng&#96; string COMMENT &#39;经度&#39;, &#96;lat&#96; string COMMENT &#39;纬度&#39;, &#96;entry&#96; string COMMENT &#39;入口: push&#x3D;1,widget&#x3D;2,icon&#x3D;3,notification&#x3D;4,lockscreen_widget&#x3D;5&#39;, &#96;open_ad_type&#96; string COMMENT &#39;开屏广告类型: 开屏原生广告&#x3D;1, 开屏插屏广告&#x3D;2&#39;, &#96;action&#96; string COMMENT &#39;状态：成功&#x3D;1 失败&#x3D;2&#39;, &#96;loading_time&#96; string COMMENT &#39;加载时长&#39;, &#96;detail&#96; string COMMENT &#39;失败码&#39;, &#96;extend1&#96; string COMMENT &#39;失败的 message&#39; ) COMMENT &#39;启动日志表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;start_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;start_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select get_json_object(line,&#39;$.mid&#39;) mid_id, get_json_object(line,&#39;$.uid&#39;) user_id, get_json_object(line,&#39;$.vc&#39;) version_code, get_json_object(line,&#39;$.vn&#39;) version_name, get_json_object(line,&#39;$.l&#39;) lang, get_json_object(line,&#39;$.sr&#39;) source, get_json_object(line,&#39;$.os&#39;) os, get_json_object(line,&#39;$.ar&#39;) area, get_json_object(line,&#39;$.md&#39;) model, get_json_object(line,&#39;$.ba&#39;) brand, get_json_object(line,&#39;$.sv&#39;) sdk_version, get_json_object(line,&#39;$.g&#39;) gmail, get_json_object(line,&#39;$.hw&#39;) height_width, get_json_object(line,&#39;$.t&#39;) app_time, get_json_object(line,&#39;$.nw&#39;) network, get_json_object(line,&#39;$.ln&#39;) lng, get_json_object(line,&#39;$.la&#39;) lat, get_json_object(line,&#39;$.entry&#39;) entry, get_json_object(line,&#39;$.open_ad_type&#39;) open_ad_type, get_json_object(line,&#39;$.action&#39;) action, get_json_object(line,&#39;$.loading_time&#39;) loading_time, get_json_object(line,&#39;$.detail&#39;) detail, get_json_object(line,&#39;$.extend1&#39;) extend1from $hive_origin_table_namewhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;9.1.2 事件日志表建表123456789101112131415161718192021222324252627282930drop table if exists dwd.mall__event_logCREATE EXTERNAL TABLE &#96;dwd.mall__event_log&#96;( &#96;mid_id&#96; string COMMENT &#39;设备唯一标识&#39;, &#96;user_id&#96; string COMMENT &#39;用户标识&#39;, &#96;version_code&#96; string COMMENT &#39;程序版本号&#39;, &#96;version_name&#96; string COMMENT &#39;程序版本名&#39;, &#96;lang&#96; string COMMENT &#39;系统语言&#39;, &#96;source&#96; string COMMENT &#39;渠道号&#39;, &#96;os&#96; string COMMENT &#39;系统版本&#39;, &#96;area&#96; string COMMENT &#39;区域&#39;, &#96;model&#96; string COMMENT &#39;手机型号&#39;, &#96;brand&#96; string COMMENT &#39;手机品牌&#39;, &#96;sdk_version&#96; string COMMENT &#39;sdkVersion&#39;, &#96;gmail&#96; string COMMENT &#39;gmail&#39;, &#96;height_width&#96; string COMMENT &#39;屏幕宽高&#39;, &#96;app_time&#96; string COMMENT &#39;客户端日志产生时的时间&#39;, &#96;network&#96; string COMMENT &#39;网络模式&#39;, &#96;lng&#96; string COMMENT &#39;经度&#39;, &#96;lat&#96; string COMMENT &#39;纬度&#39;, &#96;event_name&#96; string COMMENT &#39;事件名称&#39;, &#96;event_json&#96; string COMMENT &#39;事件详情&#39;, &#96;server_time&#96; string COMMENT &#39;服务器时间&#39; ) COMMENT &#39;事件日志表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;event_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)9.2.1 制作 UDF UDTFudf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import org.json.JSONException;import org.json.JSONObject;public class BaseFieldUDF extends UDF &#123; public String evaluate(String line, String key) throws JSONException &#123; String[] log &#x3D; line.split(&quot;\\\\|&quot;); if (log.length !&#x3D; 2 || StringUtils.isBlank(log[1])) &#123; return &quot;&quot;; &#125; JSONObject baseJson &#x3D; new JSONObject(log[1].trim()); String result &#x3D; &quot;&quot;; &#x2F;&#x2F; 获取服务器时间 if (&quot;st&quot;.equals(key)) &#123; result &#x3D; log[0].trim(); &#125; else if (&quot;et&quot;.equals(key)) &#123; &#x2F;&#x2F; 获取事件数组 if (baseJson.has(&quot;et&quot;)) &#123; result &#x3D; baseJson.getString(&quot;et&quot;); &#125; &#125; else &#123; JSONObject cm &#x3D; baseJson.getJSONObject(&quot;cm&quot;); &#x2F;&#x2F; 获取 key 对应公共字段的 value if (cm.has(key)) &#123; result &#x3D; cm.getString(key); &#125; &#125; return result; &#125; public static void main(String[] args) throws JSONException &#123; String line &#x3D; &quot; 1588319303710|&#123;\\n&quot; + &quot; \\&quot;cm\\&quot;:&#123;\\n&quot; + &quot; \\&quot;ln\\&quot;:\\&quot;-51.5\\&quot;,\\&quot;sv\\&quot;:\\&quot;V2.0.7\\&quot;,\\&quot;os\\&quot;:\\&quot;8.0.8\\&quot;,\\&quot;g\\&quot;:\\&quot;L1470998@gmail.com\\&quot;,\\&quot;mid\\&quot;:\\&quot;13\\&quot;,\\n&quot; + &quot; \\&quot;nw\\&quot;:\\&quot;4G\\&quot;,\\&quot;l\\&quot;:\\&quot;en\\&quot;,\\&quot;vc\\&quot;:\\&quot;7\\&quot;,\\&quot;hw\\&quot;:\\&quot;640*960\\&quot;,\\&quot;ar\\&quot;:\\&quot;MX\\&quot;,\\&quot;uid\\&quot;:\\&quot;13\\&quot;,\\&quot;t\\&quot;:\\&quot;1588291826938\\&quot;,\\n&quot; + &quot; \\&quot;la\\&quot;:\\&quot;-38.2\\&quot;,\\&quot;md\\&quot;:\\&quot;Huawei-14\\&quot;,\\&quot;vn\\&quot;:\\&quot;1.3.6\\&quot;,\\&quot;ba\\&quot;:\\&quot;Huawei\\&quot;,\\&quot;sr\\&quot;:\\&quot;Y\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;ap\\&quot;:\\&quot;app\\&quot;,\\n&quot; + &quot; \\&quot;et\\&quot;:[&#123;\\n&quot; + &quot; \\&quot;ett\\&quot;:\\&quot;1588228193191\\&quot;,\\&quot;en\\&quot;:\\&quot;ad\\&quot;,\\&quot;kv\\&quot;:&#123;\\&quot;activityId\\&quot;:\\&quot;1\\&quot;,\\&quot;displayMills\\&quot;:\\&quot;113201\\&quot;,\\&quot;entry\\&quot;:\\&quot;3\\&quot;,\\&quot;action\\&quot;:\\&quot;5\\&quot;,\\&quot;contentType\\&quot;:\\&quot;0\\&quot;&#125;\\n&quot; + &quot; &#125;,&#123;\\n&quot; + &quot; \\&quot;ett\\&quot;:\\&quot;1588300304713\\&quot;,\\&quot;en\\&quot;:\\&quot;notification\\&quot;,\\&quot;kv\\&quot;:&#123;\\&quot;ap_time\\&quot;:\\&quot;1588277440794\\&quot;,\\&quot;action\\&quot;:\\&quot;2\\&quot;,\\&quot;type\\&quot;:\\&quot;3\\&quot;,\\&quot;content\\&quot;:\\&quot;\\&quot;&#125;\\n&quot; + &quot; &#125;,&#123;\\n&quot; + &quot; \\&quot;ett\\&quot;:\\&quot;1588249203743\\&quot;,\\&quot;en\\&quot;:\\&quot;active_background\\&quot;,\\&quot;kv\\&quot;:&#123;\\&quot;active_source\\&quot;:\\&quot;3\\&quot;&#125;\\n&quot; + &quot; &#125;,&#123;\\n&quot; + &quot; \\&quot;ett\\&quot;:\\&quot;1588225856101\\&quot;,\\&quot;en\\&quot;:\\&quot;comment\\&quot;,\\&quot;kv\\&quot;:&#123;\\&quot;p_comment_id\\&quot;:0,\\&quot;addtime\\&quot;:\\&quot;1588263895040\\&quot;,\\&quot;praise_count\\&quot;:231,\\&quot;other_id\\&quot;:5,\\&quot;comment_id\\&quot;:5,\\&quot;reply_count\\&quot;:62,\\&quot;userid\\&quot;:7,\\&quot;content\\&quot;:\\&quot;骸汞\\&quot;&#125;\\n&quot; + &quot; &#125;,&#123;\\n&quot; + &quot; \\&quot;ett\\&quot;:\\&quot;1588254200122\\&quot;,\\&quot;en\\&quot;:\\&quot;favorites\\&quot;,\\&quot;kv\\&quot;:&#123;\\&quot;course_id\\&quot;:5,\\&quot;id\\&quot;:0,\\&quot;add_time\\&quot;:\\&quot;1588264138625\\&quot;,\\&quot;userid\\&quot;:0&#125;\\n&quot; + &quot; &#125;,&#123;\\n&quot; + &quot; \\&quot;ett\\&quot;:\\&quot;1588281152824\\&quot;,\\&quot;en\\&quot;:\\&quot;praise\\&quot;,\\&quot;kv\\&quot;:&#123;\\&quot;target_id\\&quot;:4,\\&quot;id\\&quot;:3,\\&quot;type\\&quot;:3,\\&quot;add_time\\&quot;:\\&quot;1588307696417\\&quot;,\\&quot;userid\\&quot;:8&#125;\\n&quot; + &quot; &#125;]\\n&quot; + &quot; &#125;&quot;; String s &#x3D; new BaseFieldUDF().evaluate(line, &quot;mid&quot;); String ss &#x3D; new BaseFieldUDF().evaluate(line, &quot;st&quot;); String sss &#x3D; new BaseFieldUDF().evaluate(line, &quot;et&quot;); System.out.println(s); System.out.println(ss); System.out.println(sss); &#125;&#125;结果：131588319303710[{“ett”:”1588228193191”,”en”:”ad”,”kv”:{“activityId”:”1”,”displayMills”:”113201”,”entry”:”3”,”action”:”5”,”contentType”:”0”}},{“ett”:”1588300304713”,”en”:”notification”,”kv”:{“ap_time”:”1588277440794”,”action”:”2”,”type”:”3”,”content”:””}},{“ett”:”1588249203743”,”en”:”active_background”,”kv”:{“active_source”:”3”}},{“ett”:”1588225856101”,”en”:”comment”,”kv”:{“p_comment_id”:0,”addtime”:”1588263895040”,”praise_count”:231,”other_id”:5,”comment_id”:5,”reply_count”:62,”userid”:7,”content”:”骸汞”}},{“ett”:”1588254200122”,”en”:”favorites”,”kv”:{“course_id”:5,”id”:0,”add_time”:”1588264138625”,”userid”:0}},{“ett”:”1588281152824”,”en”:”praise”,”kv”:{“target_id”:4,”id”:3,”type”:3,”add_time”:”1588307696417”,”userid”:8}}]udtf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;import org.json.JSONArray;import org.json.JSONException;import java.util.ArrayList;public class EventJsonUDTF extends GenericUDTF &#123; &#x2F;&#x2F;该方法中，我们将指定输出参数的名称和参数类型： public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException &#123; ArrayList&lt;String&gt; fieldNames &#x3D; new ArrayList&lt;String&gt;(); ArrayList&lt;ObjectInspector&gt; fieldOIs &#x3D; new ArrayList&lt;ObjectInspector&gt;(); fieldNames.add(&quot;event_name&quot;); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldNames.add(&quot;event_json&quot;); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); &#125; &#x2F;&#x2F;输入 1 条记录，输出若干条结果 @Override public void process(Object[] objects) throws HiveException &#123; &#x2F;&#x2F; 获取传入的 et String input &#x3D; objects[0].toString(); &#x2F;&#x2F; 如果传进来的数据为空，直接返回过滤掉该数据 if (StringUtils.isBlank(input)) &#123; return; &#125; else &#123; try &#123; &#x2F;&#x2F; 获取一共有几个事件（ad&#x2F;facoriters） JSONArray ja &#x3D; new JSONArray(input); if (ja &#x3D;&#x3D; null) return; &#x2F;&#x2F; 循环遍历每一个事件 for (int i &#x3D; 0; i &lt; ja.length(); i++) &#123; String[] result &#x3D; new String[2]; try &#123; &#x2F;&#x2F; 取出每个的事件名称（ad&#x2F;facoriters） result[0] &#x3D; ja.getJSONObject(i).getString(&quot;en&quot;); &#x2F;&#x2F; 取出每一个事件整体 result[1] &#x3D; ja.getString(i); &#125; catch (JSONException e) &#123; continue; &#125; &#x2F;&#x2F; 将结果返回 forward(result); &#125; &#125; catch (JSONException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#x2F;&#x2F;当没有记录处理的时候该方法会被调用，用来清理代码或者产生额外的输出 @Override public void close() throws HiveException &#123; &#125;&#125;9.1.2.2 直接永久使用UDF上传UDF资源将hive-function-1.0-SNAPSHOT包传到HDFS 的/user/hive/jars下12hadoop dfs -mkdir &#x2F;user&#x2F;hive&#x2F;jarshadoop dfs -put hive-function-1.0-SNAPSHOT.jar &#x2F;user&#x2F;hive&#x2F;jars&#x2F;hive-function-1.0-SNAPSHOT.jar在hive中创建永久UDF12create function base_analizer as &#39;com.heaton.bigdata.udf.BaseFieldUDF&#39; using jar &#39;hdfs:&#x2F;&#x2F;cdh01.cm:8020&#x2F;user&#x2F;hive&#x2F;jars&#x2F;hive-function-1.0-SNAPSHOT.jar&#39;;create function flat_analizer as &#39;com.heaton.bigdata.udtf.EventJsonUDTF&#39; using jar &#39;hdfs:&#x2F;&#x2F;cdh01.cm:8020&#x2F;user&#x2F;hive&#x2F;jars&#x2F;hive-function-1.0-SNAPSHOT.jar&#39;;9.1.2.3 Dolphin使用方式UDF在DAG图创建SQL工具中选择对应UDF函数即可使用，但是目前Dolphin1.2.0中关联函数操作保存无效。大家可以使用UDF管理功能将JAR传入到HDFS上，这样通过脚本加入临时函数，也可以很好的完成功能。临时函数语句：12create temporary function base_analizer as &#39;com.heaton.bigdata.udf.BaseFieldUDF&#39; using jar &#39;hdfs:&#x2F;&#x2F;cdh01.cm:8020&#x2F;dolphinscheduler&#x2F;dolphinscheduler&#x2F;udfs&#x2F;hive-function-1.0-SNAPSHOT.jar&#39;;create temporary function flat_analizer as &#39;com.heaton.bigdata.udtf.EventJsonUDTF&#39; using jar &#39;hdfs:&#x2F;&#x2F;cdh01.cm:8020&#x2F;dolphinscheduler&#x2F;dolphinscheduler&#x2F;udfs&#x2F;hive-function-1.0-SNAPSHOT.jar&#39;;9.2.4 数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;event_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select base_analizer(line,&#39;mid&#39;) as mid_id, base_analizer(line,&#39;uid&#39;) as user_id, base_analizer(line,&#39;vc&#39;) as version_code, base_analizer(line,&#39;vn&#39;) as version_name, base_analizer(line,&#39;l&#39;) as lang, base_analizer(line,&#39;sr&#39;) as source, base_analizer(line,&#39;os&#39;) as os, base_analizer(line,&#39;ar&#39;) as area, base_analizer(line,&#39;md&#39;) as model, base_analizer(line,&#39;ba&#39;) as brand, base_analizer(line,&#39;sv&#39;) as sdk_version, base_analizer(line,&#39;g&#39;) as gmail, base_analizer(line,&#39;hw&#39;) as height_width, base_analizer(line,&#39;t&#39;) as app_time, base_analizer(line,&#39;nw&#39;) as network, base_analizer(line,&#39;ln&#39;) as lng, base_analizer(line,&#39;la&#39;) as lat, event_name, event_json, base_analizer(line,&#39;st&#39;) as server_timefrom $hive_origin_table_name lateral view flat_analizer(base_analizer(line,&#39;et&#39;)) tmp_flat as event_name,event_jsonwhere dt&#x3D;&#39;$db_date&#39; and base_analizer(line,&#39;et&#39;)&lt;&gt;&#39;&#39;;&quot;$hive -e &quot;$sql&quot;9.1.3 商品点击表建表123456789101112131415161718192021222324252627282930313233drop table if exists dwd.mall__display_logCREATE EXTERNAL TABLE &#96;dwd.mall__display_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;action&#96; string, &#96;goodsid&#96; string, &#96;place&#96; string, &#96;extend1&#96; string, &#96;category&#96; string, &#96;server_time&#96; string ) COMMENT &#39;商品点击表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;display_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;display_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;) select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.action&#39;) action, get_json_object(event_json,&#39;$.kv.goodsid&#39;) goodsid, get_json_object(event_json,&#39;$.kv.place&#39;) place, get_json_object(event_json,&#39;$.kv.extend1&#39;) extend1, get_json_object(event_json,&#39;$.kv.category&#39;) category, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;display&#39;;&quot;$hive -e &quot;$sql&quot;9.1.4 商品列表表建表1234567891011121314151617181920212223242526272829303132333435drop table if exists dwd.mall__loading_logCREATE EXTERNAL TABLE &#96;dwd.mall__loading_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;action&#96; string, &#96;loading_time&#96; string, &#96;loading_way&#96; string, &#96;extend1&#96; string, &#96;extend2&#96; string, &#96;type&#96; string, &#96;type1&#96; string, &#96;server_time&#96; string ) COMMENT &#39;商品列表表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;loading_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;loading_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.action&#39;) action, get_json_object(event_json,&#39;$.kv.loading_time&#39;) loading_time, get_json_object(event_json,&#39;$.kv.loading_way&#39;) loading_way, get_json_object(event_json,&#39;$.kv.extend1&#39;) extend1, get_json_object(event_json,&#39;$.kv.extend2&#39;) extend2, get_json_object(event_json,&#39;$.kv.type&#39;) type, get_json_object(event_json,&#39;$.kv.type1&#39;) type1, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;loading&#39;;&quot;$hive -e &quot;$sql&quot;9.1.5 广告表建表12345678910111213141516171819202122232425262728293031323334drop table if exists dwd.mall__ad_logCREATE EXTERNAL TABLE &#96;dwd.mall__ad_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;entry&#96; string, &#96;action&#96; string, &#96;contentType&#96; string, &#96;displayMills&#96; string, &#96;itemId&#96; string, &#96;activityId&#96; string, &#96;server_time&#96; string ) COMMENT &#39;广告表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;ad_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;ad_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.entry&#39;) entry, get_json_object(event_json,&#39;$.kv.action&#39;) action, get_json_object(event_json,&#39;$.kv.contentType&#39;) contentType, get_json_object(event_json,&#39;$.kv.displayMills&#39;) displayMills, get_json_object(event_json,&#39;$.kv.itemId&#39;) itemId, get_json_object(event_json,&#39;$.kv.activityId&#39;) activityId, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;db_date&#39; and event_name&#x3D;&#39;ad&#39;;&quot;$hive -e &quot;$sql&quot;9.1.6 消息通知表建表1234567891011121314151617181920212223242526272829303132drop table if exists dwd.mall__notification_logCREATE EXTERNAL TABLE &#96;dwd.mall__notification_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;action&#96; string, &#96;noti_type&#96; string, &#96;ap_time&#96; string, &#96;content&#96; string, &#96;server_time&#96; string ) COMMENT &#39;消息通知表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;notification_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入123456789101112131415161718192021222324252627282930313233343536373839404142434445#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;notification_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.action&#39;) action, get_json_object(event_json,&#39;$.kv.noti_type&#39;) noti_type, get_json_object(event_json,&#39;$.kv.ap_time&#39;) ap_time, get_json_object(event_json,&#39;$.kv.content&#39;) content, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;notification&#39;;&quot;$hive -e &quot;$sql&quot;9.1.7 用户后台活跃表建表1234567891011121314151617181920212223242526272829drop table if exists dwd.mall__active_background_logCREATE EXTERNAL TABLE &#96;dwd.mall__active_background_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;active_source&#96; string, &#96;server_time&#96; string ) COMMENT &#39;用户后台活跃表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;active_background_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入123456789101112131415161718192021222324252627282930313233343536373839404142#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;active_background_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.active_source&#39;) active_source, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;active_background&#39;;&quot;$hive -e &quot;$sql&quot;9.1.8 评论表建表123456789101112131415161718192021222324252627282930313233343536drop table if exists dwd.mall__comment_logCREATE EXTERNAL TABLE &#96;dwd.mall__comment_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;comment_id&#96; int, &#96;userid&#96; int, &#96;p_comment_id&#96; int, &#96;content&#96; string, &#96;addtime&#96; string, &#96;other_id&#96; int, &#96;praise_count&#96; int, &#96;reply_count&#96; int, &#96;server_time&#96; string ) COMMENT &#39;评论表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;comment_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;comment_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.comment_id&#39;) comment_id, get_json_object(event_json,&#39;$.kv.userid&#39;) userid, get_json_object(event_json,&#39;$.kv.p_comment_id&#39;) p_comment_id, get_json_object(event_json,&#39;$.kv.content&#39;) content, get_json_object(event_json,&#39;$.kv.addtime&#39;) addtime, get_json_object(event_json,&#39;$.kv.other_id&#39;) other_id, get_json_object(event_json,&#39;$.kv.praise_count&#39;) praise_count, get_json_object(event_json,&#39;$.kv.reply_count&#39;) reply_count, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;comment&#39;;&quot;$hive -e &quot;$sql&quot;9.1.9 收藏表建表1234567891011121314151617181920212223242526272829303132drop table if exists dwd.mall__favorites_logCREATE EXTERNAL TABLE &#96;dwd.mall__favorites_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;id&#96; int, &#96;course_id&#96; int, &#96;userid&#96; int, &#96;add_time&#96; string, &#96;server_time&#96; string ) COMMENT &#39;收藏表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;favorites_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入123456789101112131415161718192021222324252627282930313233343536373839404142434445#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;favorites_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.id&#39;) id, get_json_object(event_json,&#39;$.kv.course_id&#39;) course_id, get_json_object(event_json,&#39;$.kv.userid&#39;) userid, get_json_object(event_json,&#39;$.kv.add_time&#39;) add_time, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;favorites&#39;;&quot;$hive -e &quot;$sql&quot;9.1.10 点赞表建表123456789101112131415161718192021222324252627282930313233drop table if exists dwd.mall__praise_logCREATE EXTERNAL TABLE &#96;dwd.mall__praise_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;id&#96; string, &#96;userid&#96; string, &#96;target_id&#96; string, &#96;type&#96; string, &#96;add_time&#96; string, &#96;server_time&#96; string ) COMMENT &#39;点赞表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;praise_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;praise_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.id&#39;) id, get_json_object(event_json,&#39;$.kv.userid&#39;) userid, get_json_object(event_json,&#39;$.kv.target_id&#39;) target_id, get_json_object(event_json,&#39;$.kv.type&#39;) type, get_json_object(event_json,&#39;$.kv.add_time&#39;) add_time, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;praise&#39;;&quot;$hive -e &quot;$sql&quot;9.1.11 错误日志表建表123456789101112131415161718192021222324252627282930drop table if exists dwd.mall__error_logCREATE EXTERNAL TABLE &#96;dwd.mall__error_log&#96;( &#96;mid_id&#96; string, &#96;user_id&#96; string, &#96;version_code&#96; string, &#96;version_name&#96; string, &#96;lang&#96; string, &#96;source&#96; string, &#96;os&#96; string, &#96;area&#96; string, &#96;model&#96; string, &#96;brand&#96; string, &#96;sdk_version&#96; string, &#96;gmail&#96; string, &#96;height_width&#96; string, &#96;app_time&#96; string, &#96;network&#96; string, &#96;lng&#96; string, &#96;lat&#96; string, &#96;errorBrief&#96; string, &#96;errorDetail&#96; string, &#96;server_time&#96; string ) COMMENT &#39;错误日志表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;error_log&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdAPP3&#x3D;odstable_name&#x3D;error_loghive_table_name&#x3D;$APP2.mall__$table_namehive_origin_table_name&#x3D;$APP3.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, user_id, version_code, version_name, lang, source, os, area, model, brand, sdk_version, gmail, height_width, app_time, network, lng, lat, get_json_object(event_json,&#39;$.kv.errorBrief&#39;) errorBrief, get_json_object(event_json,&#39;$.kv.errorDetail&#39;) errorDetail, server_timefrom dwd.mall__event_logwhere dt&#x3D;&#39;$db_date&#39; and event_name&#x3D;&#39;error&#39;;&quot;$hive -e &quot;$sql&quot;9.2 dwd层构建(业务库)此层在构建之初，增量表需要动态分区来划分时间，将数据放入指定分区事实/维度时间用户地区商品优惠卷活动编码度量订单√√√√件数/金额订单详情√√√件数/金额支付√√次数/金额加入购物车√√√件数/金额收藏√√√个数评价√√√个数退款√√√件数/金额优惠卷领用√√√个数9.2.1 商品维度表(全量)建表123456789101112131415161718192021222324252627drop table if exists dwd.mall__dim_sku_info CREATE EXTERNAL TABLE &#96;dwd.mall__dim_sku_info&#96;(&#96;id&#96; string COMMENT &#39;商品 id&#39;,&#96;spu_id&#96; string COMMENT &#39;spuid&#39;,&#96;price&#96; double COMMENT &#39;商品价格&#39;,&#96;sku_name&#96; string COMMENT &#39;商品名称&#39;,&#96;sku_desc&#96; string COMMENT &#39;商品描述&#39;,&#96;weight&#96; double COMMENT &#39;重量&#39;,&#96;tm_id&#96; string COMMENT &#39;品牌 id&#39;,&#96;tm_name&#96; string COMMENT &#39;品牌名称&#39;,&#96;category3_id&#96; string COMMENT &#39;三级分类 id&#39;,&#96;category2_id&#96; string COMMENT &#39;二级分类 id&#39;,&#96;category1_id&#96; string COMMENT &#39;一级分类 id&#39;,&#96;category3_name&#96; string COMMENT &#39;三级分类名称&#39;,&#96;category2_name&#96; string COMMENT &#39;二级分类名称&#39;,&#96;category1_name&#96; string COMMENT &#39;一级分类名称&#39;,&#96;spu_name&#96; string COMMENT &#39;spu 名称&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39; ) COMMENT &#39;商品维度表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_sku_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_sku_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select sku.id, sku.spu_id, sku.price, sku.sku_name, sku.sku_desc, sku.weight, sku.tm_id, ob.tm_name, sku.category3_id, c2.id category2_id, c1.id category1_id, c3.name category3_name, c2.name category2_name, c1.name category1_name, spu.spu_name, from_unixtime(cast(sku.create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_timefrom(select * from ods.mall__sku_info where dt&#x3D;&#39;$db_date&#39;)skujoin(select * from ods.mall__base_trademark where dt&#x3D;&#39;$db_date&#39;)ob on sku.tm_id&#x3D;ob.tm_idjoin(select * from ods.mall__spu_info where dt&#x3D;&#39;$db_date&#39;)spu on spu.id &#x3D; sku.spu_idjoin(select * from ods.mall__base_category3 where dt&#x3D;&#39;$db_date&#39;)c3 on sku.category3_id&#x3D;c3.idjoin(select * from ods.mall__base_category2 where dt&#x3D;&#39;$db_date&#39;)c2 on c3.category2_id&#x3D;c2.idjoin(select * from ods.mall__base_category1 where dt&#x3D;&#39;$db_date&#39;)c1 on c2.category1_id&#x3D;c1.id;&quot;$hive -e &quot;$sql&quot;9.2.2 优惠券信息维度表(全量)建表123456789101112131415161718192021222324252627drop table if exists dwd.mall__dim_coupon_info CREATE EXTERNAL TABLE &#96;dwd.mall__dim_coupon_info&#96;(&#96;id&#96; string COMMENT &#39;购物券编号&#39;,&#96;coupon_name&#96; string COMMENT &#39;购物券名称&#39;,&#96;coupon_type&#96; string COMMENT &#39;购物券类型 1 现金券 2 折扣券 3 满减券 4 满件打折券&#39;,&#96;condition_amount&#96; string COMMENT &#39;满额数&#39;,&#96;condition_num&#96; string COMMENT &#39;满件数&#39;,&#96;activity_id&#96; string COMMENT &#39;活动编号&#39;,&#96;benefit_amount&#96; string COMMENT &#39;减金额&#39;,&#96;benefit_discount&#96; string COMMENT &#39;折扣&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,&#96;range_type&#96; string COMMENT &#39;范围类型 1、商品 2、品类 3、品牌&#39;,&#96;spu_id&#96; string COMMENT &#39;商品 id&#39;,&#96;tm_id&#96; string COMMENT &#39;品牌 id&#39;,&#96;category3_id&#96; string COMMENT &#39;品类 id&#39;,&#96;limit_num&#96; string COMMENT &#39;最多领用次数&#39;,&#96;operate_time&#96; string COMMENT &#39;修改时间&#39;,&#96;expire_time&#96; string COMMENT &#39;过期时间&#39; ) COMMENT &#39;优惠券信息维度表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_coupon_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031323334353637#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_coupon_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select id, coupon_name, coupon_type, condition_amount, condition_num, activity_id, benefit_amount, benefit_discount, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, range_type, spu_id, tm_id, category3_id, limit_num, from_unixtime(cast(operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) operate_time, from_unixtime(cast(expire_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) expire_timefrom ods.mall__coupon_infowhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;9.2.3 活动维度表(全量)建表12345678910111213141516171819202122drop table if exists dwd.mall__dim_activity_info CREATE EXTERNAL TABLE &#96;dwd.mall__dim_activity_info&#96;(&#96;id&#96; string COMMENT &#39;编号&#39;,&#96;activity_name&#96; string COMMENT &#39;活动名称&#39;,&#96;activity_type&#96; string COMMENT &#39;活动类型&#39;,&#96;condition_amount&#96; string COMMENT &#39;满减金额&#39;,&#96;condition_num&#96; string COMMENT &#39;满减件数&#39;,&#96;benefit_amount&#96; string COMMENT &#39;优惠金额&#39;,&#96;benefit_discount&#96; string COMMENT &#39;优惠折扣&#39;,&#96;benefit_level&#96; string COMMENT &#39;优惠级别&#39;,&#96;start_time&#96; string COMMENT &#39;开始时间&#39;,&#96;end_time&#96; string COMMENT &#39;结束时间&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39; ) COMMENT &#39;活动维度表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_activity_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入1234567891011121314151617181920212223242526272829303132333435363738#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_activity_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select info.id, info.activity_name, info.activity_type, rule.condition_amount, rule.condition_num, rule.benefit_amount, rule.benefit_discount, rule.benefit_level, from_unixtime(cast(info.start_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) start_time, from_unixtime(cast(info.end_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) end_time, from_unixtime(cast(info.create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_timefrom(select * from ods.mall__activity_info where dt&#x3D;&#39;$db_date&#39;)infoleft join(select * from ods.mall__activity_rule where dt&#x3D;&#39;$db_date&#39;)rule on info.id &#x3D; rule.activity_id;&quot;$hive -e &quot;$sql&quot;9.2.4 地区维度表(特殊)建表1234567891011121314151617drop table if exists dwd.mall__dim_base_province CREATE EXTERNAL TABLE &#96;dwd.mall__dim_base_province&#96;(&#96;id&#96; string COMMENT &#39;id&#39;,&#96;province_name&#96; string COMMENT &#39;省市名称&#39;,&#96;area_code&#96; string COMMENT &#39;地区编码&#39;,&#96;iso_code&#96; string COMMENT &#39;ISO 编码&#39;,&#96;region_id&#96; string COMMENT &#39;地区 id&#39;,&#96;region_name&#96; string COMMENT &#39;地区名称&#39; ) COMMENT &#39;地区维度表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_base_province&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_base_provincehive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select bp.id, bp.name, bp.area_code, bp.iso_code, bp.region_id, br.region_namefrom ods.mall__base_province bpjoin ods.mall__base_region bron bp.region_id&#x3D;br.id;&quot;$hive -e &quot;$sql&quot;9.2.5 时间维度表(特殊)建表1234567891011121314151617181920drop table if exists dwd.mall__dim_date_info CREATE EXTERNAL TABLE &#96;dwd.mall__dim_date_info&#96;(&#96;date_id&#96; string COMMENT &#39;日&#39;,&#96;week_id&#96; int COMMENT &#39;周&#39;,&#96;week_day&#96; int COMMENT &#39;周的第几天&#39;,&#96;day&#96; int COMMENT &#39;每月的第几天&#39;,&#96;month&#96; int COMMENT &#39;第几月&#39;,&#96;quarter&#96; int COMMENT &#39;第几季度&#39;,&#96;year&#96; int COMMENT &#39;年&#39;,&#96;is_workday&#96; int COMMENT &#39;是否是周末&#39;,&#96;holiday_id&#96; int COMMENT &#39;是否是节假日&#39; ) COMMENT &#39;时间维度表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_date_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入1234567891011121314151617181920212223242526272829#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_date_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select date_id, week_id, week_day, day, month, quarter, year, is_workday, holiday_idfrom ods.mall__date_info&quot;$hive -e &quot;$sql&quot;9.2.6 用户维度表(新增及变化-缓慢变化维-拉链表)9.2.6.1 拉链表介绍拉链表，记录每条信息的生命周期，一旦一条记录的生命周期结束，就重新开始一条新的记录，并把当前日期放入生效开始日期。如果当前信息至今有效，在生效结束日期中填入一个极大值（如:9999-99-99）,下表为张三的手机号变化例子用户ID姓名手机号开始日期结束日期1张三134XXXX50502019-01-012019-01-021张三139XXXX32322019-01-032020-01-011张三137XXXX76762020-01-029999-99-99适合场景：数据会发生变化，但是大部分不变（即：缓慢变化维）比如：用户信息发生变化，但是每天变化比例不高，按照每日全量，则效率低如何使用拉链表：通过–&gt;生效开始日期&lt;=某个日期 且 生效结束日期&gt;=某个日期，能够得到某个时间点的数据全量切片。拉链表形成过程制作流程用户当日全部数据和MySQL中每天变化的数据拼接在一起，形成一个&lt;新的临时拉链表。用临时拉链表覆盖旧的拉链表数据。从而解决Hive中数据不能更新的问题9.2.6.2 用户维度表用户表中的数据每日既有可能新增，也有可能修改，属于缓慢变化维度，此处采用拉链表存储用户维度数据。建表123456789101112131415161718drop table if exists dwd.mall__dim_user_info_his CREATE EXTERNAL TABLE &#96;dwd.mall__dim_user_info_his&#96;(&#96;id&#96; string COMMENT &#39;用户 id&#39;,&#96;name&#96; string COMMENT &#39;姓名&#39;,&#96;birthday&#96; string COMMENT &#39;生日&#39;,&#96;gender&#96; string COMMENT &#39;性别&#39;,&#96;email&#96; string COMMENT &#39;邮箱&#39;,&#96;user_level&#96; string COMMENT &#39;用户等级&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,&#96;operate_time&#96; string COMMENT &#39;操作时间&#39;,&#96;start_date&#96; string COMMENT &#39;有效开始日期&#39;,&#96;end_date&#96; string COMMENT &#39;有效结束日期&#39; ) COMMENT &#39;用户拉链表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_user_info_his&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)临时表建表(结构与主表相同)123456789101112131415161718drop table if exists dwd.mall__dim_user_info_his_tmp CREATE EXTERNAL TABLE &#96;dwd.mall__dim_user_info_his_tmp&#96;(&#96;id&#96; string COMMENT &#39;用户 id&#39;,&#96;name&#96; string COMMENT &#39;姓名&#39;,&#96;birthday&#96; string COMMENT &#39;生日&#39;,&#96;gender&#96; string COMMENT &#39;性别&#39;,&#96;email&#96; string COMMENT &#39;邮箱&#39;,&#96;user_level&#96; string COMMENT &#39;用户等级&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,&#96;operate_time&#96; string COMMENT &#39;操作时间&#39;,&#96;start_date&#96; string COMMENT &#39;有效开始日期&#39;,&#96;end_date&#96; string COMMENT &#39;有效结束日期&#39; ) COMMENT &#39;用户拉链表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;dim_user_info_his_tmp&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)首先（主表）数据初始化，只做一次123456789101112131415161718192021222324252627282930#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_user_info_hishive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect id, name, from_unixtime(cast(birthday&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) birthday, gender, email, user_level, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, from_unixtime(cast(operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) operate_time, &#39;$db_date&#39;, &#39;9999-99-99&#39;from ods.mall__user_info oiwhere oi.dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;临时表数据计算导入(在主表数据之后执行)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_user_info_his_tmphive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect* from( --查询当前时间的所有信息 select cast(id as string) id, name, from_unixtime(cast(birthday&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) birthday, gender, email, user_level, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, from_unixtime(cast(operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) operate_time, &#39;$db_date&#39; start_date, &#39;9999-99-99&#39; end_date from ods.mall__user_info where dt&#x3D;&#39;$db_date&#39; union all --查询当前变化了的数据，修改日期 select uh.id, uh.name, from_unixtime(cast(uh.birthday&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) birthday, uh.gender, uh.email, uh.user_level, from_unixtime(cast(uh.create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, from_unixtime(cast(uh.operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) operate_time, uh.start_date, if(ui.id is not null and uh.end_date&#x3D;&#39;9999-99-99&#39;, date_add(ui.dt,-1),uh.end_date) end_date from dwd.mall__dim_user_info_his uh left join ( --查询当前时间的所有信息 select cast(id as string) id, name, from_unixtime(cast(birthday&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) birthday, gender, email, user_level, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, from_unixtime(cast(operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) operate_time, dt from ods.mall__user_info where dt&#x3D;&#39;$db_date&#39; ) ui on uh.id&#x3D;ui.id)hisorder by his.id, start_date;&quot;$hive -e &quot;$sql&quot;数据导入123456789101112131415161718#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;dim_user_info_hishive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect * from dwd.mall__dim_user_info_his_tmp;&quot;$hive -e &quot;$sql&quot;9.2.7 订单详情事实表(事务型快照事实表-新增)建表123456789101112131415161718192021drop table if exists dwd.mall__fact_order_detail CREATE EXTERNAL TABLE &#96;dwd.mall__fact_order_detail&#96;( &#96;id&#96; bigint COMMENT &#39;编号&#39;, &#96;order_id&#96; bigint COMMENT &#39;订单编号&#39;, &#96;user_id&#96; bigint COMMENT &#39;用户id&#39;, &#96;sku_id&#96; bigint COMMENT &#39;sku_id&#39;, &#96;sku_name&#96; string COMMENT &#39;sku名称&#39;, &#96;order_price&#96; decimal(10,2) COMMENT &#39;购买价格(下单时sku价格）&#39;, &#96;sku_num&#96; string COMMENT &#39;购买个数&#39;, &#96;create_time&#96; bigint COMMENT &#39;创建时间&#39;, &#96;province_id&#96; string COMMENT &#39;省份ID&#39;, &#96;total_amount&#96; decimal(20,2) COMMENT &#39;订单总金额&#39; ) COMMENT &#39;订单明细表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_order_detail&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入1234567891011121314151617181920212223242526272829303132#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_order_detailhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select od.id, od.order_id, od.user_id, od.sku_id, od.sku_name, od.order_price, od.sku_num, od.create_time, oi.province_id, od.order_price*od.sku_num from (select * from ods.mall__order_detail where dt&#x3D;&#39;$db_date&#39; ) od join (select * from ods.mall__order_info where dt&#x3D;&#39;$db_date&#39; ) oi on od.order_id&#x3D;oi.id;&quot;$hive -e &quot;$sql&quot;9.2.7 支付事实表(事务型快照事实表-新增)建表123456789101112131415161718192021drop table if exists dwd.mall__fact_payment_info CREATE EXTERNAL TABLE &#96;dwd.mall__fact_payment_info&#96;(&#96;id&#96; string COMMENT &#39;&#39;,&#96;out_trade_no&#96; string COMMENT &#39;对外业务编号&#39;,&#96;order_id&#96; string COMMENT &#39;订单编号&#39;,&#96;user_id&#96; string COMMENT &#39;用户编号&#39;,&#96;alipay_trade_no&#96; string COMMENT &#39;支付宝交易流水编号&#39;,&#96;payment_amount&#96; decimal(16,2) COMMENT &#39;支付金额&#39;,&#96;subject&#96; string COMMENT &#39;交易内容&#39;,&#96;payment_type&#96; string COMMENT &#39;支付类型&#39;,&#96;payment_time&#96; string COMMENT &#39;支付时间&#39;,&#96;province_id&#96; string COMMENT &#39;省份 ID&#39; ) COMMENT &#39;支付事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_payment_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入1234567891011121314151617181920212223242526272829303132333435363738#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_payment_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select pi.id, pi.out_trade_no, pi.order_id, pi.user_id, pi.alipay_trade_no, pi.total_amount, pi.subject, pi.payment_type, from_unixtime(cast(pi.payment_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) payment_time, oi.province_idfrom(select * from ods.mall__payment_info where dt&#x3D;&#39;$db_date&#39;)pijoin(select id, province_id from ods.mall__order_info where dt&#x3D;&#39;$db_date&#39;)oion pi.order_id &#x3D; oi.id;&quot;$hive -e &quot;$sql&quot;9.2.8 退款事实表(事务型快照事实表-新增)建表1234567891011121314151617181920drop table if exists dwd.mall__fact_order_refund_info CREATE EXTERNAL TABLE &#96;dwd.mall__fact_order_refund_info&#96;(&#96;id&#96; string COMMENT &#39;编号&#39;,&#96;user_id&#96; string COMMENT &#39;用户 ID&#39;,&#96;order_id&#96; string COMMENT &#39;订单 ID&#39;,&#96;sku_id&#96; string COMMENT &#39;商品 ID&#39;,&#96;refund_type&#96; string COMMENT &#39;退款类型&#39;,&#96;refund_num&#96; bigint COMMENT &#39;退款件数&#39;,&#96;refund_amount&#96; decimal(16,2) COMMENT &#39;退款金额&#39;,&#96;refund_reason_type&#96; string COMMENT &#39;退款原因类型&#39;,&#96;create_time&#96; string COMMENT &#39;退款时间&#39; ) COMMENT &#39;退款事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_order_refund_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入123456789101112131415161718192021222324252627282930#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_order_refund_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select id, user_id, order_id, sku_id, refund_type, refund_num, refund_amount, refund_reason_type, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_timefrom ods.mall__order_refund_infowhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;9.2.9 评价事实表(事务型快照事实表-新增)建表123456789101112131415161718drop table if exists dwd.mall__fact_comment_info CREATE EXTERNAL TABLE &#96;dwd.mall__fact_comment_info&#96;(&#96;id&#96; string COMMENT &#39;编号&#39;,&#96;user_id&#96; string COMMENT &#39;用户 ID&#39;,&#96;sku_id&#96; string COMMENT &#39;商品 sku&#39;,&#96;spu_id&#96; string COMMENT &#39;商品 spu&#39;,&#96;order_id&#96; string COMMENT &#39;订单 ID&#39;,&#96;appraise&#96; string COMMENT &#39;评价&#39;,&#96;create_time&#96; string COMMENT &#39;评价时间&#39; ) COMMENT &#39;评价事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_comment_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_comment_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select id, user_id, sku_id, spu_id, order_id, appraise, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_timefrom ods.mall__comment_infowhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;9.2.10 加购事实表(周期型快照事实表-全量)建表123456789101112131415161718192021drop table if exists dwd.mall__fact_cart_info CREATE EXTERNAL TABLE &#96;dwd.mall__fact_cart_info&#96;(&#96;id&#96; string COMMENT &#39;编号&#39;,&#96;user_id&#96; string COMMENT &#39;用户 id&#39;,&#96;sku_id&#96; string COMMENT &#39;skuid&#39;,&#96;cart_price&#96; string COMMENT &#39;放入购物车时价格&#39;,&#96;sku_num&#96; string COMMENT &#39;数量&#39;,&#96;sku_name&#96; string COMMENT &#39;sku 名称 (冗余)&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,&#96;operate_time&#96; string COMMENT &#39;修改时间&#39;,&#96;is_ordered&#96; string COMMENT &#39;是否已经下单。1 为已下单;0 为未下单&#39;,&#96;order_time&#96; string COMMENT &#39;下单时间&#39; ) COMMENT &#39;加购事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_cart_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728293031#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_cart_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select id, user_id, sku_id, cart_price, sku_num, sku_name, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, from_unixtime(cast(operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) operate_time, is_ordered, from_unixtime(cast(order_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) order_timefrom ods.mall__cart_infowhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;9.2.11 收藏事实表(周期型快照事实表-全量)建表123456789101112131415161718drop table if exists dwd.mall__fact_favor_info CREATE EXTERNAL TABLE &#96;dwd.mall__fact_favor_info&#96;(&#96;id&#96; string COMMENT &#39;编号&#39;,&#96;user_id&#96; string COMMENT &#39;用户 id&#39;,&#96;sku_id&#96; string COMMENT &#39;skuid&#39;,&#96;spu_id&#96; string COMMENT &#39;spuid&#39;,&#96;is_cancel&#96; string COMMENT &#39;是否取消&#39;,&#96;create_time&#96; string COMMENT &#39;收藏时间&#39;,&#96;cancel_time&#96; string COMMENT &#39;取消时间&#39; ) COMMENT &#39;收藏事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_favor_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入12345678910111213141516171819202122232425262728#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_favor_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select id, user_id, sku_id, spu_id, is_cancel, from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) create_time, from_unixtime(cast(cancel_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;) cancel_timefrom ods.mall__favor_infowhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;9.2.12 优惠券领用事实表(累积型快照事实表-新增及变化)建表12345678910111213141516171819drop table if exists dwd.mall__fact_coupon_use CREATE EXTERNAL TABLE &#96;dwd.mall__fact_coupon_use&#96;(&#96;&#96; string COMMENT &#39;编号&#39;,&#96;coupon_id&#96; string COMMENT &#39;优惠券 ID&#39;,&#96;user_id&#96; string COMMENT &#39;userid&#39;,&#96;order_id&#96; string COMMENT &#39;订单 id&#39;,&#96;coupon_status&#96; string COMMENT &#39;优惠券状态&#39;,&#96;get_time&#96; string COMMENT &#39;领取时间&#39;,&#96;using_time&#96; string COMMENT &#39;使用时间(下单)&#39;,&#96;used_time&#96; string COMMENT &#39;使用时间(支付)&#39; ) COMMENT &#39;优惠券领用事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_coupon_use&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)dt 是按照优惠卷领用时间 get_time 做为分区。get_time 为领用时间，领用过后数据就需要存在，然后在下单和支付的时候叠加更新时间数据导入12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_coupon_usehive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; set hive.exec.dynamic.partition.mode&#x3D;nonstrict;insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select if(new.id is null,old.id,new.id) id, if(new.coupon_id is null,old.coupon_id,new.coupon_id) coupon_id, if(new.user_id is null,old.user_id,new.user_id) user_id, if(new.order_id is null,old.order_id,new.order_id) order_id, if(new.coupon_status is null,old.coupon_status,new.coupon_status) coupon_status, from_unixtime(cast(if(new.get_time is null,old.get_time,new.get_time)&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;) get_time, from_unixtime(cast(if(new.using_time is null,old.using_time,new.using_time)&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;) using_time, from_unixtime(cast(if(new.used_time is null,old.used_time,new.used_time)&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;), from_unixtime(cast(if(new.get_time is null,old.get_time,new.get_time)&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;) from( select id, coupon_id, user_id, order_id, coupon_status, get_time, using_time, used_time from dwd.mall__fact_coupon_use where dt in ( select from_unixtime(cast(get_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;) from ods.mall__coupon_use where dt&#x3D;&#39;$db_date&#39; ))oldfull outer join( select id, coupon_id, user_id, order_id, coupon_status, get_time, using_time, used_time from ods.mall__coupon_use where dt&#x3D;&#39;$db_date&#39;)newon old.id&#x3D;new.id;&quot;$hive -e &quot;$sql&quot;9.2.13 订单事实表(累积型快照事实表-新增及变化)建表123456789101112131415161718192021222324252627drop table if exists dwd.mall__fact_order_info CREATE EXTERNAL TABLE &#96;dwd.mall__fact_order_info&#96;(&#96;id&#96; string COMMENT &#39;订单编号&#39;,&#96;order_status&#96; string COMMENT &#39;订单状态&#39;,&#96;user_id&#96; string COMMENT &#39;用户 id&#39;,&#96;out_trade_no&#96; string COMMENT &#39;支付流水号&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间(未支付状态)&#39;,&#96;payment_time&#96; string COMMENT &#39;支付时间(已支付状态)&#39;,&#96;cancel_time&#96; string COMMENT &#39;取消时间(已取消状态)&#39;,&#96;finish_time&#96; string COMMENT &#39;完成时间(已完成状态)&#39;,&#96;refund_time&#96; string COMMENT &#39;退款时间(退款中状态)&#39;,&#96;refund_finish_time&#96; string COMMENT &#39;退款完成时间(退款完成状态)&#39;,&#96;province_id&#96; string COMMENT &#39;省份 ID&#39;,&#96;activity_id&#96; string COMMENT &#39;活动 ID&#39;,&#96;original_total_amount&#96; string COMMENT &#39;原价金额&#39;,&#96;benefit_reduce_amount&#96; string COMMENT &#39;优惠金额&#39;,&#96;feight_fee&#96; string COMMENT &#39;运费&#39;,&#96;final_total_amount&#96; decimal(10,2) COMMENT &#39;订单金额&#39; ) COMMENT &#39;订单事实表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwd&#x2F;mall&#x2F;fact_order_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)数据导入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwdtable_name&#x3D;fact_order_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select if(new.id is null,old.id,new.id), if(new.order_status is null,old.order_status,new.order_status), if(new.user_id is null,old.user_id,new.user_id), if(new.out_trade_no is null,old.out_trade_no,new.out_trade_no), if(new.tms[&#39;1001&#39;] is null,from_unixtime(cast(old.create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;),new.tms[&#39;1001&#39;]),--1001 对应未支付状态 if(new.tms[&#39;1002&#39;] is null,from_unixtime(cast(old.payment_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;),new.tms[&#39;1002&#39;]), if(new.tms[&#39;1003&#39;] is null,from_unixtime(cast(old.cancel_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;),new.tms[&#39;1003&#39;]), if(new.tms[&#39;1004&#39;] is null,from_unixtime(cast(old.finish_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;),new.tms[&#39;1004&#39;]), if(new.tms[&#39;1005&#39;] is null,from_unixtime(cast(old.refund_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;),new.tms[&#39;1005&#39;]), if(new.tms[&#39;1006&#39;] is null,from_unixtime(cast(old.refund_finish_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd HH:mm:ss&#39;),new.tms[&#39;1006&#39;]), if(new.province_id is null,old.province_id,new.province_id), if(new.activity_id is null,old.activity_id,new.activity_id), if(new.original_total_amount is null,old.original_total_amount,new.original_total_amount), if(new.benefit_reduce_amount is null,old.benefit_reduce_amount,new.benefit_reduce_amount), if(new.feight_fee is null,old.feight_fee,new.feight_fee), if(new.final_total_amount is null,old.final_total_amount,new.final_total_amount)from( select id, order_status, user_id, out_trade_no, create_time, payment_time, cancel_time, finish_time, refund_time, refund_finish_time, province_id, activity_id, original_total_amount, benefit_reduce_amount, feight_fee, final_total_amount from dwd.mall__fact_order_info where dt in ( select from_unixtime(cast(create_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;) from ods.mall__order_info where dt&#x3D;&#39;$db_date&#39; ))oldfull outer join( select info.id, info.order_status, info.user_id, info.out_trade_no, info.province_id, act.activity_id, log.tms, info.original_total_amount, info.benefit_reduce_amount, info.feight_fee, info.final_total_amount from ( select order_id, str_to_map(concat_ws(&#39;,&#39;,collect_set(concat(order_status,&#39;&#x3D;&#39;,from_unixtime(cast(operate_time&#x2F;1000 as bigint),&#39;yyyy-MM-dd&#39;)))),&#39;,&#39;,&#39;&#x3D;&#39;) tms from ods.mall__order_status_log where dt&#x3D;&#39;$db_date&#39; group by order_id )log join ( select * from ods.mall__order_info where dt&#x3D;&#39;$db_date&#39; )info on log.order_id&#x3D;info.id left join ( select * from ods.mall__activity_order where dt&#x3D;&#39;$db_date&#39; )act on log.order_id&#x3D;act.order_id)newon old.id&#x3D;new.id;&quot;$hive -e &quot;$sql&quot;10 DWS层构建不在进行压缩处理，因为压缩对于硬盘是好的，但是对于CPU计算是差的，对于DWS层的表，会被经常使用，那么讲究的是计算效率，此层主要处理每日主题行为10.1 每日设备行为(用户行为)建表123456789101112131415161718192021222324252627drop table if exists dws.mall__uv_detail_daycountCREATE EXTERNAL TABLE &#96;dws.mall__uv_detail_daycount&#96;(&#96;mid_id&#96; string COMMENT &#39;设备唯一标识&#39;,&#96;user_id&#96; string COMMENT &#39;用户标识&#39;,&#96;version_code&#96; string COMMENT &#39;程序版本号&#39;,&#96;version_name&#96; string COMMENT &#39;程序版本名&#39;,&#96;lang&#96; string COMMENT &#39;系统语言&#39;,&#96;source&#96; string COMMENT &#39;渠道号&#39;,&#96;os&#96; string COMMENT &#39;安卓系统版本&#39;,&#96;area&#96; string COMMENT &#39;区域&#39;,&#96;model&#96; string COMMENT &#39;手机型号&#39;,&#96;brand&#96; string COMMENT &#39;手机品牌&#39;,&#96;sdk_version&#96; string COMMENT &#39;sdkVersion&#39;,&#96;gmail&#96; string COMMENT &#39;gmail&#39;,&#96;height_width&#96; string COMMENT &#39;屏幕宽高&#39;,&#96;app_time&#96; string COMMENT &#39;客户端日志产生时的时间&#39;,&#96;network&#96; string COMMENT &#39;网络模式&#39;,&#96;lng&#96; string COMMENT &#39;经度&#39;,&#96;lat&#96; string COMMENT &#39;纬度&#39;,&#96;login_count&#96; bigint COMMENT &#39;活跃次数&#39; ) COMMENT &#39;每日设备行为表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dws&#x2F;mall&#x2F;uv_detail_daycount&#x2F;&#39;导入数据12345678910111213141516171819202122232425262728293031323334353637383940#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwstable_name&#x3D;uv_detail_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_namePARTITION (dt&#x3D;&#39;$db_date&#39;)select mid_id, concat_ws(&#39;|&#39;, collect_set(user_id)) user_id, concat_ws(&#39;|&#39;, collect_set(version_code)) version_code, concat_ws(&#39;|&#39;, collect_set(version_name)) version_name, concat_ws(&#39;|&#39;, collect_set(lang))lang, concat_ws(&#39;|&#39;, collect_set(source)) source, concat_ws(&#39;|&#39;, collect_set(os)) os, concat_ws(&#39;|&#39;, collect_set(area)) area, concat_ws(&#39;|&#39;, collect_set(model)) model, concat_ws(&#39;|&#39;, collect_set(brand)) brand, concat_ws(&#39;|&#39;, collect_set(sdk_version)) sdk_version, concat_ws(&#39;|&#39;, collect_set(gmail)) gmail, concat_ws(&#39;|&#39;, collect_set(height_width)) height_width, concat_ws(&#39;|&#39;, collect_set(app_time)) app_time, concat_ws(&#39;|&#39;, collect_set(network)) network, concat_ws(&#39;|&#39;, collect_set(lng)) lng, concat_ws(&#39;|&#39;, collect_set(lat)) lat, count(*) login_countfrom dwd.mall__start_logwhere dt&#x3D;&#39;$db_date&#39;group by mid_id;&quot;$hive -e &quot;$sql&quot;10.2 每日会员行为(业务)建表1234567891011121314151617drop table if exists dws.mall__user_action_daycountCREATE EXTERNAL TABLE &#96;dws.mall__user_action_daycount&#96;(user_id string comment &#39;用户 id&#39;,login_count bigint comment &#39;登录次数&#39;,cart_count bigint comment &#39;加入购物车次数&#39;,cart_amount double comment &#39;加入购物车金额&#39;,order_count bigint comment &#39;下单次数&#39;,order_amount decimal(16,2) comment &#39;下单金额&#39;,payment_count bigint comment &#39;支付次数&#39;,payment_amount decimal(16,2) comment &#39;支付金额&#39; ) COMMENT &#39;每日会员行为表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dws&#x2F;mall&#x2F;user_action_daycount&#x2F;&#39;导入数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwstable_name&#x3D;user_action_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; withtmp_login as( select user_id, count(*) login_count from dwd.mall__start_log where dt&#x3D;&#39;$db_date&#39; and user_id is not null group by user_id),tmp_cart as( select user_id, count(*) cart_count, sum(cart_price*sku_num) cart_amount from dwd.mall__fact_cart_info where dt&#x3D;&#39;$db_date&#39; and user_id is not null and date_format(create_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39; group by user_id),tmp_order as( select user_id, count(*) order_count, sum(final_total_amount) order_amount from dwd.mall__fact_order_info where dt&#x3D;&#39;$db_date&#39; group by user_id) ,tmp_payment as( select user_id, count(*) payment_count, sum(payment_amount) payment_amount from dwd.mall__fact_payment_info where dt&#x3D;&#39;$db_date&#39; group by user_id)insert overwrite table $hive_table_name partition(dt&#x3D;&#39;$db_date&#39;)select user_actions.user_id, sum(user_actions.login_count), sum(user_actions.cart_count), sum(user_actions.cart_amount), sum(user_actions.order_count), sum(user_actions.order_amount), sum(user_actions.payment_count), sum(user_actions.payment_amount)from( select user_id, login_count, 0 cart_count, 0 cart_amount, 0 order_count, 0 order_amount, 0 payment_count, 0 payment_amount from tmp_loginunion all select user_id, 0 login_count, cart_count, cart_amount, 0 order_count, 0 order_amount, 0 payment_count, 0 payment_amount from tmp_cartunion all select user_id, 0 login_count, 0 cart_count, 0 cart_amount, order_count, order_amount, 0 payment_count, 0 payment_amount from tmp_orderunion all select user_id, 0 login_count, 0 cart_count, 0 cart_amount, 0 order_count, 0 order_amount, payment_count, payment_amount from tmp_payment) user_actionsgroup by user_id;&quot;$hive -e &quot;$sql&quot;10.3 每日商品行为(业务)建表1234567891011121314151617181920212223242526drop table if exists dws.mall__sku_action_daycountCREATE EXTERNAL TABLE &#96;dws.mall__sku_action_daycount&#96;(sku_id string comment &#39;sku_id&#39;,order_count bigint comment &#39;被下单次数&#39;,order_num bigint comment &#39;被下单件数&#39;,order_amount decimal(16,2) comment &#39;被下单金额&#39;,payment_count bigint comment &#39;被支付次数&#39;,payment_num bigint comment &#39;被支付件数&#39;,payment_amount decimal(16,2) comment &#39;被支付金额&#39;,refund_count bigint comment &#39;被退款次数&#39;,refund_num bigint comment &#39;被退款件数&#39;,refund_amount decimal(16,2) comment &#39;被退款金额&#39;,cart_count bigint comment &#39;被加入购物车次数&#39;,cart_num bigint comment &#39;被加入购物车件数&#39;,favor_count bigint comment &#39;被收藏次数&#39;,appraise_good_count bigint comment &#39;好评数&#39;,appraise_mid_count bigint comment &#39;中评数&#39;,appraise_bad_count bigint comment &#39;差评数&#39;,appraise_default_count bigint comment &#39;默认评价数&#39; ) COMMENT &#39;每日商品行为表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dws&#x2F;mall&#x2F;sku_action_daycount&#x2F;&#39;导入数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwstable_name&#x3D;sku_action_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; withtmp_order as( select cast(sku_id as string) sku_id, count(*) order_count, sum(sku_num) order_num, sum(total_amount) order_amount from dwd.mall__fact_order_detail where dt&#x3D;&#39;$db_date&#39; group by sku_id),tmp_payment as( select cast(sku_id as string) sku_id, count(*) payment_count, sum(sku_num) payment_num, sum(total_amount) payment_amount from dwd.mall__fact_order_detail where dt&#x3D;&#39;$db_date&#39; and order_id in ( select id from dwd.mall__fact_order_info where (dt&#x3D;&#39;$db_date&#39; or dt&#x3D;date_add(&#39;$db_date&#39;,-1)) and date_format(payment_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39; ) group by sku_id),tmp_refund as( select cast(sku_id as string) sku_id, count(*) refund_count, sum(refund_num) refund_num, sum(refund_amount) refund_amount from dwd.mall__fact_order_refund_info where dt&#x3D;&#39;$db_date&#39; group by sku_id),tmp_cart as( select cast(sku_id as string) sku_id, count(*) cart_count, sum(sku_num) cart_num from dwd.mall__fact_cart_info where dt&#x3D;&#39;$db_date&#39; and date_format(create_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39; group by sku_id),tmp_favor as( select cast(sku_id as string) sku_id, count(*) favor_count from dwd.mall__fact_favor_info where dt&#x3D;&#39;$db_date&#39; and date_format(create_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39; group by sku_id),tmp_appraise as( select cast(sku_id as string) sku_id, sum(if(appraise&#x3D;&#39;1201&#39;,1,0)) appraise_good_count, sum(if(appraise&#x3D;&#39;1202&#39;,1,0)) appraise_mid_count, sum(if(appraise&#x3D;&#39;1203&#39;,1,0)) appraise_bad_count, sum(if(appraise&#x3D;&#39;1204&#39;,1,0)) appraise_default_count from dwd.mall__fact_comment_info where dt&#x3D;&#39;$db_date&#39; group by sku_id)insert overwrite table $hive_table_name partition(dt&#x3D;&#39;$db_date&#39;)select sku_id, sum(order_count), sum(order_num), sum(order_amount), sum(payment_count), sum(payment_num), sum(payment_amount), sum(refund_count), sum(refund_num), sum(refund_amount), sum(cart_count), sum(cart_num), sum(favor_count), sum(appraise_good_count), sum(appraise_mid_count), sum(appraise_bad_count), sum(appraise_default_count)from( select sku_id, order_count, order_num, order_amount, 0 payment_count, 0 payment_num, 0 payment_amount, 0 refund_count, 0 refund_num, 0 refund_amount, 0 cart_count, 0 cart_num, 0 favor_count, 0 appraise_good_count, 0 appraise_mid_count, 0 appraise_bad_count, 0 appraise_default_count from tmp_orderunion all select sku_id, 0 order_count, 0 order_num, 0 order_amount, payment_count, payment_num, payment_amount, 0 refund_count, 0 refund_num, 0 refund_amount, 0 cart_count, 0 cart_num, 0 favor_count, 0 appraise_good_count, 0 appraise_mid_count, 0 appraise_bad_count, 0 appraise_default_count from tmp_paymentunion all select sku_id, 0 order_count, 0 order_num, 0 order_amount, 0 payment_count, 0 payment_num, 0 payment_amount, refund_count, refund_num, refund_amount, 0 cart_count, 0 cart_num, 0 favor_count, 0 appraise_good_count, 0 appraise_mid_count, 0 appraise_bad_count, 0 appraise_default_count from tmp_refundunion all select sku_id, 0 order_count, 0 order_num, 0 order_amount, 0 payment_count, 0 payment_num, 0 payment_amount, 0 refund_count, 0 refund_num, 0 refund_amount, cart_count, cart_num, 0 favor_count, 0 appraise_good_count, 0 appraise_mid_count, 0 appraise_bad_count, 0 appraise_default_count from tmp_cartunion all select sku_id, 0 order_count, 0 order_num, 0 order_amount, 0 payment_count, 0 payment_num, 0 payment_amount, 0 refund_count, 0 refund_num, 0 refund_amount, 0 cart_count, 0 cart_num, favor_count, 0 appraise_good_count, 0 appraise_mid_count, 0 appraise_bad_count, 0 appraise_default_count from tmp_favorunion all select sku_id, 0 order_count, 0 order_num, 0 order_amount, 0 payment_count, 0 payment_num, 0 payment_amount, 0 refund_count, 0 refund_num, 0 refund_amount, 0 cart_count, 0 cart_num, 0 favor_count, appraise_good_count, appraise_mid_count, appraise_bad_count, appraise_default_count from tmp_appraise)tmpgroup by sku_id;&quot;$hive -e &quot;$sql&quot;10.4 每日优惠券统计(业务)建表1234567891011121314151617181920212223242526drop table if exists dws.mall__coupon_use_daycountCREATE EXTERNAL TABLE &#96;dws.mall__coupon_use_daycount&#96;(&#96;coupon_id&#96; string COMMENT &#39;优惠券 ID&#39;,&#96;coupon_name&#96; string COMMENT &#39;购物券名称&#39;,&#96;coupon_type&#96; string COMMENT &#39;购物券类型 1 现金券 2 折扣券 3 满减券 4 满件打折券&#39;,&#96;condition_amount&#96; string COMMENT &#39;满额数&#39;,&#96;condition_num&#96; string COMMENT &#39;满件数&#39;,&#96;activity_id&#96; string COMMENT &#39;活动编号&#39;,&#96;benefit_amount&#96; string COMMENT &#39;减金额&#39;,&#96;benefit_discount&#96; string COMMENT &#39;折扣&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,&#96;range_type&#96; string COMMENT &#39;范围类型 1、商品 2、品类 3、品牌&#39;,&#96;spu_id&#96; string COMMENT &#39;商品 id&#39;,&#96;tm_id&#96; string COMMENT &#39;品牌 id&#39;,&#96;category3_id&#96; string COMMENT &#39;品类 id&#39;,&#96;limit_num&#96; string COMMENT &#39;最多领用次数&#39;,&#96;get_count&#96; bigint COMMENT &#39;领用次数&#39;,&#96;using_count&#96; bigint COMMENT &#39;使用(下单)次数&#39;,&#96;used_count&#96; bigint COMMENT &#39;使用(支付)次数&#39; ) COMMENT &#39;每日优惠券统计表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dws&#x2F;mall&#x2F;coupon_use_daycount&#x2F;&#39;导入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwstable_name&#x3D;coupon_use_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_name partition(dt&#x3D;&#39;$db_date&#39;)select cu.coupon_id, ci.coupon_name, ci.coupon_type, ci.condition_amount, ci.condition_num, ci.activity_id, ci.benefit_amount, ci.benefit_discount, ci.create_time, ci.range_type, ci.spu_id, ci.tm_id, ci.category3_id, ci.limit_num, cu.get_count, cu.using_count, cu.used_countfrom( select coupon_id, sum(if(date_format(get_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39;,1,0)) get_count, sum(if(date_format(using_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39;,1,0)) using_count, sum(if(date_format(used_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39;,1,0)) used_count from dwd.mall__fact_coupon_use where dt&#x3D;&#39;$db_date&#39; group by coupon_id)culeft join( select * from dwd.mall__dim_coupon_info where dt&#x3D;&#39;$db_date&#39;)ci on cu.coupon_id&#x3D;ci.id;&quot;$hive -e &quot;$sql&quot;10.5 每日活动统计(业务)建表1234567891011121314151617drop table if exists dws.mall__activity_info_daycountCREATE EXTERNAL TABLE &#96;dws.mall__activity_info_daycount&#96;(&#96;id&#96; string COMMENT &#39;编号&#39;,&#96;activity_name&#96; string COMMENT &#39;活动名称&#39;,&#96;activity_type&#96; string COMMENT &#39;活动类型&#39;,&#96;start_time&#96; string COMMENT &#39;开始时间&#39;,&#96;end_time&#96; string COMMENT &#39;结束时间&#39;,&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,&#96;order_count&#96; bigint COMMENT &#39;下单次数&#39;,&#96;payment_count&#96; bigint COMMENT &#39;支付次数&#39; ) COMMENT &#39;每日活动统计表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dws&#x2F;mall&#x2F;activity_info_daycount&#x2F;&#39;导入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwstable_name&#x3D;activity_info_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_name partition(dt&#x3D;&#39;$db_date&#39;)select oi.activity_id, ai.activity_name, ai.activity_type, ai.start_time, ai.end_time, ai.create_time, oi.order_count, oi.payment_countfrom( select activity_id, sum(if(date_format(create_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39;,1,0)) order_count, sum(if(date_format(payment_time,&#39;yyyy-MM-dd&#39;)&#x3D;&#39;$db_date&#39;,1,0)) payment_count from dwd.mall__fact_order_info where (dt&#x3D;&#39;$db_date&#39; or dt&#x3D;date_add(&#39;$db_date&#39;,-1)) and activity_id is not null group by activity_id)oijoin( select * from dwd.mall__dim_activity_info where dt&#x3D;&#39;$db_date&#39;)aion oi.activity_id&#x3D;ai.id;&quot;$hive -e &quot;$sql&quot;10.6 每日购买行为(业务)建表123456789101112131415161718192021222324252627drop table if exists dws.mall__sale_detail_daycountCREATE EXTERNAL TABLE &#96;dws.mall__sale_detail_daycount&#96;(user_id string comment &#39;用户 id&#39;,sku_id string comment &#39;商品 id&#39;,user_gender string comment &#39;用户性别&#39;,user_age string comment &#39;用户年龄&#39;,user_level string comment &#39;用户等级&#39;,order_price decimal(10,2) comment &#39;商品价格&#39;,sku_name string comment &#39;商品名称&#39;,sku_tm_id string comment &#39;品牌 id&#39;,sku_category3_id string comment &#39;商品三级品类 id&#39;,sku_category2_id string comment &#39;商品二级品类 id&#39;,sku_category1_id string comment &#39;商品一级品类 id&#39;,sku_category3_name string comment &#39;商品三级品类名称&#39;,sku_category2_name string comment &#39;商品二级品类名称&#39;,sku_category1_name string comment &#39;商品一级品类名称&#39;,spu_id string comment &#39;商品 spu&#39;,sku_num int comment &#39;购买个数&#39;,order_count bigint comment &#39;当日下单单数&#39;,order_amount decimal(16,2) comment &#39;当日下单金额&#39; ) COMMENT &#39;每日购买行为表&#39;PARTITIONED BY ( &#96;dt&#96; String COMMENT &#39;partition&#39;)row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dws&#x2F;mall&#x2F;sale_detail_daycount&#x2F;&#39;导入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwstable_name&#x3D;sale_detail_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_name partition(dt&#x3D;&#39;$db_date&#39;)select op.user_id, op.sku_id, ui.gender, months_between(&#39;$db_date&#39;, ui.birthday)&#x2F;12 age, ui.user_level, si.price, si.sku_name, si.tm_id, si.category3_id, si.category2_id, si.category1_id, si.category3_name, si.category2_name, si.category1_name, si.spu_id, op.sku_num, op.order_count, op.order_amountfrom( select user_id, sku_id, sum(sku_num) sku_num, count(*) order_count, sum(total_amount) order_amount from dwd.mall__fact_order_detail where dt&#x3D;&#39;$db_date&#39; group by user_id, sku_id)opjoin( select * from dwd.mall__dim_user_info_his where end_date&#x3D;&#39;9999-99-99&#39;)ui on op.user_id &#x3D; ui.idjoin( select * from dwd.mall__dim_sku_info where dt&#x3D;&#39;$db_date&#39;)si on op.sku_id &#x3D; si.id;&quot;$hive -e &quot;$sql&quot;11 DWT层构建此层主要针对dws层每日数据进行汇总，不建立分区，不压缩，每日进行数据覆盖11.1 设备主题宽表建表123456789101112131415161718192021222324252627drop table if exists dwt.mall__uv_topicCREATE EXTERNAL TABLE &#96;dwt.mall__uv_topic&#96;(&#96;mid_id&#96; string COMMENT &#39;设备唯一标识&#39;,&#96;user_id&#96; string COMMENT &#39;用户标识&#39;,&#96;version_code&#96; string COMMENT &#39;程序版本号&#39;,&#96;version_name&#96; string COMMENT &#39;程序版本名&#39;,&#96;lang&#96; string COMMENT &#39;系统语言&#39;,&#96;source&#96; string COMMENT &#39;渠道号&#39;,&#96;os&#96; string COMMENT &#39;安卓系统版本&#39;,&#96;area&#96; string COMMENT &#39;区域&#39;,&#96;model&#96; string COMMENT &#39;手机型号&#39;,&#96;brand&#96; string COMMENT &#39;手机品牌&#39;,&#96;sdk_version&#96; string COMMENT &#39;sdkVersion&#39;,&#96;gmail&#96; string COMMENT &#39;gmail&#39;,&#96;height_width&#96; string COMMENT &#39;屏幕宽高&#39;,&#96;app_time&#96; string COMMENT &#39;客户端日志产生时的时间&#39;,&#96;network&#96; string COMMENT &#39;网络模式&#39;,&#96;lng&#96; string COMMENT &#39;经度&#39;,&#96;lat&#96; string COMMENT &#39;纬度&#39;,&#96;login_date_first&#96; string comment &#39;首次活跃时间&#39;,&#96;login_date_last&#96; string comment &#39;末次活跃时间&#39;,&#96;login_day_count&#96; bigint comment &#39;当日活跃次数&#39;,&#96;login_count&#96; bigint comment &#39;累积活跃天数&#39; ) COMMENT &#39;设备主题宽表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwt&#x2F;mall&#x2F;uv_topic&#x2F;&#39;导入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwttable_name&#x3D;uv_topichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect nvl(new.mid_id,old.mid_id), nvl(new.user_id,old.user_id), nvl(new.version_code,old.version_code), nvl(new.version_name,old.version_name), nvl(new.lang,old.lang), nvl(new.source,old.source), nvl(new.os,old.os), nvl(new.area,old.area), nvl(new.model,old.model), nvl(new.brand,old.brand), nvl(new.sdk_version,old.sdk_version), nvl(new.gmail,old.gmail), nvl(new.height_width,old.height_width), nvl(new.app_time,old.app_time), nvl(new.network,old.network), nvl(new.lng,old.lng), nvl(new.lat,old.lat), if(old.mid_id is null,&#39;2020-03-10&#39;,old.login_date_first), if(new.mid_id is not null,&#39;2020-03-10&#39;,old.login_date_last), if(new.mid_id is not null, new.login_count,0), nvl(old.login_count,0)+if(new.login_count&gt;0,1,0)from( select * from dwt.mall__uv_topic)oldfull outer join( select * from dws.mall__uv_detail_daycount where dt&#x3D;&#39;$db_date&#39;)newon old.mid_id&#x3D;new.mid_id;&quot;$hive -e &quot;$sql&quot;11.2 会员主题宽表建表1234567891011121314151617181920212223drop table if exists dwt.mall__user_topicCREATE EXTERNAL TABLE &#96;dwt.mall__user_topic&#96;(user_id string comment &#39;用户 id&#39;,login_date_first string comment &#39;首次登录时间&#39;,login_date_last string comment &#39;末次登录时间&#39;,login_count bigint comment &#39;累积登录天数&#39;,login_last_30d_count bigint comment &#39;最近 30 日登录天数&#39;,order_date_first string comment &#39;首次下单时间&#39;,order_date_last string comment &#39;末次下单时间&#39;,order_count bigint comment &#39;累积下单次数&#39;,order_amount decimal(16,2) comment &#39;累积下单金额&#39;,order_last_30d_count bigint comment &#39;最近 30 日下单次数&#39;,order_last_30d_amount bigint comment &#39;最近 30 日下单金额&#39;,payment_date_first string comment &#39;首次支付时间&#39;,payment_date_last string comment &#39;末次支付时间&#39;,payment_count decimal(16,2) comment &#39;累积支付次数&#39;,payment_amount decimal(16,2) comment &#39;累积支付金额&#39;,payment_last_30d_count decimal(16,2) comment &#39;最近 30 日支付次数&#39;,payment_last_30d_amount decimal(16,2) comment &#39;最近 30 日支付金额&#39; ) COMMENT &#39;会员主题宽表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwt&#x2F;mall&#x2F;user_topic&#x2F;&#39;导入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwttable_name&#x3D;user_topichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect nvl(new.user_id,old.user_id), if(old.login_date_first is null and new.login_count&gt;0,&#39;$db_date&#39;,old.login_date_first), if(new.login_count&gt;0,&#39;$db_date&#39;,old.login_date_last), nvl(old.login_count,0)+if(new.login_count&gt;0,1,0), nvl(new.login_last_30d_count,0), if(old.order_date_first is null and new.order_count&gt;0,&#39;$db_date&#39;,old.order_date_first), if(new.order_count&gt;0,&#39;$db_date&#39;,old.order_date_last), nvl(old.order_count,0)+nvl(new.order_count,0), nvl(old.order_amount,0)+nvl(new.order_amount,0), nvl(new.order_last_30d_count,0), nvl(new.order_last_30d_amount,0), if(old.payment_date_first is null and new.payment_count&gt;0,&#39;$db_date&#39;,old.payment_date_first), if(new.payment_count&gt;0,&#39;$db_date&#39;,old.payment_date_last), nvl(old.payment_count,0)+nvl(new.payment_count,0), nvl(old.payment_amount,0)+nvl(new.payment_amount,0), nvl(new.payment_last_30d_count,0), nvl(new.payment_last_30d_amount,0)fromdwt.mall__user_topic oldfull outer join( select user_id, sum(if(dt&#x3D;&#39;$db_date&#39;,login_count,0)) login_count, sum(if(dt&#x3D;&#39;$db_date&#39;,order_count,0)) order_count, sum(if(dt&#x3D;&#39;$db_date&#39;,order_amount,0)) order_amount, sum(if(dt&#x3D;&#39;$db_date&#39;,payment_count,0)) payment_count, sum(if(dt&#x3D;&#39;$db_date&#39;,payment_amount,0)) payment_amount, sum(if(login_count&gt;0,1,0)) login_last_30d_count, sum(order_count) order_last_30d_count, sum(order_amount) order_last_30d_amount, sum(payment_count) payment_last_30d_count, sum(payment_amount) payment_last_30d_amount from dws.mall__user_action_daycount where dt&gt;&#x3D;date_add( &#39;$db_date&#39;,-30) group by user_id)newon old.user_id&#x3D;new.user_id;&quot;$hive -e &quot;$sql&quot;11.3 商品主题宽表建表12345678910111213141516171819202122232425262728293031323334353637383940drop table if exists dwt.mall__sku_topicCREATE EXTERNAL TABLE &#96;dwt.mall__sku_topic&#96;(sku_id string comment &#39;sku_id&#39;,spu_id string comment &#39;spu_id&#39;,order_last_30d_count bigint comment &#39;最近 30 日被下单次数&#39;,order_last_30d_num bigint comment &#39;最近 30 日被下单件数&#39;,order_last_30d_amount decimal(16,2) comment &#39;最近 30 日被下单金额&#39;,order_count bigint comment &#39;累积被下单次数&#39;,order_num bigint comment &#39;累积被下单件数&#39;,order_amount decimal(16,2) comment &#39;累积被下单金额&#39;,payment_last_30d_count bigint comment &#39;最近 30 日被支付次数&#39;,payment_last_30d_num bigint comment &#39;最近 30 日被支付件数&#39;,payment_last_30d_amount decimal(16,2) comment &#39;最近 30 日被支付金额&#39;,payment_count bigint comment &#39;累积被支付次数&#39;,payment_num bigint comment &#39;累积被支付件数&#39;,payment_amount decimal(16,2) comment &#39;累积被支付金额&#39;,refund_last_30d_count bigint comment &#39;最近三十日退款次数&#39;,refund_last_30d_num bigint comment &#39;最近三十日退款件数&#39;,refund_last_30d_amount decimal(10,2) comment &#39;最近三十日退款金额&#39;,refund_count bigint comment &#39;累积退款次数&#39;,refund_num bigint comment &#39;累积退款件数&#39;,refund_amount decimal(10,2) comment &#39;累积退款金额&#39;,cart_last_30d_count bigint comment &#39;最近 30 日被加入购物车次数&#39;,cart_last_30d_num bigint comment &#39;最近 30 日被加入购物车件数&#39;,cart_count bigint comment &#39;累积被加入购物车次数&#39;,cart_num bigint comment &#39;累积被加入购物车件数&#39;,favor_last_30d_count bigint comment &#39;最近 30 日被收藏次数&#39;,favor_count bigint comment &#39;累积被收藏次数&#39;,appraise_last_30d_good_count bigint comment &#39;最近 30 日好评数&#39;,appraise_last_30d_mid_count bigint comment &#39;最近 30 日中评数&#39;,appraise_last_30d_bad_count bigint comment &#39;最近 30 日差评数&#39;,appraise_last_30d_default_count bigint comment &#39;最近 30 日默认评价数&#39;,appraise_good_count bigint comment &#39;累积好评数&#39;,appraise_mid_count bigint comment &#39;累积中评数&#39;,appraise_bad_count bigint comment &#39;累积差评数&#39;,appraise_default_count bigint comment &#39;累积默认评价数&#39; ) COMMENT &#39;商品主题宽表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwt&#x2F;mall&#x2F;sku_topic&#x2F;&#39;导入数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwttable_name&#x3D;sku_topichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect nvl(new.sku_id,old.sku_id), sku_info.spu_id, nvl(new.order_count30,0), nvl(new.order_num30,0), nvl(new.order_amount30,0), nvl(old.order_count,0) + nvl(new.order_count,0), nvl(old.order_num,0) + nvl(new.order_num,0), nvl(old.order_amount,0) + nvl(new.order_amount,0), nvl(new.payment_count30,0), nvl(new.payment_num30,0), nvl(new.payment_amount30,0), nvl(old.payment_count,0) + nvl(new.payment_count,0), nvl(old.payment_num,0) + nvl(new.payment_count,0), nvl(old.payment_amount,0) + nvl(new.payment_count,0), nvl(new.refund_count30,0), nvl(new.refund_num30,0), nvl(new.refund_amount30,0), nvl(old.refund_count,0) + nvl(new.refund_count,0), nvl(old.refund_num,0) + nvl(new.refund_num,0), nvl(old.refund_amount,0) + nvl(new.refund_amount,0), nvl(new.cart_count30,0), nvl(new.cart_num30,0), nvl(old.cart_count,0) + nvl(new.cart_count,0), nvl(old.cart_num,0) + nvl(new.cart_num,0), nvl(new.favor_count30,0), nvl(old.favor_count,0) + nvl(new.favor_count,0), nvl(new.appraise_good_count30,0), nvl(new.appraise_mid_count30,0), nvl(new.appraise_bad_count30,0), nvl(new.appraise_default_count30,0) , nvl(old.appraise_good_count,0) + nvl(new.appraise_good_count,0), nvl(old.appraise_mid_count,0) + nvl(new.appraise_mid_count,0), nvl(old.appraise_bad_count,0) + nvl(new.appraise_bad_count,0), nvl(old.appraise_default_count,0) + nvl(new.appraise_default_count,0)from( select sku_id, spu_id, order_last_30d_count, order_last_30d_num, order_last_30d_amount, order_count, order_num, order_amount , payment_last_30d_count, payment_last_30d_num, payment_last_30d_amount, payment_count, payment_num, payment_amount, refund_last_30d_count, refund_last_30d_num, refund_last_30d_amount, refund_count, refund_num, refund_amount, cart_last_30d_count, cart_last_30d_num, cart_count, cart_num, favor_last_30d_count, favor_count, appraise_last_30d_good_count, appraise_last_30d_mid_count, appraise_last_30d_bad_count, appraise_last_30d_default_count, appraise_good_count, appraise_mid_count, appraise_bad_count, appraise_default_count from dwt.mall__sku_topic)oldfull outer join( select sku_id, sum(if(dt&#x3D;&#39;$db_date&#39;, order_count,0 )) order_count, sum(if(dt&#x3D;&#39;$db_date&#39;,order_num ,0 )) order_num, sum(if(dt&#x3D;&#39;$db_date&#39;,order_amount,0 )) order_amount , sum(if(dt&#x3D;&#39;$db_date&#39;,payment_count,0 )) payment_count, sum(if(dt&#x3D;&#39;$db_date&#39;,payment_num,0 )) payment_num, sum(if(dt&#x3D;&#39;$db_date&#39;,payment_amount,0 )) payment_amount, sum(if(dt&#x3D;&#39;$db_date&#39;,refund_count,0 )) refund_count, sum(if(dt&#x3D;&#39;$db_date&#39;,refund_num,0 )) refund_num, sum(if(dt&#x3D;&#39;$db_date&#39;,refund_amount,0 )) refund_amount, sum(if(dt&#x3D;&#39;$db_date&#39;,cart_count,0 )) cart_count, sum(if(dt&#x3D;&#39;$db_date&#39;,cart_num,0 )) cart_num, sum(if(dt&#x3D;&#39;$db_date&#39;,favor_count,0 )) favor_count, sum(if(dt&#x3D;&#39;$db_date&#39;,appraise_good_count,0 )) appraise_good_count, sum(if(dt&#x3D;&#39;$db_date&#39;,appraise_mid_count,0 ) ) appraise_mid_count , sum(if(dt&#x3D;&#39;$db_date&#39;,appraise_bad_count,0 )) appraise_bad_count, sum(if(dt&#x3D;&#39;$db_date&#39;,appraise_default_count,0 )) appraise_default_count, sum(order_count) order_count30 , sum(order_num) order_num30, sum(order_amount) order_amount30, sum(payment_count) payment_count30, sum(payment_num) payment_num30, sum(payment_amount) payment_amount30, sum(refund_count) refund_count30, sum(refund_num) refund_num30, sum(refund_amount) refund_amount30, sum(cart_count) cart_count30, sum(cart_num) cart_num30, sum(favor_count) favor_count30, sum(appraise_good_count) appraise_good_count30, sum(appraise_mid_count) appraise_mid_count30, sum(appraise_bad_count) appraise_bad_count30, sum(appraise_default_count) appraise_default_count30 from dws.mall__sku_action_daycount where dt &gt;&#x3D; date_add (&#39;$db_date&#39;, -30) group by sku_id)newon new.sku_id &#x3D; old.sku_idleft join( select * from dwd.mall__dim_sku_info where dt&#x3D;&#39;$db_date&#39;) sku_infoon nvl(new.sku_id,old.sku_id)&#x3D; sku_info.id;&quot;$hive -e &quot;$sql&quot;11.4 优惠卷主题宽表建表12345678910111213drop table if exists dwt.mall__coupon_topicCREATE EXTERNAL TABLE &#96;dwt.mall__coupon_topic&#96;(&#96;coupon_id&#96; string COMMENT &#39;优惠券 ID&#39;,&#96;get_day_count&#96; bigint COMMENT &#39;当日领用次数&#39;,&#96;using_day_count&#96; bigint COMMENT &#39;当日使用(下单)次数&#39;,&#96;used_day_count&#96; bigint COMMENT &#39;当日使用(支付)次数&#39;,&#96;get_count&#96; bigint COMMENT &#39;累积领用次数&#39;,&#96;using_count&#96; bigint COMMENT &#39;累积使用(下单)次数&#39;,&#96;used_count&#96; bigint COMMENT &#39;累积使用(支付)次数&#39; ) COMMENT &#39;优惠券主题宽表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwt&#x2F;mall&#x2F;coupon_topic&#x2F;&#39;导入数据123456789101112131415161718192021222324252627282930313233343536373839404142#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwttable_name&#x3D;coupon_topichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect nvl(new.coupon_id,old.coupon_id), nvl(new.get_count,0), nvl(new.using_count,0), nvl(new.used_count,0), nvl(old.get_count,0)+nvl(new.get_count,0), nvl(old.using_count,0)+nvl(new.using_count,0), nvl(old.used_count,0)+nvl(new.used_count,0)from( select * from dwt.mall__coupon_topic)oldfull outer join( select coupon_id, get_count, using_count, used_count from dws.mall__coupon_use_daycount where dt&#x3D;&#39;$db_date&#39;)newon old.coupon_id&#x3D;new.coupon_id;&quot;$hive -e &quot;$sql&quot;11.5 活动主题宽表建表123456789101112drop table if exists dwt.mall__activity_topicCREATE EXTERNAL TABLE &#96;dwt.mall__activity_topic&#96;(&#96;id&#96; string COMMENT &#39;活动 id&#39;,&#96;activity_name&#96; string COMMENT &#39;活动名称&#39;,&#96;order_day_count&#96; bigint COMMENT &#39;当日日下单次数&#39;,&#96;payment_day_count&#96; bigint COMMENT &#39;当日支付次数&#39;,&#96;order_count&#96; bigint COMMENT &#39;累积下单次数&#39;,&#96;payment_count&#96; bigint COMMENT &#39;累积支付次数&#39; ) COMMENT &#39;活动主题宽表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;dwt&#x2F;mall&#x2F;activity_topic&#x2F;&#39;导入数据1234567891011121314151617181920212223242526272829303132333435363738394041#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;dwttable_name&#x3D;activity_topichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert overwrite table $hive_table_nameselect nvl(new.id,old.id), nvl(new.activity_name,old.activity_name), nvl(new.order_count,0), nvl(new.payment_count,0), nvl(old.order_count,0)+nvl(new.order_count,0), nvl(old.payment_count,0)+nvl(new.payment_count,0)from( select * from dwt.mall__activity_topic)oldfull outer join( select id, activity_name, order_count, payment_count from dws.mall__activity_info_daycountwhere dt&#x3D;&#39;$db_date&#39;)newon old.id&#x3D;new.id;&quot;$hive -e &quot;$sql&quot;12 ADS层构建此层为最终数据需求层，考虑数据导出和数据数量决定是否需要压缩，不需要分区，每天刷写12.1 设备主题12.1.1 活跃设备数（日、周、月）日活：当日活跃的设备数周活：当周活跃的设备数月活：当月活跃的设备数建表12345678910111213drop table if exists ads.mall__uv_countCREATE EXTERNAL TABLE &#96;ads.mall__uv_count&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;day_count&#96; bigint COMMENT &#39;当日用户数量&#39;,&#96;wk_count&#96; bigint COMMENT &#39;当周用户数量&#39;,&#96;mn_count&#96; bigint COMMENT &#39;当月用户数量&#39;,&#96;is_weekend&#96; string COMMENT &#39;Y,N 是否是周末,用于得到本周最终结果&#39;,&#96;is_monthend&#96; string COMMENT &#39;Y,N 是否是月末,用于得到本月最终结果&#39; ) COMMENT &#39;活跃设备数表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;uv_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;uv_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39; dt, daycount.ct, wkcount.ct, mncount.ct, if(date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-1)&#x3D;&#39;$db_date&#39;,&#39;Y&#39;,&#39;N&#39;) , if(last_day(&#39;$db_date&#39;)&#x3D;&#39;$db_date&#39;,&#39;Y&#39;,&#39;N&#39;)from( select &#39;$db_date&#39; dt, count(*) ct from dwt.mall__uv_topic where login_date_last&#x3D;&#39;$db_date&#39;)daycount join( select &#39;$db_date&#39; dt, count (*) ct from dwt.mall__uv_topic where login_date_last&gt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-7) and login_date_last&lt;&#x3D; date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-1)) wkcount on daycount.dt&#x3D;wkcount.dtjoin( select &#39;$db_date&#39; dt, count (*) ct from dwt.mall__uv_topic where date_format(login_date_last,&#39;yyyy-MM&#39;)&#x3D;date_format(&#39;$db_date&#39;,&#39;yyyy-MM&#39;))mncount on daycount.dt&#x3D;mncount.dt;&quot;$hive -e &quot;$sql&quot;12.1.2 每日新增设备建表123456789drop table if exists ads.mall__new_mid_countCREATE EXTERNAL TABLE &#96;ads.mall__new_mid_count&#96;(&#96;create_date&#96; string comment &#39;创建时间&#39; ,&#96;new_mid_count&#96; bigint comment &#39;新增设备数量&#39; ) COMMENT &#39;每日新增设备表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;new_mid_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据1234567891011121314151617181920212223#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;new_mid_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect login_date_first, count(*)from dwt.mall__uv_topicwhere login_date_first&#x3D;&#39;$db_date&#39;group by login_date_first;&quot;$hive -e &quot;$sql&quot;12.1.3 沉默用户数沉默用户：只在安装当天启动过，且启动时间是在 7 天前建表123456789drop table if exists ads.mall__silent_countCREATE EXTERNAL TABLE &#96;ads.mall__silent_count&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;silent_count&#96; bigint COMMENT &#39;沉默设备数&#39; ) COMMENT &#39;沉默用户数表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;silent_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据1234567891011121314151617181920212223#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;silent_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, count(*)from dwt.mall__uv_topicwhere login_date_first&#x3D;login_date_lastand login_date_last&lt;&#x3D;date_add(&#39;$db_date&#39;,-7);&quot;$hive -e &quot;$sql&quot;12.1.4 本周回流用户数本周回流用户：上周未活跃，本周活跃的设备，且不是本周新增设备建表123456789drop table if exists ads.mall__back_countCREATE EXTERNAL TABLE &#96;ads.mall__back_count&#96;(&#96;wk_dt&#96; string COMMENT &#39;统计日期所在周&#39;,&#96;wastage_count&#96; bigint COMMENT &#39;回流设备数&#39; ) COMMENT &#39;本周回流用户数表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;back_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728293031323334353637383940#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;back_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, count(*)from( select mid_id from dwt.mall__uv_topic where login_date_last&gt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-7) and login_date_last&lt;&#x3D; date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-1) and login_date_first&lt;date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-7))current_wkleft join( select mid_id from dws.mall__uv_detail_daycount where dt&gt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-7*2) and dt&lt;&#x3D; date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-7-1) group by mid_id)last_wkon current_wk.mid_id&#x3D;last_wk.mid_idwhere last_wk.mid_id is null;&quot;$hive -e &quot;$sql&quot;12.1.5 流失用户数流失用户：最近 7 天未活跃的设备建表123456789drop table if exists ads.mall__wastage_countCREATE EXTERNAL TABLE &#96;ads.mall__wastage_count&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;wastage_count&#96; bigint COMMENT &#39;流失设备数&#39; ) COMMENT &#39;流失用户数表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;wastage_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;wastage_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, count(*)from( select mid_id from dwt.mall__uv_topic where login_date_last&lt;&#x3D;date_add(&#39;$db_date&#39;,-7) group by mid_id)t1;&quot;$hive -e &quot;$sql&quot;12.1.6 留存率建表12345678910111213drop table if exists ads.mall__user_retention_day_rateCREATE EXTERNAL TABLE &#96;ads.mall__user_retention_day_rate&#96;(&#96;stat_date&#96; string comment &#39;统计日期&#39;,&#96;create_date&#96; string comment &#39;设备新增日期&#39;,&#96;retention_day&#96; int comment &#39;截止当前日期留存天数&#39;,&#96;retention_count&#96; bigint comment &#39;留存数量&#39;,&#96;new_mid_count&#96; bigint comment &#39;设备新增数量&#39;,&#96;retention_ratio&#96; decimal(10,2) comment &#39;留存率&#39; ) COMMENT &#39;留存率表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;user_retention_day_rate&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;user_retention_day_ratehive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;,--统计日期 date_add(&#39;$db_date&#39;,-1),--新增日期 1,--留存天数 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-1) and login_date_last&#x3D;&#39;$db_date&#39;,1,0)),--2020-03-09 的 1 日留存数 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-1),1,0)),--2020-03-09 新增 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-1) and login_date_last&#x3D;&#39;$db_date&#39;,1,0))&#x2F;sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-1),1,0))*100from dwt.mall__uv_topicunion allselect &#39;$db_date&#39;,--统计日期 date_add(&#39;$db_date&#39;,-2),--新增日期 2,--留存天数 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-2) and login_date_last&#x3D;&#39;$db_date&#39;,1,0)),--2020-03-08 的 2 日留存数 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-2),1,0)),--2020-03-08 新增 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-2) and login_date_last&#x3D;&#39;$db_date&#39;,1,0))&#x2F;sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-2),1,0))*100from dwt.mall__uv_topicunion allselect &#39;$db_date&#39;,--统计日期 date_add(&#39;$db_date&#39;,-3),--新增日期 3,--留存天数 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-3) and login_date_last&#x3D;&#39;$db_date&#39;,1,0)),--2020-03-07 的 3 日留存数 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-3),1,0)),--2020-03-07 新增 sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-3) and login_date_last&#x3D;&#39;$db_date&#39;,1,0))&#x2F;sum(if(login_date_first&#x3D;date_add(&#39;$db_date&#39;,-3),1,0))*100from dwt.mall__uv_topic;&quot;$hive -e &quot;$sql&quot;12.1.7 最近连续三周活跃用户数建表12345678910drop table if exists ads.mall__continuity_wk_countCREATE EXTERNAL TABLE &#96;ads.mall__continuity_wk_count&#96;(&#96;dt&#96; string COMMENT &#39;统计日期,一般用结束周周日日期,如果每天计算一次,可用当天日期&#39;,&#96;wk_dt&#96; string COMMENT &#39;持续时间&#39;,&#96;continuity_count&#96; bigint COMMENT &#39;活跃次数&#39; ) COMMENT &#39;最近连续三周活跃用户数表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;continuity_wk_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;continuity_wk_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, concat(date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-7*3),&#39;_&#39;,date_add(next_day(&#39;$db_date&#39;,&#39;MO&#39;),-1)), count(*)from( select mid_id from ( select mid_id from dws.mall__uv_detail_daycount where dt&gt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;monday&#39;),-7) and dt&lt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;monday&#39;),-1) group by mid_id union all select mid_id from dws.mall__uv_detail_daycount where dt&gt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;monday&#39;),-7*2) and dt&lt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;monday&#39;),-7-1) group by mid_id union all select mid_id from dws.mall__uv_detail_daycount where dt&gt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;monday&#39;),-7*3) and dt&lt;&#x3D;date_add(next_day(&#39;$db_date&#39;,&#39;monday&#39;),-7*2-1) group by mid_id )t1 group by mid_id having count(*)&#x3D;3)t2&quot;$hive -e &quot;$sql&quot;12.1.8 最近七天内连续三天活跃用户数建表12345678910drop table if exists ads.mall__continuity_uv_countCREATE EXTERNAL TABLE &#96;ads.mall__continuity_uv_count&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;wk_dt&#96; string COMMENT &#39;最近 7 天日期&#39;,&#96;continuity_count&#96; bigint ) COMMENT &#39;最近七天内连续三天活跃用户数表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;continuity_uv_count&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;continuity_uv_counthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, concat(date_add(&#39;db_date&#39;,-6),&#39;_&#39;,&#39;db_date&#39;), count(*)from( select mid_id from ( select mid_id from ( select mid_id, date_sub(dt,rank) date_dif from ( select mid_id, dt, rank() over(partition by mid_id order by dt) rank from dws.mall__uv_detail_daycount where dt&gt;&#x3D;date_add(&#39;db_date&#39;,-6) and dt&lt;&#x3D;&#39;db_date&#39; )t1 )t2 group by mid_id,date_dif having count(*)&gt;&#x3D;3 )t3 group by mid_id)t4;&quot;$hive -e &quot;$sql&quot;12.2 会员主题12.2.1 会员主题信息建表12345678910111213141516drop table if exists ads.mall__user_topicCREATE EXTERNAL TABLE &#96;ads.mall__user_topic&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;day_users&#96; string COMMENT &#39;活跃会员数&#39;,&#96;day_new_users&#96; string COMMENT &#39;新增会员数&#39;,&#96;day_new_payment_users&#96; string COMMENT &#39;新增消费会员数&#39;,&#96;payment_users&#96; string COMMENT &#39;总付费会员数&#39;,&#96;users&#96; string COMMENT &#39;总会员数&#39;,&#96;day_users2users&#96; decimal(10,2) COMMENT &#39;会员活跃率&#39;,&#96;payment_users2users&#96; decimal(10,2) COMMENT &#39;会员付费率&#39;,&#96;day_new_users2users&#96; decimal(10,2) COMMENT &#39;会员新鲜度&#39; ) COMMENT &#39;会员主题信息表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;user_topic&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;user_topichive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, sum(if(login_date_last&#x3D;&#39;$db_date&#39;,1,0)), sum(if(login_date_first&#x3D;&#39;$db_date&#39;,1,0)), sum(if(payment_date_first&#x3D;&#39;$db_date&#39;,1,0)), sum(if(payment_count&gt;0,1,0)), count(*), sum(if(login_date_last&#x3D;&#39;$db_date&#39;,1,0))&#x2F;count(*), sum(if(payment_count&gt;0,1,0))&#x2F;count(*), sum(if(login_date_first&#x3D;&#39;$db_date&#39;,1,0))&#x2F;sum(if(login_date_last&#x3D;&#39;$db_date&#39;,1,0))from dwt.mall__user_topic&quot;$hive -e &quot;$sql&quot;12.2.2 漏斗分析统计“浏览-&gt;购物车-&gt;下单-&gt;支付”的转化率思路：统计各个行为的人数，然后计算比值。建表123456789101112131415drop table if exists ads.mall__user_action_convert_dayCREATE EXTERNAL TABLE &#96;ads.mall__user_action_convert_day&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;total_visitor_m_count&#96; bigint COMMENT &#39;总访问人数&#39;,&#96;cart_u_count&#96; bigint COMMENT &#39;加入购物车的人数&#39;,&#96;visitor2cart_convert_ratio&#96; decimal(10,2) COMMENT &#39;访问到加入购物车转化率&#39;,&#96;order_u_count&#96; bigint COMMENT &#39;下单人数&#39;,&#96;cart2order_convert_ratio&#96; decimal(10,2) COMMENT &#39;加入购物车到下单转化率&#39;,&#96;payment_u_count&#96; bigint COMMENT &#39;支付人数&#39;,&#96;order2payment_convert_ratio&#96; decimal(10,2) COMMENT &#39;下单到支付的转化率&#39; ) COMMENT &#39;漏斗分析表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;user_action_convert_day&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728293031323334353637#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;user_action_convert_dayhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, uv.day_count, ua.cart_count, cast(ua.cart_count&#x2F;uv.day_count as decimal(10,2)) visitor2cart_convert_ratio, ua.order_count, cast(ua.order_count&#x2F;ua.cart_count as decimal(10,2)) visitor2order_convert_ratio, ua.payment_count, cast(ua.payment_count&#x2F;ua.order_count as decimal(10,2)) order2payment_convert_ratiofrom( select dt, sum(if(cart_count&gt;0,1,0)) cart_count, sum(if(order_count&gt;0,1,0)) order_count, sum(if(payment_count&gt;0,1,0)) payment_count from dws.mall__user_action_daycount where dt&#x3D;&#39;$db_date&#39; group by dt)ua join ads.mall__uv_count uv on uv.dt&#x3D;ua.dt;&quot;$hive -e &quot;$sql&quot;12.3 商品主题12.3.1 商品个数信息建表12345678910drop table if exists ads.mall__product_infoCREATE EXTERNAL TABLE &#96;ads.mall__product_info&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;sku_num&#96; string COMMENT &#39;sku 个数&#39;,&#96;spu_num&#96; string COMMENT &#39;spu 个数&#39; ) COMMENT &#39;商品个数信息表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;product_info&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;product_infohive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39; dt, sku_num, spu_numfrom( select &#39;$db_date&#39; dt, count(*) sku_num from dwt.mall__sku_topic) tmp_sku_numjoin( select &#39;$db_date&#39; dt, count(*) spu_num from ( select spu_id from dwt.mall__sku_topic group by spu_id ) tmp_spu_id) tmp_spu_numontmp_sku_num.dt&#x3D;tmp_spu_num.dt;&quot;$hive -e &quot;$sql&quot;12.3.2 商品销量排行建表12345678910drop table if exists ads.mall__product_sale_topNCREATE EXTERNAL TABLE &#96;ads.mall__product_sale_topN&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;sku_num&#96; string COMMENT &#39;sku 个数&#39;,&#96;spu_num&#96; string COMMENT &#39;spu 个数&#39; ) COMMENT &#39;商品销量排名表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;product_sale_topN&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324252627#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;product_sale_topNhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39; dt, sku_id, payment_amountfromdws.mall__sku_action_daycountwheredt&#x3D;&#39;$db_date&#39;order by payment_amount desclimit 10;&quot;$hive -e &quot;$sql&quot;12.3.3 商品收藏排名建表12345678910drop table if exists ads.mall__product_favor_topNCREATE EXTERNAL TABLE &#96;ads.mall__product_favor_topN&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;sku_id&#96; string COMMENT &#39;商品 ID&#39;,&#96;favor_count&#96; bigint COMMENT &#39;收藏量&#39; ) COMMENT &#39;商品收藏排名表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;product_favor_topN&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324252627#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;product_favor_topNhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39; dt, sku_id, favor_countfromdws.mall__sku_action_daycountwheredt&#x3D;&#39;$db_date&#39;order by favor_count desclimit 10;&quot;$hive -e &quot;$sql&quot;12.3.4 商品加入购物车排名建表12345678910drop table if exists ads.mall__product_cart_topNCREATE EXTERNAL TABLE &#96;ads.mall__product_cart_topN&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;sku_id&#96; string COMMENT &#39;商品 ID&#39;,&#96;cart_num&#96; bigint COMMENT &#39;加入购物车数量&#39; ) COMMENT &#39;商品加入购物车排名表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;product_cart_topN&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324252627#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;product_cart_topNhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39; dt, sku_id, cart_numfromdws.mall__sku_action_daycountwheredt&#x3D;&#39;$db_date&#39;order by cart_num desclimit 10;&quot;$hive -e &quot;$sql&quot;12.3.5 商品退款率排名（近30天）建表12345678910drop table if exists ads.mall__product_refund_topNCREATE EXTERNAL TABLE &#96;ads.mall__product_refund_topN&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;sku_id&#96; string COMMENT &#39;商品 ID&#39;,&#96;refund_ratio&#96; decimal(10,2) COMMENT &#39;退款率&#39; ) COMMENT &#39;商品退款率排名(最近 30 天)表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;product_refund_topN&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;product_refund_topNhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, sku_id, refund_last_30d_count&#x2F;payment_last_30d_count*100 refund_ratiofrom dwt.mall__sku_topicorder by refund_ratio desclimit 10;&quot;$hive -e &quot;$sql&quot;12.3.6 商品差评率建表12345678910drop table if exists ads.mall__appraise_bad_topNCREATE EXTERNAL TABLE &#96;ads.mall__appraise_bad_topN&#96;(&#96;dt&#96; string COMMENT &#39;统计日期&#39;,&#96;sku_id&#96; string COMMENT &#39;商品 ID&#39;,&#96;appraise_bad_ratio&#96; decimal(10,2) COMMENT &#39;差评率&#39; ) COMMENT &#39;商品差评率表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;appraise_bad_topN&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324252627#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;appraise_bad_topNhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39; dt, sku_id, appraise_bad_count&#x2F;(appraise_good_count+appraise_mid_count+appraise_bad_count+appraise_default_count) appraise_bad_ratiofromdws.mall__sku_action_daycountwheredt&#x3D;&#39;$db_date&#39;order by appraise_bad_ratio desclimit 10;&quot;$hive -e &quot;$sql&quot;12.4 营销主题12.4.1 下单数目统计建表1234567891011drop table if exists ads.mall__order_daycountCREATE EXTERNAL TABLE &#96;ads.mall__order_daycount&#96;(dt string comment &#39;统计日期&#39;,order_count bigint comment &#39;单日下单笔数&#39;,order_amount bigint comment &#39;单日下单金额&#39;,order_users bigint comment &#39;单日下单用户数&#39; ) COMMENT &#39;下单数目统计表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;order_daycount&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据123456789101112131415161718192021222324#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;order_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect &#39;$db_date&#39;, sum(order_count), sum(order_amount), sum(if(order_count&gt;0,1,0))from dws.mall__user_action_daycountwhere dt&#x3D;&#39;$db_date&#39;;&quot;$hive -e &quot;$sql&quot;12.4.2 支付信息统计建表12345678910111213drop table if exists ads.mall__payment_daycountCREATE EXTERNAL TABLE &#96;ads.mall__payment_daycount&#96;(dt string comment &#39;统计日期&#39;,order_count bigint comment &#39;单日支付笔数&#39;,order_amount bigint comment &#39;单日支付金额&#39;,payment_user_count bigint comment &#39;单日支付人数&#39;,payment_sku_count bigint comment &#39;单日支付商品数&#39;,payment_avg_time double comment &#39;下单到支付的平均时长，取分钟数&#39; ) COMMENT &#39;支付信息统计表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;payment_daycount&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;payment_daycounthive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect tmp_payment.dt, tmp_payment.payment_count, tmp_payment.payment_amount, tmp_payment.payment_user_count, tmp_skucount.payment_sku_count, tmp_time.payment_avg_timefrom( select &#39;$db_date&#39; dt, sum(payment_count) payment_count, sum(payment_amount) payment_amount, sum(if(payment_count&gt;0,1,0)) payment_user_count from dws.mall__user_action_daycount where dt&#x3D;&#39;$db_date&#39;)tmp_paymentjoin( select &#39;$db_date&#39; dt, sum(if(payment_count&gt;0,1,0)) payment_sku_count from dws.mall__sku_action_daycount where dt&#x3D;&#39;$db_date&#39;)tmp_skucount on tmp_payment.dt&#x3D;tmp_skucount.dtjoin( select &#39;$db_date&#39; dt, sum(unix_timestamp(payment_time)-unix_timestamp(create_time))&#x2F;count(*)&#x2F;60 payment_avg_time from dwd.mall__fact_order_info where dt&#x3D;&#39;$db_date&#39; and payment_time is not null)tmp_time on tmp_payment.dt&#x3D;tmp_time.dt&quot;$hive -e &quot;$sql&quot;12.4.3 复购率建表1234567891011121314151617drop table if exists ads.mall__sale_tm_category1_stat_mnCREATE EXTERNAL TABLE &#96;ads.mall__sale_tm_category1_stat_mn&#96;(tm_id string comment &#39;品牌 id&#39;,category1_id string comment &#39;1 级品类 id &#39;,category1_name string comment &#39;1 级品类名称 &#39;,buycount bigint comment &#39;购买人数&#39;,buy_twice_last bigint comment &#39;两次以上购买人数&#39;,buy_twice_last_ratio decimal(10,2) comment &#39;单次复购率&#39;,buy_3times_last bigint comment &#39;三次以上购买人数&#39;,buy_3times_last_ratio decimal(10,2) comment &#39;多次复购率&#39;,stat_mn string comment &#39;统计月份&#39;,stat_date string comment &#39;统计日期&#39; ) COMMENT &#39;复购率表&#39;row format delimited fields terminated by &#39;\\t&#39;stored as parquetlocation &#39;&#x2F;warehouse&#x2F;ads&#x2F;mall&#x2F;sale_tm_category1_stat_mn&#x2F;&#39;tblproperties (&quot;parquet.compression&quot;&#x3D;&quot;snappy&quot;)导入数据1234567891011121314151617181920212223242526272829303132333435363738394041#!&#x2F;bin&#x2F;bashdb_date&#x3D;$&#123;date&#125;hive&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;bin&#x2F;hiveAPP1&#x3D;mallAPP2&#x3D;adstable_name&#x3D;sale_tm_category1_stat_mnhive_table_name&#x3D;$APP2.mall__$table_name# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$&#123;date&#125;&quot; ] ;then db_date&#x3D;$&#123;date&#125;else db_date&#x3D;&#96;date -d &quot;-1 day&quot; +%F&#96;fisql&#x3D;&quot; insert into table $hive_table_nameselect mn.sku_tm_id, mn.sku_category1_id, mn.sku_category1_name, sum(if(mn.order_count&gt;&#x3D;1,1,0)) buycount, sum(if(mn.order_count&gt;&#x3D;2,1,0)) buyTwiceLast, sum(if(mn.order_count&gt;&#x3D;2,1,0))&#x2F;sum( if(mn.order_count&gt;&#x3D;1,1,0)) buyTwiceLastRatio, sum(if(mn.order_count&gt;&#x3D;3,1,0)) buy3timeLast , sum(if(mn.order_count&gt;&#x3D;3,1,0))&#x2F;sum( if(mn.order_count&gt;&#x3D;1,1,0)) buy3timeLastRatio, date_format(&#39;$db_date&#39; ,&#39;yyyy-MM&#39;) stat_mn, &#39;$db_date&#39; stat_datefrom( select user_id, sd.sku_tm_id, sd.sku_category1_id, sd.sku_category1_name, sum(order_count) order_count from dws.mall__sale_detail_daycount sd where date_format(dt,&#39;yyyy-MM&#39;)&#x3D;date_format(&#39;$db_date&#39; ,&#39;yyyy-MM&#39;) group by user_id, sd.sku_tm_id, sd.sku_category1_id, sd.sku_category1_name) mngroup by mn.sku_tm_id, mn.sku_category1_id, mn.sku_category1_name;&quot;$hive -e &quot;$sql&quot;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"MySQL数据库索引-B+树","slug":"MySQL数据库索引-B-树","date":"2020-06-17T17:02:43.000Z","updated":"2020-08-16T17:04:17.620Z","comments":true,"path":"2020/06/18/MySQL数据库索引-B-树/","link":"","permalink":"cpeixin.cn/2020/06/18/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95-B-%E6%A0%91/","excerpt":"","text":"如果你对数据库的操作非常了解，针对我们现在这个问题，你就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的 SQL 语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：根据某个值查找数据，比如 select * from user where id=1234；根据区间值来查找某些数据，比如 select * from user where id &gt; 1234 and id &lt; 2345。除了这些功能性需求之外，这种问题往往还会涉及一些非功能性需求，比如安全、性能、用户体验等等。限于专栏要讨论的主要是数据结构和算法，对于非功能性需求，我们着重考虑性能方面的需求。性能方面的需求，我们主要考察时间和空间两方面，也就是执行效率和存储空间。在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉查找树、跳表。我们先来看散列表。散列表的查询性能很好，时间复杂度是 O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是 O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是 O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作 B+ 树。不过，它是通过二叉查找树演化过来的，而非跳表。为了给你还原发明 B+ 树的整个思考过程，所以，接下来，我还要从二叉查找树讲起，看它是如何一步一步被改造成 B+ 树的。改造二叉查找树来解决这个问题为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来是不是很像跳表呢？改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。比如，我们给一亿个数据构建二叉查找树索引，那索引中会包含大约 1 亿个节点，每个节点假设占用 16 个字节，那就需要大约 1GB 的内存空间。给一张表建立索引，我们需要 1GB 的内存空间。如果我们要给 10 张表建立索引，那对内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘 IO 操作。树的高度就等于每次查询数据时磁盘 IO 操作的次数。我们前面讲到，比起内存读写操作，磁盘 IO 操作非常耗时，所以我们优化的重点就是尽量减少磁盘 IO 操作，也就是，尽量降低树的高度。那如何降低树的高度呢？我们来看下，如果我们把索引构建成 m 叉树，高度是不是比二叉树要小呢？如图所示，给 16 个数据构建二叉树索引，树的高度是 4，查找一个数据，就需要 4 个磁盘 IO 操作（如果根节点存储在内存中，其他节点存储在磁盘中），如果对 16 个数据构建五叉树索引，那高度只有 2，查找一个数据，对应只需要 2 次磁盘操作。如果 m 叉树中的 m 是 100，那对一亿个数据构建索引，树的高度也只是 3，最多只要 3 次磁盘 IO 就能获取到数据。磁盘 IO 变少了，查找数据的效率也就提高了。)如果我们将 m 叉树实现 B+ 树索引，用代码实现出来，就是下面这个样子（假设我们给 int 类型的数据库字段添加索引，所以代码中的 keywords 是 int 类型的）：1234567891011121314151617181920212223242526272829303132333435&#x2F;** * 这是B+树非叶子节点的定义。 * * 假设keywords&#x3D;[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE &#x3D; (m-1)*4[keywordss大小]+m*8[children大小] *&#x2F;public class BPlusTreeNode &#123; public static int m &#x3D; 5; &#x2F;&#x2F; 5叉树 public int[] keywords &#x3D; new int[m-1]; &#x2F;&#x2F; 键值，用来划分数据区间 public BPlusTreeNode[] children &#x3D; new BPlusTreeNode[m];&#x2F;&#x2F;保存子节点指针&#125;&#x2F;** * 这是B+树中叶子节点的定义。 * * B+树中的叶子节点跟内部节点是不一样的, * 叶子节点存储的是值，而非区间。 * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。 * * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE &#x3D; k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小] *&#x2F;public class BPlusTreeLeafNode &#123; public static int k &#x3D; 3; public int[] keywords &#x3D; new int[k]; &#x2F;&#x2F; 数据的键值 public long[] dataAddress &#x3D; new long[k]; &#x2F;&#x2F; 数据地址 public BPlusTreeLeafNode prev; &#x2F;&#x2F; 这个结点在链表中的前驱结点 public BPlusTreeLeafNode next; &#x2F;&#x2F; 这个结点在链表中的后继结点&#125;我稍微解释一下这段代码。对于相同个数的数据构建 m 叉树索引，m 叉树中的 m 越大，那树的高度就越小，那 m 叉树中的 m 是不是越大越好呢？到底多大才最合适呢？不管是内存中的数据，还是磁盘中的数据，操作系统都是按页（一页大小通常是 4KB，这个值可以通过 getconfig PAGE_SIZE 命令查看）来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次 IO 操作。所以，我们在选择 m 大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘 IO 操作。尽管索引可以提高数据库的查询效率，但是，作为一名开发工程师，你应该也知道，索引有利也有弊，它也会让写入数据的效率下降。这是为什么呢？数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。对于一个 B+ 树来说，m 值是根据页的大小事先计算好的，也就是说，每个节点最多只能有 m 个子节点。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过 m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘 IO 操作。我们该如何解决这个问题呢？实际上，处理思路并不复杂。我们只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过 m 个。不过这也没关系，我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，你可以结合着下面这个图一块看，会更容易理解（图中的 B+ 树是一个三叉树。我们限定叶子节点中，数据的个数超过 2 个就分裂节点；非叶子节点中，子节点的个数超过 3 个就分裂节点）。正是因为要时刻保证 B+ 树索引是一个 m 叉树，所以，索引的存在会导致数据库写入的速度降低。实际上，不光写入数据会变慢，删除数据也会变慢。这是为什么呢？我们在删除某个数据的时候，也要对应地更新索引节点。这个处理思路有点类似跳表中删除数据的处理思路。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。我们可以设置一个阈值。在 B+ 树中，这个阈值等于 m/2。如果某个节点的子节点个数小于 m/2，我们就将它跟相邻的兄弟节点合并。不过，合并之后节点的子节点个数有可能会超过 m。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点。文字描述不是很直观，我举了一个删除操作的例子，你可以对比着看下（图中的 B+ 树是一个五叉树。我们限定叶子节点中，数据的个数少于 2 个就合并节点；非叶子节点中，子节点的个数少于 3 个就合并节点。）。数据库索引以及 B+ 树的由来，到此就讲完了。你有没有发现，B+ 树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代 B+ 树，作为数据库的索引实现的。B+ 树发明于 1972 年，跳表发明于 1989 年，我们可以大胆猜想下，跳表的作者有可能就是受了 B+ 树的启发，才发明出跳表来的。不过，这个也无从考证了。总结今天，我们讲解了数据库索引实现，依赖的底层数据结构，B+ 树。它通过存储在磁盘的多叉树结构，做到了时间、空间的平衡，既保证了执行效率，又节省了内存。前面的讲解中，为了一步一步详细地给你介绍 B+ 树的由来，内容看起来比较零散。为了方便你掌握和记忆，我这里再总结一下 B+ 树的特点：每个节点中子节点的个数不能超过 m，也不能小于 m/2；根节点的子节点个数可以不超过 m/2，这是一个例外；m 叉树只存储索引，并不真正存储数据，这个有点儿类似跳表；通过链表将叶子节点串联在一起，这样可以方便按区间查找；一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。除了 B+ 树，你可能还听说过 B 树、B- 树，我这里简单提一下。实际上，B- 树就是 B 树，英文翻译都是 B-Tree，这里的“-”并不是相对 B+ 树中的“+”，而只是一个连接符。这个很容易误解，所以我强调下。而 B 树实际上是低级版的 B+ 树，或者说 B+ 树是 B 树的改进版。B 树跟 B+ 树的不同点主要集中在这几个地方：B+ 树中的节点不存储数据，只是索引，而 B 树中的节点存储数据；B 树中的叶子节点并不需要链表来串联。也就是说，B 树只是一个每个节点的子节点个数不能小于 m/2 的 m 叉树。","categories":[{"name":"DataBases","slug":"DataBases","permalink":"cpeixin.cn/categories/DataBases/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"cpeixin.cn/tags/mysql/"}]},{"title":"Flink apply()&process() 讲解","slug":"Flink-apply-process-讲解","date":"2020-05-29T17:01:15.000Z","updated":"2020-09-07T13:41:16.457Z","comments":true,"path":"2020/05/30/Flink-apply-process-讲解/","link":"","permalink":"cpeixin.cn/2020/05/30/Flink-apply-process-%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"在处理流数据计算时，我们在对流数据使用了keyby()和window()后，需要对分组后的数据做分组处理，那么除了对分组数据直接做reduce()，aggregate()等聚合操作之外，还有另一种场景就是对分组后的数据，每一个key对应的Iterable做稍微复杂一点的数据计算或者数据的整合或者变换。那么这里我们就可以使用apply()或者process()来实现更底层的计算逻辑。那么两者之间有什么区别呢？我们来看一下两者的源码：从上面的图片中，我们可以看到，两个方法的注释居然是一样的。内部的实现则不同，分别调用了ScalaProcessWindowFunctionWrapper和ScalaWindowFunctionWrapper我们来看ScalaProcessWindowFunctionWrapper，内部实现的process中，传入了context上下文，还有其他window相关有用的参数，从功能上来讲，process已经包含了apply能提供的功能，apply()是旧版的process()，并且没有提供上下文信息和其他window的高级功能，例如每个窗口的keyed state。apply()将在某个时间被弃用。process算子常见的有ProcessFunction和KeyedProcessFunction两种计算的方式，具体的实现可以看源码。process和apply算子最大的区别在于process可以自己定时触发计算的定时器,在processElement方法定义定时器 context.timerService().registerEventTimeTimer(timestamp);当定时器时间到达，会回调onTimer()方法的计算任务时器允许应用程序对processing time和event time的变化做出反应，每次对processElement()的调用都会得到一个Context对象，该对象允许访问元素事件时间的时间戳和TimeServer。TimeServer可以用来为尚未发生的event-time或者processing-time注册回调，当定时器的时间到达时，onTimer(…)方法会被调用。在这个调用期间，所有的状态都会限定到创建定时器的键，并允许定时器操纵键控状态(keyed states)。apply算子和process算子**(1) window数据apply算子对应的fuctionWindowFunction(keyed window) 与 AllWindowFunction(no key window)(2) window数据process算子对应的fuctionprocess算子可以获取到window窗口的Context信息 而 apply算子无法获取到ProcessWindowFunction(keyed-window) 和 ProcessAllWindowFunction(no keyd window)keyed-window 和 nokeyed-window(3) 普通流数据，process算子,对应的fuctionKeyedProcessFunction： A keyed function that processes elements of a stream. （可实现定时任务）ProcessFunction：A function that processes elements of a stream.（可实现定时任务）下面我们用一个小实例来演示两种方法的使用需求：求每个user，周一至周日，每天成绩的排序详情数据格式：user001,1,95（用户ID，星期几，成绩）程序如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package data_stream.functionimport org.apache.flink.streaming.api.scala.function.&#123;ProcessWindowFunction, WindowFunction&#125;import org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, WindowedStream&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.streaming.api.scala._import org.apache.flink.util.Collectorimport scala.collection.mutable.ListBufferimport scala.collection.parallel.immutableobject keyby_datastream &#123; case class UserGrade(user_id: String, weekday: String, grade: Int) case class UserGradeList(user_id: String, gradeList: List[Int]) def main(args: Array[String]): Unit = &#123; val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val socketStream: DataStream[String] = env.socketTextStream(\"localhost\", 8888) val userGradeStream: DataStream[UserGrade] = socketStream.map((data: String) =&gt; &#123; val user_array: Array[String] = data.split(\",\") UserGrade(user_array(0), user_array(1), user_array(2).toInt) &#125;) val keyWondowStream: WindowedStream[UserGrade, String, TimeWindow] = userGradeStream.keyBy((_: UserGrade).user_id) .timeWindow(Time.seconds(5)) // Base interface for functions that are evaluated over keyed (grouped) windows. // trait WindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function with Serializable// val resultStream: DataStream[(String, List[Int])] = keyWondowStream.apply(new WindowFunction[UserGrade, (String, List[Int]), String, TimeWindow] &#123;// override def apply(key: String, window: TimeWindow, input: Iterable[UserGrade], out: Collector[(String, List[Int])]): Unit = &#123;// var gradeList: ListBuffer[Int] = ListBuffer[Int]()// for (data &lt;- input) &#123;// gradeList += data.grade// &#125;// out.collect((key,gradeList.toList.sorted))// &#125;// &#125;)// val resultStream: DataStream[String] = keyWondowStream.apply(new WindowFunction[UserGrade, String, String, TimeWindow] &#123;// override def apply(key: String, window: TimeWindow, input: Iterable[UserGrade], out: Collector[String]): Unit = &#123;// var gradeList: ListBuffer[Int] = ListBuffer[Int]()// for (data &lt;- input)&#123;// gradeList+=data.grade// &#125;// val result: String = key + \"===\" + gradeList.toList.sorted.toString// out.collect(result)// &#125;// &#125;) val resultStream: DataStream[UserGradeList] = keyWondowStream.process(new ProcessWindowFunction[UserGrade, UserGradeList, String, TimeWindow]&#123; override def process(key: String, context: Context, elements: Iterable[UserGrade], out: Collector[UserGradeList]): Unit = &#123; var gradeList: ListBuffer[Int] = ListBuffer[Int]() for (data &lt;- elements) &#123; gradeList += data.grade &#125; out.collect(UserGradeList(key,gradeList.toList.sorted)) &#125; &#125;) resultStream.print(\"===\") env.execute(\"process apply function\") &#125;&#125;控制台输入：1234567891011121314user001,1,95user001,2,200user001,3,97user001,4,189user001,5,99user001,6,100user001,7,40user002,1,1user002,2,5user002,3,2user002,4,88user002,5,4user002,6,33user002,7,22结果打印：12&#x3D;&#x3D;&#x3D;:3&gt; UserGradeList(user001,List(40, 95, 97, 99, 100, 189, 200))&#x3D;&#x3D;&#x3D;:2&gt; UserGradeList(user002,List(1, 2, 4, 5, 22, 33, 88))这里有一点需要注意，我在程序中使用了scala可变集合ListBuffer，程序执行后，则报错Caused by: java.lang.NumberFormatException: Not a version: 9后来Google得到，这是Flink版本和Scala版本之间引起的错误，这里要注意，原来的版本对应是Flink 1.10和Scala 2.11.8，后来Scala版本修改为2.11.12，这个问题就解决了。**","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Linux直接下载Google Drive文件","slug":"Linux直接下载Google-Drive文件","date":"2020-05-27T17:03:54.000Z","updated":"2020-05-28T15:08:18.891Z","comments":true,"path":"2020/05/28/Linux直接下载Google-Drive文件/","link":"","permalink":"cpeixin.cn/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/","excerpt":"","text":"在Google Colab中训练完模型，保存在Google Drive中，整个项目大小有3、4GB大小，此时想将这整个项目放到服务器中进行部署测试。首选方案是将整个项目上传到GitHub中，随后在服务器中直接wget，但是模型文件过大，GitHub单个文件的限制是100MB。突然想到可不可以直接从Google Drive上进行下载模型文件到服务器😅下载小文件：选择要下载的文件右键点击“共享”点击“更改”，设置分享权限这是复制图中选中部分的ID拼接下载链接，进行下载wget https://drive.google.com/uc?id=复制下来的共享id -O your_file_name下载大文件：上面的方法，适合下载一些小文件，大文件就不可以了。更换下面命令的id选项，并且准备好cookies.txt关于cookies.txt，可以在Chrome浏览器中下载cookie.txt这个插件，点击下载，上传到服务器中/tmp目录下即可关于文件id，和上面方法获取一致，接下来运行下面命令即可。1wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZag' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&amp;id=15q9WdqjpZKiVXUo7FOII7O2WLxZagARe\" -O pytorch_model.bin","categories":[{"name":"Tools","slug":"Tools","permalink":"cpeixin.cn/categories/Tools/"}],"tags":[]},{"title":"Flink 数据倾斜","slug":"Flink-数据倾斜","date":"2020-05-06T17:46:55.000Z","updated":"2020-09-07T03:34:23.429Z","comments":true,"path":"2020/05/07/Flink-数据倾斜/","link":"","permalink":"cpeixin.cn/2020/05/07/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/","excerpt":"","text":"无论是对于 Flink、Spark 这样的实时计算框架还是 Hive 等离线计算框架，数据量从来都不是问题，真正引起问题导致严重后果的是数据倾斜。所谓数据倾斜，是指在大规模并行处理的数据中，其中某个运行节点处理的数据远远超过其他部分，这会导致该节点压力极大，最终出现运行失败从而导致整个任务的失败。我们在这一课时中将分析出现数据倾斜的原因，Flink 任务中最容易出现数据倾斜的几个算子并且给出解决方案。数据倾斜背景和危害数据倾斜产生的原因和危害和解决方案有哪些呢？我们一一来看。数据倾斜原理目前我们所知道的大数据处理框架，比如 Flink、Spark、Hadoop 等之所以能处理高达千亿的数据，是因为这些框架都利用了分布式计算的思想，集群中多个计算节点并行，使得数据处理能力能得到线性扩展。在实际生产中 Flink 都是以集群的形式在运行，在运行的过程中包含了两类进程。其中 TaskManager 实际负责执行计算的 Worker，在其上执行 Flink Job 的一组 Task，Task 则是我们执行具体代码逻辑的容器。理论上只要我们的任务 Task 足够多就可以对足够大的数据量进行处理。但是实际上大数据量经常出现，一个 Flink 作业包含 200 个 Task 节点，其中有 199 个节点可以在很短的时间内完成计算。但是有一个节点执行时间远超其他结果，并且随着数据量的持续增加，导致该计算节点挂掉，从而整个任务失败重启。我们可以在 Flink 的管理界面中看到任务的某一个 Task 数据量远超其他节点。数据倾斜原因和解决方案Flink 任务出现数据倾斜的直观表现是任务节点频繁出现反压，但是增加并行度后并不能解决问题；部分节点出现 OOM 异常，是因为大量的数据集中在某个节点上，导致该节点内存被爆，任务失败重启。产生数据倾斜的原因主要有 2 个方面：业务上有严重的数据热点，比如滴滴打车的订单数据中北京、上海等几个城市的订单量远远超过其他地区；技术上大量使用了 KeyBy、GroupBy 等操作，错误的使用了分组 Key，人为产生数据热点。因此解决问题的思路也很清晰：业务上要尽量避免热点 key 的设计，例如我们可以把北京、上海等热点城市分成不同的区域，并进行单独处理；技术上出现热点时，要调整方案打散原来的 key，避免直接聚合；此外 Flink 还提供了大量的功能可以避免数据倾斜。那么我们就从典型的场景入手，看看在 Flink 任务中出现数据倾斜的主要场景和解决方案。Flink 任务数据倾斜场景和解决方案两阶段聚合解决 KeyBy 热点KeyBy 是我们经常使用的分组聚合函数之一。在实际的业务中经常会碰到这样的场景：双十一按照下单用户所在的省聚合求订单量最高的前 10 个省，或者按照用户的手机类型聚合求访问量最高的设备类型等。上述场景在我们进行 KeyBy 时就会出现严重的数据倾斜，如下图所示：如果我们直接简单地使用 KeyBy 算子，模拟一个简单的统计 PV 的场景如下：123456DataStream sourceStream &#x3D; ...;windowedStream &#x3D; sourceStream.keyBy(&quot;type&quot;) .window(TumblingEventTimeWindows.of(Time.minutes(1)));windowedStream.process(new MyPVFunction()) .addSink(new MySink())... env.execute()...我们在根据 type 进行 KeyBy 时，如果数据的 type 分布不均匀就会导致大量的数据分配到一个 task 中去，发生数据倾斜。那么我们的解决思路是：首先把分组的 key 打散，比如加随机后缀；对打散后的数据进行聚合；把打散的 key 还原为真正的 key；二次 KeyBy 进行结果统计，然后输出。123456789101112131415161718192021DataStream sourceStream &#x3D; ...;resultStream &#x3D; sourceStream .map(record -&gt; &#123; Record record &#x3D; JSON.parseObject(record, Record.class); String type &#x3D; record.getType(); record.setType(type + &quot;#&quot; + new Random().nextInt(100)); return record; &#125;) .keyBy(0) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .aggregate(new CountAggregate()) .map(count -&gt; &#123; String key &#x3D; count.getKey.substring(0, count.getKey.indexOf(&quot;#&quot;)); return RecordCount(key,count.getCount); &#125;) &#x2F;&#x2F;二次聚合 .keyBy(0) .process(new CountProcessFunction);resultStream.sink()...env.execute()...其中 CountAggregate 函数实现如下：12345678910111213141516171819202122public class CountAggregate implements AggregateFunction&lt;Record,CountRecord,CountRecord&gt; &#123; @Override public CountRecord createAccumulator() &#123; return new CountRecord(null, 0L); &#125; @Override public CountRecord add(Record value, CountRecord accumulator) &#123; if(accumulator.getKey() &#x3D;&#x3D; null)&#123; accumulator.setKey(value.key); &#125; accumulator.setCount(value.count); return accumulator; &#125; @Override public CountRecord getResult(CountRecord accumulator) &#123; return accumulator; &#125; @Override public CountRecord merge(CountRecord a, CountRecord b) &#123; return new CountRecord(a.getKey(),a.getCount()+b.getCount()) ; &#125;&#125;CountProcessFunction 的实现如下：123456789101112131415161718192021public class CountProcessFunction extends KeyedProcessFunction&lt;String, CountRecord, CountRecord&gt; &#123; private ValueState&lt;Long&gt; state &#x3D; this.getRuntimeContext().getState(new ValueStateDescriptor(&quot;count&quot;,Long.class)); @Override public void processElement(CountRecord value, Context ctx, Collector&lt;CountRecord&gt; out) throws Exception &#123; if(state.value()&#x3D;&#x3D;0)&#123; state.update(value.count); ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 1000L * 5); &#125;else&#123; state.update(state.value() + value.count); &#125; &#125; @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;CountRecord&gt; out) throws Exception &#123; &#x2F;&#x2F;这里可以做业务操作，例如每 5 分钟将统计结果发送出去 &#x2F;&#x2F;out.collect(...); &#x2F;&#x2F;清除状态 state.clear(); &#x2F;&#x2F;其他操作 ... &#125;&#125;通过上面打散聚合再二次聚合的方式，我们就可以实现热点 Key 的打散，消除数据倾斜。GroupBy + Aggregation 分组聚合热点问题业务上通过 GroupBy 进行分组，然后紧跟一个 SUM、COUNT 等聚合操作是非常常见的。我们都知道 GroupBy 函数会根据 Key 进行分组，完全依赖 Key 的设计，如果 Key 出现热点，那么会导致巨大的 shuffle，相同 key 的数据会被发往同一个处理节点；如果某个 key 的数据量过大则会直接导致该节点成为计算瓶颈，引起反压。我们还是按照上面的分组统计 PV 的场景，SQL 语句如下：12345678select date, type, sum(count) as pvfrom table group by date, type;我们可以通过内外两层聚合的方式将 SQL 改写为：1234567891011121314151617select date, type, sum(pv) as pvfrom( select date, type, sum(count) as pv from table group by date, type, floor(rand()*100) --随机打散成100份 ) group by date, type;在上面的 SQL 拆成了内外两层，第一层通过随机打散 100 份的方式减少数据热点，当然这个打散的方式可以根据业务灵活指定。Flink 消费 Kafka 上下游并行度不一致导致的数据倾斜通常我们在使用 Flink 处理实时业务时，上游一般都是消息系统，Kafka 是使用最广泛的大数据消息系统。当使用 Flink 消费 Kafka 数据时，也会出现数据倾斜。需要十分注意的是，我们 Flink 消费 Kafka 的数据时，是推荐上下游并行度保持一致，即 Kafka 的分区数等于 Flink Consumer 的并行度。但是会有一种情况，为了加快数据的处理速度，来设置 Flink 消费者的并行度大于 Kafka 的分区数。如果你不做任何的设置则会导致部分 Flink Consumer 线程永远消费不到数据。这时候你需要设置 Flink 的 Redistributing，也就是数据重分配。Flink 提供了多达 8 种重分区策略，类图如下图所示：在我们接收到 Kafka 消息后，可以通过自定义数据分区策略来实现数据的负载均衡，例如：复制123456dataStream .setParallelism(2) &#x2F;&#x2F; 采用REBALANCE分区策略重分区 .rebalance() &#x2F;&#x2F;.rescale() .print() .setParallelism(4);其中，Rebalance 分区策略，数据会以 round-robin 的方式对数据进行再次分区，可以全局负载均衡。Rescale 分区策略基于上下游的并行度，会将数据以循环的方式输出到下游的每个实例中。其他Flink 一直在不断地迭代，不断出现各种各样的手段解决我们遇到的数据倾斜问题。例如，MiniBatch 微批处理手段等，需要我们开发者不断地去发现，并学习新的解决问题的办法。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink State 状态管理","slug":"Flink-State-状态管理","date":"2020-04-29T01:51:40.000Z","updated":"2020-10-09T07:47:14.702Z","comments":true,"path":"2020/04/29/Flink-State-状态管理/","link":"","permalink":"cpeixin.cn/2020/04/29/Flink-State-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/","excerpt":"","text":"我们先回顾一下到底什么是 state，流式计算的数据往往是转瞬即逝， 当然，真实业务场景不可能说所有的数据都是进来之后就走掉，没有任何东西留下来，那么留下来的东西其实就是称之为 state，中文可以翻译成状态。在下面这个图中，我们的所有的原始数据进入用户代码之后再输出到下游，如果中间涉及到 state 的读写，这些状态会存储在本地的 state backend（可以对标成嵌入式本地 kv 存储）当中。一. 状态管理的基本概念1. 什么是状态首先举一个无状态计算的例子：消费延迟计算。假设现在有一个消息队列，消息队列中有一个生产者持续往消费队列写入消息，多个消费者分别从消息队列中读取消息。从图上可以看出，生产者已经写入 16 条消息，Offset 停留在 15 ；有 3 个消费者，有的消费快，而有的消费慢。消费快的已经消费了 13 条数据，消费者慢的才消费了 7、8 条数据。如何实时统计每个消费者落后多少条数据，如图给出了输入输出的示例。可以了解到输入的时间点有一个时间戳，生产者将消息写到了某个时间点的位置，每个消费者同一时间点分别读到了什么位置。刚才也提到了生产者写入了 15 条，消费者分别读取了 10、7、12 条。那么问题来了，怎么将生产者、消费者的进度转换为右侧示意图信息呢？consumer 0 落后了 5 条，consumer 1 落后了 8 条，consumer 2 落后了 3 条，根据 Flink 的原理，此处需进行 Map 操作。Map 首先把消息读取进来，然后分别相减，即可知道每个 consumer 分别落后了几条。Map 一直往下发，则会得出最终结果。大家会发现，在这种模式的计算中，无论这条输入进来多少次，输出的结果都是一样的，因为单条输入中已经包含了所需的所有信息。消费落后等于生产者减去消费者。生产者的消费在单条数据中可以得到，消费者的数据也可以在单条数据中得到，所以相同输入可以得到相同输出，这就是一个无状态的计算。相应的什么是有状态的计算？以访问日志统计量的例子进行说明，比如当前拿到一个 Nginx 访问日志，一条日志表示一个请求，记录该请求从哪里来，访问的哪个地址，需要实时统计每个地址总共被访问了多少次，也即每个 API 被调用了多少次。可以看到下面简化的输入和输出，输入第一条是在某个时间点请求 GET 了 /api/a；第二条日志记录了某个时间点 Post /api/b ; 第三条是在某个时间点 GET 了一个 /api/a，总共有 3 个 Nginx 日志。从这 3 条 Nginx 日志可以看出，第一条进来输出 /api/a 被访问了一次，第二条进来输出 /api/b 被访问了一次，紧接着又进来一条访问 api/a，所以 api/a 被访问了 2 次。不同的是，两条 /api/a 的 Nginx 日志进来的数据是一样的，但输出的时候结果可能不同，第一次输出 count=1 ，第二次输出 count=2，说明相同输入可能得到不同输出。输出的结果取决于当前请求的 API 地址之前累计被访问过多少次。第一条过来累计是 0 次，count = 1，第二条过来 API 的访问已经有一次了，所以 /api/a 访问累计次数 count=2。单条数据其实仅包含当前这次访问的信息，而不包含所有的信息。要得到这个结果，还需要依赖 API 累计访问的量，即状态。这个计算模式是将数据输入算子中，用来进行各种复杂的计算并输出数据。这个过程中算子会去访问之前存储在里面的状态。另外一方面，它还会把现在的数据对状态的影响实时更新，如果输入 200 条数据，最后输出就是 200 条结果。Flink 的状态数据可以存在 JVM 的堆内存或者堆外内存中，当然也可以借助第三方存储，例如 Flink 已经实现的对 RocksDB 支持。Flink 的官网同样给出了适用于状态计算的几种情况：去重：比如上游的系统数据可能会有重复，落到下游系统时希望把重复的数据都去掉。去重需要先了解哪些数据来过，哪些数据还没有来，也就是把所有的主键都记录下来，当一条数据到来后，能够看到在主键当中是否存在。窗口计算：比如统计每分钟 Nginx 日志 API 被访问了多少次。窗口是一分钟计算一次，在窗口触发前，如 08:00 ~ 08:01 这个窗口，前 59 秒的数据来了需要先放入内存，即需要把这个窗口之内的数据先保留下来，等到 8:01 时一分钟后，再将整个窗口内触发的数据输出。未触发的窗口数据也是一种状态。机器学习 / 深度学习：如训练的模型以及当前模型的参数也是一种状态，机器学习可能每次都用有一个数据集，需要在数据集上进行学习，对模型进行一个反馈。访问历史数据：比如与昨天的数据进行对比，需要访问一些历史数据。如果每次从外部去读，对资源的消耗可能比较大，所以也希望把这些历史数据也放入状态中做对比。2. 为什么要管理状态管理状态最直接的方式就是将数据都放到内存中，这也是很常见的做法。比如在做 WordCount 时，Word 作为输入，Count 作为输出。在计算的过程中把输入不断累加到 Count。但对于流式作业有以下要求：7*24 小时运行，高可靠；数据不丢不重，恰好计算一次；数据实时产出，不延迟；基于以上要求，内存的管理就会出现一些问题。由于内存的容量是有限制的。如果要做 24 小时的窗口计算，将 24 小时的数据都放到内存，可能会出现内存不足；另外，作业是 7*24，需要保障高可用，机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，保证运行的作业不受影响；此外，考虑横向扩展，假如网站的访问量不高，统计每个 API 访问次数的程序可以用单线程去运行，但如果网站访问量突然增加，单节点无法处理全部访问数据，此时需要增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点也问题之一。因此，将数据都放到内存中，并不是最合适的一种状态管理方式。3. 理想的状态管理最理想的状态管理需要满足易用、高效、可靠三点需求：易用，Flink 提供了丰富的数据结构、多样的状态组织形式以及简洁的扩展接口，让状态管理更加易用；高效，实时作业一般需要更低的延迟，一旦出现故障，恢复速度也需要更快；当处理能力不够时，可以横向扩展，同时在处理备份时，不影响作业本身处理性能；可靠，Flink 提供了状态持久化，包括不丢不重的语义以及具备自动的容错能力，比如 HA，当节点挂掉后会自动拉起，不需要人工介入。二.Flink 状态的类型与使用示例1.Managed State &amp; Raw StateManaged State 是 Flink 自动管理的 State，而 Raw State 是原生态 State，两者的区别如下：从状态管理方式的方式来说，Managed State 由 Flink Runtime 管理，自动存储，自动恢复，在内存管理上有优化；而 Raw State 需要用户自己管理，需要自己序列化，Flink 不知道 State 中存入的数据是什么结构，只有用户自己知道，需要最终序列化为可存储的数据结构。从状态数据结构来说，Managed State 支持已知的数据结构，如 Value、List、Map 等。而 Raw State 只支持字节数组 ，所有状态都要转换为二进制字节数组才可以。从推荐使用场景来说，Managed State 大多数情况下均可使用，而 Raw State 是当 Managed State 不够用时，比如需要自定义 Operator 时，推荐使用 Raw State。2.Keyed State &amp; Operator StateManaged State 分为两种，一种是 Keyed State；另外一种是 Operator State。在 Flink Stream 模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream 。每个 Key 对应一个 State，即一个 Operator 实例处理多个 Key，访问相应的多个 State，并由此就衍生了 Keyed State。Keyed State 只能用在 KeyedStream 的算子中，即在整个程序中没有 keyBy 的过程就没有办法使用 KeyedStream。相比较而言，Operator State 可以用于所有算子，相对于数据源有一个更好的匹配方式，常用于 Source，例如 FlinkKafkaConsumer。相比 Keyed State，一个 Operator 实例对应一个 State，随着并发的改变，Keyed State 中，State 随着 Key 在实例间迁移，比如原来有 1 个并发，对应的 API 请求过来，/api/a 和 /api/b 都存放在这个实例当中；如果请求量变大，需要扩容，就会把 /api/a 的状态和 /api/b 的状态分别放在不同的节点。由于 Operator State 没有 Key，并发改变时需要选择状态如何重新分配。其中内置了 2 种分配方式：一种是均匀分配，另外一种是将所有 State 合并为全量 State 再分发给每个实例。在访问上，Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个 Rich Function。Operator State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。在数据结构上，Keyed State 支持的数据结构，比如 ValueState、ListState、ReducingState、AggregatingState 和 MapState；而 Operator State 支持的数据结构相对较少，如 ListState。3.Keyed State &amp; Operator State 区别接下来我们会在四个维度来区分两种不同的 state：operator state 以及 keyed state。1. 是否存在当前处理的 key（current key）：operator state 是没有当前 key 的概念，而 keyed state 的数值总是与一个 current key 对应。2. 存储对象是否 on heap: 目前 operator state backend 仅有一种 on-heap 的实现；而 keyed state backend 有 on-heap 和 off-heap（RocksDB）的多种实现。3. 是否需要手动声明快照（snapshot）和恢复 (restore) 方法：operator state 需要手动实现 snapshot 和 restore 方法；而 keyed state 则由 backend 自行实现，对用户透明。4. 数据大小：一般而言，我们认为 operator state 的数据规模是比较小的；认为 keyed state 规模是相对比较大的。需要注意的是，这是一个经验判断，不是一个绝对的判断区分标准。4.Keyed State 使用示例Keyed State 有很多种，如图为几种 Keyed State 之间的关系。首先 State 的子类中一级子类有 ValueState、MapState、AppendingState。AppendingState 又有一个子类 MergingState。MergingState 又分为 3 个子类分别是 ListState、ReducingState、AggregatingState。这个继承关系使它们的访问方式、数据结构也存在差异。几种 Keyed State 的差异具体体现在：ValueState 存储单个值，比如 Wordcount，用 Word 当 Key，State 就是它的 Count。这里面的单个值可能是数值或者字符串，作为单个值，访问接口可能有两种，get 和 set。在 State 上体现的是 update(T) / T value()。MapState 的状态数据类型是 Map，在 State 上有 put、remove 等。需要注意的是在 MapState 中的 key 和 Keyed state 中的 key 不是同一个。ListState 状态数据类型是 List，访问接口如 add、update 等。ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值，原因在于其中的 add 方法不是把当前的元素追加到列表中，而是把当前元素直接更新进了 Reducing 的结果中。AggregatingState 的区别是在访问接口，ReducingState 中 add（T）和 T get() 进去和出来的元素都是同一个类型，但在 AggregatingState 输入的 IN，输出的是 OUT。下面以 ValueState 为例，来阐述一下具体如何使用，我们先从简单的示例来演示：（1）Flink 提供了 StateDesciptor 方法专门用来访问不同的 state，类图如下：下面演示一下如何使用 StateDesciptor 和 ValueState，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import org.apache.flink.api.common.functions.RichFlatMapFunction;import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.state.ValueState;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.time.Time;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;public class StateTest &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 5L), Tuple2.of(1L, 2L)) .keyBy(0) .flatMap(new CountWindowAverage()) .printToErr(); env.execute(\"submit job\"); &#125; public static class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; &#123; private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; Tuple2&lt;Long, Long&gt; currentSum; // 访问ValueState if(sum.value()==null)&#123; currentSum = Tuple2.of(0L, 0L); &#125;else &#123; currentSum = sum.value(); &#125; // 更新 currentSum.f0 += 1; // 第二个元素加1 currentSum.f1 += input.f1; // 更新state sum.update(currentSum); // 如果count的值大于等于2，求知道并清空state if (currentSum.f0 &gt;= 2) &#123; out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); &#125; &#125; public void open(Configuration config) &#123; ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( \"average\", // state的名字 TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;) ); // 设置默认值 StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(10)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build(); descriptor.enableTimeToLive(ttlConfig); sum = getRuntimeContext().getState(descriptor); &#125; &#125;&#125;我们通过继承 RichFlatMapFunction 来访问 State，通过 getRuntimeContext().getState(descriptor) 来获取状态的句柄。而真正的访问和更新状态则在 Map 函数中实现。我们这里的输出条件为，每当第一个元素的和达到二，就把第二个元素的和与第一个元素的和相除，最后输出。我们直接运行，在控制台可以看到结果：Operator State 的实际应用场景不如 Keyed State 多，一般来说它会被用在 Source 或 Sink 等算子上，用来保存流入数据的偏移量或对输出数据做缓存，以保证 Flink 应用的 Exactly-Once 语义。同样，我们对于任何状态数据还可以设置它们的过期时间。如果一个状态设置了 TTL，并且已经过期，那么我们之前保存的值就会被清理。想要使用 TTL，我们需要首先构建一个 StateTtlConfig 配置对象；然后，可以通过传递配置在任何状态描述符中启用 TTL 功能。StateTtlConfig 这个类中有一些配置需要我们注意：UpdateType 表明了过期时间什么时候更新，而对于那些过期的状态，是否还能被访问则取决于 StateVisibility 的配置。（2）接下来以更复杂的状态机的案例来讲解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172import org.apache.flink.api.common.functions.RichFlatMapFunction;import org.apache.flink.api.common.state.ValueState;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.configuration.Configuration;import org.apache.flink.contrib.streaming.state.RocksDBStateBackend;import org.apache.flink.core.fs.FileSystem;import org.apache.flink.runtime.state.filesystem.FsStateBackend;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.source.SourceFunction;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;import org.apache.flink.streaming.examples.statemachine.dfa.State;import org.apache.flink.streaming.examples.statemachine.event.Alert;import org.apache.flink.streaming.examples.statemachine.event.Event;import org.apache.flink.streaming.examples.statemachine.generator.EventsGeneratorSource;import org.apache.flink.streaming.examples.statemachine.kafka.EventDeSerializer;import org.apache.flink.util.Collector;import java.util.Properties;/** * Main class of the state machine example. * This class implements the streaming application that receives the stream of events and evaluates * a state machine (per originating address) to validate that the events follow * the state machine's rules. */public class StateMachineExample &#123; /** * Main entry point for the program. * * @param args The command line arguments. */ public static void main(String[] args) throws Exception &#123; // ---- print some usage help ---- System.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate &lt;probability-of-invalid-transition&gt;] [--sleep &lt;sleep-per-record-in-ms&gt;]\"); System.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic &lt;topic&gt; [--brokers &lt;brokers&gt;]\"); System.out.println(\"Options for both the above setups: \"); System.out.println(\"\\t[--backend &lt;file|rocks&gt;]\"); System.out.println(\"\\t[--checkpoint-dir &lt;filepath&gt;]\"); System.out.println(\"\\t[--async-checkpoints &lt;true|false&gt;]\"); System.out.println(\"\\t[--incremental-checkpoints &lt;true|false&gt;]\"); System.out.println(\"\\t[--output &lt;filepath&gt; OR null for stdout]\"); System.out.println(); // ---- determine whether to use the built-in source, or read from Kafka ---- final SourceFunction&lt;Event&gt; source; final ParameterTool params = ParameterTool.fromArgs(args); if (params.has(\"kafka-topic\")) &#123; // set up the Kafka reader String kafkaTopic = params.get(\"kafka-topic\"); String brokers = params.get(\"brokers\", \"localhost:9092\"); System.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers); System.out.println(); Properties kafkaProps = new Properties(); kafkaProps.setProperty(\"bootstrap.servers\", brokers); FlinkKafkaConsumer&lt;Event&gt; kafka = new FlinkKafkaConsumer&lt;&gt;(kafkaTopic, new EventDeSerializer(), kafkaProps); kafka.setStartFromLatest(); kafka.setCommitOffsetsOnCheckpoints(false); source = kafka; &#125; else &#123; double errorRate = params.getDouble(\"error-rate\", 0.0); int sleep = params.getInt(\"sleep\", 1); System.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep); System.out.println(); source = new EventsGeneratorSource(errorRate, sleep); &#125; // ---- main program ---- // create the environment to create streams and configure execution final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(2000L); final String stateBackend = params.get(\"backend\", \"memory\"); if (\"file\".equals(stateBackend)) &#123; final String checkpointDir = params.get(\"checkpoint-dir\"); boolean asyncCheckpoints = params.getBoolean(\"async-checkpoints\", false); env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints)); &#125; else if (\"rocks\".equals(stateBackend)) &#123; final String checkpointDir = params.get(\"checkpoint-dir\"); boolean incrementalCheckpoints = params.getBoolean(\"incremental-checkpoints\", false); env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints)); &#125; final String outputFile = params.get(\"output\"); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); DataStream&lt;Event&gt; events = env.addSource(source); DataStream&lt;Alert&gt; alerts = events // partition on the address to make sure equal addresses // end up in the same state machine flatMap function .keyBy(Event::sourceAddress) // the function that evaluates the state machine over the sequence of events .flatMap(new StateMachineMapper()); // output the alerts to std-out if (outputFile == null) &#123; alerts.print(); &#125; else &#123; alerts .writeAsText(outputFile, FileSystem.WriteMode.OVERWRITE) .setParallelism(1); &#125; // trigger program execution env.execute(\"State machine job\"); &#125; // ------------------------------------------------------------------------ /** * The function that maintains the per-IP-address state machines and verifies that the * events are consistent with the current state of the state machine. If the event is not * consistent with the current state, the function produces an alert. */ @SuppressWarnings(\"serial\") static class StateMachineMapper extends RichFlatMapFunction&lt;Event, Alert&gt; &#123; /** The state for the current key. */ private ValueState&lt;State&gt; currentState; @Override public void open(Configuration conf) &#123; // get access to the state object currentState = getRuntimeContext().getState( new ValueStateDescriptor&lt;&gt;(\"state\", State.class)); &#125; @Override public void flatMap(Event evt, Collector&lt;Alert&gt; out) throws Exception &#123; // get the current state for the key (source address) // if no state exists, yet, the state must be the state machine's initial state State state = currentState.value(); if (state == null) &#123; state = State.Initial; &#125; // ask the state machine what state we should go to based on the given event State nextState = state.transition(evt.type()); if (nextState == State.InvalidTransition) &#123; // the current event resulted in an invalid transition // raise an alert! out.collect(new Alert(evt.sourceAddress(), state, evt.type())); &#125; else if (nextState.isTerminal()) &#123; // we reached a terminal state, clean up the current state currentState.clear(); &#125; else &#123; // remember the new state currentState.update(nextState); &#125; &#125; &#125;&#125;首先 events 是一个 DataStream，通过 env.addSource 加载数据进来，接下来有一个 DataStream 叫 alerts，先 keyby 一个 sourceAddress，然后在 flatMap 一个 StateMachineMapper。StateMachineMapper 就是一个状态机，状态机指有不同的状态与状态间有不同的转换关系的结合，以买东西的过程简单举例。首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货。已付款，待发货的状态再来一个事件发货，订单状态将会变为配送中，配送中的状态再来一个事件签收，则该订单的状态就变为已签收。在整个过程中，随时都可以来一个事件，取消订单，无论哪个状态，一旦触发了取消订单事件最终就会将状态转移到已取消，至此状态就结束了。Flink 写状态机是如何实现的？首先这是一个 RichFlatMapFunction，要用 Keyed State getRuntimeContext，getRuntimeContext 的过程中需要 RichFunction，所以需要在 open 方法中获取 currentState ，然后 getState，currentState 保存的是当前状态机上的状态。如果刚下订单，那么 currentState 就是待付款状态，初始化后，currentState 就代表订单完成。订单来了后，就会走 flatMap 这个方法，在 flatMap 方法中，首先定义一个 State，从 currentState 取出，即 Value，Value 取值后先判断值是否为空，如果 sourceAddress state 是空，则说明没有被使用过，那么此状态应该为刚创建订单的初始状态，即待付款。然后赋值 state = State.Initial，注意此处的 State 是本地的变量，而不是 Flink 中管理的状态，将它的值从状态中取出。接下来在本地又会来一个变量，然后 transition，将事件对它的影响加上，刚才待付款的订单收到付款成功的事件，就会变成已付款，待发货，然后 nextState 即可算出。此外，还需要判断 State 是否合法，比如一个已签收的订单，又来一个状态叫取消订单，会发现已签收订单不能被取消，此时这个状态就会下发，订单状态为非法状态。如果不是非法的状态，还要看该状态是否已经无法转换，比如这个状态变为已取消时，就不会在有其他的状态再发生了，此时就会从 state 中 clear。clear 是所有的 Flink 管理 keyed state 都有的公共方法，意味着将信息删除，如果既不是一个非法状态也不是一个结束状态，后面可能还会有更多的转换，此时需要将订单的当前状态 update ，这样就完成了 ValueState 的初始化、取值、更新以及清零，在整个过程中状态机的作用就是将非法的状态进行下发，方便下游进行处理。其他的状态也是类似的使用方式。三. 容错机制与故障恢复1. 状态如何保存及恢复Flink 状态保存主要依靠 Checkpoint 机制，Checkpoint 会定时制作分布式快照，对程序中的状态进行备份，这里就不在阐述分布式快照具体是如何实现的。分布式快照 Checkpoint 完成后，当作业发生故障了如何去恢复？假如作业分布跑在 3 台机器上，其中一台挂了。这个时候需要把进程或者线程移到 active 的 2 台机器上，此时还需要将整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态，然后从该点开始继续处理。如果要从 Checkpoint 恢复，必要条件是数据源需要支持数据重新发送。Checkpoint 恢复后， Flink 提供两种一致性语义，一种是恰好一次，一种是至少一次。在做 Checkpoint 时，可根据 Barries 对齐来判断是恰好一次还是至少一次，如果对齐，则为恰好一次，否则没有对齐即为至少一次。如果作业是单线程处理，也就是说 Barries 是不需要对齐的；如果只有一个 Checkpoint 在做，不管什么时候从 Checkpoint 恢复，都会恢复到刚才的状态；如果有多个节点，假如一个数据的 Barries 到了，另一个 Barries 还没有来，内存中的状态如果已经存储。那么这 2 个流是不对齐的，恢复的时候其中一个流可能会有重复。Checkpoint 通过代码的实现方法如下：首先从作业的运行环境 env.enableCheckpointing 传入 1000，意思是做 2 个 Checkpoint 的事件间隔为 1 秒。Checkpoint 做的越频繁，恢复时追数据就会相对减少，同时 Checkpoint 相应的也会有一些 IO 消耗。接下来是设置 Checkpoint 的 model，即设置了 Exactly_Once 语义，并且需要 Barries 对齐，这样可以保证消息不会丢失也不会重复。setMinPauseBetweenCheckpoints 是 2 个 Checkpoint 之间最少是要等 500ms，也就是刚做完一个 Checkpoint。比如某个 Checkpoint 做了 700ms，按照原则过 300ms 应该是做下一个 Checkpoint，因为设置了 1000ms 做一次 Checkpoint 的，但是中间的等待时间比较短，不足 500ms 了，需要多等 200ms，因此以这样的方式防止 Checkpoint 太过于频繁而导致业务处理的速度下降。setCheckpointTimeout 表示做 Checkpoint 多久超时，如果 Checkpoint 在 1min 之内尚未完成，说明 Checkpoint 超时失败。setMaxConcurrentCheckpoints 表示同时有多少个 Checkpoint 在做快照，这个可以根据具体需求去做设置。enableExternalizedCheckpoints 表示下 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。上面讲过，除了故障恢复之外，还需要可以手动去调整并发重新分配这些状态。手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在，那么作业如何恢复数据？一方面 Flink 在 Cancel 时允许在外部介质保留 Checkpoint ；另一方面，Flink 还有另外一个机制是 SavePoint。Savepoint 与 Checkpoint 类似，同样是把状态存储到外部介质。当作业失败时，可以从外部恢复。Savepoint 与 Checkpoint 有什么区别呢？从触发管理方式来讲，Checkpoint 由 Flink 自动触发并管理，而 Savepoint 由用户手动触发并人肉管理；从用途来讲，Checkpoint 在 Task 发生异常时快速恢复，例如网络抖动或超时异常，而 Savepoint 有计划地进行备份，使作业能停止后再恢复，例如修改代码、调整并发；最后从特点来讲，Checkpoint 比较轻量级，作业出现问题会自动从故障中恢复，在作业停止后默认清除；而 Savepoint 比较持久，以标准格式存储，允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复。2. 可选的状态存储方式Checkpoint 的存储，第一种是内存存储，即 MemoryStateBackend，构造方法是设置最大的 StateSize，选择是否做异步快照，这种存储状态本身存储在 TaskManager 节点也就是执行节点内存中的，因为内存有容量限制，所以单个 State maxStateSize 默认 5 M，且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M。Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。推荐使用的场景为：本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。另一种就是在文件系统上的 FsStateBackend ，构建方法是需要传一个文件路径和是否异步快照。State 依然在 TaskManager 内存中，但不会像 MemoryStateBackend 有 5 M 的设置上限，Checkpoint 存储在外部文件系统（本地或 HDFS），打破了总大小 Jobmanager 内存的限制。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。推荐使用的场景、常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启 HA 的作业。还有一种存储为 RocksDBStateBackend ，RocksDB 是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着每次用户不需要将所有状态都写进去，将增量的改变的状态写进去即可。它的 Checkpoint 存储在外部文件系统（本地或 HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存 + 磁盘，单 Key 最大 2G，总大小不超过配置的文件系统容量即可。推荐使用的场景为：超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。**3. StateBackend 的分类下面这张图对目前广泛使用的三类 state backend 做了区分，其中绿色表示所创建的operator/keyed state backend 是 on-heap 的，黄色则表示是 off-heap 的。一般而言，在生产中，我们会在 FsStateBackend 和 RocksDBStateBackend 间选择：FsStateBackend：性能更好；日常存储是在堆内存中，面临着 OOM 的风险，不支持增量 checkpoint。RocksDBStateBackend：无需担心 OOM 风险，是大部分时候的选择。4.RocksDB StateBackend 概览和相关配置讨论RocksDB 是 Facebook 开源的 LSM 的键值存储数据库，被广泛应用于大数据系统的单机组件中。Flink 的 keyed state 本质上来说就是一个键值对，所以与 RocksDB 的数据模型是吻合的。下图分别是 “window state” 和 “value state” 在 RocksDB 中的存储格式，所有存储的 key，value 均被序列化成 bytes 进行存储。在 RocksDB 中，每个 state 独享一个 Column Family，而每个 Column family 使用各自独享的 write buffer 和 block cache，上图中的 window state 和 value state实际上分属不同的 column family。下面介绍一些对 RocksDB 性能比较有影响的参数，并整理了一些相关的推荐配置，至于其他配置项，可以参阅社区相关文档。状态建议state.backend.rocksdb.thread.num后台 flush 和 compaction 的线程数. 默认值 ‘1‘. 建议调大state.backend.rocksdb.writebuffer.count每个 column family 的 write buffer 数目，默认值 ‘2‘. 如果有需要可以适当调大state.backend.rocksdb.writebuffer.size每个 write buffer 的 size，默认值‘64MB‘. 对于写频繁的场景，建议调大state.backend.rocksdb.block.cache-size每个 column family 的 block cache大小，默认值‘8MB’，如果存在重复读的场景，建议调大5.State best practice：一些使用 state 的心得Operator state 使用建议■ 慎重使用长 list下图展示的是目前 task 端 operator state 在执行完 checkpoint 返回给 job master 端的 StateMetaInfo 的代码片段。由于 operator state 没有 key group 的概念，所以为了实现改并发恢复的功能，需要对 operator state 中的每一个序列化后的元素存储一个位置偏移 offset，也就是构成了上图红框中的 offset 数组。那么如果你的 operator state 中的 list 长度达到一定规模时，这个 offset 数组就可能会有几十 MB 的规模，关键这个数组是会返回给 job master，当 operator 的并发数目很大时，很容易触发 job master 的内存超用问题。我们遇到过用户把 operator state 当做黑名单存储，结果这个黑名单规模很大，导致一旦开始执行 checkpoint，job master 就会因为收到 task 发来的“巨大”的 offset 数组，而内存不断增长直到超用无法正常响应。■ 正确使用 UnionListState**union list state 目前被广泛使用在 kafka connector 中，不过可能用户日常开发中较少遇到，他的语义是从检查点恢复之后每个并发 task 内拿到的是原先所有operator 上的 state，如下图所示：kafka connector 使用该功能，为的是从检查点恢复时，可以拿到之前的全局信息，如果用户需要使用该功能，需要切记恢复的 task 只取其中的一部分进行处理和用于下一次 snapshot，否则有可能随着作业不断的重启而导致 state 规模不断增长。6. Keyed state 使用建议■ 如何正确清空当前的 statestate.clear() 实际上只能清理当前 key 对应的 value 值，如果想要清空整个 state，需要借助于 applyToAllKeys 方法，具体代码片段如下：如果你的需求中只是对 state 有过期需求，借助于 state TTL 功能来清理会是一个性能更好的方案。■ RocksDB 中考虑 value 值很大的极限场景受限于 JNI bridge API 的限制，单个 value 只支持 2^31 bytes 大小，如果存在很极限的情况，可以考虑使用 MapState 来替代 ListState 或者 ValueState，因为RocksDB 的 map state 并不是将整个 map 作为 value 进行存储，而是将 map 中的一个条目作为键值对进行存储。■ 如何知道当前 RocksDB 的运行情况比较直观的方式是打开 RocksDB 的 native metrics ，在默认使用 Flink managed memory 方式的情况下，state.backend.rocksdb.metrics.block-cache-usage ，state.backend.rocksdb.metrics.mem-table-flush-pending，state.backend.rocksdb.metrics.num-running-compactions 以及 state.backend.rocksdb.metrics.num-running-flushes 是比较重要的相关 metrics。下面这张图是 Flink-1.10 之后，打开相关 metrics 的示例图：而下面这张是 Flink-1.10 之前或者关闭 state.backend.rocksdb.memory.managed 的效果：■ 容器内运行的 RocksDB 的内存超用问题在 Flink-1.10 之前，由于一个 state 独占若干 write buffer 和一块 block cache，所以我们会建议用户不要在一个 operator 内创建过多的 state，否则需要考虑到相应的额外内存使用量，否则容易造成在容器内运行时，相关进程被容器环境所杀。对于用户来说，需要考虑一个 slot 内有多少 RocksDB 实例在运行，一个 RocksDB 中有多少 state，整体的计算规则就很复杂，很难真得落地实施。Flink-1.10 之后，由于引入了 RocksDB 的内存托管机制，在绝大部分情况下， RocksDB 的这一部分 native 内存是可控的，不过受限于 RocksDB 的相关 cache 实现限制（这里暂不展开，后续会有文章讨论），在某些场景下，无法做到完美控制，这时候建议打开上文提到的 native metrics，观察相关 block cache 内存使用是否存在超用情况，可以将相关内存添加到 taskmanager.memory.task.off-heap.size 中，使得 Flink 有更多的空间给 native 内存使用。7.一些使用 checkpoint 的使用建议■ Checkpoint 间隔不要太短虽然理论上 Flink 支持很短的 checkpoint 间隔，但是在实际生产中，过短的间隔对于底层分布式文件系统而言，会带来很大的压力。另一方面，由于检查点的语义，所以实际上 Flink 作业处理 record 与执行 checkpoint 存在互斥锁，过于频繁的 checkpoint，可能会影响整体的性能。当然，这个建议的出发点是底层分布式文件系统的压力考虑。■ 合理设置超时时间默认的超时时间是 10min，如果 state 规模大，则需要合理配置。最坏情况是分布式地创建速度大于单点（job master 端）的删除速度，导致整体存储集群可用空间压力较大。建议当检查点频繁因为超时而失败时，增大超时时间。**四. 总结1. 为什么要使用状态？前面提到有状态的作业要有有状态的逻辑，有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。所以需要通过状态来满足业务逻辑。2. 为什么要管理状态？使用了状态，为什么要管理状态？因为实时作业需要 7*24 不间断的运行，需要应对不可靠的因素而带来的影响。3. 如何选择状态的类型和存储方式？那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。转载自：Flink State 最佳实践Apache Flink 零基础入门（六）：状态管理及容错机制","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"def neverGrowUp()","slug":"def-neverGrowUp","date":"2020-04-05T16:00:00.000Z","updated":"2020-04-05T15:10:24.541Z","comments":true,"path":"2020/04/06/def-neverGrowUp/","link":"","permalink":"cpeixin.cn/2020/04/06/def-neverGrowUp/","excerpt":"","text":"123456789def neverGrowUp() while true: 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心 开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心开心","categories":[],"tags":[]},{"title":"抗疫英雄","slug":"抗疫英雄","date":"2020-04-04T14:45:15.000Z","updated":"2020-04-05T14:46:33.308Z","comments":true,"path":"2020/04/04/抗疫英雄/","link":"","permalink":"cpeixin.cn/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/","excerpt":"","text":"致敬缅怀每一位抗疫英雄","categories":[],"tags":[]},{"title":"python Flask & Ajax 数据传输","slug":"python-Flask-Ajax-数据传输","date":"2020-03-11T14:43:01.000Z","updated":"2020-04-04T17:13:00.080Z","comments":true,"path":"2020/03/11/python-Flask-Ajax-数据传输/","link":"","permalink":"cpeixin.cn/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/","excerpt":"","text":"帮朋友写个小工具，没想到还要搞定JS，大学毕业后就没有写过JS，真的是难为我了😂忙活三个小时，终于把前端和后端打通了～～前端demo：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;script src=\"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\"&gt;&lt;/script&gt;&lt;body&gt;&lt;!-- 发送数据，表单方式 （注意：后端接收数据对应代码不同）--&gt;&lt;form action=\"&#123;&#123; url_for('send_message') &#125;&#125;\" method=\"post\"&gt; &lt;textarea name =\"domain\" rows=\"30\" cols=\"100\" placeholder=\"请输入需要查询的域名,如cq5999.com\"&gt;&lt;/textarea&gt; &lt;!--&lt;input id=\"submit\" type=\"submit\" value=\"发送\"&gt;--&gt; &lt;button type=\"submit\" id=\"btn-bq\" data-toggle=\"modal\" data-target=\"#myModal\"&gt;查询&lt;/button&gt;&lt;/form&gt;&lt;!-- 发送数据，input方式 （注意：后端接收数据对应代码不同） --&gt;&lt;div&gt; &lt;label for=\"send_content\"&gt;向后台发送消息：&lt;/label&gt; &lt;input id=\"send_content\" type=\"text\" name=\"send_content\"&gt; &lt;input id=\"send\" type=\"button\" value=\"发送\"&gt;&lt;/div&gt;&lt;div&gt; &lt;label for=\"recv_content\"&gt;从后台接收消息：&lt;/label&gt; &lt;input id=\"recv_content\" type=\"text\" name=\"recv_content\"&gt;&lt;/div&gt;&lt;!-- input方式 对应的js代码，如用表单方式请注释掉 --&gt;&lt;!-- 发送 --&gt;&lt;script type=\"text/javascript\"&gt; $(\"#send\").click(function () &#123; var message = $(\"#send_content\").val() alert(message) $.ajax(&#123; url:\"/send_message\", type:\"POST\", data:&#123; message:message &#125;, dataType: 'json', success:function (data) &#123; &#125; &#125;) &#125;)&lt;/script&gt;&lt;!-- 接收 --&gt;&lt;script type=\"text/javascript\"&gt; $(\"#send\").click(function () &#123; $.getJSON(\"/change_to_json\",function (data) &#123; $(\"#recv_content\").val(data.message) //将后端数据显示在前端 console.log(\"传到前端的数据的类型：\" + typeof (data.message)) $(\"#send_content\").val(\"\")//发送的输入框清空 &#125;) &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;后端demo:1234567891011121314151617181920212223242526272829303132333435from flask import Flask, render_template, request, jsonifyapp = Flask(__name__)@app.route('/')def index(): return render_template(\"index_v6.html\")@app.route('/send_message', methods=['POST'])def send_message(): global message_get message_get = \"\" message_get = request.form[\"domain\"].split('\\n') # message_get = request.form['message'] #input提交 print(\"收到前端发过来的信息：%s\" % message_get) print(\"收到数据的类型为：\" + str(type(message_get))) return \"收到消息\"@app.route('/change_to_json', methods=['GET'])def change_to_json(): global message_get message_json = &#123; \"message\": message_get &#125; return jsonify(message_json)if __name__ == '__main__': app.run(host='0.0.0.0', port=80,debug=True)","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"}]},{"title":"Python Flask接口设计-示例","slug":"Python-Flask接口设计-示例","date":"2020-03-10T15:08:35.000Z","updated":"2020-04-04T17:12:52.356Z","comments":true,"path":"2020/03/10/Python-Flask接口设计-示例/","link":"","permalink":"cpeixin.cn/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"Get 请求开发一个只接受get方法的接口，接受参数为name和age，并返回相应内容。**方法 1:****123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask import requestfrom flask import redirectfrom flask import jsonifyimport jsonapp = Flask(__name__)@app.route(\"/test_1.0\", methods=[\"GET\"])def check(): # 默认返回内容 return_dict = &#123;'return_code': '200', 'return_info': '处理成功', 'result': False&#125; # 判断入参是否为空 if request.args is None: return_dict['return_code'] = '5004' return_dict['return_info'] = '请求参数为空' return json.dumps(return_dict, ensure_ascii=False) # 获取传入的params参数 get_data = request.args.to_dict() name = get_data.get('name') age = get_data.get('age') # 对参数进行操作 return_dict['result'] = tt(name, age) return json.dumps(return_dict, ensure_ascii=False)# 功能函数def tt(name, age): result_str = \"%s今年%s岁\" % (name, age) return result_strif __name__ == '__main__': app.run(host='0.0.0.0', port=80)此种方式对应的request请求方式：拼接请求链接, 直接请求：http://0.0.0.0/test_1.0?name=ccc&amp;age=18request 请求中带有参数，如下图方法 2:123@app.route('/api/banWordSingle/&lt;string:word&gt;', methods=['GET'])def banWordSingleStart(word): return getWordStatus(word)此方法 与 方法 1 中的拼接链接相似，但是不用输入关键字请求链接：http://0.0.0.0/api/banWordSingle/输入词Post 请求123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask import requestfrom flask import redirectfrom flask import jsonifyimport jsonapp = Flask(__name__)@app.route(\"/test_1.0\", methods=[\"POST\"])def check(): # 默认返回内容 return_dict = &#123;'return_code': '200', 'return_info': '处理成功', 'result': False&#125; # 判断入参是否为空 if request.args is None: return_dict['return_code'] = '5004' return_dict['return_info'] = '请求参数为空' return json.dumps(return_dict, ensure_ascii=False) # 获取传入的params参数 get_data = request.args.to_dict() name = get_data.get('name') age = get_data.get('age') # 对参数进行操作 return_dict['result'] = tt(name, age) return json.dumps(return_dict, ensure_ascii=False)# 功能函数def tt(name, age): result_str = \"%s今年%s岁\" % (name, age) return result_strif __name__ == '__main__': app.run(host='0.0.0.0', port=8080)请求方式：","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"}]},{"title":"Flink 1.10版本发布","slug":"Flink-1-10版本发布","date":"2020-02-12T17:22:22.000Z","updated":"2020-06-03T14:38:01.179Z","comments":true,"path":"2020/02/13/Flink-1-10版本发布/","link":"","permalink":"cpeixin.cn/2020/02/13/Flink-1-10%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","excerpt":"","text":"Apache Flink社区很高兴能达到两位数并宣布发布Flink 1.10.0！由于迄今为止社区最大的努力，已实施了1.2k个问题，贡献了200多个贡献者，此版本引入了对Flink作业的整体性能和稳定性的重大改进，原生Kubernetes集成的预览以及Python支持的巨大进步（PyFlink）。(Spark对Python的支持也越来越好😂)Flink 1.10还标志着Blink集成的完成，强化了流数据SQL并通过可用于生产的Hive集成和TPC-DS覆盖将成熟的批处理引入Flink。这篇博客文章描述了所有主要的新功能和改进，需要注意的重要更改以及预期的发展。现在可以在Flink网站的更新的“ 下载”页面上找到二进制分发文件和源工件。有关更多详细信息，请查看完整的发行变更日志和更新的文档。我们鼓励您下载发行版，并通过Flink邮件列表或JIRA与社区分享您的反馈。新功能和改进改进的内存管理和配置目前Flink中的TaskExecutor内存配置存在一些缺点，这些缺点使得难以推理或优化资源利用率，例如：流处理和批处理执行中用于内存占用的不同配置模型；流处理执行中堆外状态后端（即RocksDB）的复杂且依赖用户的配置。为了使内存选项对用户更明确和直观，Flink 1.10对TaskExecutor内存模型和配置逻辑（FLIP-49）进行了重大更改。这些更改使Flink更适合于各种部署环境（例如Kubernetes，Yarn，Mesos），从而使用户可以严格控制其内存消耗。托管内存扩展托管内存已经扩展扩展，当然还考虑了RocksDB StateBackend的内存使用情况。虽然批处理作业可以使用堆内（on-heap）或堆外（off-heap）内存，但具有这些功能的流作业RocksDBStateBackend只能使用堆内内存。因此，为了允许用户在流执行和批处理执行之间切换而不必修改群集配置，托管内存现在始终处于堆外状态。简化RocksDB配置**曾经配置像RocksDB这样的off-heap (堆外)state backend涉及大量的手动调整，例如减小JVM堆大小或将Flink设置为使用堆外内存。现在可以通过Flink的现成配置来实现，并且调整RocksDBStateBackend内存预算就像调整内存大小一样简单。另一个重要的改进是允许Flink绑定RocksDB本地内存使用情况（FLINK-7289），从而防止其超出总内存预算-这在Kubernetes等容器化环境中尤其重要。有关如何启用和调整此功能的详细信息，请参阅Tuning RocksDB。注意 FLIP-49更改了群集资源配置的过程，这可能需要调整群集以从以前的Flink版本进行升级。有关所引入更改和调整指南的全面概述，请参阅此设置。提交作业的统一逻辑在此版本之前，提交作业是执行环境的一部分职责，并且与不同的部署目标（例如，Yarn，Kubernetes，Mesos）紧密相关。这导致关注点分离不佳，并且随着时间的流逝，用户需要单独配置和管理的定制环境越来越多。在Flink 1.10中，作业提交逻辑被抽象到通用Executor接口（FLIP-73）中。另外ExecutorCLI（FLIP-81）引入了一个统一的方式去指定配置参数对于任何 执行对象。为了完善这项工作，结果检索的过程也与作业提交分离，引入了JobClient（FLINK-74）来负责获取JobExecutionResult。特别是，这些更改通过为用户提供Flink的统一入口点，使在下游框架（例如Apache Beam或Zeppelin交互式笔记本）中以编程方式使用Flink变得更加容易。对于跨多个目标环境使用Flink的用户，向基于配置的执行过程的过渡还可以显着减少样板代码和可维护性开销。原生Kubernetes集成（测试版）对于希望在容器化环境上开始使用Flink的用户，在Kubernetes之上部署和管理独立集群需要有关容器，算子和环境工具kubectl的一些知识。在Flink 1.10中，我们推出了Active Kubernetes集成（FLINK-9953）的第一阶段，其中，“主动”指 Flink ResourceManager (K8sResMngr) 原生地与 Kubernetes 通信，像 Flink 在 Yarn 和 Mesos 上一样按需申请 pod。用户可以利用 namespace，在多租户环境中以较少的资源开销启动 Flink。这需要用户提前配置好 RBAC 角色和有足够权限的服务账号。正如刚刚讲到的，Flink 1.10中的所有命令行选项都映射到统一配置。因此，用户可以简单地引用Kubernetes配置选项，然后使用以下命令在CLI中将作业提交到Kubernetes上的现有Flink会话：1.&#x2F;bin&#x2F;flink run -d -e kubernetes-session -Dkubernetes.cluster-id&#x3D;&lt;ClusterId&gt; examples&#x2F;streaming&#x2F;WindowJoin.jar如果您想尝试使用此预览功能，我们建议您逐步完成本机Kubernetes的安装，试用并与社区分享反馈。Table API / SQL：生产就绪的Hive集成Hive集成在Flink 1.9中宣布为预览功能。此预览允许用户使用SQL DDL将Flink-specific元数据（例如Kafka表）保留在Hive Metastore中，调用Hive中定义的UDF并使用Flink读取和写入Hive表。Flink 1.10通过进一步的开发使这项工作更加圆满，这些开发使可立即投入生产的Hive集成到Flink，并具有与大多数Hive版本的完全兼容性。批处理SQL的本地分区支持1.10版本以前，仅支持对未分区的Hive表进行写入。在Flink 1.10中，Flink SQL语法已通过INSERT OVERWRITE和PARTITION（FLIP-63）进行了扩展，使用户能够在Hive中写入静态和动态分区。静态分区写入**1INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;动态分区编写**1INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 select_statement1 FROM from_statement;Flink对于分区表的全面支持，允许用户利用读取时的分区修剪功能，通过减少需要扫描的数据量来显着提高这些操作的性能。进一步优化除了分区修剪外，Flink 1.10还为Hive集成引入了更多读取优化，例如：投影下推： Flink通过省略表扫描中不必要的字段，利用投影下推来最大程度地减少Flink和Hive表之间的数据传输。这对于具有大量列的表尤其有利。LIMIT下推：对于带有LIMIT子句的查询，Flink将尽可能限制输出记录的数量，以最大程度地减少通过网络传输的数据量。读取时进行ORC矢量化：为了提高ORC文件的读取性能，Flink现在默认将本机ORC矢量化阅读器用于2.0.0以上的Hive版本以及具有非复杂数据类型的列。可插拔模块作为Flink系统对象（Beta）Flink 1.10引入了Flink Table核心中可插拔模块的通用机制，首先关注系统功能（FLIP-68）。使用该模块，用户可以扩展Flink的系统对象，例如，使用行为类似于Flink系统功能的Hive内置函数。该版本附带一个预先实现的HiveModule，支持多个Hive版本的版本，但用户也可以编写自己的可插拔模块。Table API / SQL的其他改进SQL DDL中的水印和计算列Flink 1.10支持特定于流的语法扩展，以在Flink SQL DDL（FLIP-66）中定义时间属性和水印生成。这允许基于时间的操作（例如加窗），以及在使用DDL语句创建的表上定义水印策略。12345CREATE TABLE table_name ( WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;) WITH ( ...)此版本还引入了对虚拟计算列（FLIP-70）的支持，该列可基于同一表中的其他列或确定性表达式（即，文字值，UDF和内置函数）派生。在Flink中，计算列可用于在创建表时定义时间属性。SQL DDL的其他扩展现在，temporary/persistent 和 system/catalog（FLIP-57）之间有明显的区别。这不仅消除了函数引用中的歧义，而且允许确定性的函数解析顺序（即，在命名冲突的情况下，系统函数将优先于目录函数，而临时函数的优先级高于两个维度的持久性函数）。遵循FLIP-57的基础知识，我们扩展了SQL DDL语法以支持目录功能，临时功能和临时系统功能（FLIP-79）的创建：123CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF NOT EXISTS] [catalog_name.][db_name.]function_name AS identifier [LANGUAGE JAVA|SCALA]有关Flink SQL中DDL支持的当前状态的完整概述，请查看更新的文档。注意**为了将来能正确处理和保证元对象（表，视图，函数）之间的行为一致，不建议使用Table API中的某些对象声明方法，而应使用更接近标准SQL DDL的方法（FLIP -64）。TPC-DS的完整覆盖范围可批量处理TPC-DS是一种广泛使用的行业标准决策支持基准，用于评估和衡量基于SQL的数据处理引擎的性能。在Flink 1.10中，端到端（FLINK-11491）支持所有TPC-DS查询，这反映了它的SQL引擎已准备就绪，可以满足类似现代数据仓库的工作负载的需求。PyFlink：支持本机用户定义的函数（UDF）在以前的发行版中引入了PyFlink的预览版，朝着实现Flink中完全Python支持的目标迈进了一步。对于此发行版，重点是使用户能够在表API / SQL（FLIP-58）中注册和使用Python用户定义函数（UDF，已计划UDTF / UDAF ）。如果您对基础实现感兴趣（利用Apache Beam的可移植性框架），请参考FLIP-58的“架构”部分，也请参考FLIP-78。这些数据结构为Pandas支持和PyFlink最终到达DataStream API奠定了必要的基础。从Flink 1.10开始，用户还可以pip使用以下方法轻松安装PyFlink ：1pip install apache-flink有关PyFlink计划进行的其他改进的预览，请查看FLINK-14500并参与有关所需用户功能的讨论。重要变化[ FLINK-10725 ] Flink现在可以编译并在Java 11上运行。[ FLINK-15495 ] Blink计划程序现在是SQL Client中的默认设置，因此用户可以从所有最新功能和改进中受益。在下一个版本中，还计划从Table API中的旧计划程序进行切换，因此我们建议用户开始熟悉Blink计划程序。[ FLINK-13025 ]有一个新的Elasticsearch接收器连接器，完全支持Elasticsearch 7.x版本。[ FLINK-15115 ] Kafka 0.8和0.9的连接器已标记为不推荐使用，将不再得到积极支持。如果您仍在使用这些版本或有任何其他相关问题，请联系@dev邮件列表。[ FLINK-14516 ]删除了非基于信用的网络流控制代码以及配置选项taskmanager.network.credit.model。展望未来，Flink将始终使用基于信用的流量控制。[ FLINK-12122 ] FLIP-6在Flink 1.5.0中推出，并引入了与从中分配插槽方式有关的代码回归TaskManagers。要使用更接近FLIP之前行为的调度策略（Flink尝试将工作负载分散到所有当前可用的行为中）TaskManagers，用户可以cluster.evenly-spread-out-slots: true在中设置flink-conf.yaml。[ FLINK-11956 ] s3-hadoop和s3-presto文件系统不再使用类重定位，而应通过插件加载，但现在可以与所有凭据提供程序无缝集成。强烈建议将其他文件系统仅用作插件，因为我们将继续删除重定位。Flink 1.9带有重构的Web UI，保留了旧版的UI作为备份，以防万一某些功能无法正常工作。到目前为止，尚未报告任何问题，因此社区投票决定在Flink 1.10中删除旧版Web UI。发行说明如果您打算将设置升级到Flink 1.10，请仔细查看发行说明，以获取详细的更改和新功能列表。此版本与以前的1.x版本的API兼容，这些版本的API使用@Public注释进行了注释。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"IDEA install TabNine","slug":"IDEA-install-TabNine","date":"2020-01-22T02:26:15.000Z","updated":"2020-04-04T11:06:48.223Z","comments":true,"path":"2020/01/22/IDEA-install-TabNine/","link":"","permalink":"cpeixin.cn/2020/01/22/IDEA-install-TabNine/","excerpt":"","text":"TabNine是我目前遇到过最好的智能补全工具TabNine基于GPT-2的插件安装IDEA编译器，找到pluginsWindows pycharm：File&gt;settings&gt;plugins;Mac pycharm：performence&gt;plugins&gt;marketplace or plugins&gt;Install JetBrains Plugins查找 TabNine, 点击 install, 随后 restart重启后：Help&gt;Edit Custom Properties…&gt;Create;在跳出来的idea.properties中输入（注：英文字符） TabNine::config随即会自动弹出TabNine激活页面；激活点击Activation Key下面的here；输入你的邮箱号；复制粘贴邮件里面的API Key到Activation Key下面；（得到的 key 可以在各种编译器中共用）等待自动安装，观察页面（最下面有log可以看当前进度）；激活完成后TabNine Cloud为Enabled状态，你也可以在安装进度完成后刷新页面手动选择Enabled；确认激活完成，重启pycharm即可；","categories":[{"name":"开发工具","slug":"开发工具","permalink":"cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"cpeixin.cn/tags/IDEA/"}]},{"title":"Flink和Spark Streaming中的Back Pressure","slug":"Flink和Spark-Streaming中的Back-Pressure","date":"2020-01-15T15:09:25.000Z","updated":"2020-09-06T09:04:50.797Z","comments":true,"path":"2020/01/15/Flink和Spark-Streaming中的Back-Pressure/","link":"","permalink":"cpeixin.cn/2020/01/15/Flink%E5%92%8CSpark-Streaming%E4%B8%AD%E7%9A%84Back-Pressure/","excerpt":"","text":"Spark Streaming BackPressureSpark Streaming的back pressure出现的原因呢，我想大家应该都知道，是为了应对短期数据尖峰。Spark Streaming的back pressure是从spark 1.5以后引入的，在之前呢，只能通过限制最大消费速度（这个要人为压测预估），对于基于Receiver 形式，我们可以通过配置 spark.streaming.receiver.maxRate 参数来限制每个 receiver 每秒最大可以接收的记录的数据；对于 Direct Approach 的数据接收，我们可以通过配置 spark.streaming.kafka.maxRatePerPartition 参数来限制每次作业中每个 Kafka 分区最多读取的记录条数。这种限速的弊端很明显，比如假如我们后端处理能力超过了这个最大的限制，会导致资源浪费。需要对每个spark Streaming任务进行压测预估。成本比较高。由此，从1.5开始引入了back pressure，这种机制呢实际上是基于自动控制理论的pid这个概念。我们就简单讲一下其中思路：为了实现自动调节数据的传输速率，在原有的架构上新增了一个名为 RateController 的组件，这个组件继承自 StreamingListener，其监听所有作业的 onBatchCompleted 事件，并且基于 processingDelay 、schedulingDelay 、当前 Batch 处理的记录条数以及处理完成事件来估算出一个速率；这个速率主要用于更新流每秒能够处理的最大记录的条数。这样就可以实现处理能力好的话就会有一个较大的最大值，处理能力下降了就会生成一个较小的最大值。来保证Spark Streaming流畅运行。pid速率计算源码**每一个Batch处理完成后都会调用此方法1234567891011121314override def onBatchCompleted(batchCompleted: StreamingListenerBatchCompleted) &#123; val elements = batchCompleted.batchInfo.streamIdToInputInfo for &#123; // 处理结束时间 processingEnd &lt;- batchCompleted.batchInfo.processingEndTime // 处理时间,即`processingEndTime` - `processingStartTime` workDelay &lt;- batchCompleted.batchInfo.processingDelay // 在调度队列中的等待时间,即`processingStartTime` - `submissionTime` waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay // 当前批次处理的记录数 elems &lt;- elements.get(streamUID).map(_.numRecords) &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay) &#125;接着又调用的是computeAndPublish方法1234567891011private def computeAndPublish(time: Long, elems: Long, workDelay: Long, waitDelay: Long): Unit = Future[Unit] &#123; // 根据处理时间、调度时间、当前Batch记录数，预估新速率 val newRate = rateEstimator.compute(time, elems, workDelay, waitDelay) newRate.foreach &#123; s =&gt; // 设置新速率 rateLimit.set(s.toLong) // 发布新速率 publish(getLatestRate()) &#125; &#125;更深一层，具体调用的是rateEstimator.compute方法来预估新速率，如下:12345def compute( time: Long, elements: Long, processingDelay: Long, schedulingDelay: Long): Option[Double]该方法是接口RateEstimator中的方法，会计算出新的批次每秒应摄入的记录数。PIDRateEstimator，即PID速率估算器,是RateEstimator的唯一实现,具体估算逻辑可看PIDRateEstimator.compute方法，逻辑很复杂，用到了微积分相关的知识，总之，一句话，即根据当前Batch的结果和期望的差值来估算新的输入速率。配置Spark Streaming的back pressurespark.streaming.backpressure.initialRate： 启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置。spark.streaming.backpressure.rateEstimator：速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现。spark.streaming.backpressure.pid.proportional：用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。weight for response to “error” (change between last batch and this batch)spark.streaming.backpressure.pid.integral：错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。weight for the response to the accumulation of error. This has a dampening effect.spark.streaming.backpressure.pid.derived：对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.spark.streaming.backpressure.pid.minRate：可以估算的最低费率是多少。默认值为 100，只能设置成非负值。Flink BackPressure如果你看到一个task的back pressure告警（比如，high），这意味着生产数据比下游操作算子消费的速度快。Record的在你工作流的传输方向是向下游，比如从source到sink，而back pressure正好是沿着反方向，往上游传播。举个简单的例子，一个工作流，只有source到sink两个步骤。假如你看到source端有个告警，这意味着sink消费数据速率慢于生产者的生产数据速率。Sink正在向上游进行back pressure。采样线程Back Pressure（后面翻译成背压）是通过重复采样正在运行的tasks的tack trace样本数据来监控任务的。JobManager会针对你的job的tasks重复触发调用Thread.getStackTrace()。如果样本数据显示任务线程卡在某个内部方法调用中（从网络堆栈请求缓冲区），则表示该任务存在背压。默认情况，为了判断是否进行背压，jobmanager会每50ms触发100次stack traces。Web界面中显示的比率，告诉你在这些stack traces中，阻塞在内部方法调用的stack traces占所有的百分比，例如，0.01，代表着100次中有一次阻塞在内部调用。• **OK: 0 &lt;= Ratio &lt;= 0.10• LOW: 0.10 &lt; Ratio &lt;= 0.5• HIGH: 0.5 &lt; Ratio &lt;= 1为例避免stack trace采样导致task managers压力过大，web 界面仅仅在60s刷新一次。配置可以通过下面的属性进行配置1， jobmanager.web.backpressure.refresh-interval:在这个时间之后，统计数据将会废弃，需要重新刷新。默认是60000，也即是1min。2， jobmanager.web.backpressure.num-samples:判断背压需要进行stack trace采样的个数，默认是1003， jobmanager.web.backpressure.delay-between-samples:两次stack trace个采用间隔。例子在flink的webui 的job界面中可以看到背压。下图中红色的Sampling in progress…, 这意味着JobManager对正在运行的tasks触发stack trace采样。默认配置，这将会花费五秒钟完成。点击之后，就触发了该操作算子所有task的采样。背压状态如果您看到任务的状态ok，则表示没有背压指示。另一方面，HIGH意味着任务被加压。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"【转载】字节跳动 EB 级 HDFS 实践","slug":"【转载】字节跳动-EB-级-HDFS-实践","date":"2020-01-02T15:25:55.000Z","updated":"2020-05-31T15:28:54.936Z","comments":true,"path":"2020/01/02/【转载】字节跳动-EB-级-HDFS-实践/","link":"","permalink":"cpeixin.cn/2020/01/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-EB-%E7%BA%A7-HDFS-%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"转载自字节跳动 EB 级 HDFS 实践，学习学习字节跳动基础架构部门对上万节点的HDFS集群的管理方式文章的最后，有写上自己的总结，工作这几年，确实没有遇到过这么庞大的数据量和集群，那么就不能先实战再总结了，目前是站在巨人的肩膀上看看远处的风景，当我到达的那一天，就会更从容的融入风景了。HDFS 简介因为 HDFS 这样一个系统已经存在了非常长的时间，应用的场景已经非常成熟了，所以这部分我们会比较简单地介绍。HDFS 全名 Hadoop Distributed File System，是业界使用最广泛的开源分布式文件系统。原理和架构与 Google 的 GFS 基本一致。它的特点主要有以下几项：和本地文件系统一样的目录树视图Append Only 的写入（不支持随机写）顺序和随机读超大数据规模易扩展，容错率高字节跳动特色的 HDFS字节跳动应用 HDFS 已经非常长的时间了，经历了 7 年的发展，目前已直接支持了十多种数据平台，间接支持了上百种业务发展。从集群规模和数据量来说，HDFS 平台在公司内部已经成长为总数几万台服务器的大平台，支持了 EB 级别的数据量。在深入相关的技术细节之前，我们先看看字节跳动的 HDFS 架构。架构介绍接入层接入层是区别于社区版本最大的一层，社区版本中并无这一层定义。在字节跳动的落地实践中，由于集群的节点过于庞大，我们需要非常多的 NameNode 实现联邦机制来接入不同上层业务的数据服务。但当 NameNode 数量也变得非常多了以后，用户请求的统一接入及统一视图的管理也会有很大的问题。为了解决用户接入过于分散，我们需要一个独立的接入层来支持用户请求的统一接入，转发路由；同时也能结合业务提供用户权限和流量控制能力；另外，该接入层也需要提供对外的目录树统一视图。该接入层从部署形态上来讲，依赖于一些外部组件如 Redis，MySQL 等，会有一批无状态的 NNProxy 组成，他们提供了请求路由，Quota 限制，Tracing 能力及流量限速等能力。元数据层这一层主要模块有 Name Node，ZKFC，和 BookKeeper（不同于 QJM，BookKeeper 在大规模多节点数据同步上来讲会表现得更稳定可靠）。Name Node 负责存储整个 HDFS 集群的元数据信息，是整个系统的大脑。一旦故障，整个集群都会陷入不可用状态。因此 Name Node 有一套基于 ZKFC 的主从热备的高可用方案。Name Node 还面临着扩展性的问题，单机承载能力始终受限。于是 HDFS 引入了联邦（Federation）机制。一个集群中可以部署多组 Name Node，它们独立维护自己的元数据，共用 Data Node 存储资源。这样，一个 HDFS 集群就可以无限扩展了。但是这种 Federation 机制下，每一组 Name Node 的目录树都互相割裂的。于是又出现了一些解决方案，能够使整个 Federation 集群对外提供一个完整目录树的视图。数据层相比元数据层，数据层主要节点是 Data Node。Data Node 负责实际的数据存储和读取。用户文件被切分成块复制成多副本，每个副本都存在不同的 Data Node 上，以达到容错容灾的效果。每个副本在 Data Node 上都以文件的形式存储，元信息在启动时被加载到内存中。Data Node 会定时向 Name Node 做心跳汇报，并且周期性将自己所存储的副本信息汇报给 Name Node。这个过程对 Federation 中的每个集群都是独立完成的。在心跳汇报的返回结果中，会携带 Name Node 对 Data Node 下发的指令，例如，需要将某个副本拷贝到另外一台 Data Node 或者将某个副本删除等。主要业务先来看一下当前在字节跳动 HDFS 承载的主要业务：Hive，HBase，日志服务，Kafka 数据存储Yarn，Flink 的计算框架平台数据Spark，MapReduce 的计算相关数据存储发展阶段在字节跳动，随着业务的快速发展，HDFS 的数据量和集群规模快速扩大，原来的 HDFS 的集群从几百台，迅速突破千台和万台的规模。这中间，踩了无数的坑，大的阶段归纳起来会有这样几个阶段。第一阶段业务增长初期，集群规模增长趋势非常陡峭，单集群规模很快在元数据服务器 Name Node 侧遇到瓶颈。引入联邦机制（Federation）实现集群的横向扩展。联邦又带来统一命名空间问题，因此，需要统一视图空间帮助业务构建统一接入。为了解决这个问题，我们引入了 Name Node Proxy 组件实现统一视图和多租户管理等功能，这部分会在下文的 NNProxy 章节中介绍。第二阶段数据量继续增大，Federation 方式下的目录树管理也存在瓶颈，主要体现在数据量增大后，Java 版本的 GC 变得更加频繁，跨子树迁移节点代价过大，节点启动时间太长等问题。因此我们通过重构的方式，解决了 GC，锁优化，启动加速等问题，将原 Name Node 的服务能力进一步提高。容纳更多的元数据信息。为了解决这个问题，我们也实现了字节跳动特色的 DanceNN 组件，兼容了原有 Java 版本 NameNode 的全部功能基础上，大大增强了稳定性和性能。相关详细介绍会在下面的 DanceNN 章节中介绍。第三阶段当数据量跨过 EB，集群规模扩大到几万台的时候，慢节点问题，更细粒度服务分级问题，成本问题和元数据瓶颈进一步凸显。我们在架构上进一步在包括完善多租户体系构建，重构数据节点和元数据分层等方向进一步演进。这部分目前正在进行中，因为优化的点会非常多，本文会给出慢节点优化的落地实践。关键改进在整个架构演进的过程中，我们做了非常多的探索和尝试。如上所述，结合之前提到的几个大的挑战和问题，我们就其中关键的 Name Node Proxy 和 Dance Name Node 这两个重点组件做一下介绍，同时，也会介绍一下我们在慢节点方面的优化和改进。NNProxy（Name Node Proxy）作为系统的元数据操作接入端，NNProxy 提供了联邦模式下统一元数据视图，解决了用户请求的统一转发，业务流量的统一管控的问题。先介绍一下 NNProxy 所处的系统上下游。我们先来看一下 NNProxy 都做了什么工作。路由管理在上面 Federation 的介绍中提到，每个集群都维护自己独立的目录树，无法对外提供一个完整的目录树视图。NNProxy 中的路由管理就解决了这个问题。路由管理存储了一张 mount table，表中记录若干条路径到集群的映射关系。例如 /user -&gt; hdfs://namenodeB，这条映射关系的含义就是 /user 及其子目录这个目录在 namenodeB 这个集群上，所有对 /user 及其子目录的访问都会由 NNProxy 转发给 namenodeB，获取结果后再返回给 Client。匹配原则为最长匹配，例如我们还有另外一条映射 /user/tiger/dump -&gt; hdfs://namenodeC，那么 /user/tiger/dump 及其所有子目录都在 namenodeC，而 /user 目录下其他子目录都在 namenodeB 上。如下图所示：Quota 限制使用过 HDFS 的同学会知道 Quota 这个概念。我们给每个目录集合分配了额定的空间资源，一旦使用超过这个阈值，就会被禁止写入。这个工作就是由 NNProxy 完成的。NNProxy 会通过 Quota 实时监控系统获取最新 Quota 使用情况，当用户进行元数据操作的时候，NNProxy 就会根据用户的 Quota 情况作出判断，决定通过或者拒绝。Trace 支持ByteTrace 是一个 Trace 系统，记录追踪用户和系统以及系统之间的调用行为，以达到分析和运维的目的。其中的 Trace 信息会附在向 NNProxy 的请求 RPC 中。NNProxy 拿到 ByteTrace 以后就可以知道当前请求的上游模块，USER 及 Application ID 等信息。NNProxy 一方面将这些信息发到 Kafka 做一些离线分析，一方面实时聚合并打点，以便追溯线上流量。流量限制虽然 NNProxy 非常轻量，可以承受很高的 QPS，但是后端的 Name Node 承载能力是有限的。因此突发的大作业造成高 QPS 的读写请求被全量转发到 Name Node 上时，会造成 Name Node 过载，延时变高，甚至出现 OOM，影响集群上所有用户。因此 NNProxy 另一个非常重要的任务就是限流，以保护后端 Name Node。目前限流基于路径+RPC 以及 用户+RPC 维度，例如我们可以限制 /user/tiger/warhouse 路径的 create 请求为 100 QPS，或者某个用户的 delete 请求为 5 QPS。一旦该用户的访问量超过这个阈值，NNProxy 会返回一个可重试异常，Client 收到这个异常后会重试。因此被限流的路径或用户会感觉到访问 HDFS 变慢，但是并不会失败。Dance NN（Dance Name Node）解决的问题如前所述，在数据量上到 EB 级别的场景后，原有的 Java 版本的 Name Node 存在了非常多的线上问题需要解决。以下是在实践过程中我们遇到的一些问题总结：Java 版本 Name Node 采用 Java 语言开发，在 INode 规模上亿时，不可避免的会带来严重的 GC 问题；Java 版本 Name Node 将 INode meta 信息完全放置于内存，10 亿 INode 大约占用 800GB 内存（包含 JVM 自身占用的部分 native memory），更进一步加重了 GC；我们目前的集群规模下，Name Node 从重启到恢复服务需要 6 个小时，在主备同时发生故障的情况下，严重影响上层业务；Java 版本 Name Node 全局一把读写锁，任何对目录树的修改操作都会阻塞其他的读写操作，并发度较低；从上可以看出，在大数据量场景下，我们亟需一个新架构版本的 Name Node 来承载我们的海量元数据。除了 C++语言重写来规避 Java 带来的 GC 问题以外，我们还在一些场景下做了特殊的优化。目录树锁设计HDFS 对内是一个分布式集群，对外提供的是一个 unified 的文件系统，因此对文件及目录的操作需要像操作 Linux 本地文件系统一样。这就要求 HDFS 满足类似于数据库系统中 ACID 特性一样的原子性，一致性、隔离性和持久性。因此 DanceNN 在面对多个用户同时操作同一个文件或者同一个目录时，需要保证不会破坏掉 ACID 属性，需要对操作做锁保护。不同于传统的 KV 存储和数据库表结构，DanceNN 上维护的是一棵树状的数据结构，因此单纯的 key 锁或者行锁在 DanceNN 下不适用。而像数据库的表锁或者原生 NN 的做法，对整棵目录树加单独一把锁又会严重的影响整体吞吐和延迟，因此 DanceNN 重新设计了树状锁结构，做到保证 ACID 的情况下，读吞吐能够到 8w，写吞吐能够到 2w，是原生 NN 性能的 10 倍以上。这里，我们会重新对 RPC 做分类，像 createFile，getFileInfo，setXAttr 这类 RPC 依然是简单的对某一个 INode 进行 CURD 操作；像 delete RPC，有可能删除一个文件，也有可能会删除目录，后者会影响整棵子树下的所有文件；像 rename RPC，则是更复杂的另外一类操作，可能会涉及到多个 INode，甚至是多棵子树下的所有 INode。DanceNN 启动优化由于我们的 DanceNN 底层元数据实现了本地目录树管理结构，因此我们 DanceNN 的启动优化都是围绕着这样的设计来做的。多线程扫描和填充 BlockMap在系统启动过程中，第一步就是读取目录树中保存的信息并且填入 BlockMap 中，类似 Java 版 NN 读取 FSImage 的操作。在具体实现过程中，首先起多个线程并行扫描静态目录树结构。将扫描的结果放入一个加锁的 Buffer 中。当 Buffer 中的元素个数达到设定的数量以后，重新生成一个新的 Buffer 接收请求，并在老 Buffer 上起一个线程将数据填入 BlockMap。接收块上报优化DanceNN 启动以后会首先进入安全模式，接收所有 Date Node 的块上报，完善 BlockMap 中保存的信息。当上报的 Date Node 达到一定比例以后，才会退出安全模式，这时候才能正式接收 client 的请求。所以接收块上报的速度也会影响 Date Node 的启动时长。DanceNN 这里做了一个优化，根据 BlockID 将不同请求分配给不同的线程处理，每个线程负责固定的 Slice，线程之间无竞争，这样就极大的加快了接收块上报的速度。如下图所示：慢节点优化慢节点问题在很多分布式系统中都存在。其产生的原因通常为上层业务的热点或者底层资源故障。上层业务热点，会导致一些数据在较短的时间段内被集中访问。而底层资源故障，如出现慢盘或者盘损坏，更多的请求就会集中到某一个副本节点上从而导致慢节点。通常来说，慢节点问题的优化和上层业务需求及底层资源量有很大的关系，极端情况，上层请求很小，下层资源充分富裕的情况下，慢节点问题将会非常少，反之则会变得非常严重。在字节跳动的 HDFS 集群中，慢节点问题一度非常严重，尤其是磁盘占用百分比非常高以后，各种慢节点问题层出不穷。其根本原因就是资源的平衡滞后，许多机器的磁盘占用已经触及红线导致写降级；新增热资源则会集中到少量机器上，这种情况下，当上层业务的每秒请求数升高后，对于 P999 时延要求比较高的一些大数据分析查询业务就容易出现一大批数据访问（&gt;10000 请求）被卡在某个慢请求的处理上。我们优化的方向会分为读慢节点和写慢节点两个方面。读慢节点优化我们经历了几个阶段：最早，使用社区版本，其 Switch Read 以读取一个 packet 的时长为统计单位，当读取一个 packet 的时间超过阈值时，认为读取当前 packet 超时。如果一定时间窗口内超时 packet 的数量过多，则认为当前节点是慢节点。但这个问题在于以 packet 作为统计单位使得算法不够敏感，这样使得每次读慢节点发生的时候，对于小 IO 场景（字节跳动的一些业务是以大量随机小 IO 为典型使用场景的），这些个积攒的 Packet 已经造成了问题。后续，我们研发了 Hedged Read 的读优化。Hedged Read 对每一次读取设置一个超时时间。如果读取超时，那么会另开一个线程，在新的线程中向第二个副本发起读请求，最后取第一第二个副本上优先返回的 response 作为读取的结果。但这种情况下，在慢节点集中发生的时候，会导致读流量放大。严重的时候甚至导致小范围带宽短时间内不可用。基于之前的经验，我们进一步优化，开启了 Fast Switch Read 的优化，该优化方式使用吞吐量作为判断慢节点的标准，当一段时间窗口内的吞吐量小于阈值时，认为当前节点是慢节点。并且根据当前的读取状况动态地调整阈值，动态改变时间窗口的长度以及吞吐量阈值的大小。下表是当时线上某业务测试的值：Host:X.X.X.X3 副本 Switch Read2 副本 Hedged Read3 副本 Hedged Read3 副本 Fast Switch Read（优化后算法）读取时长 p999977 ms549 ms192 ms128 ms最长读取时间300 s125 s60 s15.5 s长尾出现次数（大于 500ms）238 次/天75 次/天15 次/天3 次/天长尾出现次数（大于 1000ms）196 次/天64 次/天6 次/天3 次/天进一步的相关测试数据：写慢节点优化写慢节点优化的适用场景会相对简单一些。主要解决的是写过程中，Pipeline 的中间节点变慢的情况。为了解决这个问题，我们也发展了 Fast Failover 和 Fast Failover+两种算法。Fast FailoverFast Failover 会维护一段时间内 ACK 时间过长的 packet 数目，当超时 ACK 的数量超过阈值后，会结束当前的 block，向 namenode 申请新块继续写入。Fast Failover 的问题在于，随意结束当前的 block 会造成系统的小 block 数目增加，给之后的读取速度以及 namenode 的元数据维护都带来负面影响。所以 Fast Failover 维护了一个切换阈值，如果已写入的数据量（block 的大小）大于这个阈值，才会进行 block 切换。但是往往为了达到这个写入数据大小阈值，就会造成用户难以接收的延迟，因此当数据量小于阈时需要进额外的优化。Fast Failover+为了解决上述的问题，当已写入的数据量（block 的大小）小于阈值时，我们引入了新的优化手段——Fast Failover+。该算法首先从 pipeline 中筛选出速度较慢的 datanode，将慢节点从当前 pipeline 中剔除，并进入 Pipeline Recovery 阶段。Pipeline Recovery 会向 namenode 申请一个新的 datanode，与剩下的 datanode 组成一个新的 pipeline，并将已写入的数据同步到新的 datanode 上（该步骤称为 transfer block）。由于已经写入的数据量较小，transfer block 的耗时并不高。统计 p999 平均耗时只有 150ms。由 Pipeline Recovery 所带来的额外消耗是可接受的。下表是当时线上某业务测试的值：Host:X.X.X.XFast Failover p99Fast Failover+ p99 (优化后算法)Fast Failover p95Fast Failover+ p95 (优化后算法)平均 Flush 时长1.49 s1.23 s182 ms147 ms最长 Flush 时间80 s66 s9.7 s6.5 s长尾出现次数（p99 大于 10s, p95 大于 1s）63 次/天38 次/天94 次/天55 次/天长尾出现次数（p99 大于 5s, p95 大于 0.5s）133 次/天101 次/天173 次/天156 次/天一些进一步的实际效果对比：结尾HDFS 在字节跳动的发展历程已经非常长了。从最初的几百台的集群规模支持 PB 级别的数据量，到现在几万台级别多集群的平台支持 EB 级别的数据量，我们经历了 7 年的发展。伴随着业务的快速上量，我们团队也经历了野蛮式爆发，规模化发展，平台化运营的阶段。这过程中我们踩了不少坑，也积累了相当丰富的经验。当然，最重要的，公司还在持续高速发展，而我们仍旧不忘初心，坚持“DAY ONE”，继续在路上。学习总结接入层：接入层是字节设计假如的一层，在上万节点的HDFS集群中，必然要使用多NameNode模式，那么对于用户大量的请求统一管理，字节引入了接入层，具体实现借用Redis，Mysql以及NNProxy转发路由等外界组件实现。元数据层：这里面有一点，在字节的HDFS集群中，并没有使用社区版的QJM HA高可用方案，而是使用了BookKeeper。Apache bookkeeper是一个分布式，可扩展，容错（多副本），低延迟的存储系统，其提供了高性能，高吞吐的存储能力。而QJM/Qurom Journal Manager是Clouera提出的，这是一个基于Paxos算法实现的HDFS HA方案数据层倒是没什么特别的改善可以看出，字节的数据暴涨阶段，首先遇到的问题是Name Node的瓶颈，而此时字节的集群环境为单集群，此时的解决方案是采用Federation。数据持续高速增长，Federation 方式下的目录树管理也存在瓶颈，主要原因是Java频繁GC，那么字节的解决方案就显得有些硬核了，重写了NameNode，在字节中叫做DanceNN 🐂🍺。在数据超过EB级别之后，遇到的问题就更多了。不同粒度服务分级，元数据存储瓶颈，慢节点等问题。那么字节的解决方案则是考虑到一方面从存储方面的数据节点进行重构，另一方面对于大块的元数据进行分级。这里要说一下上面提到的NNProxy，好用！！主要有两个功能很吸引我，在Hadoop集群原有的基础上，字节添加了NNProxy，一个是根据用户请求的路径转发到不同的HDFS空间，二呢，对多租户的场景下，对每个用户的请求做判断，如果某个请求量过大，则会对其限流。在这里，我也领略到了一个场景，那就是在字节EB级别的集群规模下，集群重启到全部服务恢复，需要6个小时左右。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"GPT-2 Chinese 自动生成文章 - 环境准备","slug":"GPT-2-Chinese-自动生成文章-环境准备","date":"2020-01-01T14:28:43.000Z","updated":"2020-04-13T09:28:23.224Z","comments":true,"path":"2020/01/01/GPT-2-Chinese-自动生成文章-环境准备/","link":"","permalink":"cpeixin.cn/2020/01/01/GPT-2-Chinese-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E7%AB%A0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"Google ColabColaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用。利用Colaboratory ，可以方便的使用Keras,TensorFlow,PyTorch等框架进行深度学习应用的开发。缺点是最多只能运行12小时，时间一到就会清空VM上所有数据。这包括我们安装的软件，包括我们下载的数据，存放的计算结果， 所以最好不要直接在colab上进行文件的修改，以防保存不及时而造成丢失，而且Google Drive只有免费的15G空间，如果训练文件很大的话，需要扩容。优点 免费！ 免费！免费！**谷歌云盘当登录账号进入谷歌云盘时，系统会给予15G免费空间大小。由于Colab需要依靠谷歌云盘，故需要在云盘上新建一个文件夹，来存放你的代码或者数据。可以看到上图，我的存储空间几乎快满了，在选择进行扩容的时候呢，则需要国外银行卡和国外支付方式，这一点就有点头痛，但是不要忘记万能的淘宝，最后通过淘宝的，花费20元左右，就升级到了无限空间，这里需要注意一下，升级存储空间的方式是添加一块共享云盘，如下图：引入Colab设置GPU环境打开colab后，我们要设置运行环境。”修改”—&gt;”笔记本设置”挂载和切换工作目录1234567from google.colab import drivedrive.mount('/content/drive')import os# os.chdir('/content/drive/My Drive/code/GPT2-Chinese') # 原本Google drive的目录os.chdir('/content/drive/Shared drives/brentfromchina/code_warehouse/GPT2-Chinese') ## 共享云盘的目录其中： My Drive 代表你的google网盘根目录code/GPT2-Chinese 或者 code_warehouse/GPT2-Chinese 代表网盘中你的程序文件目录在Colab中运行任务下图是我google drive中的文件结构， 在项目文件中，创建一个.ipynb文件，来执行你的所有操作。.ipynb文件内容","categories":[{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"}],"tags":[{"name":"GPT-2","slug":"GPT-2","permalink":"cpeixin.cn/tags/GPT-2/"}]},{"title":"架构思想","slug":"架构思想","date":"2019-12-20T02:26:15.000Z","updated":"2020-04-04T11:23:45.206Z","comments":true,"path":"2019/12/20/架构思想/","link":"","permalink":"cpeixin.cn/2019/12/20/%E6%9E%B6%E6%9E%84%E6%80%9D%E6%83%B3/","excerpt":"","text":"关于什么是架构，一种比较通俗的说法是 “最高层次的规划，难以改变的决定”，这些规划和决定奠定了事物未来发展的方向和最终的蓝图。从这个意义上说，人生规划也是一种架构。选什么学校、学什么专业、进什么公司、找什么对象，过什么样的生活，都是自己人生的架构。具体到软件架构，维基百科是这样定义的：“有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计”。系统的各个重要组成部分及其关系构成了系统的架构，这些组成部分可以是具体的功能模块，也可以是非功能的设计与决策，他们相互关系组成一个整体，共同构成了软件系统的架构。架构其实就是把复杂的问题抽象化、简单化，可能你会觉得“说起来容易但做起来难”，如何能快速上手。可以多观察，根据物质决定意识，借助生活真实场景（用户故事，要很多故事）来还原这一系列问题，抓住并提取核心特征。架构思想CPU运算速度&gt;&gt;&gt;&gt;&gt;内存的读写速度&gt;&gt;&gt;&gt;磁盘读写速度满足业务发展需求是最高准则业务建模，抽象和枚举是两种方式，需要平衡，不能走极端模型要能更真实的反应事物的本质，不是名词概念的堆砌，不能过度设计基础架构最关键的是分离不同业务领域、不同技术领域，让整个系统具有持续优化的能力。分离基础服务、业务规则、业务流程，选择合适的工具外化业务规则和业务流程分离业务组件和技术组件，高类聚，低耦合 - 业务信息的执行可以分散，但业务信息的管理要尽量集中不要让软件的逻辑架构与最后物理部署绑死 - 选择合适的技术而不是高深的技术，随着业务的发展调整使用的技术好的系统架构需要合适的组织架构去保障 - 团队成员思想的转变，漫长而艰难业务架构、系统架构、数据模型面对一块新业务，如何系统架构？业务分析：输出业务架构图，这个系统里有多少个业务模块，从前台用户到底层一共有多少层。系统划分：根据业务架构图输出系统架构图，需要思考的是这块业务划分成多少个系统，可能一个系统能支持多个业务。基于什么原则将一个系统拆分成多个系统？又基于什么原则将两个系统合并成一个系统？系统分层：系统是几层架构，基于什么原则将一个系统进行分层，分成多少层？模块化：系统里有多少个模块，哪些需要模块化？基于什么原则将一类代码变成一个模块。如何模块化基于水平切分。把一个系统按照业务类型进行水平切分成多个模块，比如权限管理模块，用户管理模块，各种业务模块等。基于垂直切分。把一个系统按照系统层次进行垂直切分成多个模块，如DAO层，SERVICE层，业务逻辑层。基于单一职责。将代码按照职责抽象出来形成一个一个的模块。将系统中同一职责的代码放在一个模块里。比如我们开发的系统要对接多个渠道的数据，每个渠道的对接方式和数据解析方式不一样，为避免不同渠道代码的相互影响，我们把各个渠道的代码放在各自的模块里。基于易变和不易变。将不易变的代码抽象到一个模块里，比如系统的比较通用的功能。将易变的代码放在另外一个或多个模块里，比如业务逻辑。因为易变的代码经常修改，会很不稳定，分开之后易变代码在修改时候，不会将BUG传染给不变的代码。提升系统的稳定性流控双11期间，对于一些重要的接口（比如帐号的查询接口，店铺首页）做流量控制，超过阈值直接返回失败。另外对于一些不重要的业务也可以考虑采用降级方案，大促—&gt;邮件系统。根据28原则，提前将大卖家约1W左右在缓存中预热，并设置起止时间，活动期间内这部分大卖家不发交易邮件提醒，以减轻SA邮件服务器的压力。容灾最大程度保证主链路的可用性，比如我负责交易的下单，而下单过程中有优惠的业务逻辑，此时需要考虑UMP系统挂掉，不会影响用户下单（后面可以通过修改价格弥补），采用的方式是，如果优惠挂掉，重新渲染页面，并增加ump屏蔽标记，下单时会自动屏蔽ump的代码逻辑。另外还会记录ump系统不可用次数，一定时间内超过阈值，系统会自动报警。稳定性第三方系统可能会不稳定，存在接口超时或宕机，为了增加系统的健壮性，调用接口时设置超时时间以及异常捕获处理。容量规划做好容量规划、系统间强弱依赖关系梳理。如：冷热数据不同处理，早期的订单采用oracle存储，随着订单的数量越来越多，查询缓慢，考虑数据迁移，引入历史表，将已归档的记录迁移到历史表中。当然最好的方法是分库分表。分布式架构分布式系统分布式缓存分布式数据API 和乐高积木有什么相似之处？相信我们大多数人在儿童时期都喜欢玩乐高积木。乐高积木的真正乐趣和吸引力在于，尽管包装盒外面都带有示意图片，但你最终都可以随心所欲得搭出各种样子或造型。对 API 的最佳解释就是它们像乐高积木一样。我们可以用创造性的方式来组合它们，而不用在意它们原本的设计和实现意图。你可以发现很多 API 和乐高积木的相似之处：标准化：通用、标准化的组件，作为基本的构建块（building blocks）；可用性：强调可用性，附有文档或使用说明；可定制：为不同功能使用不同的API；创造性：能够组合不同的 API 来创造混搭的结果；乐高和 API 都有超简单的界面/接口，并且借助这样简单的界面/接口，它可以非常直观、容易、快速得构建。虽然乐高和 API 一样可能附带示意图片或使用文档，大概描述了推荐玩法或用途，但真正令人兴奋的结果或收获恰恰是通过创造力产生的。让我们仔细地思考下上述的提法。在很多情况下，API 的使用者构建出了 API 的构建者超出预期的服务或产品，API 使用者想要的，和 API 构建者认为使用者想要的，这二者之间通常有个断层。事实也确实如此，在 IoT 领域，我们使用 API 创造出了一些非常有创造性的使用场景。","categories":[{"name":"架构","slug":"架构","permalink":"cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[]},{"title":"设计模式 - 单例模式（上）","slug":"设计模式-单例模式（上）","date":"2019-12-09T07:38:03.000Z","updated":"2020-08-09T07:40:39.845Z","comments":true,"path":"2019/12/09/设计模式-单例模式（上）/","link":"","permalink":"cpeixin.cn/2019/12/09/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"大纲为什么要使用单例？单例存在哪些问题？单例与静态类的区别？有何替代的解决方案？为什么要使用单例？单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。对于单例的概念，我觉得没必要解释太多，你一看就能明白。我们重点看一下，为什么我们需要单例这种设计模式？它能解决哪些问题？接下来我通过两个实战案例来讲解。实战案例一：处理资源访问冲突我们先来看第一个例子。在这个例子中，我们自定义实现了一个往文件中打印日志的 Logger 类。具体的代码实现如下所示：1234567891011121314151617181920212223242526272829303132public class Logger &#123; private FileWriter writer; public Logger() &#123; File file = new File(\"/Users/wangzheng/log.txt\"); writer = new FileWriter(file, true); //true表示追加写入 &#125; public void log(String message) &#123; writer.write(mesasge); &#125;&#125;// Logger类的应用示例：public class UserController &#123; private Logger logger = new Logger(); public void login(String username, String password) &#123; // ...省略业务逻辑代码... logger.log(username + \" logined!\"); &#125;&#125;public class OrderController &#123; private Logger logger = new Logger(); public void create(OrderVo order) &#123; // ...省略业务逻辑代码... logger.log(\"Created an order: \" + order.toString()); &#125;&#125;看完代码之后，先别着急看我下面的讲解，你可以先思考一下，这段代码存在什么问题。在上面的代码中，我们注意到，所有的日志都写入到同一个文件 /Users/wangzheng/log.txt 中。在 UserController 和 OrderController 中，我们分别创建两个 Logger 对象。在 Web 容器的 Servlet 多线程环境下，如果两个 Servlet 线程同时分别执行 login() 和 create() 两个函数，并且同时写日志到 log.txt 文件中，那就有可能存在日志信息互相覆盖的情况。为什么会出现互相覆盖呢？我们可以这么类比着理解。在多线程环境下，如果两个线程同时给同一个共享变量加 1，因为共享变量是竞争资源，所以，共享变量最后的结果有可能并不是加了 2，而是只加了 1。同理，这里的 log.txt 文件也是竞争资源，两个线程同时往里面写数据，就有可能存在互相覆盖的情况。那如何来解决这个问题呢？我们最先想到的就是通过加锁的方式：给 log() 函数加互斥锁（Java 中可以通过 synchronized 的关键字），同一时刻只允许一个线程调用执行 log() 函数。具体的代码实现如下所示：123456789101112131415public class Logger &#123; private FileWriter writer; public Logger() &#123; File file = new File(\"/Users/wangzheng/log.txt\"); writer = new FileWriter(file, true); //true表示追加写入 &#125; public void log(String message) &#123; synchronized(this) &#123; writer.write(mesasge); &#125; &#125;&#125;不过，你仔细想想，这真的能解决多线程写入日志时互相覆盖的问题吗？答案是否定的。这是因为，这种锁是一个对象级别的锁，一个对象在不同的线程下同时调用 log() 函数，会被强制要求顺序执行。但是，不同的对象之间并不共享同一把锁。在不同的线程下，通过不同的对象调用执行 log() 函数，锁并不会起作用，仍然有可能存在写入日志互相覆盖的问题。我这里稍微补充一下，在刚刚的讲解和给出的代码中，我故意“隐瞒”了一个事实：我们给 log() 函数加不加对象级别的锁，其实都没有关系。因为 FileWriter 本身就是线程安全的，它的内部实现中本身就加了对象级别的锁，因此，在外层调用 write() 函数的时候，再加对象级别的锁实际上是多此一举。因为不同的 Logger 对象不共享 FileWriter 对象，所以，FileWriter 对象级别的锁也解决不了数据写入互相覆盖的问题。那我们该怎么解决这个问题呢？实际上，要想解决这个问题也不难，我们只需要把对象级别的锁，换成类级别的锁就可以了。让所有的对象都共享同一把锁。这样就避免了不同对象之间同时调用 log() 函数，而导致的日志覆盖问题。具体的代码实现如下所示：123456789101112131415public class Logger &#123; private FileWriter writer; public Logger() &#123; File file = new File(\"/Users/wangzheng/log.txt\"); writer = new FileWriter(file, true); //true表示追加写入 &#125; public void log(String message) &#123; synchronized(Logger.class) &#123; // 类级别的锁 writer.write(mesasge); &#125; &#125;&#125;除了使用类级别锁之外，实际上，解决资源竞争问题的办法还有很多，分布式锁是最常听到的一种解决方案。不过，实现一个安全可靠、无 bug、高性能的分布式锁，并不是件容易的事情。除此之外，并发队列（比如 Java 中的 BlockingQueue）也可以解决这个问题：多个线程同时往并发队列里写日志，一个单独的线程负责将并发队列中的数据，写入到日志文件。这种方式实现起来也稍微有点复杂。相对于这两种解决方案，单例模式的解决思路就简单一些了。单例模式相对于之前类级别锁的好处是，不用创建那么多 Logger 对象，一方面节省内存空间，另一方面节省系统文件句柄（对于操作系统来说，文件句柄也是一种资源，不能随便浪费）。我们将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题。按照这个设计思路，我们实现了 Logger 单例类。具体代码如下所示：123456789101112131415161718192021222324252627282930313233public class Logger &#123; private FileWriter writer; private static final Logger instance = new Logger(); private Logger() &#123; File file = new File(\"/Users/wangzheng/log.txt\"); writer = new FileWriter(file, true); //true表示追加写入 &#125; public static Logger getInstance() &#123; return instance; &#125; public void log(String message) &#123; writer.write(mesasge); &#125;&#125;// Logger类的应用示例：public class UserController &#123; public void login(String username, String password) &#123; // ...省略业务逻辑代码... Logger.getInstance().log(username + \" logined!\"); &#125;&#125;public class OrderController &#123; public void create(OrderVo order) &#123; // ...省略业务逻辑代码... Logger.getInstance().log(\"Created a order: \" + order.toString()); &#125;&#125;**实战案例二：表示全局唯一类从业务概念上，如果有些数据在系统中只应保存一份，那就比较适合设计为单例类。比如，配置信息类。在系统中，我们只有一个配置文件，当配置文件被加载到内存之后，以对象的形式存在，也理所应当只有一份。再比如，唯一递增 ID 号码生成器（第 34 讲中我们讲的是唯一 ID 生成器，这里讲的是唯一递增 ID 生成器），如果程序中有两个对象，那就会存在生成重复 ID 的情况，所以，我们应该将 ID 生成器类设计为单例。12345678910111213141516171819import java.util.concurrent.atomic.AtomicLong;public class IdGenerator &#123; // AtomicLong是一个Java并发库中提供的一个原子变量类型, // 它将一些线程不安全需要加锁的复合操作封装为了线程安全的原子操作， // 比如下面会用到的incrementAndGet(). private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() &#123;&#125; public static IdGenerator getInstance() &#123; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125;// IdGenerator使用举例long id = IdGenerator.getInstance().getId();实际上，今天讲到的两个代码实例（Logger、IdGenerator），设计的都并不优雅，还存在一些问题。如何实现一个单例？尽管介绍如何实现一个单例模式的文章已经有很多了，但为了保证内容的完整性，我这里还是简单介绍一下几种经典实现方式。概括起来，要实现一个单例，我们需要关注的点无外乎下面几个：构造函数需要是 private 访问权限的，这样才能避免外部通过 new 创建实例；考虑对象创建时的线程安全问题；考虑是否支持延迟加载；考虑 getInstance() 性能是否高（是否加锁）。如果你对这块已经很熟悉了，你可以当作复习。注意，下面的几种单例实现方式是针对 Java 语言语法的，如果你熟悉的是其他语言，不妨对比 Java 的这几种实现方式，自己试着总结一下，利用你熟悉的语言，该如何实现。1. 饿汉式饿汉式的实现方式比较简单。在类加载的时候，instance 静态实例就已经创建并初始化好了，所以，instance 实例的创建过程是线程安全的。不过，这样的实现方式不支持延迟加载（在真正用到 IdGenerator 的时候，再创建实例），从名字中我们也可以看出这一点。具体的代码实现如下所示：123456789101112public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() &#123;&#125; public static IdGenerator getInstance() &#123; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125;有人觉得这种实现方式不好，因为不支持延迟加载，如果实例占用资源多（比如占用内存多）或初始化耗时长（比如需要加载各种配置文件），提前初始化实例是一种浪费资源的行为。最好的方法应该在用到的时候再去初始化。不过，我个人并不认同这样的观点。如果初始化耗时长，那我们最好不要等到真正要用它的时候，才去执行这个耗时长的初始化过程，这会影响到系统的性能（比如，在响应客户端接口请求的时候，做这个初始化操作，会导致此请求的响应时间变长，甚至超时）。采用饿汉式实现方式，将耗时的初始化操作，提前到程序启动的时候完成，这样就能避免在程序运行的时候，再去初始化导致的性能问题。如果实例占用资源多，按照 fail-fast 的设计原则（有问题及早暴露），那我们也希望在程序启动时就将这个实例初始化好。如果资源不够，就会在程序启动的时候触发报错（比如 Java 中的 PermGen Space OOM），我们可以立即去修复。这样也能避免在程序运行一段时间后，突然因为初始化这个实例占用资源过多，导致系统崩溃，影响系统的可用性。2. 懒汉式有饿汉式，对应的，就有懒汉式。懒汉式相对于饿汉式的优势是支持延迟加载。具体的代码实现如下所示：123456789101112131415public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() &#123;&#125; public static synchronized IdGenerator getInstance() &#123; if (instance == null) &#123; instance = new IdGenerator(); &#125; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125;不过懒汉式的缺点也很明显，我们给 getInstance() 这个方法加了一把大锁（synchronzed），导致这个函数的并发度很低。量化一下的话，并发度是 1，也就相当于串行操作了。而这个函数是在单例使用期间，一直会被调用。如果这个单例类偶尔会被用到，那这种实现方式还可以接受。但是，如果频繁地用到，那频繁加锁、释放锁及并发度低等问题，会导致性能瓶颈，这种实现方式就不可取了。3. 双重检测饿汉式不支持延迟加载，懒汉式有性能问题，不支持高并发。那我们再来看一种既支持延迟加载、又支持高并发的单例实现方式，也就是双重检测实现方式。在这种实现方式中，只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。所以，这种实现方式解决了懒汉式并发度低的问题。具体的代码实现如下所示：12345678910111213141516171819public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() &#123;&#125; public static IdGenerator getInstance() &#123; if (instance == null) &#123; synchronized(IdGenerator.class) &#123; // 此处为类级别的锁 if (instance == null) &#123; instance = new IdGenerator(); &#125; &#125; &#125; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125;网上有人说，这种实现方式有些问题。因为指令重排序，可能会导致 IdGenerator 对象被 new 出来，并且赋值给 instance 之后，还没来得及初始化（执行构造函数中的代码逻辑），就被另一个线程使用了。要解决这个问题，我们需要给 instance 成员变量加上 volatile 关键字，禁止指令重排序才行。实际上，只有很低版本的 Java 才会有这个问题。我们现在用的高版本的 Java 已经在 JDK 内部实现中解决了这个问题（解决的方法很简单，只要把对象 new 操作和初始化操作设计为原子操作，就自然能禁止重排序）。关于这点的详细解释，跟特定语言有关，我就不展开讲了，感兴趣的同学可以自行研究一下。4. 静态内部类我们再来看一种比双重检测更加简单的实现方法，那就是利用 Java 的静态内部类。它有点类似饿汉式，但又能做到了延迟加载。具体是怎么做到的呢？我们先来看它的代码实现。1234567891011121314151617public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private IdGenerator() &#123;&#125; private static class SingletonHolder&#123; private static final IdGenerator instance = new IdGenerator(); &#125; public static IdGenerator getInstance() &#123; return SingletonHolder.instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125;SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。5. 枚举最后，我们介绍一种最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。具体的代码如下所示：123456789public enum IdGenerator &#123; INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() &#123; return id.incrementAndGet(); &#125;&#125;总结好了，今天的内容到此就讲完了。我们来总结回顾一下，你需要掌握的重点内容。1. 单例的定义单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者叫实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。2. 单例的用处从业务概念上，有些数据在系统中只应该保存一份，就比较适合设计为单例类。比如，系统的配置信息类。除此之外，我们还可以使用单例解决资源访问冲突的问题。3. 单例的实现单例有下面几种经典的实现方式。饿汉式饿汉式的实现方式，在类加载的期间，就已经将 instance 静态实例初始化好了，所以，instance 实例的创建是线程安全的。不过，这样的实现方式不支持延迟加载实例。懒汉式懒汉式相对于饿汉式的优势是支持延迟加载。这种实现方式会导致频繁加锁、释放锁，以及并发度低等问题，频繁的调用会产生性能瓶颈。双重检测双重检测实现方式既支持延迟加载、又支持高并发的单例实现方式。只要 instance 被创建之后，再调用 getInstance() 函数都不会进入到加锁逻辑中。所以，这种实现方式解决了懒汉式并发度低的问题。静态内部类利用 Java 的静态内部类来实现单例。这种实现方式，既支持延迟加载，也支持高并发，实现起来也比双重检测简单。枚举最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"cpeixin.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"单例模式","slug":"单例模式","permalink":"cpeixin.cn/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}]},{"title":"kali中文设置","slug":"kali中文设置","date":"2019-12-01T02:26:15.000Z","updated":"2020-04-04T11:06:21.313Z","comments":true,"path":"2019/12/01/kali中文设置/","link":"","permalink":"cpeixin.cn/2019/12/01/kali%E4%B8%AD%E6%96%87%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"更新源https://blog.csdn.net/qq_38333291/article/details/89764967设置编码和中文字体安装http://www.linuxdiyf.com/linux/20701.html","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"kali","slug":"kali","permalink":"cpeixin.cn/tags/kali/"}]},{"title":"分布式下的数据hash分布","slug":"分布式下的数据hash分布","date":"2019-11-19T15:05:08.000Z","updated":"2020-04-04T11:24:04.737Z","comments":true,"path":"2019/11/19/分布式下的数据hash分布/","link":"","permalink":"cpeixin.cn/2019/11/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AEhash%E5%88%86%E5%B8%83/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"【转载】字节跳动在Spark SQL上的核心优化实践","slug":"【转载】字节跳动在Spark-SQL上的核心优化实践","date":"2019-11-12T10:57:27.000Z","updated":"2020-05-16T10:59:03.835Z","comments":true,"path":"2019/11/12/【转载】字节跳动在Spark-SQL上的核心优化实践/","link":"","permalink":"cpeixin.cn/2019/11/12/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8Spark-SQL%E4%B8%8A%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"本文转载自字节跳动技术团队在掘金网上发表的文章，主要是Spark SQL在字节数据仓库应用方面的优化实践原文链接：https://juejin.im/post/5dc3ed336fb9a04a7847f25cSpark SQL 架构简介我们先简单聊一下Spark SQL 的架构。下面这张图描述了一条 SQL 提交之后需要经历的几个阶段，结合这些阶段就可以看到在哪些环节可以做优化。很多时候，做数据仓库建模的同学更倾向于直接写 SQL 而非使用 Spark 的 DSL。一条 SQL 提交之后会被 Parser 解析并转化为 Unresolved Logical Plan。它的重点是 Logical Plan 也即逻辑计划，它描述了希望做什么样的查询。Unresolved 是指该查询相关的一些信息未知，比如不知道查询的目标表的 Schema 以及数据位置。上述信息存于 Catalog 内。在生产环境中，一般由 Hive Metastore 提供 Catalog 服务。Analyzer 会结合 Catalog 将 Unresolved Logical Plan 转换为 Resolved Logical Plan。到这里还不够。不同的人写出来的 SQL 不一样，生成的 Resolved Logical Plan 也就不一样，执行效率也不一样。为了保证无论用户如何写 SQL 都可以高效的执行，Spark SQL 需要对 Resolved Logical Plan 进行优化，这个优化由 Optimizer 完成。Optimizer 包含了一系列规则，对 Resolved Logical Plan 进行等价转换，最终生成 Optimized Logical Plan。该 Optimized Logical Plan 不能保证是全局最优的，但至少是接近最优的。上述过程只与 SQL 有关，与查询有关，但是与 Spark 无关，因此无法直接提交给 Spark 执行。Query Planner 负责将 Optimized Logical Plan 转换为 Physical Plan，进而可以直接由 Spark 执行。由于同一种逻辑算子可以有多种物理实现。如 Join 有多种实现，ShuffledHashJoin、BroadcastHashJoin、BroadcastNestedLoopJoin、SortMergeJoin 等。因此 Optimized Logical Plan 可被 Query Planner 转换为多个 Physical Plan。如何选择最优的 Physical Plan 成为一件非常影响最终执行性能的事情。一种比较好的方式是，构建一个 Cost Model，并对所有候选的 Physical Plan 应用该 Model 并挑选 Cost 最小的 Physical Plan 作为最终的 Selected Physical Plan。Physical Plan 可直接转换成 RDD 由 Spark 执行。我们经常说“计划赶不上变化”，在执行过程中，可能发现原计划不是最优的，后续执行计划如果能根据运行时的统计信息进行调整可能提升整体执行效率。这部分动态调整由 Adaptive Execution 完成。后面介绍字节跳动在 Spark SQL 上做的一些优化，主要围绕这一节介绍的逻辑计划优化与物理计划优化展开。Spark SQL引擎优化Bucket Join改进在 Spark 里，实际并没有 Bucket Join 算子。这里说的 Bucket Join 泛指不需要 Shuffle 的 Sort Merge Join。下图展示了 Sort Merge Join 的基本原理。用虚线框代表的 Table 1 和 Table 2 是两张需要按某字段进行 Join 的表。虚线框内的 partition 0 到 partition m 是该表转换成 RDD 后的 Partition，而非表的分区。假设 Table 1 与 Table 2 转换为 RDD 后分别包含 m 和 k 个 Partition。为了进行 Join，需要通过 Shuffle 保证相同 Join Key 的数据在同一个 Partition 内且 Partition 内按 Key 排序，同时保证 Table 1 与 Table 2 经过 Shuffle 后的 RDD 的 Partition 数相同。如下图所示，经过 Shuffle 后只需要启动 n 个 Task，每个 Task 处理 Table 1 与 Table 2 中对应 Partition 的数据进行 Join 即可。如 Task 0 只需要顺序扫描 Shuffle 后的左右两边的 partition 0 即可完成 Join。该方法的优势是适用场景广，几乎可用于任意大小的数据集。劣势是每次 Join 都需要对全量数据进行 Shuffle，而 Shuffle 是最影响 Spark SQL 性能的环节。如果能避免 Shuffle 往往能大幅提升 Spark SQL 性能。对于大数据的场景来讲，数据一般是一次写入多次查询。如果经常对两张表按相同或类似的方式进行 Join，每次都需要付出 Shuffle 的代价。与其这样，不如让数据在写的时候，就让数据按照利于 Join 的方式分布，从而使得 Join 时无需进行 Shuffle。如下图所示，Table 1 与 Table 2 内的数据按照相同的 Key 进行分桶且桶数都为 n，同时桶内按该 Key 排序。对这两张表进行 Join 时，可以避免 Shuffle，直接启动 n 个 Task 进行 Join。字节跳动对 Spark SQL 的 BucketJoin 做了四项比较大的改进。改进一：支持与 Hive 兼容**在过去一段时间，字节跳动把大量的 Hive 作业迁移到了 SparkSQL。而 Hive 与 Spark SQL 的 Bucket 表不兼容。对于使用 Bucket 表的场景，如果直接更新计算引擎，会造成 Spark SQL 写入 Hive Bucket 表的数据无法被下游的 Hive 作业当成 Bucket 表进行 Bucket Join，从而造成作业执行时间变长，可能影响 SLA。为了解决这个问题，我们让 Spark SQL 支持 Hive 兼容模式，从而保证 Spark SQL 写入的 Bucket 表与 Hive 写入的 Bucket 表效果一致，并且这种表可以被 Hive 和 Spark SQL 当成 Bucket 表进行 Bucket Join 而不需要 Shuffle。通过这种方式保证 Hive 向 Spark SQL 的透明迁移。第一个需要解决的问题是，Hive 的一个 Bucket 一般只包含一个文件，而 Spark SQL 的一个 Bucket 可能包含多个文件。解决办法是动态增加一次以 Bucket Key 为 Key 并且并行度与 Bucket 个数相同的 Shuffle。第二个需要解决的问题是，Hive 1.x 的哈希方式与 Spark SQL 2.x 的哈希方式（Murmur3Hash）不同，使得相同的数据在 Hive 中的 Bucket ID 与 Spark SQL 中的 Bucket ID 不同而无法直接 Join。在 Hive 兼容模式下，我们让上述动态增加的 Shuffle 使用 Hive 相同的哈希方式，从而解决该问题。改进二：支持倍数关系Bucket Join**Spark SQL 要求只有 Bucket 相同的表才能（必要非充分条件）进行 Bucket Join。对于两张大小相差很大的表，比如几百 GB 的维度表与几十 TB （单分区）的事实表，它们的 Bucket 个数往往不同，并且个数相差很多，默认无法进行 Bucket Join。因此我们通过两种方式支持了倍数关系的 Bucket Join，即当两张 Bucket 表的 Bucket 数是倍数关系时支持 Bucket Join。第一种方式，Task 个数与小表 Bucket 个数相同。如下图所示，Table A 包含 3 个 Bucket，Table B 包含 6 个 Bucket。此时 Table B 的 bucket 0 与 bucket 3 的数据合集应该与 Table A 的 bucket 0 进行 Join。这种情况下，可以启动 3 个 Task。其中 Task 0 对 Table A 的 bucket 0 与 Table B 的 bucket 0 + bucket 3 进行 Join。在这里，需要对 Table B 的 bucket 0 与 bucket 3 的数据再做一次 merge sort 从而保证合集有序。如果 Table A 与 Table B 的 Bucket 个数相差不大，可以使用上述方式。如果 Table B 的 Bucket 个数是 Bucket A Bucket 个数的 10 倍，那上述方式虽然避免了 Shuffle，但可能因为并行度不够反而比包含 Shuffle 的 SortMergeJoin 速度慢。此时可以使用另外一种方式，即 Task 个数与大表 Bucket 个数相等，如下图所示。在该方案下，可将 Table A 的 3 个 Bucket 读多次。在上图中，直接将 Table A 与 Table A 进行 Bucket Union （新的算子，与 Union 类似，但保留了 Bucket 特性），结果相当于 6 个 Bucket，与 Table B 的 Bucket 个数相同，从而可以进行 Bucket Join。改进三：支持BucketJoin 降级**公司内部过去使用 Bucket 的表较少，在我们对 Bucket 做了一系列改进后，大量用户希望将表转换为 Bucket 表。转换后，表的元信息显示该表为 Bucket 表，而历史分区内的数据并未按 Bucket 表要求分布，在查询历史数据时会出现无法识别 Bucket 的问题。同时，由于数据量上涨快，平均 Bucket 大小也快速增长。这会造成单 Task 需要处理的数据量过大进而引起使用 Bucket 后的效果可能不如直接使用基于 Shuffle 的 Join。为了解决上述问题，我们实现了支持降级的 Bucket 表。基本原理是，每次修改 Bucket 信息（包含上述两种情况——将非 Bucket 表转为 Bucket 表，以及修改 Bucket 个数）时，记录修改日期。并且在决定使用哪种 Join 方式时，对于 Bucket 表先检查所查询的数据是否只包含该日期之后的分区。如果是，则当成 Bucket 表处理，支持 Bucket Join；否则当成普通无 Bucket 的表。改进四：支持超集对于一张常用表，可能会与另外一张表按 User 字段做 Join，也可能会与另外一张表按 User 和 App 字段做 Join，与其它表按 User 与 Item 字段进行 Join。而 Spark SQL 原生的 Bucket Join 要求 Join Key Set 与表的 Bucket Key Set 完全相同才能进行 Bucket Join。在该场景中，不同 Join 的 Key Set 不同，因此无法同时使用 Bucket Join。这极大的限制了 Bucket Join 的适用场景。针对此问题，我们支持了超集场景下的 Bucket Join。只要 Join Key Set 包含了 Bucket Key Set，即可进行 Bucket Join。如下图所示，Table X 与 Table Y，都按字段 A 分 Bucket。而查询需要对 Table X 与 Table Y 进行 Join，且 Join Key Set 为 A 与 B。此时，由于 A 相等的数据，在两表中的 Bucket ID 相同，那 A 与 B 各自相等的数据在两表中的 Bucket ID 肯定也相同，所以数据分布是满足 Join 要求的，不需要 Shuffle。同时，Bucket Join 还需要保证两表按 Join Key Set 即 A 和 B 排序，此时只需要对 Table X 与 Table Y 进行分区内排序即可。由于两边已经按字段 A 排序了，此时再按 A 与 B 排序，代价相对较低。物化列**Spark SQL 处理嵌套类型数据时，存在以下问题：读取大量不必要的数据：对于 Parquet / ORC 等列式存储格式，可只读取需要的字段，而直接跳过其它字段，从而极大节省 IO。而对于嵌套数据类型的字段，如下图中的 Map 类型的 people 字段，往往只需要读取其中的子字段，如 people.age。却需要将整个 Map 类型的 people 字段全部读取出来然后抽取出 people.age 字段。这会引入大量的无意义的 IO 开销。在我们的场景中，存在不少 Map 类型的字段，而且很多包含几十至几百个 Key，这也就意味着 IO 被放大了几十至几百倍。无法进行向量化读取：而向量化读能极大的提升性能。但截止到目前（2019年10月26日），Spark 不支持包含嵌套数据类型的向量化读取。这极大的影响了包含嵌套数据类型的查询性能不支持 Filter 下推：目前（2019年10月26日）的 Spark 不支持嵌套类型字段上的 Filter 的下推重复计算：JSON 字段，在 Spark SQL 中以 String 类型存在，严格来说不算嵌套数据类型。不过实践中也常用于保存不固定的多个字段，在查询时通过 JSON Path 抽取目标子字段，而大型 JSON 字符串的字段抽取非常消耗 CPU。对于热点表，频繁重复抽取相同子字段非常浪费资源。对于这个问题，做数仓的同学也想了一些解决方案。如下图所示，在名为 base_table 的表之外创建了一张名为 sub_table 的表，并且将高频使用的子字段 people.age 设置为一个额外的 Integer 类型的字段。下游不再通过 base_table 查询 people.age，而是使用 sub_table 上的 age 字段代替。通过这种方式，将嵌套类型字段上的查询转为了 Primitive 类型字段的查询，同时解决了上述问题。)这种方案存在明显缺陷：额外维护了一张表，引入了大量的额外存储/计算开销。无法在新表上查询新增字段的历史数据（如要支持对历史数据的查询，需要重跑历史作业，开销过大，无法接受）。表的维护方需要在修改表结构后修改插入数据的作业。需要下游查询方修改查询语句，推广成本较大。运营成本高：如果高频子字段变化，需要删除不再需要的独立子字段，并添加新子字段为独立字段。删除前，需要确保下游无业务使用该字段。而新增字段需要通知并推进下游业务方使用新字段。为解决上述所有问题，我们设计并实现了物化列。它的原理是：新增一个 Primitive 类型字段，比如 Integer 类型的 age 字段，并且指定它是 people.age 的物化字段。插入数据时，为物化字段自动生成数据，并在 Partition Parameter 内保存物化关系。因此对插入数据的作业完全透明，表的维护方不需要修改已有作业。查询时，检查所需查询的所有 Partition，如果都包含物化信息（people.age 到 age 的映射），直接将 select people.age 自动重写为 select age，从而实现对下游查询方的完全透明优化。同时兼容历史数据。下图展示了在某张核心表上使用物化列的收益：物化视图在 OLAP 领域，经常会对相同表的某些固定字段进行 Group By 和 Aggregate / Join 等耗时操作，造成大量重复性计算，浪费资源，且影响查询性能，不利于提升用户体验。我们实现了基于物化视图的优化功能：如上图所示，查询历史显示大量查询根据 user 进行 group by，然后对 num 进行 sum 或 count 计算。此时可创建一张物化视图，且对 user 进行 gorup by，对 num 进行 avg（avg 会自动转换为 count 和 sum）。用户对原始表进行 select user, sum(num) 查询时，Spark SQL 自动将查询重写为对物化视图的 select user, sum_num 查询。Spark SQL 引擎上的其它优化下图展示了我们在 Spark SQL 上进行的其它部分优化工作：Spark Shuffle稳定性提升与性能优化Spark Shuffle 存在的问题Shuffle的原理，很多同学应该已经很熟悉了。鉴于时间关系，这里不介绍过多细节，只简单介绍下基本模型。)如上图所示，我们将 Shuffle 上游 Stage 称为 Mapper Stage，其中的 Task 称为 Mapper。Shuffle 下游 Stage 称为 Reducer Stage，其中的 Task 称为 Reducer。每个 Mapper 会将自己的数据分为最多 N 个部分，N 为 Reducer 个数。每个 Reducer 需要去最多 M （Mapper 个数）个 Mapper 获取属于自己的那部分数据。这个架构存在两个问题：稳定性问题：Mapper 的 Shuffle Write 数据存于 Mapper 本地磁盘，只有一个副本。当该机器出现磁盘故障，或者 IO 满载，CPU 满载时，Reducer 无法读取该数据，从而引起 FetchFailedException，进而导致 Stage Retry。Stage Retry 会造成作业执行时间增长，直接影响 SLA。同时，执行时间越长，出现 Shuffle 数据无法读取的可能性越大，反过来又会造成更多 Stage Retry。如此循环，可能导致大型作业无法成功执行。性能问题：每个 Mapper 的数据会被大量 Reducer 读取，并且是随机读取不同部分。假设 Mapper 的 Shuffle 输出为 512MB，Reducer 有 10 万个，那平均每个 Reducer 读取数据 512MB / 100000 = 5.24KB。并且，不同 Reducer 并行读取数据。对于 Mapper 输出文件而言，存在大量的随机读取。而 HDD 的随机 IO 性能远低于顺序 IO。最终的现象是，Reducer 读取 Shuffle 数据非常慢，反映到 Metrics 上就是 Reducer Shuffle Read Blocked Time 较长，甚至占整个 Reducer 执行时间的一大半，如下图所示。基于HDFS的Shuffle稳定性提升经观察，引起 Shuffle 失败的最大因素不是磁盘故障等硬件问题，而是 CPU 满载和磁盘 IO 满载。)如上图所示，机器的 CPU 使用率接近 100%，使得 Mapper 侧的 Node Manager 内的 Spark External Shuffle Service 无法及时提供 Shuffle 服务。下图中 Data Node 占用了整台机器 IO 资源的 84%，部分磁盘 IO 完全打满，这使得读取 Shuffle 数据非常慢，进而使得 Reducer 侧无法在超时时间内读取数据，造成 FetchFailedException。无论是何种原因，问题的症结都是 Mapper 侧的 Shuffle Write 数据只保存在本地，一旦该节点出现问题，会造成该节点上所有 Shuffle Write 数据无法被 Reducer 读取。解决这个问题的一个通用方法是，通过多副本保证可用性。最初始的一个简单方案是，Mapper 侧最终数据文件与索引文件不写在本地磁盘，而是直接写到 HDFS。Reducer 不再通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据，而是直接从 HDFS 上获取数据，如下图所示。快速实现这个方案后，我们做了几组简单的测试。结果表明：Mapper 与 Reducer 不多时，Shuffle 读写性能与原始方案相比无差异。Mapper 与 Reducer 较多时，Shuffle 读变得非常慢。在上面的实验过程中，HDFS 发出了报警信息。如下图所示，HDFS Name Node Proxy 的 QPS 峰值达到 60 万。（注：字节跳动自研了 Node Name Proxy，并在 Proxy 层实现了缓存，因此读 QPS 可以支撑到这个量级）。原因在于，总共 10000 Reducer，需要从 10000 个 Mapper 处读取数据文件和索引文件，总共需要读取 HDFS 10000 * 1000 * 2 = 2 亿次。如果只是 Name Node 的单点性能问题，还可以通过一些简单的方法解决。例如在 Spark Driver 侧保存所有 Mapper 的 Block Location，然后 Driver 将该信息广播至所有 Executor，每个 Reducer 可以直接从 Executor 处获取 Block Location，然后无须连接 Name Node，而是直接从 Data Node 读取数据。但鉴于 Data Node 的线程模型，这种方案会对 Data Node 造成较大冲击。最后我们选择了一种比较简单可行的方案，如下图所示。Mapper 的 Shuffle 输出数据仍然按原方案写本地磁盘，写完后上传到 HDFS。Reducer 仍然按原始方案通过 Mapper 侧的 External Shuffle Service 读取 Shuffle 数据。如果失败了，则从 HDFS 读取。这种方案极大减少了对 HDFS 的访问频率。该方案上线近一年：覆盖 57% 以上的 Spark Shuffle 数据。使得 Spark 作业整体性能提升 14%。天级大作业性能提升 18%。小时级作业性能提升 12%。该方案旨在提升 Spark Shuffle 稳定性从而提升作业稳定性，但最终没有使用方差等指标来衡量稳定性的提升。原因在于每天集群负载不一样，整体方差较大。Shuffle 稳定性提升后，Stage Retry 大幅减少，整体作业执行时间减少，也即性能提升。最终通过对比使用该方案前后的总的作业执行时间来对比性能的提升，用于衡量该方案的效果。Shuffle性能优化实践与探索如上文所分析，Shuffle 性能问题的原因在于，Shuffle Write 由 Mapper 完成，然后 Reducer 需要从所有 Mapper 处读取数据。这种模型，我们称之为以 Mapper 为中心的 Shuffle。它的问题在于：Mapper 侧会有 M 次顺序写 IO。Mapper 侧会有 M * N * 2 次随机读 IO（这是最大的性能瓶颈）。Mapper 侧的 External Shuffle Service 必须与 Mapper 位于同一台机器，无法做到有效的存储计算分离，Shuffle 服务无法独立扩展。针对上述问题，我们提出了以 Reducer 为中心的，存储计算分离的 Shuffle 方案，如下图所示。该方案的原理是，Mapper 直接将属于不同 Reducer 的数据写到不同的 Shuffle Service。在上图中，总共 2 个 Mapper，5 个 Reducer，5 个 Shuffle Service。所有 Mapper 都将属于 Reducer 0 的数据远程流式发送给 Shuffle Service 0，并由它顺序写入磁盘。Reducer 0 只需要从 Shuffle Service 0 顺序读取所有数据即可，无需再从 M 个 Mapper 取数据。该方案的优势在于：将 M * N * 2 次随机 IO 变为 N 次顺序 IO。Shuffle Service 可以独立于 Mapper 或者 Reducer 部署，从而做到独立扩展，做到存储计算分离。Shuffle Service 可将数据直接存于 HDFS 等高可用存储，因此可同时解决 Shuffle 稳定性问题。我的分享就到这里，谢谢大家。QA集锦- 提问：物化列新增一列，是否需要修改历史数据？回答：历史数据太多，不适合修改历史数据。- 提问：如果用户的请求同时包含新数据和历史数据，如何处理？回答：一般而言，用户修改数据都是以 Partition 为单位。所以我们在 Partition Parameter 上保存了物化列相关信息。如果用户的查询同时包含了新 Partition 与历史 Partition，我们会在新 Partition 上针对物化列进行 SQL Rewrite，历史 Partition 不 Rewrite，然后将新老 Partition 进行 Union，从而在保证数据正确性的前提下尽可能充分利用物化列的优势。- 提问：你好，你们针对用户的场景，做了很多挺有价值的优化。像物化列、物化视图，都需要根据用户的查询 Pattern 进行设置。目前你们是人工分析这些查询，还是有某种机制自动去分析并优化？回答：目前我们主要是通过一些审计信息辅助人工分析。同时我们也正在做物化列与物化视图的推荐服务，最终做到智能建设物化列与物化视图。- 提问：刚刚介绍的基于 HDFS 的 Spark Shuffle 稳定性提升方案，是否可以异步上传 Shuffle 数据至 HDFS？回答：这个想法挺好，我们之前也考虑过，但基于几点考虑，最终没有这样做。第一，单 Mapper 的 Shuffle 输出数据量一般很小，上传到 HDFS 耗时在 2 秒以内，这个时间开销可以忽略；第二，我们广泛使用 External Shuffle Service 和 Dynamic Allocation，Mapper 执行完成后可能 Executor 就回收了，如果要异步上传，就必须依赖其它组件，这会提升复杂度，ROI 较低。最后字节的这篇分享，我真的太喜欢了，所有的优化点，都是拿真实的业务场景进行举例，虽然上文有的技术点在我们的场景中还没必要去做到这种优化程度，但是如此实在的来源于线上的方案，非常容易理解。我们公司目前的数据量要比字节的量小很多，最近在新闻上看到抖音的日活已经达到了4亿，所以我们在数仓中的数据，还没有用到更细粒度的Bucket表，分区表就已经完全可以满足我们的需求。上文中的物化列方案，我觉得很新颖，但是从工程师的角度来讲，在物化列方案中，多维护一张表，添加了复杂度和运营成本，我们在数据的存储中，尽可量的回去避免复杂结构的数据类型，这样会降低存储端和计算端代码的复杂度。这篇文章是针对Spark SQL的优化方面，可以说基本上每个大数据公司都会用到Spark SQL，上述的优化方案肯定会帮助到更多的大数据团队 💪","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Flink 侧输出流SideOutput","slug":"Flink-侧输出流SideOutput","date":"2019-10-20T13:30:11.000Z","updated":"2020-09-06T09:05:01.600Z","comments":true,"path":"2019/10/20/Flink-侧输出流SideOutput/","link":"","permalink":"cpeixin.cn/2019/10/20/Flink-%E4%BE%A7%E8%BE%93%E5%87%BA%E6%B5%81SideOutput/","excerpt":"","text":"在Spark和Flink的流式处理方面，有很多相似之处，例如map()，flatmap()等算子使用方法都是相似的。那这里我举出一个在Spark Streaming中没有的一个功能，侧出流SideOutput。在Flink以前的版本中，是使用split()算子来实现这个功能的，但是目前1.4官方不推荐使用，在编译器中也被标示过时方法，推荐使用sideoutput。具体能实现的功能是将一个流数据，按照你自定义的规则，在流数据内部来分成一个或者多个流，并且同时输出给你。在Spark Streaming中，如果我们想要对一个流数据进行分割，我们需要对一个流数据分别做两次filter算子，这样就会进行数据复制，相对来说耗用资源更高一些。flink中的侧输出就是将数据流进行分割，而不对流进行复制的一种分流机制。flink的侧输出的另一个作用就是对延时迟到的数据进行处理，这样就可以不必丢弃迟到的数据。实例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package data_streamimport java.util.Propertiesimport org.apache.flink.streaming.api.scala._import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.functions.ProcessFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.Collectorimport utils.KafkaUtilobject sideoutput_datastream &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val processStream: DataStream[Raw] = word_stream.process(new SideOutput()) processStream.print(\"original data\") //通过getSideOutput获取侧输出流，并打印输出 processStream.getSideOutput(new OutputTag[String](\"dirty_data\")).print(\"side output data\") env.execute(\"side output test\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125; class SideOutput() extends ProcessFunction[Raw, Raw] &#123; //定义一个侧输出流标签 lazy val dirty_data: OutputTag[String] = new OutputTag[String](\"dirty_data\") override def processElement(value: Raw, ctx: ProcessFunction[Raw, Raw]#Context, out: Collector[Raw]): Unit = &#123; if (value.keywordList == \"dirty_data\") &#123; ctx.output(dirty_data, s\"$&#123;value.date_time&#125;+$&#123;value.keywordList&#125; is dirty data\") &#125; else &#123; out.collect(value) &#125; &#125; &#125;&#125;结果输出","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"数仓-规范","slug":"数仓-规范","date":"2019-10-20T12:42:13.000Z","updated":"2020-08-04T12:44:04.462Z","comments":true,"path":"2019/10/20/数仓-规范/","link":"","permalink":"cpeixin.cn/2019/10/20/%E6%95%B0%E4%BB%93-%E8%A7%84%E8%8C%83/","excerpt":"","text":"模型是整个数仓建设基石，规范是数仓建设的保障。为了避免出现指标重复建设和数据质量差的情况，我们统一按照最详细、可落地的方法进行规范建设。(1) 词根词根是维度和指标管理的基础，划分为普通词根与专有词根，提高词根的易用性和关联性。普通词根：描述事物的最小单元体，如：交易-trade。专有词根：具备约定成俗或行业专属的描述体，如：美元-USD。(2) 表命名规范通用规范表名、字段名采用一个下划线分隔词根（示例：clienttype-&gt;client_type）。每部分使用小写英文单词，属于通用字段的必须满足通用字段信息的定义。表名、字段名需以字母为开头。表名、字段名最长不超过64个英文字符。优先使用词根中已有关键字（数仓标准配置中的词根管理），定期Review新增命名的不合理性。在表名自定义部分禁止采用非标准的缩写。表命名规则表名称 = 类型 + 业务主题 + 子主题 + 表含义 + 存储格式 + 更新频率 +结尾，如下图所示：图6 统一的表命名规范(3) 指标命名规范结合指标的特性以及词根管理规范，将指标进行结构化处理。A. 基础指标词根，即所有指标必须包含以下基础词根：基础指标词根英文全称Hive数据类型MySQL数据类型长度精度词根样例数量countBigintBigint100cnt金额类amoutDecimalDecimal204amt比率/占比ratioDecimalDecimal104ratio0.9818…………………………B.业务修饰词，用于描述业务场景的词汇，例如trade-交易。C.日期修饰词，用于修饰业务发生的时间区间。日期类型全称词根备注日dailyd周weeklyw………………D.聚合修饰词，对结果进行聚集操作。聚合类型全称词根备注平均averageavg周累计wtdwtd本周一截止到当天累计………………E.基础指标，单一的业务修饰词+基础指标词根构建基础指标 ，例如：交易金额-trade_amt。F.派生指标，多修饰词+基础指标词根构建派生指标。派生指标继承基础指标的特性，例如：安装门店数量-install_poi_cnt。G.普通指标命名规范，与字段命名规范一致，由词汇转换即可以。图7 普通指标规范H.日期类型指标命名规范，命名时要遵循：业务修饰词+基础指标词根+日期修饰词/聚合修饰词。将日期后缀加到名称后面，如下图所示：图8 日期类型指标规范I.聚合类型指标，命名时要遵循：业务修饰词+基础指标词根+聚合类型+日期修饰词。将累积标记加到名称后面，如下图所示：图9 聚合类指标规范(4) 清洗规范确认了字段命名和指标命名之后，根据指标与字段的部分特性，我们整理出了整个数仓可预知的24条清洗规范：数据类型数据类别Hive类型MySQL类型长度精度词根格式说明备注日期类型字符日期类stringvarchar10dateYYYY-MM-DD日期清洗为相应的格式数据类型数量类bigintbigint100cnt活跃门店数量…………………………………………区分使用 TINYINT、SMALLINT、INT、BIGINT 数据类型,；建议使用 TINYINT 代替 BOOLEAN, 提高扩展性和类型转换兼容性；尽量使用低存储数据类型以提高运行效率；金额字段统一使用DECIMAL，时间字段(精确到十分秒)字段统一使用TIMESTAMP以提升比较效率， 分区字段及日期字段（没有时分秒）使用 String（格式统一为 yyyyMMdd）。(4) 编码规范1、程序代码：每层一个代码目录，用于存放对应层的模型开发工程。2、HQL代码：(1) 使用 left semi join 代替 in/exists；(2) join 时小表尽量放在左边，如小表足够小可使用 map join，hive 已支持自动判断大小表；(3) 排序尽量使用 distribute+sort 组合，避免全表 order by；(4) 尽量使用静态分区，提升运行效率；例行补数建议使用动态分区简化代码；(5) 慎用笛卡尔积 join，卡历史数据建议使用日期维度表作笛卡尔积，以并行循环操作；(6) 尽量使用窗口函数、udf 简化 sql 逻辑，提升代码可读性；(7) join/group by/distinct 注意处理 NULL 值，尽量避免数据倾斜；(8) union 会去重, 不用去重时使用 union all；(9) 表查询如果是分区表, 尽量加上分区限制。结合模型与规范，形成模型设计及模型评审两者的工作职责，如下图所示：图10 模型设计和审计职责统一应用归口在对原有的应用支持流程进行梳理的时候，我们发现整个研发过程是烟囱式。如果不进行改善就会导致前面的建设”毁于一旦“，所以需对原有应用支持流程进行改造，如下图所示：图11 应用归口从图中可以看出，重构前一个应用存在多次迭代，每次迭代都各自维护自己的文档。烟囱式开发会导致业务信息混乱、应用无法与文档对齐、知识传递成本、维护成本和迭代成本大大增加等问题。重构后，应用与知识库相对应，保证一个应用只对应一份文档，且应用统一要求在一份文档上进行迭代，从根源上改变应用支持流程。同时，针对核心业务细节和所支撑的数据信息，进行了全局调研并归纳到知识库。综合统一的知识库，降低了知识传递、理解、维护和迭代成本。统一归口策略包含业务归口统一、设计归口统一和应用归口统一，从底层保证了数仓建设的三特性和三效果。统一数据出口数仓建设不仅仅是为了数据内容而建设，同时也为了提高交付的数据质量与数据使用的便利性。如何保证数据质量以及推广数据的使用，我们提出了统一数据出口策略。在进行数据资产管理和统一数据出口之前，必须高质量地保障输出的数据质量，从而树立OneData数据服务体系的权威性。1.交付标准化如何保证数据质量，满足什么样的数据才是可交付的，是数据建设者一直探索的问题。为了保证交付的严谨性，在具体化测试方案之前，我们结合数仓的特点明确了数据交付标准的五个特性，如下图所示：图12 交付标准化《交付标准化》完善了整个交付细节，从根本上保证了数据的质量，如：业务测试保障数据的合理性、一致性；技术测试保障数据的唯一性、准确性；数据平台的稳定性和后期人工维护保障数据的及时性。2.数据资产管理针对如何解决数据质量中维度与指标一致性以及如何提高数据易用性的问题，我们提出数据资产的概念，借助公司内部平台工具“起源数据平台”实现了整个数据资产管理，它的功能如下图所示：图13 起源功能体系借用起源数据平台，我们实现了：统一指标管理，保证了指标定义、计算口径、数据来源的一致性。统一维度管理，保证了维度定义、维度值的一致性。统一数据出口，实现了维度和指标元数据信息的唯一出口，维值和指标数据的唯一出口。通过交付标准化和数据资产管理，保证了数据质量与数据的易用性，同时我们也构建出OneData数据体系中数据指标管理的核心。转载自 https://tech.meituan.com/2019/10/17/meituan-saas-data-warehouse.html真心感觉美团技术团队🐂🍺","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"Spark Shuffle内幕","slug":"Spark-Shuffle内幕","date":"2019-10-19T14:22:08.000Z","updated":"2020-09-03T14:25:32.212Z","comments":true,"path":"2019/10/19/Spark-Shuffle内幕/","link":"","permalink":"cpeixin.cn/2019/10/19/Spark-Shuffle%E5%86%85%E5%B9%95/","excerpt":"","text":"Shuffle 一般被翻译为数据混洗，是类 MapReduce 分布式计算框架独有的机制，也是这类分布式计算框架最重要的执行机制。本文主要从两个层面讲解 Shuffle，主要分为：逻辑层面；物理层面。逻辑层面主要从 RDD 的血统机制出发，从 DAG 的角度来讲解 Shuffle，另外也会讲解 Spark 容错机制，而物理层面是从执行角度来剖析 Shuffle 是如何发生的。RDD 血统与 Spark 容错在 DAG 中，最初的 RDD 被称为基础 RDD，后续生成的 RDD 都是由算子以及依赖关系生成的，也就是说，无论哪个 RDD 出现问题，都可以由这种依赖关系重新计算而成。这种依赖关系被称为 RDD 血统（lineage）。血统的表现形式主要分为宽依赖（wide dependency）与窄依赖（narrow dependency），如下图所示：窄依赖的准确定义是：子 RDD 中的分区与父 RDD 中的分区只存在一对一的映射关系，而宽依赖则是子 RDD 中的分区与父 RDD 中的分区存在一对多的映射关系，那么从这个角度来说，map、 filter、 union 等就是窄依赖，而 groupByKey、 coGroup 就是典型的宽依赖，如下图所示：宽依赖还有个名字，叫 Shuffle 依赖，也就是说宽依赖必然会发生 Shuffle 操作，在前面也提到过 Shuffle 也是划分 Stage 的依据。而窄依赖由于不需要发生 Shuffle，所有计算都是在分区所在节点完成，它类似于 MapReduce 中的 ChainMapper。所以说，在你自己的 DAG 中，如果你选取的算子形成了宽依赖，那么就一定会触发 Shuffle。当 RDD 中的某个分区出现故障，那么只需要按照这种依赖关系重新计算即可，窄依赖最简单，只涉及某个节点内的计算，而宽依赖，则会按照依赖关系由父分区计算而得到，如下图所示：如果 P1_0 分区发生故障，那么按照依赖关系，则需要 P0_0 与 P0_1 的分区重算，如果 P0_0与 P0_1 没有持久化，就会不断回溯，直到找到存在的父分区为止。当计算逻辑复杂时，就会引起依赖链过长，这样重算的代价会极其高昂，所以用户可以在计算过程中，适时调用 RDD 的 checkpoint 方法，保存当前算好的中间结果，这样依赖链就会大大缩短。RDD 的血统机制就是 RDD 的容错机制。Spark 的容错主要分为资源管理平台的容错和 Spark 应用的容错， Spark 应用是基于资源管理平台运行，所以资源管理平台的容错也是 Spark 容错的一部分，如 YARN 的 ResourceManager HA 机制。在 Spark 应用执行的过程中，可能会遇到以下几种失败的情况：Driver 报错；Executor 报错；Task 执行失败。Driver 执行失败是 Spark 应用最严重的一种情况，标志整个作业彻底执行失败，需要开发人员手动重启 Driver；Executor 报错通常是因为 Executor 所在的机器故障导致，这时 Driver 会将执行失败的 Task 调度到另一个 Executor 继续执行，重新执行的 Task 会根据 RDD 的依赖关系继续计算，并将报错的 Executor 从可用 Executor 的列表中去掉；Spark 会对执行失败的 Task 进行重试，重试 3 次后若仍然失败会导致整个作业失败。在这个过程中，Task 的数据恢复和重新执行都用到了 RDD 的血统机制。Spark Shuffle很多算子都会引起 RDD 中的数据进行重分区，新的分区被创建，旧的分区被合并或者被打碎，在重分区的过程中，如果数据发生了跨节点移动，就被称为 Shuffle，在 Spark 中， Shuffle 负责将 Map 端（这里的 Map 端可以理解为宽依赖的左侧）的处理的中间结果传输到 Reduce 端供 Reduce 端聚合（这里的 Reduce 端可以理解为宽依赖的右侧），它是 MapReduce 类型计算框架中最重要的概念，同时也是很消耗性能的步骤。Shuffle 体现了从函数式编程接口到分布式计算框架的实现。与 MapReduce 的 Sort-based Shuffle 不同，Spark 对 Shuffle 的实现方式有两种：Hash Shuffle 与 Sort-based Shuffle，这其实是一个优化的过程。在较老的版本中，Spark Shuffle 的方式可以通过 spark.shuffle.manager 配置项进行配置，而在最新的 Spark 版本中，已经去掉了该配置，统一称为 Sort-based Shuffle。Hash Shuffle在 Spark 1.6.3 之前， Hash Shuffle 都是 Spark Shuffle 的解决方案之一。 Shuffle 的过程一般分为两个部分：Shuffle Write 和 Shuffle Fetch，前者是 Map 任务划分分区、输出中间结果，而后者则是 Reduce 任务获取到的这些中间结果。Hash Shuffle 的过程如下图所示：在图中，Shuffle Write 发生在一个节点上，该节点用来执行 Shuffle 任务的 CPU 核数为 2，每个核可以同时执行两个任务，每个任务输出的分区数与 Reducer（这里的 Reducer 指的是 Reduce 端的 Executor）数相同，即为 3，每个分区都有一个缓冲区（bucket）用来接收结果，每个缓冲区的大小由配置 spark.shuffle.file.buffer.kb 决定。这样每个缓冲区写满后，就会输出到一个文件段（filesegment），而 Reducer 就会去相应的节点拉取文件。这样的实现很简单，但是问题也很明显。主要有两个：生成的中间结果文件数太大。理论上，每个 Shuffle 任务输出会产生 R 个文件（ R为Reducer 的个数），而 Shuffle 任务的个数往往由 Map 任务个数 M 决定，所以总共会生成 M * R 个中间结果文件，而往往在一个作业中 M 和 R 都是很大的数字，在大型作业中，经常会出现文件句柄数突破操作系统限制。缓冲区占用内存空间过大。单节点在执行 Shuffle 任务时缓存区大小消耗为 m * R * spark.shuffle.file.buffer.kb，m 为该节点运行的 Shuffle 任务数，如果一个核可以执行一个任务，m 就与 CPU 核数相等。这对于动辄有 32、64 物理核的服务器来说，是比不小的内存开销。为了解决第一个问题， Spark 推出过 File Consolidation 机制，旨在通过共用输出文件以降低文件数，如下图所示：每当 Shuffle 任务输出时，同一个 CPU 核心处理的 Map 任务的中间结果会输出到同分区的一个文件中，然后 Reducer 只需一次性将整个文件拿到即可。这样，Shuffle 产生的文件数为 C（CPU 核数）* R。 Spark 的 FileConsolidation 机制默认开启，可以通过 spark.shuffle.consolidateFiles 配置项进行配置。Sort-based Shuffle在 Spark 先后引入了 Hash Shuffle 与 FileConsolidation 后，还是无法根本解决中间文件数太大的问题，所以 Spark 在 1.2 之后又推出了与 MapReduce 一样（你可以参照《Hadoop 海量数据处理》（第 2 版）的 Shuffle 相关章节）的 Shuffle 机制： Sort-based Shuffle，才真正解决了 Shuffle 的问题，再加上 Tungsten 计划的优化， Spark 的 Sort-based Shuffle 比 MapReduce 的 Sort-based Shuffle 青出于蓝。如下图所示：每个 Map 任务会最后只会输出两个文件（一个数据文件，一个是索引文件），其中间过程采用的是与 MapReduce 一样的归并排序，但是会用索引文件记录每个分区的偏移量，输出完成后，Reducer 会根据索引文件得到属于自己的分区，在这种情况下，Shuffle 产生的中间结果文件数为 2 * M（M 为 Map 任务数）。在基于排序的 Shuffle 中， Spark 还提供了一种折中方案——Bypass Sort-based Shuffle，当 Reduce 任务小于 spark.shuffle.sort.bypassMergeThreshold 配置（默认 200）时，Spark Shuffle 开始按照 Hash Shuffle 的方式处理数据，而不用进行归并排序，只是在 Shuffle Write 步骤的最后，将其合并为 1 个文件，并生成索引文件。这样实际上还是会生成大量的中间文件，只是最后合并为 1 个文件并省去排序所带来的开销，该方案的准确说法是 Hash Shuffle 的Shuffle Fetch 优化版。Spark 在1.5 版本时开始了 Tungsten 计划，也在 1.5.0、 1.5.1、 1.5.2 的时候推出了一种 tungsten-sort 的选项，这是一种成果应用，类似于一种实验，该类型 Shuffle 本质上还是给予排序的 Shuffle，只是用 UnsafeShuffleWriter 进行 Map 任务输出，并采用了要在后面介绍的 BytesToBytesMap 相似的数据结构，把对数据的排序转化为对指针数组的排序，能够基于二进制数据进行操作，对 GC 有了很大提升。但是该方案对数据量有一些限制，随着 Tungsten 计划的逐渐成熟，该方案在 1.6 就消失不见了。从上面整个过程的变化来看， Spark Shuffle 也是经过了一段时间才趋于成熟和稳定，这也正像学习的过程，不用一蹴而就，贵在坚持。常见面试题用 Spark 算子实现对 1TB 的数据进行排序？关于这道题的解法，你可能很自然地想到了归并排序的原理，首先每个分区对自己分区进行排序，最后汇总到一个分区内进行全排序，如下图所示：可想而知，最后 1TB 的数据都会汇总到 1 个 Executor，就算这个 Executor 分配到的资源再充足，面对这种情况，无疑也是以失败告终。所以这道题的解法应该是另一种方案，首先数据会按照键的区间进行分发，也就是 Shuffle，如 [0，100000]、 [100000，200000）和 [200000，300000]，每个分区没有交集。照此规则分发后，分区内再进行排序，就可以在满足性能要求的前提下完成全排序，如下图：这种方式的全排序无疑实现了计算的并行化，很多测试性能的场景也用这种方式对 1TB 的数据进行排序，目前世界纪录是腾讯在 2016 年达到的 98.8 秒。对于这种排序方式，Spark 也将其封装为 sortByKey 算子，它采用的分区器则是 RangePartitioner。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"HBase高性能点","slug":"HBase高性能点","date":"2019-10-06T13:54:42.000Z","updated":"2020-10-06T13:55:40.664Z","comments":true,"path":"2019/10/06/HBase高性能点/","link":"","permalink":"cpeixin.cn/2019/10/06/HBase%E9%AB%98%E6%80%A7%E8%83%BD%E7%82%B9/","excerpt":"","text":"在计算机数据存储领域，一直是关系数据库（RDBMS）的天下，以至于在传统企业的应用领域，许多应用系统设计都是面向数据库设计，也就是先设计数据库然后设计程序，从而导致关系模型绑架对象模型，并由此引申出旷日持久的业务对象贫血模型与充血模型之争。业界为了解决关系数据库的不足，提出了诸多方案，比较有名的是对象数据库，但是这些数据库的出现似乎只是进一步证明关系数据库的优越而已。直到人们遇到了关系数据库难以克服的缺陷——糟糕的海量数据处理能力及僵硬的设计约束，局面才有所改善。从 Google 的 BigTable 开始，一系列的可以进行海量数据存储与访问的数据库被设计出来，更进一步说，NoSQL 这一概念被提了出来。NoSQL，主要指非关系的、分布式的、支持海量数据存储的数据库设计模式。也有许多专家将 NoSQL 解读为 Not Only SQL，表示 NoSQL 只是关系数据库的补充，而不是替代方案。其中，HBase 是这一类 NoSQL 系统的杰出代表。HBase 之所以能够具有海量数据处理能力，其根本在于和传统关系型数据库设计的不同思路。传统关系型数据库对存储在其上的数据有很多约束，学习关系数据库都要学习数据库设计范式，事实上，是在数据存储中包含了一部分业务逻辑。而 NoSQL 数据库则简单暴力地认为，数据库就是存储数据的，业务逻辑应该由应用程序去处理，有时候不得不说，简单暴力也是一种美。HBase 可伸缩架构我们先来看看 HBase 的架构设计。HBase 为可伸缩海量数据储存而设计，实现面向在线业务的实时数据访问延迟。HBase 的伸缩性主要依赖其可分裂的 HRegion 及可伸缩的分布式文件系统 HDFS 实现。HRegion 是 HBase 负责数据存储的主要进程，应用程序对数据的读写操作都是通过和 HRegion 通信完成。上面是 HBase 架构图，我们可以看到在 HBase 中，数据以 HRegion 为单位进行管理，也就是说应用程序如果想要访问一个数据，必须先找到 HRegion，然后将数据读写操作提交给 HRegion，由 HRegion 完成存储层面的数据操作。HRegionServer 是物理服务器，每个 HRegionServer 上可以启动多个 HRegion 实例。当一个 HRegion 中写入的数据太多，达到配置的阈值时，一个 HRegion 会分裂成两个 HRegion，并将 HRegion 在整个集群中进行迁移，以使 HRegionServer 的负载均衡。每个 HRegion 中存储一段 Key 值区间[key1, key2) 的数据，所有 HRegion 的信息，包括存储的 Key 值区间、所在 HRegionServer 地址、访问端口号等，都记录在 HMaster 服务器上。为了保证 HMaster 的高可用，HBase 会启动多个 HMaster，并通过 ZooKeeper 选举出一个主服务器。下面是一张调用时序图，应用程序通过 ZooKeeper 获得主 HMaster 的地址，输入 Key 值获得这个 Key 所在的 HRegionServer 地址，然后请求 HRegionServer 上的 HRegion，获得所需要的数据。数据写入过程也是一样，需要先得到 HRegion 才能继续操作。HRegion 会把数据存储在若干个 HFile 格式的文件中，这些文件使用 HDFS 分布式文件系统存储，在整个集群内分布并高可用。当一个 HRegion 中数据量太多时，这个 HRegion 连同 HFile 会分裂成两个 HRegion，并根据集群中服务器负载进行迁移。如果集群中有新加入的服务器，也就是说有了新的 HRegionServer，由于其负载较低，也会把 HRegion 迁移过去并记录到 HMaster，从而实现 HBase 的线性伸缩。HBase 的核心设计目标是解决海量数据的分布式存储，和 Memcached 这类分布式缓存的路由算法不同，HBase 的做法是按 Key 的区域进行分片，这个分片也就是 HRegion。应用程序通过 HMaster 查找分片，得到 HRegion 所在的服务器 HRegionServer，然后和该服务器通信，就得到了需要访问的数据。HBase 可扩展数据模型传统的关系数据库为了保证关系运算（通过 SQL 语句）的正确性，在设计数据库表结构的时候，需要指定表的 schema 也就是字段名称、数据类型等，并要遵循特定的设计范式。这些规范带来了一个问题，就是僵硬的数据结构难以面对需求变更带来的挑战，有些应用系统设计者通过预先设计一些冗余字段来应对，但显然这种设计也很糟糕。那有没有办法能够做到可扩展的数据结构设计呢？不用修改表结构就可以新增字段呢？当然有的，许多 NoSQL 数据库使用的列族（ColumnFamily）设计就是其中一个解决方案。列族最早在 Google 的 BigTable 中使用，这是一种面向列族的稀疏矩阵存储格式，如下图所示。这是一个学生的基本信息表，表中不同学生的联系方式各不相同，选修的课程也不同，而且将来还会有更多联系方式和课程加入到这张表里，如果按照传统的关系数据库设计，无论提前预设多少冗余字段都会捉襟见肘、疲于应付。而使用支持列族结构的 NoSQL 数据库，在创建表的时候，只需要指定列族的名字，无需指定字段（Column）。那什么时候指定字段呢？可以在数据写入时再指定。通过这种方式，数据表可以包含数百万的字段，这样就可以随意扩展应用程序的数据结构了。并且这种数据库在查询时也很方便，可以通过指定任意字段名称和值进行查询。HBase 这种列族的数据结构设计，实际上是把字段的名称和字段的值，以 Key-Value 的方式一起存储在 HBase 中。实际写入的时候，可以随意指定字段名称，即使有几百万个字段也能轻松应对。而使用支持列族结构的 NoSQL 数据库，在创建表的时候，只需要指定列族的名字，无需指定字段（Column）。那什么时候指定字段呢？可以在数据写入时再指定。通过这种方式，数据表可以包含数百万的字段，这样就可以随意扩展应用程序的数据结构了。并且这种数据库在查询时也很方便，可以通过指定任意字段名称和值进行查询。HBase 这种列族的数据结构设计，实际上是把字段的名称和字段的值，以 Key-Value 的方式一起存储在 HBase 中。实际写入的时候，可以随意指定字段名称，即使有几百万个字段也能轻松应对。HBase 的高性能存储传统的机械式磁盘的访问特性是连续读写很快，随机读写很慢。这是因为机械磁盘靠电机驱动访问磁盘上的数据，电机要将磁头落到数据所在的磁道上，这个过程需要较长的寻址时间。如果数据不连续存储，磁头就要不停地移动，浪费了大量的时间。为了提高数据写入速度，HBase 使用了一种叫作 LSM 树的数据结构进行数据存储。LSM 树的全名是 Log Structed Merge Tree，翻译过来就是 Log 结构合并树。数据写入的时候以 Log 方式连续写入，然后异步对磁盘上的多个 LSM 树进行合并。LSM 树可以看作是一个 N 阶合并树。数据写操作（包括插入、修改、删除）都在内存中进行，并且都会创建一个新记录（修改会记录新的数据值，而删除会记录一个删除标志）。这些数据在内存中仍然还是一棵排序树，当数据量超过设定的内存阈值后，会将这棵排序树和磁盘上最新的排序树合并。当这棵排序树的数据量也超过设定阈值后，会和磁盘上下一级的排序树合并。合并过程中，会用最新更新的数据覆盖旧的数据（或者记录为不同版本）。在需要进行读操作时，总是从内存中的排序树开始搜索，如果没有找到，就从磁盘 上的排序树顺序查找。在 LSM 树上进行一次数据更新不需要磁盘访问，在内存即可完成。当数据访问以写操作为主，而读操作则集中在最近写入的数据上时，使用 LSM 树可以极大程度地减少磁盘的访问次数，加快访问速度。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"}]},{"title":"Hive 窗口函数","slug":"Hive-窗口函数","date":"2019-10-05T00:22:52.000Z","updated":"2020-08-26T07:58:15.731Z","comments":true,"path":"2019/10/05/Hive-窗口函数/","link":"","permalink":"cpeixin.cn/2019/10/05/Hive-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/","excerpt":"","text":"简介本文主要介绍hive中的窗口函数，hive中的窗口函数和sql中的窗口函数相类似,都是用来做一些数据分析类的工作,一般用于OLAP分析（在线分析处理）。概念窗口函数是用于分析用的一类函数，要理解窗口函数要先从聚合函数说起，我们都知道在sql中有一类函数叫做聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的.但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数。窗口函数可以在本行内做运算，得到多行的结果，即每一行对应一行的值。 通用的窗口函数可以用下面的语法来概括：1Function() Over (Partition By Column1，Column2，Order By Column3)窗口函数又分为以下三类：聚合型窗口函数分析型窗口函数取值型窗口函数over()子句我们可以形象的把over()子句理解成开窗子句，即打开一个窗口，窗口内包含多条记录，over()会给每一行开一个窗口。如下图，总共有5条记录，每一行代表一条记录，over()在每一条记录的基础上打开一个窗口，给r1记录打开w1窗口，窗口内只包含自己，给r2打开w2窗口，窗口内包含r1、r2，给r3打开w3窗口，窗口内包含r1、r2、r3，以此类推…..over()子句的开窗范围先看一张图：current row代表查询的当前行，1 preceding代表前一行，1 following代表后一行，unbounded preceding代表第一行，unbounded following代表最后一行。（注意这里的第一行和最后一行并不是严格的第一行和最后一行，根据具体情况而定）由上我们不难发现，在使用over()子句进行查询的时候， 不仅可以查询到每条记录的信息，还可以查询到这条记录对应窗口内的所有记录的聚合信息，所以我们通常结合聚合函数和over()子句一起使用。那么over()是如何进行开窗的呢？即每条记录对应的窗口内应该包含哪些记录呢？这些都是在over()子句的括号内进行定义。order by如果over()子句中接order by，例如：over(order by date)，则默认的开窗范围为根据date排序后的rows between unbounded preceding and current row，即第一行到当前行，意思是over(order by date)和over(order by date rows rows between unbounded preceding and current row)的效果是一样的。partition by如果over子句中接partition by（和group by类似，都是根据列值对行进行分组），例如over(partition by month(date))，则每一行的默认的开窗范围为当前行所在分组的所有记录。注意partition by子句不能单独和window clause子句一起使用，必须结合order by子句，下面会讨论。partition by + order by先分组，再排序，即组内排序。同样的，如果 order by后不接window clause，则每一行的默认的开窗范围为：当前行所在分组的第一行到当前行，即over(partition by (month(date)) order by orderdate)和over(partition by (month(date)) order by orderdate rows between undounded preceding and current row)是一样的。窗口大小over()子句的开窗范围可以通过window 子句（window clause）在over()的括号中定义，window clause的规范如下：123456789101112(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWINGpartition by …order by…rows between unbounded preceding and current row窗口大小为从起始行得到当前行。partition by …order by… rows between 3 preceding and current row窗口大小为从当前行到之前三行partition by …order by… rows between 3 preceding and 1 following窗口大小为当前行的前三行到之后的一行partition by …order by… rows between 3 preceding and unbounded following窗口大小为当前行的前三行到之后的所有行例如 select ,sum(column_name) over( rows between unbounded preceding and unbounded following) from table_name 表示查询每一行的所有列值，同时给每一行打开一个从第一行到最后一行的窗口，并统计窗口内所有记录的column_name列值的和。最后给每一行输出该行的所有属性以及该行对应窗口内记录的聚合值。如果over()子句中什么都不写的话，默认开窗范围是：rows between unbounded preceding and unbounded following*在深入研究Over子句之前，一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于Order by字句之前。实践准备创建Hive表12345CREATE TABLE user_match_temp (user_name string,opponent string,result int,create_time timestamp);数据量较少，就直接手动插入了123456789INSERT INTO TABLE user_match_temp values(&#39;A&#39;,&#39;B&#39;,1,&#39;2019-07-18 23:19:00&#39;),(&#39;B&#39;,&#39;A&#39;,0,&#39;2019-07-18 23:19:00&#39;),(&#39;A&#39;,&#39;C&#39;,0,&#39;2019-07-18 23:20:00&#39;),(&#39;C&#39;,&#39;A&#39;,1,&#39;2019-07-18 23:20:00&#39;),(&#39;A&#39;,&#39;D&#39;,1,&#39;2019-07-19 22:19:00&#39;),(&#39;D&#39;,&#39;A&#39;,0,&#39;2019-07-19 22:19:00&#39;),(&#39;C&#39;,&#39;B&#39;,0,&#39;2019-07-19 23:19:00&#39;),(&#39;B&#39;,&#39;C&#39;,1,&#39;2019-07-19 23:19:00&#39;);数据包含4列，分别为 user_name，opponent，result，create_time。 我们将基于这些数据来介绍下窗口函数的一些使用场景。原始数据**user_name opponent result create_timeA B 1 2019-07-18 23:19:00B A 0 2019-07-18 23:19:00A C 0 2019-07-18 23:20:00C A 1 2019-07-18 23:20:00A D 1 2019-07-19 22:19:00D A 0 2019-07-19 22:19:00C B 0 2019-07-19 23:19:00B C 1 2019-07-19 23:19:00聚合型窗口函数聚合型即SUM(), MIN(),MAX(),AVG(),COUNT()这些常见的聚合函数。 聚合函数配合窗口函数使用可以使计算更加灵活，例如以下场景：至今累计分数1234567891011select *, sum(result) over(partition by user_name order by create_time) result_sum from user_match_temp;user_name opponent result create_time result_sumA B 1 2019-07-18 23:19:00 1A C 0 2019-07-18 23:20:00 1A D 1 2019-07-19 22:19:00 2B A 0 2019-07-18 23:19:00 0B C 1 2019-07-19 23:19:00 1C A 1 2019-07-18 23:20:00 1C B 0 2019-07-19 23:19:00 1D A 0 2019-07-19 22:19:00 0之前3场平均胜场1234567891011hive (test)&gt; SELECT *,avg(result) over (partition by user_name order by create_time rows between 3 preceding and current row) as recently_wins from user_match_temp;user_match_temp.user_name user_match_temp.opponent user_match_temp.result user_match_temp.create_time recently_winsA B 1 2019-07-18 23:19:00 1.0A C 0 2019-07-18 23:20:00 0.5A D 1 2019-07-19 22:19:00 0.6666666666666666B A 0 2019-07-18 23:19:00 0.0B C 1 2019-07-19 23:19:00 0.5C A 1 2019-07-18 23:20:00 1.0C B 0 2019-07-19 23:19:00 0.5D A 0 2019-07-19 22:19:00 0.0我们通过rows between 即可定义窗口的范围，这里我们定义了窗口的范围为之前3行到该行。累计遇到的对手数量需要注意的是count(distinct xxx)在窗口函数里是不允许使用的，不过我们也可以用size(collect_set() over(partition by order by))来替代实现我们的需求1234567891011hive (test)&gt; SELECT *,size(collect_set(opponent) over (partition by user_name order by create_time)) as recently_wins from user_match_temp;user_match_temp.user_name user_match_temp.opponent user_match_temp.result user_match_temp.create_time recently_winsA B 1 2019-07-18 23:19:00 1A C 0 2019-07-18 23:20:00 2A D 1 2019-07-19 22:19:00 3B A 0 2019-07-18 23:19:00 1B C 1 2019-07-19 23:19:00 2C A 1 2019-07-18 23:20:00 1C B 0 2019-07-19 23:19:00 2D A 0 2019-07-19 22:19:00 1collect_set()也是一个聚合函数，作用是将多行聚合进一行的某个set内，再用size()统计集合内的元素个数，即可实现我们的需求。分析型窗口函数分析型即RANk(),ROW_NUMBER(),DENSE_RANK()等常见的排序用的窗口函数，不过他们也是有区别的。排名函数不支持window子句，即不支持自定义窗口大小**1234567891011hive (test)&gt; SELECT *,rank() over (order by create_time) as user_rank,row_number() over (order by create_time) as user_row_number,dense_rank() over (order by create_time) as user_dense_rank FROM user_match_temp;user_name opponent result create_time user_rank user_row_number user_dense_rankB A 0 2019-07-18 23:19:00 1 1 1A B 1 2019-07-18 23:19:00 1 2 1C A 1 2019-07-18 23:20:00 3 3 2A C 0 2019-07-18 23:20:00 3 4 2D A 0 2019-07-19 22:19:00 5 5 3A D 1 2019-07-19 22:19:00 5 6 3B C 1 2019-07-19 23:19:00 7 7 4C B 0 2019-07-19 23:19:00 7 8 4如上所示： row_number函数：生成连续的序号（相同元素序号相同）；rank函数：如两元素排序相同则序号相同，并且会跳过下一个序号；dense_rank函数：如两元素排序相同则序号相同，不会跳过下一个序号；除了这三个排序用的函数，还有 _CUME_DIST函数 ：小于等于当前值的行在所有行中的占比 _PERCENT_RANK() ：小于当前值的行在所有行中的占比 * NTILE() ：如果把数据按行数分为n份，那么该行所属的份数是第几份 这三种窗口函数 sql如下：1SELECT *, CUME_DIST() over (order by create_time) as user_CUME_DIST, PERCENT_RANK() over (order by create_time) as user_PERCENT_RANK, NTILE(3) over (order by create_time) as user_NTILE FROM user_match_temp; ## 取值型窗口函数 这几个函数可以通过字面意思记得，LAG是迟滞的意思，也就是对某一列进行往后错行；LEAD是LAG的反义词，也就是对某一列进行提前几行；FIRST_VALUE是对该列到目前为止的首个值，而LAST_VALUE是到目前行为止的最后一个值。LAG()和LEAD() 可以带3个参数，第一个是返回的值，第二个是前置或者后置的行数，第三个是默认值。下一个对手，上一个对手，最近3局的第一个对手及最后一个对手，如下：12345678hive&gt; SELECT *,hive&gt; lag(opponent,1) hive&gt; over (partition by user_name order by create_time) as lag_opponent,hive&gt; lead(opponent,1) over hive&gt; (partition by user_name order by create_time) as lead_opponent,hive&gt; first_value(opponent) over (partition by user_name order by create_time rows hive&gt; between 3 preceding and 3 following) as first_opponent,hive&gt; last_value(opponent) over (partition by user_name order by create_time rows hive&gt; between 3 preceding and 3 following) as last_opponenthive&gt; From user_match_temp;参考文章：[https://blog.csdn.net/czr11616/article/details/101645693](https://blog.csdn.net/czr11616/article/details/101645693)[https://zhuanlan.zhihu.com/p/77705681](https://zhuanlan.zhihu.com/p/77705681)[https://blog.csdn.net/qq_37296285/article/details/90940591](https://blog.csdn.net/qq_37296285/article/details/90940591)","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"Flink window窗口操作","slug":"Flink-window窗口操作","date":"2019-09-17T13:10:54.000Z","updated":"2020-09-06T09:04:37.715Z","comments":true,"path":"2019/09/17/Flink-window窗口操作/","link":"","permalink":"cpeixin.cn/2019/09/17/Flink-window%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C/","excerpt":"","text":"前言Flink中的window窗口概念和Spark Streaming中的window是一样的。对一直流动的数据划定一个固定的窗口。以交通传感器为例，该传感器每15秒统计通过某个位置的车辆数量。结果流看起来像：如果您想知道有多少辆车经过该位置，您只需将各个计数相加即可。但是，传感器流的本质是它连续产生数据。这样的流永远不会结束，并且不可能计算可以返回的最终和。相反，可以计算滚动总和，即为每个输入事件返回更新的总和记录。这将产生新的部分和流。但是，部分求和流可能不是我们想要的，因为它会不断更新计数，更重要的是，某些信息（例如随时间变化）会丢失。因此，我们可能想改一下我们的问题，并询问每分钟通过该位置的汽车数量。这要求我们将流的元素分组为有限的集合，每个集合对应于60秒。此操作称为_滚动Windows_操作。（不重叠）滚动窗口将流离散化为不重叠的窗口。对于某些应用程序，重要的是窗口不可分离，因为应用程序可能需要平滑的聚合。例如，我们可以每30秒计算最后一分钟通过的汽车数量。这种窗户称为_滑动窗户_。（重叠）如前所述，在数据流上定义窗口是非并行操作。这是因为流的每个元素必须由决定该元素应添加到哪个窗口的同一窗口运算符处理。完整流中的Windows 在Flink 中称为_AllWindows_。对于许多应用程序，数据流需要分组为多个逻辑流，每个逻辑流都可以应用窗口运算符。例如，考虑来自多个交通传感器（而不是像前面的示例中的一个传感器）的车辆计数流，其中每个传感器监视一个不同的位置。通过按传感器ID对流进行分组，我们可以并行计算每个位置的窗口流量统计信息。在Flink中，我们称此类分区窗口为_Windows_，因为它们是分布式流的常见情况。下图显示了翻转窗口，该窗口收集了(sensorId, count)成对元素流中的两个元素。一般来说，窗口在无界流上定义了一组有限的元素。该集合可以基于时间（如我们前面的示例中所示），元素计数，计数和时间的组合，或一些将元素分配给窗口的自定义逻辑。Flink的DataStream API为最常见的窗口操作提供了简洁的运算符，并提供了一种通用的窗口机制，该机制允许用户定义非常自定义的窗口逻辑。在下面的内容中，我们将介绍Flink的时间和计数窗口，然后再详细讨论其窗口机制。WindowWindows是处理无限流的核心。Windows将流分成有限大小的“buckets”，我们可以在其上应用计算。Flink window窗口程序的一般结构如下所示。第一段指的是KeyedStream[T]流，第二段指的是non-keyed**DataStream[T]流。以下两段代码对比，唯一的区别是数据流stream经过keyBy(…)后，需要调用window(…)来创建窗口，而non-keyed stream则需要调用windowAll(…)创建窗口。Keyed window**123456789stream .keyBy(...) &lt;- keyed versus non-keyed windows .window(...) &lt;- required: \"assigner\" [.trigger(...)] &lt;- optional: \"trigger\" (else default trigger) [.evictor(...)] &lt;- optional: \"evictor\" (else no evictor) [.allowedLateness(...)] &lt;- optional: \"lateness\" (else zero) [.sideOutputLateData(...)] &lt;- optional: \"output tag\" (else no side output for late data) .reduce/aggregate/fold/apply() &lt;- required: \"function\" [.getSideOutput(...)] &lt;- optional: \"output tag\"Non-Keyed Windows**12345678stream .windowAll(...) &lt;- required: \"assigner\" [.trigger(...)] &lt;- optional: \"trigger\" (else default trigger) [.evictor(...)] &lt;- optional: \"evictor\" (else no evictor) [.allowedLateness(...)] &lt;- optional: \"lateness\" (else zero) [.sideOutputLateData(...)] &lt;- optional: \"output tag\" (else no side output for late data) .reduce/aggregate/fold/apply() &lt;- required: \"function\" [.getSideOutput(...)] &lt;- optional: \"output tag\"在上面，方括号（[…]）中的命令是可选的。这表明Flink允许您以多种不同方式自定义窗口逻辑，从而使其最适合您的需求。window生命周期简而言之，一旦应属于该窗口的第一个元素到达，就会创建一个窗口，并且当时间（event or processing time）超过其结束时间戳加上用户指定的延迟时间，该窗口将被完全删除。Flink保证只删除基于时间（time-based windows）的窗口，而不保证其他类型（_例如 _global windows）的删除。例如，采用基于event time的窗口化策略，该策略每5分钟创建一次不重叠的滚动窗口，并允许延迟1分钟，因此Flink将会创建一个时间在12:00～12:05之间的串口，当带有时间戳的第一个元素落入此间隔时，当水印通过12:06 时间戳时，它将删除它。此外，每个窗口将具有Trigger触发器 和一个函数（ProcessWindowFunction，ReduceFunction， AggregateFunction或FoldFunction等窗口功能。该函数将包含要应用于窗口内容的计算，而则Trigger指定了在什么条件下可以将窗口视为要应用该函数的条件。触发策略可能类似于“当窗口中的元素数大于4时”或“当watermark水印通过窗口末尾时”。触发器还可以决定在创建和删除窗口之间的任何时间清除窗口的内容。在这种情况下，清除仅是指窗口中的元素，而不是窗口元数据。这意味着仍可以将新数据添加到该窗口。除上述内容外，您还可以指定一个Evictor（请参阅Evictors），它将在触发触发器后以及应用此功能之前和/或之后从窗口中删除元素。在下文中，我们将对上述每个组件进行更详细的介绍。我们先从上面的代码片段中的必需部分开始（请参见Keyed vs Non- Keyed Windows，Window Assigner和 Window Function）。Keyed vs Non-Keyed Windows在我们接入流数据后，第一件事情就是我们要在定义window函数前，需要决定你的数据流是否需要进行 keyed。如果我们使用了keyBy()，那么输入的数据流将会被切分成logical keyed streams，否则，将不会对数据流进行切分，在使用 keyed streams的情况下，传入事件的任何属性都可以用作key。拥有keyed streams将使您的窗口化计算可以由多个任务并行执行，因为logical keyed streams都可以独立于其余logical keyed streams进行处理。引用同一键的所有元素将被发送到同一并行任务task中。对于non-keyed streams，您的原始流将不会拆分为多个逻辑流，并且所有窗口逻辑将由单个任务执行，即并行度为1。Window Assigners窗口分配器在决定是否要对数据流keyed后，下一步则是定义window assigners窗口分配器，window assigners定义了数据流元素怎样通过窗口，这完全取决于你的window assigners的选择，window()还是windowAll()WindowAssigner负责将每个传入元素分配给一个或多个窗口。Flink带有针对最常见用例的预定义窗口分配器，即tumbling windows,滚动窗口， sliding windows滑动窗口，session windows会话窗口和global windows全局窗口 。您还可以通过扩展WindowAssigner类来实现自定义窗口分配器。所有内置窗口分配器（global windows**全局窗口除外**）均基于时间将元素分配给窗口，时间可以是process time，也可以是event time。process time和event time之间的差异以及timestamps和watermarks的生成方式稍后再讲。基于时间的窗口具有开始时间戳（包括端点）和结束时间戳（包括端点），它们共同描述了窗口的大小。在代码中，Flink在使用TimeWindow基于时间的窗口时使用，该方法具有查询开始和结束时间戳记的方法maxTimestamp()，还具有返回给定窗口允许的最大时间戳的附加方法。在下面，我们展示Flink的预定义窗口分配器如何工作以及如何在DataStream程序中使用它们。下图显示了每个分配器的工作情况。紫色圆圈表示流的元素，这些元素由某个键（在这种情况下为用户1，用户2和用户3）划分。x轴显示时间进度。Tumbling Windows滚动窗口滚动_窗口_分配器分配每个元素到指定_窗口大小_的窗口。滚动窗口具有固定的大小，并且不重叠。例如，如果您指定大小为5分钟的滚动窗口，则将评估当前窗口，并且每五分钟将启动一个新窗口。那么在滚动窗口上的实现，Spark Streming和Flink有明显的不同，最大的区别是spark streaming在一开始便指定了切分流数据的方式val ssc = new StreamingContext(conf, Seconds(10)) 即10秒切分一次，然后对这一批进行处理。而Flink是后面自己通过Window算子timeWindow(Time.seconds(10))来指定每10秒作为一个数据窗口进行处理。以上可以看出Spark Streaming的底层为micro batch处理方式，Flink为流处理方式。Flink滚动窗口如下图所示。为了更好的演示Flink的各种窗口操作，我花了一点时间用python写了一个模拟实时产生用户数据的程序，随后将产生的数据写入kafka中，顺便提一下Faker这个python中的第三方库，模拟数据很好用，以前只在爬虫程序中，用过模拟浏览器信息。Flink tumbling window 代码结构12345678910111213141516171819val input: DataStream[T] = ...// tumbling event-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// tumbling processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// daily tumbling event-time windows offset by -8 hours.input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;)窗口函数，这里也可以使用timeWindow(Time.seconds(5))，时间间隔可以通过使用一个指定Time.milliseconds(x)，Time.seconds(x)， Time.minutes(x)，等等。如最后一个示例所示，滚动窗口分配器还采用一个可选offset 参数，该参数可用于更改窗口的对齐方式。例如，如果没有偏移，则每小时滚动窗口与epoch对齐，即您将获得诸如的窗口 1:00:00.000 - 1:59:59.999，2:00:00.000 - 2:59:59.999依此类推。如果要更改，可以提供一个偏移量。随着15分钟的偏移量，你会，例如，拿 1:15:00.000 - 2:14:59.999，2:15:00.000 - 3:14:59.999等。一个重要的用例的偏移是窗口调整到比UTC-0时区等。例如，在中国，您必须指定的偏移量Time.hours(-8)。Sliding Windows_滑动窗口_这个滑动窗口概念和Spark Streaming中的也是一样的，在Spark Streaming中是这样实现的1val window_dstream: DStream[(String, Int)] = wordcount_dstream.reduceByKeyAndWindow((x: Int,y: Int)=&gt;x+y,Seconds(30), Seconds(10))_滑动窗口_分配器让元素以固定长度的窗口通过。类似于滚动窗口分配器，_窗口的大小由窗口大小_参数配置。附加的_窗口滑动_参数控制滑动窗口启动的频率。因此，如果滑动参数小于窗口大小，则滑动窗口会重叠。在这种情况下，元素被分配给多个窗口。例如，您可以将大小为10分钟的窗口滑动5分钟。这样，您每隔5分钟就会得到一个窗口，其中包含最近10分钟内到达的事件，如下图所示。sliding windows滑动窗口代码结构**12345678910111213141516val input: DataStream[T] = ...// sliding event-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// sliding processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// sliding processing-time windows offset by -8 hoursinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;)窗口函数，这里也可以使用timeWindow(Time.seconds(10), Time.seconds(5))，时间间隔可以通过使用一个指定Time.milliseconds(x)，Time.seconds(x)， Time.minutes(x)，等等。如最后一个示例所示，滑动窗口分配器还采用一个可选offset参数，该参数可用于更改窗口的对齐方式。例如，在没有偏移的情况下，每小时滑动30分钟的窗口将与epoch对齐，即您将获得诸如的窗口 1:00:00.000 - 1:59:59.999，1:30:00.000 - 2:29:59.999依此类推。如果要更改，可以提供一个偏移量。随着15分钟的偏移量，你会，例如，拿 1:15:00.000 - 2:14:59.999，1:45:00.000 - 2:44:59.999等一个重要的用例的偏移是窗口调整到比UTC-0时区等。例如，在中国，您必须指定的偏移量Time.hours(-8)。Session Windows会话窗口会话窗口中的元素是正在活跃的会话产生的数据。与滚动窗口和滑动窗口相比，会话窗口不重叠且没有固定的窗口开始和结束时间。相反，当会话窗口在一定时间段内未收到元素时（即不发生活动数据间隙时），它将关闭。会话窗口分配器配置静态会话间隔，其限定当会话不活动周期有多长。当此时间段到期时，当前会话将关闭，随后的元素将分配给新的会话窗口。在现实中的意义就是，当数据处于连续活跃的状态下，并不划分窗口，但是当数据处于不活跃的状态，相邻数据的时间超过了设定的gap，则划定一个新窗口。12345678910111213141516171819202122232425262728293031323334val input: DataStream[T] = ...// event-time session windows with static gapinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// event-time session windows with dynamic gapinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] &#123; override def extract(element: String): Long = &#123; // determine and return session gap &#125; &#125;)) .&lt;windowed transformation&gt;(&lt;window function&gt;)// processing-time session windows with static gapinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// processing-time session windows with dynamic gapinput .keyBy(&lt;key selector&gt;) .window(DynamicProcessingTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] &#123; override def extract(element: String): Long = &#123; // determine and return session gap &#125; &#125;)) .&lt;windowed transformation&gt;(&lt;window function&gt;)静态间隙可以通过使用中的一个来指定Time.milliseconds(x)，Time.seconds(x)， Time.minutes(x)。动态间隙是通过实现SessionWindowTimeGapExtractor接口指定的。由于会话窗口没有固定的开始和结束，因此对它们的评估不同于滚动窗口和滑动窗口。在内部，会话窗口运算符会为每个到达的记录创建一个新窗口，如果窗口彼此之间的距离比已定义的间隔小，则将它们合并在一起。为了可以将数据合并，会话窗口需要一个合并触发器以及合并窗口函数，如ReduceFunction，AggregateFunction，或ProcessWindowFunction （FoldFunction不能合并。）接下来，所有的窗口函数操作的数据，都是基于下面我用python下的模拟生产用户数据。python模拟数据代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import randomimport timefrom datetime import datetimefrom faker import Fakerfrom kafka import KafkaProducerimport json# 初始化fake = Faker(locale='zh_CN')producer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8'),bootstrap_servers=['localhost:9092'])\"\"\"生成随机时间（注册时间和最近登陆时间）\"\"\"register_start = '2017-06-02 12:12:12'register_end = '2017-11-01 00:00:00'login_start = '2018-05-01 12:12:12'login_end = '2018-12-01 00:00:00'def strTimeProp(start, end, prop, frmt): stime = time.mktime(time.strptime(start, frmt)) etime = time.mktime(time.strptime(end, frmt)) ptime = stime + prop * (etime - stime) return int(ptime)def register_randomDate(register_start, register_end, frmt='%Y-%m-%d %H:%M:%S'): register_time = time.strftime(frmt, time.localtime(strTimeProp(register_start, register_end, random.random(), frmt))) return register_timedef login_randomDate(login_start, login_end, frmt='%Y-%m-%d %H:%M:%S'): login_time = time.strftime(frmt, time.localtime(strTimeProp(login_start, login_end, random.random(), frmt))) return login_time\"\"\"生成渠道，设备\"\"\"def get_device(): web_devices = ['Chrome', '360_browser', 'QQ_browser', 'Firefox', 'Opera', 'UC_browser'] app_devices = ['xiaomi', 'iphone', 'Samsung', 'meizu', '1+', 'huawei','vivo', 'oppo'] pc_devices = ['Windows','Mac'] channels = &#123;'web':web_devices, 'app': app_devices, 'pc': pc_devices&#125; channel = random.choice(list(channels.keys())) device = random.choice(channels[channel]) return channel, devicedef main(): sex_list = ['male', 'female'] while True: channel, device = get_device() register_date_time = register_randomDate(register_start, register_end, frmt='%Y-%m-%d %H:%M:%S') last_login_time = login_randomDate(login_start, login_end, frmt='%Y-%m-%d %H:%M:%S') real_name, user_name, age, sex, phone_number, province, email, ip_address, company, channel, device, register_date_time, last_login_time = fake.name(), fake.user_name(), random.choice(range(18,65)) ,random.choice(sex_list), fake.phone_number(), fake.province(), fake.ascii_free_email(), fake.ipv4(), fake.company(), channel, device, register_date_time, last_login_time raw_data = &#123;'real_name': real_name, 'user_name': user_name, 'age':age, 'sex':sex, 'phone_number':phone_number,'province':province, 'email':email, 'ip_address':ip_address, 'company':company, 'channel':channel, 'device':device, 'register_date_time':register_date_time, 'last_login_time':last_login_time&#125; print(raw_data) producer.send('user_information', raw_data) time.sleep(0.5)if __name__ == '__main__': main()数据样式ReduceFunctionReduceFunction指定如何将输入中的两个元素组合在一起以产生相同类型的输出元素。Flink使用a ReduceFunction来逐步增量的聚合窗口元素。ReduceFunction的源码：123456789101112131415161718/** * Applies a reduce function to the window. The window function is called for each evaluation * of the window for each key individually. The output of the reduce function is interpreted * as a regular non-windowed stream. * * This window will try and pre-aggregate data as much as the window policies permit. For example, * tumbling time windows can perfectly pre-aggregate the data, meaning that only one element per * key is stored. Sliding time windows will pre-aggregate on the granularity of the slide * interval, so a few elements are stored per key (one per slide interval). * Custom windows may not be able to pre-aggregate, or may need to store extra values in an * aggregation tree. * * @param function The reduce function. * @return The data stream that is the result of applying the reduce function to the window. */def reduce(function: ReduceFunction[T]): DataStream[T] = &#123; asScalaStream(javaStream.reduce(clean(function)))&#125;利用tumbling window举例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package data_stream.windowimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.streaming.api.scala._import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport utils.KafkaUtil/** * reduce Function * 增量累加 * 输入输出类型要一致 */object window_function_reduce &#123; case class UserLogData(real_name: String, user_name: String, age: Int, sex: String, phone_number: String, province: String, email: String, ip_address: String, company: String, channel: String, device: String, register_date_time: String, last_login_time: String) private val KAFKA_TOPIC: String = \"user_information\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource/tumbling\")) val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(KAFKA_TOPIC) //计算窗口内5秒的数据 val original_stream: DataStream[String] = env.addSource(kafkaSource) \"\"\" |original_stream&gt; UserLogData(纪淑兰,ehao,37,female,18529008264,辽宁省,apeng@gmail.com,110.75.179.140,易动力科技有限公司,app,xiaomi,2017-08-18 12:29:29,2018-09-02 05:05:56) |original_stream&gt; UserLogData(刘玉英,caojing,42,male,15647635274,香港特别行政区,houwei@hotmail.com,50.188.173.136,同兴万点传媒有限公司,web,Firefox,2017-10-27 12:20:20,2018-10-11 09:26:20) |original_stream&gt; UserLogData(薛萍,leizhong,32,male,18928404956,云南省,xxiao@yahoo.com,115.123.146.193,网新恒天传媒有限公司,web,360_browser,2017-06-17 16:42:24,2018-09-19 19:36:39) \"\"\".stripMargin /** * 这里我们设定一个具体的需求，统计过去的5秒内，各省份访问的用户数量统计 * 下面第二次map操作=》(String, Int)，是因为接下来要使用reduce函数 * reduce函数聚合是要求输入和输出格式相同的。 * * 以下的转换步骤可以串联的写在一起，但是在这里分开写是想清晰的展示每一步转换后，Stream的类型变换 */ val original_format_stream: DataStream[(String, Int)] = original_stream .map(match_data(_: String)) .map((raw: UserLogData) =&gt;(raw.province, 1)) val province_window_stream: WindowedStream[(String, Int), String, TimeWindow] = original_format_stream .keyBy((_: (String, Int))._1) .timeWindow(Time.seconds(5)) val result_stream: DataStream[(String, Int)] = province_window_stream .reduce((p1: (String, Int), p2: (String, Int)) =&gt; (p1._1, p1._2 + p2._2)) result_stream.print(\"province_count\").setParallelism(1) env.execute(\"province count stream\") &#125; def match_data(original_str: String): UserLogData = &#123; val original_json: JSONObject = JSON.parseObject(original_str) UserLogData(original_json.getString(\"real_name\"), original_json.getString(\"user_name\"), original_json.getInteger(\"age\"), original_json.getString(\"sex\"), original_json.getString(\"phone_number\"), original_json.getString(\"province\"), original_json.getString(\"email\"), original_json.getString(\"ip_address\"), original_json.getString(\"company\"), original_json.getString(\"channel\"), original_json.getString(\"device\"), original_json.getString(\"register_date_time\"), original_json.getString(\"last_login_time\")) &#125;&#125;123456789101112131415161718192021222324province_count&gt; (安徽省,2)province_count&gt; (贵州省,1)province_count&gt; (江苏省,2)province_count&gt; (北京市,1)province_count&gt; (重庆市,1)province_count&gt; (湖南省,1)province_count&gt; (海南省,2)province_count&gt; (甘肃省,2)province_count&gt; (河南省,2)province_count&gt; (香港特别行政区,4)province_count&gt; (四川省,2)province_count&gt; (云南省,2)province_count&gt; (辽宁省,2)province_count&gt; (青海省,3)province_count&gt; (澳门特别行政区,1)province_count&gt; (天津市,1)province_count&gt; (新疆维吾尔自治区,1)province_count&gt; (河北省,2)province_count&gt; (吉林省,1)province_count&gt; (宁夏回族自治区,1)province_count&gt; (江西省,1)province_count&gt; (黑龙江省,1)province_count&gt; (山西省,5)province_count&gt; (西藏自治区,1)AggregateFunctionAggregateFunction是一个普通版本ReduceFunction，其具有三种类型：输入类型（IN），累加器类型（ACC），和一个输出类型（OUT）。输入类型是输入流中数据的类型，并且AggregateFunction具有将一个输入元素添加到累加器的方法。该接口还具有创建初始累加器，将两个累加器合并为一个累加器以及OUT从累加器提取输出（类型）的方法。在下面的示例中，我们将了解其工作原理。与ReduceFunction一样，Flink将在窗口输入元素到达时逐步增量的聚合它们。一个AggregateFunction可以被这样定义并使用：12345678910111213141516171819202122/** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] &#123; override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2)&#125;val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(new AverageAggregate)利用sliding window举例aggregate()方法中不能像reduce(function: (T, T) =&gt; T)一样使用匿名函数，而是需要自定义继承了AggregateFunction的对象。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package data_stream.windowimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.streaming.api.scala._import org.apache.flink.api.common.functions.AggregateFunctionimport org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.function.WindowFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, WindowedStream&#125;import org.apache.flink.streaming.api.windowing.assigners.SlidingProcessingTimeWindowsimport org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.Collectorimport utils.KafkaUtilobject window_function_aggregate &#123; /** * 增量处理，来一条数据处理一条。 */ class aggregate_channel_devices extends AggregateFunction[((String,String), Int),Long, Long]&#123; // 数据累加逻辑 override def add(in: ((String, String), Int), acc: Long): Long = acc + in._2 // 累加器初始化 override def createAccumulator(): Long = 0L // 累加器计算的结果 override def getResult(acc: Long): Long = acc // 不同分区之间，累加器的值相加 override def merge(acc: Long, acc1: Long): Long = acc + acc1 &#125; /** * 作用与窗口内，aggregate后数据的函数，窗口结束时运行 * 基于接口WindowFunction[IN, OUT, KEY, W &lt;: Window] * IN ：输入参数，为 AggregateFunction的输出 * OUT：输出参数 * */ class channel_devices_windowFuction extends WindowFunction[Long, ((String, String), Long), (String, String), TimeWindow]&#123; override def apply(key: (String, String), window: TimeWindow, input: Iterable[Long], out: Collector[((String, String), Long)]): Unit = &#123; out.collect((key, input.iterator.next())) &#125; &#125; case class UserLogData(real_name: String, user_name: String, age: Int, sex: String, phone_number: String, province: String, email: String, ip_address: String, company: String, channel: String, device: String, register_date_time: String, last_login_time: String) private val KAFKA_TOPIC: String = \"user_information\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource/tumbling\")) val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(KAFKA_TOPIC) //计算窗口内5秒的数据 val original_stream: DataStream[String] = env.addSource(kafkaSource) \"\"\" |original_stream&gt; UserLogData(纪淑兰,ehao,37,female,18529008264,辽宁省,apeng@gmail.com,110.75.179.140,易动力科技有限公司,app,xiaomi,2017-08-18 12:29:29,2018-09-02 05:05:56) |original_stream&gt; UserLogData(刘玉英,caojing,42,male,15647635274,香港特别行政区,houwei@hotmail.com,50.188.173.136,同兴万点传媒有限公司,web,Firefox,2017-10-27 12:20:20,2018-10-11 09:26:20) |original_stream&gt; UserLogData(薛萍,leizhong,32,male,18928404956,云南省,xxiao@yahoo.com,115.123.146.193,网新恒天传媒有限公司,web,360_browser,2017-06-17 16:42:24,2018-09-19 19:36:39) \"\"\".stripMargin /** * 这里我们设定一个具体的需求，每5秒统计过去10秒中，用户渠道来源以及使用设备的排行 */ val original_format_stream: DataStream[((String, String),Int)] = original_stream .map(match_data(_: String)) .map((raw: UserLogData) =&gt; ((raw.channel, raw.device), 1)) original_format_stream .keyBy((_: ((String, String), Int))._1) .window(SlidingProcessingTimeWindows.of(Time.seconds(10),Time.seconds(5))) .aggregate(new aggregate_channel_devices, new channel_devices_windowFuction) .print(\"channel_device\") env.execute(\"channel_device stream\") &#125; def match_data(original_str: String): UserLogData = &#123; val original_json: JSONObject = JSON.parseObject(original_str) UserLogData(original_json.getString(\"real_name\"), original_json.getString(\"user_name\"), original_json.getInteger(\"age\"), original_json.getString(\"sex\"), original_json.getString(\"phone_number\"), original_json.getString(\"province\"), original_json.getString(\"email\"), original_json.getString(\"ip_address\"), original_json.getString(\"company\"), original_json.getString(\"channel\"), original_json.getString(\"device\"), original_json.getString(\"register_date_time\"), original_json.getString(\"last_login_time\")) &#125;&#125;1234567891011121314channel_device:1&gt; ((app,Samsung),27)channel_device:2&gt; ((app,vivo),20)channel_device:3&gt; ((app,1+),19)channel_device:8&gt; ((pc,Mac),84)channel_device:4&gt; ((web,Chrome),26)channel_device:6&gt; ((app,meizu),22)channel_device:4&gt; ((app,oppo),21)channel_device:8&gt; ((pc,Windows),90)channel_device:3&gt; ((web,360_browser),25)channel_device:1&gt; ((web,QQ_browser),32)channel_device:2&gt; ((web,Opera),26)channel_device:3&gt; ((web,Firefox),34)channel_device:8&gt; ((web,UC_browser),25)channel_device:3&gt; ((app,huawei),19)ProcessWindowFunctionProcessWindowFunction是处理窗口函数中，底层的处理数据API，获取一个Iterable，该Iterable包含窗口的所有元素，以及一个Context对象，该对象可以访问时间和状态信息，从而使其比其他窗口函数更具灵活性。这是以性能和资源消耗为代价的，因为无法增量聚合元素，而是需要在内部对其进行缓冲，直到将窗口视为已准备好进行处理为止。ProcessWindowFunctionlook 的抽象类如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748abstract class ProcessWindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** * The context holding window metadata */ abstract class Context &#123; /** * Returns the window that is being evaluated. */ def window: W /** * Returns the current processing time. */ def currentProcessingTime: Long /** * Returns the current event-time watermark. */ def currentWatermark: Long /** * State accessor for per-key and per-window state. */ def windowState: KeyedStateStore /** * State accessor for per-key global state. */ def globalState: KeyedStateStore &#125;&#125;Sliding Window 举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package data_stream.windowimport java.text.SimpleDateFormatimport java.util.&#123;Date, Properties&#125;import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.streaming.api.scala._import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.scala.function.ProcessWindowFunctionimport org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.Collectorimport utils.KafkaUtilobject window_function_process &#123; case class UserLogData(real_name: String, user_name: String, age: Int, sex: String, phone_number: String, province: String, email: String, ip_address: String, company: String, channel: String, device: String, register_date_time: String, last_login_time: String) private val KAFKA_TOPIC: String = \"user_information\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource/tumbling\")) val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(KAFKA_TOPIC) //计算窗口内5秒的数据 val original_stream: DataStream[String] = env.addSource(kafkaSource) \"\"\" |original_stream&gt; UserLogData(纪淑兰,ehao,37,female,18529008264,辽宁省,apeng@gmail.com,110.75.179.140,易动力科技有限公司,app,xiaomi,2017-08-18 12:29:29,2018-09-02 05:05:56) |original_stream&gt; UserLogData(刘玉英,caojing,42,male,15647635274,香港特别行政区,houwei@hotmail.com,50.188.173.136,同兴万点传媒有限公司,web,Firefox,2017-10-27 12:20:20,2018-10-11 09:26:20) |original_stream&gt; UserLogData(薛萍,leizhong,32,male,18928404956,云南省,xxiao@yahoo.com,115.123.146.193,网新恒天传媒有限公司,web,360_browser,2017-06-17 16:42:24,2018-09-19 19:36:39) \"\"\".stripMargin /** * 这里我们设定一个具体的需求，每5秒统计过去10秒中，用户渠道来源以及使用设备的排行 * 这里是使用ProcessWindowFunction底层函数来做处理 * 为了清楚的显示，在每个窗口上打印了时间。 */ val original_format_stream: DataStream[((String, String), Int)] = original_stream .map(match_data(_: String)) .map((raw: UserLogData) =&gt; ((raw.channel, raw.device), 1)) original_format_stream .keyBy((_: ((String, String), Int))._1) .timeWindow(Time.seconds(10), Time.seconds(5)) .process(new ProcessWindowFunction[((String, String), Int), ((String, String), Long), (String, String), TimeWindow] &#123; override def process(key: (String, String), context: Context, elements: Iterable[((String, String), Int)], out: Collector[((String, String), Long)]): Unit = &#123; val process_time: String = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date) println(s\"=========$process_time=============\") out.collect((key, elements.size)) &#125; &#125;).print(\"processFunctionWindow\") env.execute(\"process function stream\") &#125; def match_data(original_str: String): UserLogData = &#123; val original_json: JSONObject = JSON.parseObject(original_str) UserLogData(original_json.getString(\"real_name\"), original_json.getString(\"user_name\"), original_json.getInteger(\"age\"), original_json.getString(\"sex\"), original_json.getString(\"phone_number\"), original_json.getString(\"province\"), original_json.getString(\"email\"), original_json.getString(\"ip_address\"), original_json.getString(\"company\"), original_json.getString(\"channel\"), original_json.getString(\"device\"), original_json.getString(\"register_date_time\"), original_json.getString(\"last_login_time\")) &#125;&#125;结果打印12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((web,QQ_browser),76)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((pc,Mac),217)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,oppo),50)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,Samsung),58)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,huawei),54)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((web,UC_browser),54)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((web,Firefox),63)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((web,360_browser),66)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,meizu),56)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,xiaomi),53)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,vivo),49)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,iphone),61)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((web,Chrome),57)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((pc,Windows),230)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((web,Opera),73)=========2018-05-17 19:29:15=============processFunctionWindow&gt; ((app,1+),50)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((web,UC_browser),55)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((web,Chrome),60)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,1+),50)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,huawei),54)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((web,QQ_browser),76)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,Samsung),58)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((pc,Windows),231)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,oppo),51)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((web,Opera),74)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((web,Firefox),64)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,vivo),49)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,xiaomi),53)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,meizu),57)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((pc,Mac),218)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((app,iphone),61)=========2018-05-17 19:29:20=============processFunctionWindow&gt; ((web,360_browser),66)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((web,Opera),3)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((web,Chrome),3)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((app,meizu),1)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((web,Firefox),5)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((pc,Mac),2)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((app,oppo),1)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((pc,Windows),1)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((web,UC_browser),3)=========2018-05-17 19:29:25=============processFunctionWindow&gt; ((app,huawei),1)","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Redis简介","slug":"Redis简介","date":"2019-09-15T05:59:41.000Z","updated":"2020-08-30T06:02:55.638Z","comments":true,"path":"2019/09/15/Redis简介/","link":"","permalink":"cpeixin.cn/2019/09/15/Redis%E7%AE%80%E4%BB%8B/","excerpt":"","text":"Redis是什么Redis是现在最受欢迎的NoSQL数据库之一，Redis是一个使用ANSI C编写的开源、包含多种数据结构、支持网络、基于内存、可选持久性的键值对存储数据库，其具备如下特性：基于内存运行，性能高效支持分布式，理论上可以无限扩展key-value存储系统开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的APIRedis的应用场景有哪些？Redis 的应用场景包括：缓存系统（“热点”数据：高频读、低频写）、计数器、消息队列系统、排行榜、社交网络和实时系统。Redis的数据类型及主要特性Redis提供的数据类型主要分为5种自有类型和一种自定义类型，这5种自有类型包括：String类型、哈希类型、列表类型、集合类型和顺序集合类型。Redis的数据结构Redis的数据结构如下图所示：关于上表中的部分释义：压缩列表是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项，并且每个列表项要么就是小整数，要么就是长度比较短的字符串，Redis就会使用压缩列表来做列表键的底层实现整数集合是集合键的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis就会使用整数集合作为集合键的底层实现","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"}]},{"title":"我的服务器被黑了（二）","slug":"我的服务器被黑了（二）","date":"2019-09-09T02:26:15.000Z","updated":"2020-04-04T12:00:14.168Z","comments":true,"path":"2019/09/09/我的服务器被黑了（二）/","link":"","permalink":"cpeixin.cn/2019/09/09/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"苦逼的周一开始了，苦逼的工作开始了，坐到工位上，上班气正在逐渐的减弱，但是当我发现，我的三台服务器又被那些无情的小黑人们盯上了的时候，我的怒气值达到了顶点，同时还感觉有点丢脸，哈哈哈。由于这三台服务器属于我个人的，没有经过运维兄弟的照顾，所以在安全方面，基本上没有防护。这次是怎么发现的呢，是因为我服务器上的爬虫突然停止了，我带着疑问去看了下系统日志。于是敲下了下面的命令1journalctl -xe映入眼帘的是满屏的扫描和ssh尝试登陆1234567891011121314151617181920212223242526272829303132333435363738394041424344Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Failed password for invalid user admin from 117.132.175.25 port 42972 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Received disconnect from 117.132.175.25 port 42972:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Disconnected from 117.132.175.25 port 42972 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Failed password for invalid user ansible from 149.56.96.78 port 44980 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Received disconnect from 149.56.96.78 port 44980:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Disconnected from 149.56.96.78 port 44980 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Failed password for root from 218.92.0.163 port 45157 ssh2Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: error: maximum authentication attempts exceeded for root from 218.92.0.163 port 45157 ssh2 [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: error: maximum authentication attempts exceeded for root from 49.88.112.54 port 24184 ssh2 [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:54 4Z-J16-A47 sshd[314]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:54 4Z-J16-A47 sshd[314]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"lines 1105-1127/1127 (END)Sep 09 11:02:49 4Z-J16-A47 sshd[65522]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Failed password for invalid user admin from 117.132.175.25 port 42972 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Received disconnect from 117.132.175.25 port 42972:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[303]: Disconnected from 117.132.175.25 port 42972 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Failed password for invalid user ansible from 149.56.96.78 port 44980 ssh2Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Received disconnect from 149.56.96.78 port 44980:11: Bye Bye [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[302]: Disconnected from 149.56.96.78 port 44980 [preauth]Sep 09 11:02:50 4Z-J16-A47 sshd[65525]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Failed password for root from 218.92.0.163 port 45157 ssh2Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: error: maximum authentication attempts exceeded for root from 218.92.0.163 port 45157 ssh2 [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:51 4Z-J16-A47 sshd[65522]: PAM service(sshd) ignoring max retries; 6 &gt; 3Sep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.92.0.163 user=rootSep 09 11:02:52 4Z-J16-A47 sshd[310]: pam_succeed_if(sshd:auth): requirement \"uid &gt;= 1000\" not met by user \"root\"Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Failed password for root from 49.88.112.54 port 24184 ssh2Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: error: maximum authentication attempts exceeded for root from 49.88.112.54 port 24184 ssh2 [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: Disconnecting: Too many authentication failures [preauth]Sep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=49.88.112.54 user=rootSep 09 11:02:53 4Z-J16-A47 sshd[65525]: PAM service(sshd) ignoring max retries; 6 &gt; 3看到这里，感觉自己家的鸡，随时都要被偷走呀。。。。这还了得。于是马上开始了加固防护对待这种情况，就是要禁止root用户远程登录，使用新建普通用户，进行远程登录，还有重要的一点，修改默认22端口。12[root@*** ~]# useradd one #创建用户[root@*** ~]# passwd one #设置密码输入新用户密码首先确保文件 /etc/sudoers 中1234567%wheel ALL=(ALL) ALL``` 没有被注释```linuxusermod -g wheel onerocket设置只有指定用户组才能使用su命令切换到root用户在linux中，有一个默认的管理组 wheel。在实际生产环境中，即使我们有系统管理员root的权限，也不推荐用root用户登录。一般情况下用普通用户登录就可以了，在需要root权限执行一些操作时，再su登录成为root用户。但是，任何人只要知道了root的密码，就都可以通过su命令来登录为root用户，这无疑为系统带来了安全隐患。所以，将普通用户加入到wheel组，被加入的这个普通用户就成了管理员组内的用户。然后设置只有wheel组内的成员可以使用su命令切换到root用户。1234567#! /bin/bash# Function: 修改配置文件，使得只有wheel组的用户可以使用 su 权限sed -i '/pam_wheel.so use_uid/c\\auth required pam_wheel.so use_uid ' /etc/pam.d/sun=`cat /etc/login.defs | grep SU_WHEEL_ONLY | wc -l`if [ $n -eq 0 ];thenecho SU_WHEEL_ONLY yes &gt;&gt; /etc/login.defsfi打开SSHD的配置文件1vim /etc/ssh/sshd_config查找“#PermitRootLogin yes”，将前面的“#”去掉，短尾“yes”改为“no”（不同版本可能区分大小写），并保存文件。修改sshd默认端口虽然更改端口无法在根本上抵御端口扫描，但是，可以在一定程度上提高防御。打开sshd配置文件1vi /etc/ssh/sshd_config找到#Port 22 删掉注释服务器端口最大可以开到65536同时再添加一个Port 61024 （随意设置）Port 22Port 61024重启sshd服务123service sshd restart #centos6系列systemctl restart sshd #centos7系列firewall-cmd --add-port=61024/tcp测试，使用新用户，新端口进行登录如果登陆成功后，再将Port22注释掉，重启sshd服务。到这里，关于远程登录的防护工作，就做好了。最后，告诫大家，亲身体验，没有防护裸奔的服务器，真的太容易被抓肉鸡了！！！！！","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"}]},{"title":"Flink SQL","slug":"Flink-SQL","date":"2019-09-07T16:56:14.000Z","updated":"2020-09-06T09:04:21.585Z","comments":true,"path":"2019/09/08/Flink-SQL/","link":"","permalink":"cpeixin.cn/2019/09/08/Flink-SQL/","excerpt":"","text":"Flink SQL这里内容较少，并且除了窗口定义上和其他有点不一样，其余的都是大家熟悉的SQL语句，或者是熟悉的算子。这里直接写出两个实例，还是和以前的需求一样，求滚动窗口或者滑动窗口，用户对应的游戏总得分。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package SQLimport org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractorimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.table.api.scala.StreamTableEnvironmentimport org.apache.flink.table.api.&#123;EnvironmentSettings, Table, Tumble&#125;import org.apache.flink.types.Rowobject sql_stream_window &#123; case class GameData(user_id: String, game_id: String, game_time: Long, game_score: Int) def main(args: Array[String]): Unit = &#123; val stream_env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment stream_env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val settings: EnvironmentSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build() val table_env: StreamTableEnvironment = StreamTableEnvironment.create(stream_env, settings) stream_env.setParallelism(1) val socketStream: DataStream[String] = stream_env.socketTextStream(\"localhost\", 8888) val gameStream: DataStream[GameData] = socketStream.map((line: String) =&gt; &#123; val array_data: Array[String] = line.split(\",\") GameData(array_data(0), array_data(1), array_data(2).toLong, array_data(3).toInt) &#125;).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[GameData](Time.seconds(3)) &#123; override def extractTimestamp(element: GameData): Long = &#123; element.game_time &#125; &#125;) import org.apache.flink.table.api.scala._ table_env.registerDataStream(\"t_game_detail\", gameStream, 'user_id, 'game_id, 'game_time.rowtime, 'game_score) //滚动窗口 // val game_score_sum: Table = table_env // .sqlQuery(\"select user_id, \" + // \"sum(game_score) \" + // \"from t_game_detail \" + // \"group by tumble(game_time, interval '5' second), user_id\") // 滑动窗口 // val game_score_sum: Table = table_env.sqlQuery(\"select user_id, \" + // \"sum(game_score) \" + // \"from t_game_detail \" + // \"group by hop(game_time, interval '5' second, interval '10' second), user_id\") // 滑动窗口，打印窗口时间 val game_score_sum: Table = table_env.sqlQuery(\"select user_id, \" + \"hop_start(game_time, interval '5' second, interval '10' second),\" + \"hop_end(game_time, interval '5' second, interval '10' second),\" + \"sum(game_score) \" + \"from t_game_detail \" + \"group by hop(game_time, interval '5' second, interval '10' second), user_id\") table_env .toRetractStream[Row](game_score_sum) .filter((_: (Boolean, Row))._1 == true) .print(\"user_sum_score\") table_env.execute(\"user_sum_score——job\") &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 数据流容错","slug":"Flink-数据流容错","date":"2019-08-30T10:09:11.000Z","updated":"2020-09-06T09:03:34.165Z","comments":true,"path":"2019/08/30/Flink-数据流容错/","link":"","permalink":"cpeixin.cn/2019/08/30/Flink-%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99/","excerpt":"","text":"介绍Apache Flink提供了一种容错机制，可以一致地恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态最终也将exactly once反映出数据流中的每个记录。请注意，有一个开关可以至少_将担保_降级一次容错机制连续绘制分布式流数据流的快照。对于状态小的流应用程序，这些快照非常轻巧，可以在不影响性能的情况下频繁绘制。流应用程序的状态存储在可配置的位置（例如主节点或HDFS）。如果发生程序故障（由于机器，网络或软件故障），Flink将停止分布式流数据流。然后，系统重新启动operators，并将其重置为最新的成功检查点。输入流将重置为状态快照的点。确保作为重新启动的并行数据流的一部分处理的任何记录都不属于先前的检查点状态。_注意：_默认情况下，检查点是禁用的。有关如何启用和配置检查点的详细信息，请参见检查点。_注意：_为了使该机制实现其全部保证，数据源（例如消息队列或代理）必须能够将流后退到定义的最近点。Apache Kafka具有此功能(kafka 0.11版本后支持)，Flink与Kafka的连接器利用了此功能。有关Flink连接器提供的保证的更多信息，请参见数据源和接收器的容错保证。_注意：_由于Flink的checkpoints是通过分布式快照实现的，因此我们可以交替使用_snapshot_和checkpoint一词。CheckpointFlink容错机制的核心部分是绘制分布式数据流和operator state的一致快照。这些快照充当一致的检查点，如果发生故障，系统可以回退到这些检查点。Flink绘制这些快照的机制在“ 分布式数据流的轻量级异步快照 ”中进行了介绍。它受用于分布式快照的标准Chandy-Lamport算法的启发，并且专门针对Flink的执行模型进行了量身定制。Barriers流屏障(barriers)是Flink分布式快照中的核心元素。这些barriers将注入到数据流中，并与记录一起作为数据流的一部分流动。壁垒从不超越记录，它们严格按照顺序排列。屏障将数据流中的记录分为进入当前快照的记录集和进入下一个快照的记录集。每个屏障都带有快照的ID，快照的记录已推送到快照的前面。屏障不会中断流的流动，因此非常轻便。来自不同快照的多个障碍可以同时出现在流中，这意味着各种快照可能同时发生。_流屏障(barriers)在流源处注入并行数据流中。快照n的屏障被注入的点（我们称其为_S _）是快照中覆盖数据的源流中的位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。该位置_S _被报告给_检查点协调器_（Flink的JobManager）。然后，屏障向下游流动。当中间operator从其所有输入流中收到快照n的屏障时，它会将快照n的屏障发射到其所有输出流中。接收器运算符（流式DAG的末尾）从其所有输入流接收到屏障n后，便将快照n确认给检查点协调器。所有接收器都确认快照后，就认为快照已完成。一旦快照n完成，该作业将再也不会向源询问_S _之前的记录，因为此时这些记录（及其后代记录）将通过整个数据流拓扑。接收多个输入流的operator需要在快照barriers上_对齐_输入流。上图说明了这一点：operator一旦从传入流接收到快照屏障n，就无法处理该流中的任何其他记录，直到它也从其他输入接收到屏障n为止。否则，它将混合属于快照_n的_记录和属于快照_n + 1的记录_。报告屏障_n的_流被暂时搁置。从这些流接收到的记录将不进行处理，而是放入输入缓冲区中。一旦最后一个流接收到屏障n，操作员将发出所有未决的传出记录，然后自身发出快照n屏障。此后，它将恢复处理所有输入流中的记录，处理输入缓冲中的记录，然后再处理流中的记录。State当运算符包含任何形式的_状态时_，该状态也必须是快照的一部分。operator状态以不同的形式出现：_用户定义的状态_：这是由转换功能（如map()或filter()）直接创建和修改的状态。有关详细信息，请参见流应用程序中的状态。_系统状态_：此状态是指作为操作员计算的一部分的数据缓冲区。这种状态的一个典型示例是_窗口缓冲区_，系统在其中收集（并汇总）窗口记录，直到评估并逐出窗口为止。操作员在从输入流接收到所有快照barriers的时间点，以及在将barriers发送到其输出流之前，对其状态进行快照。届时，将对进行障碍之前的记录进行的所有状态更新，并且不依赖于应用障碍后的记录进行的任何更新。由于快照的状态可能很大，因此将其存储在可配置state _backend中_。默认情况下，这是JobManager的内存，但对于生产用途，应配置分布式可靠存储（例如HDFS）。存储状态后，操作员确认检查点，将快照屏障发送到输出流中，然后继续。现在生成的快照包含：对于每个并行流数据源，快照启动时流中的偏移量/位置对于每个运算符，指向作为快照一部分存储的状态的指针Exactly Once vs. At Least Once对准步骤可以向流传输程序增加等待时间。通常，这种额外的延迟大约是几毫秒，但是我们看到一些异常值的延迟显着增加的情况。对于要求所有记录始终具有超低延迟（几毫秒）的应用程序，Flink可以进行切换以在检查点期间跳过流对齐。一旦操作员从每个输入看到检查点障碍，仍然会绘制检查点快照。跳过对齐后，即使到达检查点n的_某些检查点障碍，操作员仍会继续处理所有输入。这样，操作员还可以在获取检查点n的状态快照之前处理属于检查点_n + 1的_元素。在还原时，这些记录将作为重复记录出现，因为它们都包含在检查点n的状态快照中，并将在检查点n之后作为数据的一部分重播。_注意_：对齐仅适用于具有多个前任（联接）的运算符以及具有多个发件人的运算符（在流重新分区/混洗之后）。正因为如此，有数据流只有尴尬的并行流操作（map()，flatMap()，filter()，…）实际上给_正好一次_甚至在保证_至少一次_的模式。Asynchronous State Snapshots注意，上述机制意味着操作员在将输入状态的快照存储在_状态后端时_停止处理输入记录。每次拍摄快照时，此_同步_状态快照都会带来延迟。可以让operator在存储其状态快照时继续进行处理，从而有效地使状态快照在后台_异步_发生。为此，operator必须能够产生状态对象，该状态对象的存储方式应确保对操作员状态的进一步修改不会影响该状态对象。例如，在RocksDB中使用_的写时复制_数据结构具有此行为。在其输入上收到检查点屏障后，操作员将开始对其状态进行异步快照复制。它立即对输出发出障碍，并继续进行常规流处理。后台复制过程完成后，它将向检查点协调器（JobManager）确认检查点。现在，只有在所有接收器都接收到障碍并且所有有状态操作员都已确认完成备份后（可能是在障碍到达接收器之后），检查点才完成。有关状态快照的详细信息，请参见状态后端。Recovery在这种机制下的恢复非常简单：失败时，Flink选择最新完成的检查点k。然后，系统重新部署整个分布式数据流，并为每个操作员提供作为检查点_k的_一部分快照的状态。设置源以开始从位置_S _读取流。例如，在Apache Kafka中，这意味着告诉使用者开始从偏移量_S _获取。如果状态是增量快照，则操作员将从最新完整快照的状态开始，然后对该状态应用一系列增量快照更新。有关更多信息，请参见重新启动策略。Operator Snapshot Implementation拍摄操作员快照时，有两个部分：同步部分和异步部分。运算符和状态后端以Java形式提供其快照FutureTask。该任务包含_同步_部分已完成而_异步_部分未决的状态。然后，异步部分由该检查点的后台线程执行。纯粹检查点的操作员将同步返回已完成的操作FutureTask。如果需要执行异步操作，则以run()that 的方法执行FutureTask。这些任务是可取消的，因此可以释放流和其他消耗资源的句柄。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink Watermark","slug":"Flink-Watermark","date":"2019-08-27T09:03:48.000Z","updated":"2020-09-06T09:04:08.480Z","comments":true,"path":"2019/08/27/Flink-Watermark/","link":"","permalink":"cpeixin.cn/2019/08/27/Flink-Watermark/","excerpt":"","text":"如果您正在构建实时流媒体应用程序，则事件时间处理是您必须迟早使用的功能之一。由于在大多数现实世界的用例中，消息到达无序，应该有一些方法，您建立的系统了解消息可能迟到并且相应地处理的事实。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中启用它。EventTime是事件在现实世界中发生的时间，ProcessingTime是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于Process Time的系统，看看它的缺点。我们将创建一个大小为10秒的SlidingWindow，每5秒滑动一次，在窗口结束时，系统将发出在此期间收到的消息数。一旦了解EventTime处理如何与SlidingWindow相关的工作，那么了解如何在TumblingWindow中工作也不难。所以让我们开始吧。基于处理时间的系统对于这个例子，我们的消息格式是 value，timestamp，其中value是消息，timestamp是在源生成此消息的时间。由于我们正在构建基于Process Time的系统，因此以下代码忽略了时间戳部分。了解消息应包含生成时间的信息是一个重要的方面。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己形成这个。稍后我们将看到，事件时间处理提取此时间戳信息以处理较晚的消息。支持事件时间的流处理器需要一种测量事件时间进度的方法。例如，当事件时间超过一个小时结束时，需要通知构建每小时窗口的窗口算子，以便该窗口算子可以关闭正在进行的窗口。Event Time可以独立于Process Time进行。例如，在一个程序中，窗口算子的当前事件时间可能会稍微落后于处理时间 （考虑到事件接收的延迟），而两者均以相同的速度进行。另一方面，另一个流媒体程序可以通过快速转发已经在Kafka主题（或另一个消息队列）中缓存的一些历史数据来在数周的事件时间内进行处理，而处理时间仅为几秒钟。情况1消息到达不间断假设源分别在时间13秒生产两条消息和第16秒产生一条，类型为a的三个消息。（小时和分钟不重要，因为窗口大小只有10秒）。这些消息将落入Windows中，如下所示。在第13秒产生的前两个消息将落入window1 [5s-15s]和window2 [10s-20s]，第16个时间生成的第三个消息将落入window2 [ 10s-20s]和window3 [15s-25s] ]。每个window发出的最终计数分别为（a，2），（a，3）和（a，1）。该输出可以被认为是预期的行为。现在我们将看看当一个消息到达系统的时候会发生什么。情况2消息到达延迟现在假设其中一条消息（在第13秒生成）到达延迟6秒（即第19秒才到达），可能是由于某些网络拥塞。你能猜测这个消息会落入哪个window？延迟的消息落入window2和window3，因为19秒在10-20和15-25之间。在window2中计算没有任何问题（因为消息应该落入该窗口），但是它影响了window1和window3的结果。我们现在将尝试使用EventTime处理来解决这个问题。基于EventTime的系统要启用EventTime处理，我们需要一个时间戳提取器，从输入的数据中提取Event Time信息。请记住，数据的格式是Value，Timestamp。该extractTimestamp方法获取Timestamp部分，并以Long型进行返回。现在忽略getCurrentWatermark方法，我们稍后再回来。12345678class TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable &#123; override def extractTimestamp(e: String, prevElementTimestamp: Long) = &#123; e.split(\",\")(1).toLong &#125; override def getCurrentWatermark(): Watermark = &#123; new Watermark(System.currentTimeMillis) &#125;&#125;我们现在需要设置这个时间戳提取器，并将TimeCharactersistic设置为EventTime。其余的代码与ProcessingTime的情况保持一致。123456789senv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val text = senv.socketTextStream(\"localhost\", 9999) .assignTimestampsAndWatermarks(new TimestampExtractor) val counts = text.map &#123;(m: String) =&gt; (m.split(\",\")(0), 1) &#125; .keyBy(0) .timeWindow(Time.seconds(10), Time.seconds(5)) .sum(1)counts.printsenv.execute(\"EventTime processing example\")运行上述代码的结果如下图所示。结果看起来好一些，window 2和window 3现在得到了正确的结果，但是window 1仍然是错误的。Flink没有将延迟的消息分配给window 3，因为它现在检查了消息的事件时间，并且理解它不在该窗口中。但是为什么没有将消息分配给window 1 呢？原因是在延迟的信息到达系统时（第19秒），window 1的窗口计算已经完成了（第15秒）。现在让我们尝试通过使用watermark来解决这个问题。ps:请注意，在window 2中，延迟的消息仍然位于第19秒，而不是第13秒（事件时间）。该图中的描述是故意表示窗口中的消息不会根据事件时间进行排序。（这可能会在将来改变）WaterMarkwatermark是一个非常重要和有趣的想法，watermark本质上是一个时间戳。当Flink中的运算符接收到watermark时，它明白（假设）它不会看到比该时间戳更早的消息。（ 原文：A Watermark is essentially a timestamp. When an Operator in Flink receives a watermark, it understands(assumes) that it is not going to see any message older than that timestamp.我这里理解是翻译为更早的信息或者更晚的信息都可以，因为从时间上来说，1990年比1994年要老，但是1990年比1994年也是可以说是早的，那么就是它看不到更早/老的信息）因此，在“EventTime”中，watermark也可以被认为是一种告诉Flink它有多远的一种方式。Flink中衡量Event Time进度的机制是watermark水印。watermark简单的来说，就是一个延迟触发机制，watermark伴随着每一条数据，作为数据流的一部分流动，并带有时间戳_t，_其值为当前已流入数据中最大的Event Time-最大延迟时间 maxEventTime(程序中自定义)，当watermark的值大于等于某个窗口的结束时间，则那个窗口立即触发执行。这个例子的目的，把它看作是一种告诉Flink一个消息会延迟多少的方式。在上一次尝试中，我们将watermark设置为当前系统时间。因此，不要指望任何延迟的消息。我们现在将水印设置为当前时间-5秒，这告诉Flink希望消息最多有5s的延迟，这是因为每个窗口仅在水印通过时被评估。由于我们的水印是当前时间-5秒，所以第一个窗口[5s-15s]将仅在第20秒被评估。类似地，窗口[10s-20s]将在第25秒进行评估，依此类推。123override def getCurrentWatermark(): Watermark &#x3D; &#123; new Watermark(System.currentTimeMillis - 5000) &#125;通常最好保持接收到的最大时间戳，并创建具有最大预期延迟的水印，而不是从当前系统时间减去。进行上述更改后运行代码的结果是：最后我们得到了正确的结果，所有这三个窗口现在都按照预期的方式发射计数，这是（a，2），（a，3）和（a，1）。注意：我们也可以使用AllowedLateness功能设置消息的最大允许时间来解决这个问题，这个后面我们再讲Watermarks in Parallel Streams并行流中的水印watermark在源函数处或源函数之后直接生成。源函数的每个并行子任务通常独立生成其水印。这些水印定义了该并行源处的event time。随着watermark在流式传输程序中的流动，它们可能会比operators处的事件时间提前到达。每当一个operators提前它的事件时间，它为它的后继操作符产生一个新的下游水印。一些算子将消耗多个输入流；例如，并集，keyBy（）或partition（）函数的运算符。该operator的当前事件时间是其输入流的事件时间中的最小值。随着其输入流更新其事件时间，operator也将更新。也就是operator会在各个到达的event time中，选择最小的event time进行“对齐”。下图显示了流过并行流的事件和水印的示例，操作员跟踪事件时间。请注意，Kafka源支持按分区添加水印，您可以在此处阅读更多信息。Late Elements迟到元素**某些元素可能会违反水印条件，这意味着在_水印（t）_之后发生，也会出现更多时间戳为_t’&lt;= t的_元素。实际上，在许多现实世界的设置中，某些元素可以任意延迟，这使得不可能指定某个事件时间戳的所有元素发生的时间。此外，即使可以限制延迟，通常也不希望将水印延迟太多，因为这会导致事件时间窗的评估延迟过多。由于这个原因，流式传输程序可能会明确期望某些_后期_元素。延迟元素是在系统的事件时间时钟（由水印指示）已经经过延迟元素时间戳的时间之后到达的元素。有关如何在事件时间窗口中使用延迟元素的更多信息请参见允许延迟。生成时间戳/水印本节与在event time运行的程序有关。有关事件时间， 处理时间和摄取时间的简介，请参阅事件时间的简介。为了处理**事件时间，流式传输程序需要相应地设置时间特征**。12val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)分配时间戳为了使用事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳。这通常是通过从元素的某个字段访问/提取时间戳来完成的。时间戳分配与生成水印齐头并进，水印告诉系统事件时间的进展。有两种分配时间戳和生成水印的方法：直接在数据流源中通过时间戳分配器/水印生成器：在Flink中，时间戳分配器还定义要发送的水印注意**自1970年1月1日T00：00：00Z的Java时代以来，时间戳和水印都指定为毫秒。带有时间戳和水印的源函数流源可以将时间戳直接分配给它们产生的元素，并且它们还可以发出水印。完成此操作后，无需时间戳分配器。请注意，如果使用时间戳分配器，则源所提供的任何时间戳和水印都将被覆盖。要将时间戳直接分配给源中的元素，源必须使用上的collectWithTimestamp() 方法SourceContext。要生成水印，源必须调用该emitWatermark(Watermark)函数。下面是一个简单的示例（非检查点）源，该源分配时间戳并生成水印：12345678910override def run(ctx: SourceContext[MyType]): Unit &#x3D; &#123; while (&#x2F;* condition *&#x2F;) &#123; val next: MyType &#x3D; getNext() ctx.collectWithTimestamp(next, next.eventTimestamp) if (next.hasWatermarkTime) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime)) &#125; &#125;&#125;**带有定期水印AssignerWithPeriodicWatermarks 分配时间戳并定期生成水印。通过定义生成水印的间隔（每n毫秒） ExecutionConfig.setAutoWatermarkInterval()。分配器的getCurrentWatermark()方法每次都会被调用，如果返回的水印非空且大于前一个水印，则将发出新的水印。Flink提供了抽象，允许程序员分配自己的时间戳并发出自己的水印。更具体地说，根据使用情况，可以通过实现AssignerWithPeriodicWatermarks和AssignerWithPunctuatedWatermarks接口之一来实现。简而言之，第一个将定期发出水印，而第二个则根据传入记录的某些属性发出水印，例如，每当流中遇到特殊元素时。为了进一步简化此类任务的编程工作，Flink附带了一些预先实现的时间戳分配器。本节提供了它们的列表。除了开箱即用的功能外，它们的实现还可以作为自定义实现的示例。**时间戳递增的分配者_定期生成水印的最简单的特殊情况是给定源任务看到的时间戳以升序出现的情况。在这种情况下，当前时间戳始终可以充当水印，因为没有更早的时间戳会到达。请注意，每个并行数据源任务只需要增加时间戳记即可。例如，如果在一个特定的设置中一个并行数据源实例读取一个Kafka分区，则仅在每个Kafka分区内将时间戳记递增是必要的。每当对并行流进行混洗，合并，连接或合并时，Flink的水印合并机制都会生成正确的水印。12val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )**分配器允许固定的延迟时间周期性水印生成的另一个示例是水印在流中看到的最大（事件时间）时间戳落后固定时间量的情况。这种情况涵盖了预先知道流中可能遇到的最大延迟的场景，例如，当创建包含时间戳的元素的自定义源时，该时间戳在固定时间段内传播以进行测试。对于这些情况，Flink提供了BoundedOutOfOrdernessTimestampExtractor，将用作参数maxOutOfOrderness，即在计算给定窗口的最终结果时允许元素延迟到被忽略之前的最长时间。延迟对应于的结果t - t_w，其中t元素的（事件时间）时间戳和t_w前一个水印的时间戳。如果lateness &gt; 0那么该元素将被认为是较晚的元素，默认情况下，在为其相应窗口计算作业结果时将其忽略。有关 使用延迟元素的更多信息，请参见有关允许延迟的文档。12val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))**带标点的水印**要在特定事件表明可能会生成新的水印时生成水印，请使用 AssignerWithPunctuatedWatermarks。对于此类，Flink将首先调用该extractTimestamp()方法为元素分配时间戳，然后立即checkAndGetNextWatermark()在该元素上调用该 方法。该checkAndGetNextWatermark()方法会传递该方法中分配的时间戳extractTimestamp() ，并可以决定是否要生成水印。每当该checkAndGetNextWatermark() 方法返回一个非空水印，并且该水印大于最新的先前水印时，就会发出新的水印。12345678910class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123; if (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null &#125;&#125;_注意：_可以在每个事件上生成水印。但是，由于每个水印都会在下游引起一些计算，因此过多的水印会降低性能。每个Kafka分区的时间戳当使用Apache Kafka作为数据源时，每个Kafka分区可能都有一个简单的事件时间模式（时间戳升序或有界乱序）。但是，在使用来自Kafka的流时，通常会并行使用多个分区，从而交错插入分区中的事件并破坏每个分区的模式（这是Kafka的客户客户端工作方式所固有的）。在这种情况下，您可以使用Flink的Kafka分区感知水印生成。使用该功能，将在Kafka使用者内部针对每个Kafka分区生成水印，并且按与合并水印在流shuffle上相同的方式合并每个分区的水印。例如，如果事件时间戳严格按照每个Kafka分区递增，则使用递增时间戳水印生成器生成按分区的水印 将产生完美的整体水印。下图显示了如何使用按kafka分区的水印生成，以及在这种情况下水印如何通过流数据流传播。123456val kafkaSource = new FlinkKafkaConsumer09[MyType](\"myTopic\", schema, props)kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123; def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp&#125;)val stream: DataStream[MyType] = env.addSource(kafkaSource)预定义的时间戳提取器/水印发射器如时间戳和水印处理中所述，Flink提供了抽象，允许程序员分配自己的时间戳并发出自己的水印。更具体地说，根据使用情况，可以通过实现AssignerWithPeriodicWatermarks和AssignerWithPunctuatedWatermarks接口之一来实现。简而言之，第一个将定期发出水印，而第二个则根据传入记录的某些属性发出水印，例如，每当流中遇到特殊元素时。为了进一步简化此类任务的编程工作，Flink附带了一些预先实现的时间戳分配器。本节提供了它们的列表。除了开箱即用的功能外，它们的实现还可以作为自定义实现的示例。在 Flink watermark这一模块，相对于Spark Streaming来说，确实有些晦涩难懂，在前面的一些章节中，Flink程序举例我都是直接对接kafka读取数据进行处理，但是在watermark模块中，还是使用socket发送数据，可以更清晰的看出Flink watermark机制下窗口的触发。下面就是针对watermark进行的程序实例。基于有序数据的watermark，以下代码需求场景，我根据公司游戏监控项目的最基本的原理，使用简洁的数据进行演示，每5秒计算近10秒中，网关传递数据延迟时间最大的步骤。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package data_stream.watermarkimport java.text.SimpleDateFormatimport java.util.Dateimport org.apache.flink.api.common.functions.ReduceFunctionimport org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.scala.function.WindowFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.util.Collectorobject watermark_orderness &#123; case class game_gateway_data(user_id: String, step_num: String, gateway_time: Long, gateway_name: String, delay_time: Int) def main(args: Array[String]): Unit = &#123; val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val socketStream: DataStream[String] = env.socketTextStream(\"localhost\",8888) val gameGatewayStream: DataStream[game_gateway_data] = socketStream.map((line: String) =&gt; &#123; val array_data: Array[String] = line.split(\",\") game_gateway_data(array_data(0), array_data(1), array_data(2).toLong, array_data(3), array_data(4).toInt) &#125;) //设置时间语义 .assignAscendingTimestamps((_: game_gateway_data).gateway_time) gameGatewayStream .keyBy((_: game_gateway_data).user_id) .timeWindow(Time.seconds(10), Time.seconds(5)) .reduce(new MyReduceFunction, new stepTimeWindowFunction) .print(\"====&gt;\") env.execute(\"orderness case\") &#125; class MyReduceFunction extends ReduceFunction[game_gateway_data]&#123; override def reduce(t: game_gateway_data, t1: game_gateway_data): game_gateway_data = &#123; if (t.delay_time &gt;= t1.delay_time) t else t1 &#125; &#125; class stepTimeWindowFunction extends WindowFunction[game_gateway_data, game_gateway_data, String, TimeWindow]&#123; // 在窗口结束时调用 override def apply(key: String, window: TimeWindow, input: Iterable[game_gateway_data], out: Collector[game_gateway_data]): Unit = &#123; // input中只有一条数据 println(s\"$&#123;window.getStart&#125;===$&#123;window.getEnd&#125;\") val result: game_gateway_data = input.iterator.next() out.collect(result) &#125; &#125;&#125;下面需要特别注意，也可以像我一样使用nc来调试窗口打印，可以清楚的了解窗口之间的关系，首先，我依次在控制台输入以下数据，一共五个批次的数据，每条数据最后一个字段是“delay_time 延迟时间”：1234567891011121314151617user_001,1,1591016077123,visitor_gateway,2user_001,2,1591016078123,login_gateway,3user_001,3,1591016080123,userCenter_gateway,3user_001,4,1591016081123,gameCenter_gateway,4user_001,5,1591016085123,payCenter_gateway,5user_001,6,1591016087123,exitCenter_gateway,6user_001,10,1591016090123,exitCenter_gateway,10user_001,11,1591016095123,exitCenter_gateway,11user_001,12,1591016100123,exitCenter_gateway,12依次打印以下五个窗口结果，仔细观察各个窗口的划分，其中我们来看第三个批次数据，当我输入延迟时间分别为6，10的时候，1591016080000===1591016090000窗口触发，返回当前窗口延迟时间最大的记录为6，为什么不是10呢？？ 这是因为窗口计算中，每个窗口的数据，左闭右开，也就是延迟时间为10的这条数据不加入1591016080000===1591016090000窗口的计算：12345678910111213141591016070000&#x3D;&#x3D;&#x3D;1591016080000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,2,1591016078123,login_gateway,3)1591016075000&#x3D;&#x3D;&#x3D;1591016085000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,4,1591016081123,gameCenter_gateway,4)1591016080000&#x3D;&#x3D;&#x3D;1591016090000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,6,1591016087123,exitCenter_gateway,6)1591016085000&#x3D;&#x3D;&#x3D;1591016095000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,10,1591016090123,exitCenter_gateway,10)1591016090000&#x3D;&#x3D;&#x3D;1591016100000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,11,1591016095123,exitCenter_gateway,11)以下是窗口划分的源码：基于乱序数据：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package data_stream.watermarkimport org.apache.flink.api.common.functions.ReduceFunctionimport org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarksimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractorimport org.apache.flink.streaming.api.scala.function.WindowFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.api.watermark.Watermarkimport org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindowsimport org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.util.Collectorobject watermark_outoforderness &#123; case class game_gateway_data(user_id: String, step_num: String, gateway_time: Long, gateway_name: String, delay_time: Int) def main(args: Array[String]): Unit = &#123; val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) env.getConfig.setAutoWatermarkInterval(100L) //watermark周期 val socketStream: DataStream[String] = env.socketTextStream(\"localhost\", 8888) val gameGatewayStream: DataStream[game_gateway_data] = socketStream.map((line: String) =&gt; &#123; val array_data: Array[String] = line.split(\",\") game_gateway_data(array_data(0), array_data(1), array_data(2).toLong, array_data(3), array_data(4).toInt) &#125;) //乱序数据处理，采用周期性watermark,设置延迟时间为5秒 //第一种写法，采用AssignerWithPeriodicWatermarks .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[game_gateway_data](Time.seconds(3)) &#123; \"\"\"抽取时间戳，设置event time\"\"\" override def extractTimestamp(element: game_gateway_data): Long = &#123; element.gateway_time &#125; &#125;) //第二种写法，自己定义实现类 // .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[game_gateway_data] &#123; // var maxEventTime: Long = _ // // //周期性生成watermark // override def getCurrentWatermark = &#123; // new Watermark(maxEventTime - 3000L) // &#125; // // //设定event time // override def extractTimestamp(element: game_gateway_data, previousElementTimestamp: Long) = &#123; // //设置maxEventTime // maxEventTime = maxEventTime.max(element.gateway_time) // element.gateway_time // &#125; // &#125;) gameGatewayStream .keyBy((_: game_gateway_data).user_id) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .reduce(new MyReduceFunction, new stepTimeWindowFunction) .print(\"====&gt;\") env.execute(\"orderness case\") &#125; class MyReduceFunction extends ReduceFunction[game_gateway_data] &#123; override def reduce(t: game_gateway_data, t1: game_gateway_data): game_gateway_data = &#123; if (t.delay_time &gt;= t1.delay_time) t else t1 &#125; &#125; class stepTimeWindowFunction extends WindowFunction[game_gateway_data, game_gateway_data, String, TimeWindow] &#123; // 在窗口结束时调用 override def apply(key: String, window: TimeWindow, input: Iterable[game_gateway_data], out: Collector[game_gateway_data]): Unit = &#123; // input中只有一条数据 println(s\"$&#123;window.getStart&#125;===$&#123;window.getEnd&#125;\") val result: game_gateway_data = input.iterator.next() out.collect(result) &#125; &#125;&#125;我依次在控制台输入以下数据，请注意，窗口对于延迟数据，是怎样处理的123456789101112131415161718192021222324252627282930313233343570-80（窗口范围，此条无需输入）user_001,1,1591016077123,visitor_gateway,2user_001,2,1591016078123,login_gateway,3user_001,3,1591016080123,userCenter_gateway,3user_001,4,1591016083123,gameCenter_gateway,475-85（窗口范围，此条无需输入）user_001,4,1591016081123,gameCenter_gateway,4user_001,5,1591016085123,payCenter_gateway,5user_001,6,1591016088123,exitCenter_gateway,680-90（窗口范围，此条无需输入）user_001,6,1591016079123,exitCenter_gateway,20user_001,6,1591016080123,exitCenter_gateway,40user_001,10,1591016093123,exitCenter_gateway,1085-95（窗口范围，此条无需输入）user_001,6,1591016084123,exitCenter_gateway,84user_001,10,1591016098123,exitCenter_gateway,1090-100（窗口范围，此条无需输入）user_001,10,1591016090123,exitCenter_gateway,90user_001,10,1591016094123,exitCenter_gateway,94user_001,10,1591016091123,exitCenter_gateway,91user_001,10,1591016092123,exitCenter_gateway,92user_001,10,1591016098123,exitCenter_gateway,98user_001,10,1591016099123,exitCenter_gateway,99user_001,10,1591016095123,exitCenter_gateway,95user_001,10,1591016096123,exitCenter_gateway,96user_001,10,15910160100123,exitCenter_gateway,100结果如下，根据下面的结果，我们可以看出，基于延迟机制watermark并且晚于窗口触发到达的数据，将会被抛弃1234567891011121591016070000&#x3D;&#x3D;&#x3D;1591016080000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,2,1591016078123,login_gateway,3)1591016075000&#x3D;&#x3D;&#x3D;1591016085000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,4,1591016083123,gameCenter_gateway,4)1591016080000&#x3D;&#x3D;&#x3D;1591016090000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,6,1591016080123,exitCenter_gateway,40)1591016085000&#x3D;&#x3D;&#x3D;1591016095000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,10,1591016093123,exitCenter_gateway,10)1591016090000&#x3D;&#x3D;&#x3D;1591016100000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,10,1591016099123,exitCenter_gateway,99)1591016095000&#x3D;&#x3D;&#x3D;1591016105000&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt; game_gateway_data(user_001,10,1591016099123,exitCenter_gateway,99)实时流处理系统的重要性日益增长，必须处理延迟的消息是您构建的任何此类系统的一部分。在这篇博文中，我们看到到达的消息迟到会影响系统的结果，以及如何使用ApacheFlink的事件时间处理功能来解决它们。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"我的服务器被黑了","slug":"我的服务器被黑了","date":"2019-08-24T02:26:15.000Z","updated":"2020-04-04T12:00:09.871Z","comments":true,"path":"2019/08/24/我的服务器被黑了/","link":"","permalink":"cpeixin.cn/2019/08/24/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E9%BB%91%E4%BA%86/","excerpt":"","text":"服务器自述我是一台8核，16G内存，4T的Linux (centOS 7)服务器… 还有两台和我一起被买来的苦主，我们一同长大，配置一样，都是从香港被贩卖到国外，我们三个组成了分布式爬虫框架，另两位苦主分别负责异步爬取连接，多进程爬取连接和scrapy-redis分布式爬取解析。而我比较清闲，只负责存储. 网页链接放在我的redis中，而解析好的文章信息放在我的MySQL中。然而故事的开始，就是在安装redis的那天，主人的粗心大意，为了节省时间，从而让他今天花费了小半天来对我进行维修！！😢为什么黑我的服务器这样一台配置的服务器，一个月的价格大概在1000RMB一个月，怎么说呢… 这个价格的服务器对于个人用户搭建自己玩的环境还是有些小贵的。例如我现在写博客，也是托管在GitHub上的，我也可以租用一台服务器来托管的博客，但是目前我的这种级别，也是要考虑到投入产出比是否合适，哈哈哈。但是对于，服务器上运行的任务和服务产出的价值要远远大于服务器价值的时候，这1000多RMB就可以忽略不计了。同时，还有黑衣人，他们需要大量的服务器，来运行同样的程序，产出的价值他们也无法衡量，有可能很多有可能很少。。那么这时候，他们为了节约成本，降低成本，就会用一些黑色的手法，例如渗透，sql注入，根据漏洞扫描等方法来 抓“肉鸡”，抓到大量的可侵入的服务器，然后在你的服务器上的某一个角落，放上他的程序，一直在运行，一直在运行，占用着你的cpu,占用着你的带宽…那么上面提到的黑衣人，就有那么一类角色，“矿工”！！！！曾经，我也专注过区块链，我也短暂的迷失在数字货币的浪潮中，但是没有吃到红利👀👀👀 就是这些数字世界的矿工，利用我服务器的漏洞黑了我的服务器如何发现被黑回到这篇博客的正题，我是如何发现，我的服务器被黑了呢？？最近我在做scrapy分布式爬虫方面的工作，准备了三台服务器，而这台被黑的服务器是我用来做存储的，其中用到了redis和mysql。其中引发这件事情的就是redis，我在安装redis的时候，可以说责任完全在我，我为了安装节约时间，以后使用方便等，做了几个很错误的操作1.关闭了Linux防火墙2.没有设置redis访问密码3.没有更改redis默认端口4.开放了任意IP可以远程连接以上四个很傻的操作,都是因为以前所用的redis都是有公司运维同事进行安装以及安全策略方面的配置，以至我这一次没有注意到安装方面。当我的爬虫程序已经平稳的运行了两天了，我就开始放心了，静静地看着spider疯狂的spider,可是就是在随后，redis服务出现异常，首先是我本地客户端连接不上远程redis-server，我有想过是不是网络不稳定的问题。在我重启redis后，恢复正常，又平稳的运行了一天。但是接下来redis频繁出问题，我就想，是不是爬虫爬取了大量的网页链接，对redis造成了阻塞。于是，我开启了对redis.conf，还有程序端的connect两方面360度的优化，然并卵。。。1lsof -i tcp:6379使用上面的命令后，发现redis服务正常运行，6379端口也是开启的。我陷入了深深地迷惑。。。。。但是这时其实就应该看出一些端倪了，因为正常占用 6379 端口的进程名是 ： redis-ser 。但是现在占用 6379 端口的进程名是 ：xmrig-no (忘记截图了)，但是这时我也没有多想直到我运行：1top发现了占用 6379 端口的进程全名称xmrig…，我才恍然大悟，我的端口被占用了。我在google上一查，才发现。。我被黑了做了哪些急救工作这时，感觉自己开始投入了一场对抗战1.首先查找植入程序的位置。在/tmp/目录下，一般植入程序都会放在 /tmp 临时目录下，其实回过头一想，放在这里，也是挺妙的。2.删除清理可疑文件杀死进程删除了正在运行的程序文件还有安装包3.查看所有用户的定时任务1cat /etc/passwd |cut -f 1 -d:crontab -uXXX -l4.开启防火墙仅开放会使用到的端口5.修改redis默认端口redis.conf中的port6.添加redis授权密码redis.conf中的requirepass7.修改绑定远程绑定ipredis.conf中的bind最后重启redis服务！从中学到了什么明明是自己被黑了，但是在补救的过程中，却得到了写程序给不了的满足感。感觉因为这件事情，上帝给我打开了另一扇窗户～～～最后说下，这个木马是怎么进来的呢，查了一下原来是利用Redis端口漏洞进来的，它可以对未授权访问redis的服务器登录，定时下载并执行脚本，脚本运行，挖矿，远程调用等。所以除了执行上述操作，linux服务器中的用户权限，服务权限精细化，防止再次被入侵。","categories":[{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"}],"tags":[{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"}]},{"title":"Flink 合并流 connect & union","slug":"Flink-合并流-connect-union","date":"2019-08-19T13:30:11.000Z","updated":"2020-09-06T09:03:48.123Z","comments":true,"path":"2019/08/19/Flink-合并流-connect-union/","link":"","permalink":"cpeixin.cn/2019/08/19/Flink-%E5%90%88%E5%B9%B6%E6%B5%81-connect-union/","excerpt":"","text":"Flink 合并流 connect &amp; union* connect &amp; union(合并流)**connect之后生成ConnectedStreams，会对两个流的数据应用不同的处理方法，并且双流 之间可以共享状态(比如计数)。这在第一个流的输入会影响第二个流时, 会非常有用;union 合并多个流，新的流包含所有流的数据。union是DataStream → DataStream。connect是DataStream* → ConnectedStreams。connect只能连接两个流，而union可以连接多于两个流 。connect连接的两个流类型可以不一致，但是合并数据的数据类型要一致，而union连接的流的类型必须一致。connect算子后面跟的map 和 flatMap和普通的map, flatmap类似，只不过作用在ConnectedStreams上会改变流的类型，由ConnectedStreams → DataStream下面的实例，是以sideoutput那篇文章的基础上，对主数据流和侧出流进行connect，代码中也给出了union合并流方法，unino方法使用起来则很简单，并且和spark streaming中union的用法是一样的1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.datastream.DataStreamSinkimport org.apache.flink.streaming.api.functions.ProcessFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.Collectorimport utils.KafkaUtilobject connect_datastream &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val processStream: DataStream[Raw] = word_stream.process(new SideOutput()) val dirty_stream: DataStream[Raw] = processStream.getSideOutput(new OutputTag[Raw](\"dirty_data\"))// val connect_datastream: DataStream[String] = processStream.connect(dirty_stream)// .map(// (originalRaw: Raw) =&gt; originalRaw.keywordList,// (dirtyRaw: Raw) =&gt; dirtyRaw.keywordList// )//// connect_datastream.print(\"ALL \") val union_datastream: DataStream[Raw] = processStream.union(dirty_stream) union_datastream.print(\"union_datastream \") env.execute(\"connect stream test\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125; class SideOutput() extends ProcessFunction[Raw, Raw] &#123; //定义一个侧输出流标签 lazy val dirty_data: OutputTag[Raw] = new OutputTag[Raw](\"dirty_data\") override def processElement(value: Raw, ctx: ProcessFunction[Raw, Raw]#Context, out: Collector[Raw]): Unit = &#123; if (value.keywordList == \"dirty_data\") &#123; ctx.output(dirty_data, value) &#125; else &#123; out.collect(value) &#125; &#125; &#125;&#125;打印结果：1234567891011121314151617181920212223242526ALL :8&gt; dirty_dataALL :7&gt; 网购翻车的经历,经历ALL :1&gt; 彭昱畅的自拍和他拍,彭昱畅ALL :7&gt; dirty_dataALL :1&gt; 彭昱畅的自拍和他拍,彭昱畅ALL :7&gt; dirty_dataALL :1&gt; 网购翻车的经历,经历ALL :7&gt; dirty_dataALL :1&gt; 网购翻车的经历,经历ALL :8&gt; dirty_dataALL :7&gt; 痛仰演唱会,演唱会ALL :1&gt; 痛仰演唱会,演唱会ALL :8&gt; dirty_dataALL :1&gt; dirty_dataALL :7&gt; dirty_dataALL :1&gt; 痛仰演唱会,演唱会ALL :1&gt; dirty_dataALL :8&gt; dirty_dataALL :7&gt; 痛仰演唱会,演唱会ALL :8&gt; dirty_dataALL :1&gt; 痛仰演唱会,演唱会ALL :8&gt; dirty_dataALL :1&gt; dirty_dataALL :7&gt; dirty_dataALL :1&gt; 幸福触手可及片花,片花ALL :7&gt; 幸福触手可及片花,片花","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 数据输出 Sink","slug":"Flink-数据输出-Sink","date":"2019-08-09T18:07:42.000Z","updated":"2020-09-06T09:03:22.295Z","comments":true,"path":"2019/08/10/Flink-数据输出-Sink/","link":"","permalink":"cpeixin.cn/2019/08/10/Flink-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA-Sink/","excerpt":"","text":"在Flink中，主要分为source，transform，sink三个部分，前面我们使用Flink利用封装好的方法对接Kafka，就是属于source部分，那么在sink阶段也是一样，我们只能通过定义好的sink方法，将数据落地到我们想存储的地方，这里虽看看起来没有Spark那么灵活，要么foreachRDD，foreachPartition遍历输出，或者是第三方组件方法进行输出，但是Flink这样设计更有利于工程师降低代码复杂度，更多精力去关心业务开发就可以。一些比较基本的 Sink 已经内置在 Flink 里。数据接收器使用DataStream，并将其转发到文件，套接字，外部系统或打印它们。Flink带有各种内置的输出格式，这些格式封装在DataStream的操作后面：writeAsText()/ TextOutputFormat-将元素按行写为字符串。通过调用每个元素的toString（）方法获得字符串。writeAsCsv(…)/ CsvOutputFormat-将元组写为逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的toString（）方法。print()/ printToErr() - 在标准输出/标准错误流上打印每个元素的toString（）值。可选地，可以提供前缀（msg），该前缀在输出之前。这可以帮助区分不同的打印调用。如果并行度大于1，则输出之前还将带有产生输出的任务的标识符。writeUsingOutputFormat()/ FileOutputFormat-自定义文件输出的方法和基类。支持自定义对象到字节的转换。writeToSocket -根据以下内容将元素写入套接字 SerializationSchemaaddSink-调用自定义接收器功能。Flink捆绑有连接到其他系统（例如Apache Kafka）的连接器，这些连接器实现为接收器功能。请注意，上的write*()方法DataStream主要用于调试目的。它们不参与Flink的检查点，这意味着这些功能通常具有至少一次的语义。刷新到目标系统的数据取决于OutputFormat的实现。这意味着并非所有发送到OutputFormat的元素都立即显示在目标系统中。同样，在失败的情况下，这些记录可能会丢失。为了将流可靠，准确地一次传输到文件系统中，请使用flink-connector-filesystem。同样，通过该.addSink(…)方法的自定义实现可以参与Flink一次精确语义的检查点。Sink原理SinkFunction 是一个接口，类似于SourceFunction接口。SinkFunction中主要包含一个方法，那就是用于数据输出的invoke 方法,每条记录都会执行一次invoke方法，用于执行输出操作。123456789101112131415// Writes the given value to the sink. This function is called for every record.default void invoke(IN value) throws Exception default void invoke(IN value, Context context) throws Exception // Context接口中返回关于时间的信息interface Context&lt;T&gt; &#123; /** Returns the current processing time. */ long currentProcessingTime(); /** Returns the current event-time watermark. */ long currentWatermark(); /** * Returns the timestamp of the current input record or &#123;@code null&#125; if the element does not * have an assigned timestamp. */ Long timestamp(); &#125;我们一般自定义Sink的时候，都是继承AbstractRichFunction，他是一个抽象类，实现了RichFunction接口。1public abstract class AbstractRichFunction implements RichFunction, Serializable并且提供了关于RuntimContext的操作和open，clone方法。AbstractRichFunction 有很多实现类，比如关于msyql操作的JDBCSinkFunction，比如直接输出结果的 PrintSinkFunction，在我们开发的过程中，我们进程用print语句来打印结果，但是print函数中就是讲PrintSinkFunction类传递到addSink方法中。1234public DataStreamSink&lt;T&gt; print() &#123; PrintSinkFunction&lt;T&gt; printFunction = new PrintSinkFunction&lt;&gt;(); return addSink(printFunction).name(\"Print to Std. Out\");&#125;PrintSinkFunction我们这里分析一下PrintSinkFunction这个类，这个类就是将没个元素输出到标准输出或者是标准错误输出流中。1234567891011121314151617181920212223242526272829303132333435363738394041public class PrintSinkFunction&lt;IN&gt; extends RichSinkFunction&lt;IN&gt; &#123; private static final long serialVersionUID = 1L; private final PrintSinkOutputWriter&lt;IN&gt; writer; /** * Instantiates a print sink function that prints to standard out. */ public PrintSinkFunction() &#123; writer = new PrintSinkOutputWriter&lt;&gt;(false); &#125; /** * Instantiates a print sink function that prints to standard out. * * @param stdErr True, if the format should print to standard error instead of standard out. */ public PrintSinkFunction(final boolean stdErr) &#123; writer = new PrintSinkOutputWriter&lt;&gt;(stdErr); &#125; /** * Instantiates a print sink function that prints to standard out and gives a sink identifier. * * @param stdErr True, if the format should print to standard error instead of standard out. * @param sinkIdentifier Message that identify sink and is prefixed to the output of the value */ public PrintSinkFunction(final String sinkIdentifier, final boolean stdErr) &#123; writer = new PrintSinkOutputWriter&lt;&gt;(sinkIdentifier, stdErr); &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext(); writer.open(context.getIndexOfThisSubtask(), context.getNumberOfParallelSubtasks()); &#125; @Override public void invoke(IN record) &#123; writer.write(record); &#125; @Override public String toString() &#123; return writer.toString(); &#125;&#125;分析：1、调用构造函数来创建一个PrintSinkOutputWriter2、调用open方法中在调用PrintSinkOutputWriter 的open方法，进行初始化3、调用invoke方法，通过PrintSinkOutputWriter 的writer方法吧record输出第三方Sink下面我们将使用 Elasticsearch Connector 作为Sink 为例示范Sink的使用。Elasticsearch Connector 提供了 *at least once *语义支持，at lease once 支持需要用到Flink的checkpoint 机制。要使用Elasticsearch Connector 需要根据Elasticsearch 版本添加依赖，版本参考12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;ES版本6.7， 写入示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package data_streamimport java.net.&#123;InetAddress, InetSocketAddress&#125;import java.utilimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.api.common.functions.RuntimeContextimport org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.elasticsearch.util.IgnoringFailureHandlerimport org.apache.flink.streaming.connectors.elasticsearch.&#123;ActionRequestFailureHandler, ElasticsearchSinkFunction, RequestIndexer&#125;import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.util.ExceptionUtilsimport org.apache.http.HttpHostimport org.elasticsearch.ElasticsearchParseExceptionimport org.elasticsearch.action.ActionRequestimport org.elasticsearch.action.index.IndexRequestimport org.elasticsearch.client.Requestsimport org.elasticsearch.common.util.concurrent.EsRejectedExecutionExceptionimport utils.KafkaUtilobject datastream_2_es_HandFail &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val httpHosts = new java.util.ArrayList[HttpHost] httpHosts.add(new HttpHost(\"127.0.0.1\", 9200, \"http\")) val esSinkBuilder = new ElasticsearchSink.Builder[Raw] ( httpHosts, new ElasticsearchSinkFunction[Raw] &#123; override def process(data: Raw, runtimeContext: RuntimeContext, requestIndexer: RequestIndexer): Unit = &#123; print(\"saving data\" + data) //包装成一个Map或者JsonObject val hashMap = new util.HashMap[String, String]() hashMap.put(\"date_time\", data.date_time) hashMap.put(\"keyword_list\", data.keywordList) hashMap.put(\"rowkey\", \"i am rowkey haha\") //创建index request,准备发送数据 val indexRequest: IndexRequest = Requests.indexRequest().index(\"weibo_keyword-2018-04-30\").`type`(\"default\").source(hashMap) //发送请求,写入数据 requestIndexer.add(indexRequest) println(\"data saved successfully\") &#125; &#125; ) esSinkBuilder.setBulkFlushMaxActions(2) esSinkBuilder.setBulkFlushInterval(1000L) // 自定义异常处理 esSinkBuilder.setFailureHandler(new ActionRequestFailureHandler &#123; override def onFailure(actionRequest: ActionRequest, throwable: Throwable, i: Int, requestIndexer: RequestIndexer): Unit = &#123; println(\"@@@@@@@On failure from ElasticsearchSink:--&gt;\" + throwable.getMessage) &#125; &#125;) word_stream.addSink(esSinkBuilder.build()) env.execute(\"Flink Streaming—————es sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;自定义Sinkmysql在向MySQL中写数据的时候，千万不要忘记引入驱动依赖：12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt;&lt;/dependency&gt;示例代码：自定义Sink123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.sql.&#123;Connection, DriverManager, PreparedStatement, SQLException&#125;import data_stream.datastream_2_mysql.Rawimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;import org.slf4j.&#123;Logger, LoggerFactory&#125;class MysqlSink_v2 extends RichSinkFunction[Raw] &#123; val logger: Logger = LoggerFactory.getLogger(\"MysqlSink\") var conn: Connection = _ var ps: PreparedStatement = _ val jdbcUrl = \"jdbc:mysql://localhost:3306?useSSL=false&amp;allowPublicKeyRetrieval=true\" val username = \"root\" val password = \"password\" val driverName = \"com.mysql.jdbc.Driver\" override def open(parameters: Configuration): Unit = &#123; Class.forName(driverName) try &#123; Class.forName(driverName) conn = DriverManager.getConnection(jdbcUrl, username, password) // close auto commit conn.setAutoCommit(false) &#125; catch &#123; case e@(_: ClassNotFoundException | _: SQLException) =&gt; logger.error(\"init mysql error\") e.printStackTrace() System.exit(-1); &#125; &#125; /** * 吞吐量不够话，可以将数据暂存在状态中，批量提交的方式提高吞吐量（如果oom，可能就是数据量太大，资源没有及时释放导致的） * @param raw * @param context */ override def invoke(raw: Raw, context: SinkFunction.Context[_]): Unit = &#123; println(\"data : \" + raw.toString) ps = conn.prepareStatement(\"insert into test.t_weibo_keyword(date_time,keywordList) values(?,?)\") ps.setString(1, raw.date_time) ps.setString(2, raw.keywordList) ps.execute() conn.commit() &#125; override def close(): Unit = &#123; if (conn != null)&#123; conn.commit() conn.close() &#125; &#125;&#125;主程序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport utils.KafkaUtilobject datastream_2_mysql &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val sink = new MysqlSink_v2() word_stream.addSink(sink) env.execute(\"Flink Streaming—————mysql sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;HBase自定义sink1234567891011121314151617181920212223242526272829303132333435363738package data_streamimport data_stream.datastream_2_hbase.Rawimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;import org.apache.hadoop.hbase.&#123;HBaseConfiguration, HConstants, TableName&#125;import org.apache.hadoop.hbase.client._import org.apache.hadoop.hbase.util.Bytesclass HBaseSink_v4(tableName: String, family: String) extends RichSinkFunction[Raw] &#123; var conn: Connection = _ override def open(parameters: Configuration): Unit = &#123; super.open(parameters) val conf = HBaseConfiguration.create() conf.set(HConstants.ZOOKEEPER_QUORUM, \"localhost:2181\") conn = ConnectionFactory.createConnection(conf) &#125; override def invoke(value: Raw, context: SinkFunction.Context[_]): Unit = &#123; println(value) val t: Table = conn.getTable(TableName.valueOf(tableName)) val put: Put = new Put(Bytes.toBytes(value.date_time)) put.addColumn(Bytes.toBytes(family), Bytes.toBytes(\"name\"), Bytes.toBytes(value.date_time.toString)) put.addColumn(Bytes.toBytes(family), Bytes.toBytes(\"id_no\"), Bytes.toBytes(value.keywordList.toString)) t.put(put) t.close() &#125; override def close(): Unit = &#123; super.close() conn.close() &#125;&#125;主程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.datastream.DataStreamSinkimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport utils.KafkaUtilobject datastream_2_hbase &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer_hbase\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) env.enableCheckpointing(5000) env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource/hbase\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic).setStartFromLatest() val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) val value: DataStreamSink[Raw] = word_stream.addSink(new HBaseSink_v4(\"t_weibo_keyword_2\",\"cf1\")).name(\"write_2_hbase\") env.execute(\"Flink Streaming—————hbase sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;Redis12345&lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;创建Sink工具类123456789101112131415161718192021222324package utilsimport data_stream.datastream_2_redis.Rawimport org.apache.flink.streaming.connectors.redis.RedisSinkimport org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfigimport org.apache.flink.streaming.connectors.redis.common.mapper.&#123;RedisCommand, RedisCommandDescription, RedisMapper&#125;object RedisUtil &#123; private val config: FlinkJedisPoolConfig = new FlinkJedisPoolConfig.Builder().setHost(\"127.0.0.1\").setPort(6379).build() def getRedisSink(): RedisSink[Raw] = &#123; new RedisSink[Raw](config, new MyRedisMapper) &#125; class MyRedisMapper extends RedisMapper[Raw] &#123; override def getCommandDescription: RedisCommandDescription = &#123; new RedisCommandDescription(RedisCommand.HSET, \"weibo_keyword\") &#125; // value override def getValueFromData(t: Raw): String = t.keywordList //key override def getKeyFromData(t: Raw): String = t.date_time &#125;&#125;主程序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseimport org.apache.flink.streaming.connectors.redis.RedisSinkimport utils.&#123;KafkaUtil, RedisUtil&#125;object datastream_2_redis &#123; case class Raw(date_time: String, keywordList: String) private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val topic: String = \"weibo_keyword\" val kafkaSource: FlinkKafkaConsumerBase[String] = KafkaUtil.getKafkaSource(topic) val word_stream: DataStream[Raw] = env.addSource(kafkaSource) .map((x: String) =&gt; &#123; val date_time: String = get_value(x)._1 val keywordList: String = get_value(x)._2 Raw(date_time, keywordList) &#125;) word_stream.print(\"2_redis\").setParallelism(1) val redis_sink: RedisSink[Raw] = RedisUtil.getRedisSink() word_stream.addSink(redis_sink).name(\"write_2_redis\") env.execute(\"Flink Streaming—————redis sink\") &#125; def get_value(string_data: String): (String, String) = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"团灭 LeetCode 股票买卖问题","slug":"团灭-LeetCode-股票买卖问题","date":"2019-08-04T17:00:08.000Z","updated":"2020-07-04T17:01:29.086Z","comments":true,"path":"2019/08/05/团灭-LeetCode-股票买卖问题/","link":"","permalink":"cpeixin.cn/2019/08/05/%E5%9B%A2%E7%81%AD-LeetCode-%E8%82%A1%E7%A5%A8%E4%B9%B0%E5%8D%96%E9%97%AE%E9%A2%98/","excerpt":"","text":"思路介绍很多读者抱怨 LeetCode 的股票系列问题奇技淫巧太多，如果面试真的遇到这类问题，基本不会想到那些巧妙的办法，怎么办？所以本文拒绝奇技淫巧，而是稳扎稳打，只用一种通用方法解决所用问题，以不变应万变。这篇文章用状态机的技巧来解决，可以全部提交通过。不要觉得这个名词高大上，文学词汇而已，实际上就是 DP table，看一眼就明白了。PS：本文参考自英文版 LeetCode 的一篇题解。先随便抽出一道题，看看别人的解法：123456789101112int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.empty()) return 0; int s1=-prices[0],s2=INT_MIN,s3=INT_MIN,s4=INT_MIN; for(int i=1;i&lt;prices.size();++i) &#123; s1 = max(s1, -prices[i]); s2 = max(s2, s1+prices[i]); s3 = max(s3, s2-prices[i]); s4 = max(s4, s3+prices[i]); &#125; return max(0,s4);&#125;能看懂吧？会做了吗？不可能的，你看不懂，这才正常。就算你勉强看懂了，下一个问题你还是做不出来。为什么别人能写出这么诡异却又高效的解法呢？因为这类问题是有框架的，但是人家不会告诉你的，因为一旦告诉你，你五分钟就学会了，该算法题就不再神秘，变得不堪一击了。本文就来告诉你这个框架，然后带着你一道一道秒杀。这篇文章用状态机的技巧来解决，可以全部提交通过。不要觉得这个名词高大上，文学词汇而已，实际上就是 DP table，看一眼就明白了。这 6 道题目是有共性的，我就抽出来第 4 道题目，因为这道题是一个最泛化的形式，其他的问题都是这个形式的简化，看下题目：第一题是只进行一次交易，相当于 k = 1；第二题是不限交易次数，相当于 k = +infinity（正无穷）；第三题是只进行 2 次交易，相当于 k = 2；剩下两道也是不限次数，但是加了交易「冷冻期」和「手续费」的额外条件，其实就是第二题的变种，都很容易处理。一、穷举框架首先，还是一样的思路：如何穷举？这里的穷举思路和上篇文章递归的思想不太一样。递归其实是符合我们思考的逻辑的，一步步推进，遇到无法解决的就丢给递归，一不小心就做出来了，可读性还很好。缺点就是一旦出错，你也不容易找到错误出现的原因。比如上篇文章的递归解法，肯定还有计算冗余，但确实不容易找到。而这里，我们不用递归思想进行穷举，而是利用「状态」进行穷举。我们具体到每一天，看看总共有几种可能的「状态」，再找出每个「状态」对应的「选择」。我们要穷举所有「状态」，穷举的目的是根据对应的「选择」更新状态。听起来抽象，你只要记住「状态」和「选择」两个词就行，下面实操一下就很容易明白了。1234for 状态1 in 状态1的所有取值： for 状态2 in 状态2的所有取值： for ... dp[状态1][状态2][...] = 择优(选择1，选择2...)比如说这个问题，每天都有三种「选择」：买入、卖出、无操作，我们用 buy, sell, rest 表示这三种选择。但问题是，并不是每天都可以任意选择这三种选择的，因为 sell 必须在 buy 之后，buy 必须在 sell 之后。那么 rest 操作还应该分两种状态，一种是 buy 之后的 rest（持有了股票），一种是 sell 之后的 rest（没有持有股票）。而且别忘了，我们还有交易次数 k 的限制，就是说你 buy 还只能在 k &gt; 0 的前提下操作。很复杂对吧，不要怕，我们现在的目的只是穷举，你有再多的状态，老夫要做的就是一把梭全部列举出来。这个问题的「状态」有三个，第一个是天数，第二个是允许交易的最大次数，第三个是当前的持有状态（即之前说的 rest 的状态，我们不妨用 1 表示持有，0 表示没有持有）。然后我们用一个三维数组就可以装下这几种状态的全部组合：12345678dp[i][k][0 or 1]0 &lt;= i &lt;= n-1, 1 &lt;= k &lt;= Kn 为天数，大 K 为最多交易数此问题共 n × K × 2 种状态，全部穷举就能搞定。for i in range(n): for k in range(1,K+1): for s in &#123;0, 1&#125;: dp[i][k][s] = max(buy, sell, rest)而且我们可以用自然语言描述出每一个状态的含义，比如说 dp[3][2][1] 的含义就是：今天是第三天，我现在手上持有着股票，至今最多进行 2 次交易。再比如 dp[2][3][0] 的含义：今天是第二天，我现在手上没有持有股票，至今最多进行 3 次交易。很容易理解，对吧？我们想求的最终答案是 dp[n - 1][K][0]，即最后一天，最多允许 K 次交易，最多获得多少利润。读者可能问为什么不是 dp[n - 1][K][1]？因为 [1] 代表手上还持有股票，[0] 表示手上的股票已经卖出去了，很显然后者得到的利润一定大于前者。记住如何解释「状态」，一旦你觉得哪里不好理解，把它翻译成自然语言就容易理解了。二、状态转移框架现在，我们完成了「状态」的穷举，我们开始思考每种「状态」有哪些「选择」，应该如何更新「状态」。只看「持有状态」，可以画个状态转移图。通过这个图可以很清楚地看到，每种状态（0 和 1）是如何转移而来的。根据这个图，我们来写一下状态转移方程：12dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]) #￥max( 选择 rest , 选择 sell )解释：今天我没有持有股票，有两种可能：要么是我昨天就没有持有，然后今天选择 rest，所以我今天还是没有持有；要么是我昨天持有股票，但是今天我 sell 了，所以我今天没有持有股票了。12dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) #max( 选择 rest , 选择 buy )解释：今天我持有着股票，有两种可能：要么我昨天就持有着股票，然后今天选择 rest，所以我今天还持有着股票；要么我昨天本没有持有，但今天我选择 buy，所以今天我就持有股票了。这个解释应该很清楚了，如果 buy，就要从利润中减去 prices[i]，如果 sell，就要给利润增加 prices[i]。今天的最大利润就是这两种可能选择中较大的那个。而且注意 k 的限制和变化，根据题意，**允许完成一笔交易（即买入和卖出一支股票一次），**我们在选择 buy 的时候，把 k 减小了 1，所以在卖出的时候就不用减小1了，很好理解吧，当然你也可以在 sell 的时候减 1，一样的。**现在，我们已经完成了动态规划中最困难的一步：状态转移方程。如果之前的内容你都可以理解，那么你已经可以秒杀所有问题了，只要套这个框架就行了。不过还差最后一点点，就是定义 base case，即最简单的情况。12345678dp[-1][k][0] = 0解释：因为 i 是从 0 开始的，所以 i = -1 意味着还没有开始，这时候的利润当然是 0 。dp[-1][k][1] = -infinity解释：还没开始的时候，是不可能持有股票的，用负无穷表示这种不可能。dp[i][0][0] = 0解释：因为 k 是从 1 开始的，所以 k = 0 意味着根本不允许交易，这时候利润当然是 0 。dp[i][0][1] = -infinity解释：不允许交易的情况下，是不可能持有股票的，用负无穷表示这种不可能。把上面的状态转移方程总结一下：123456base case：dp[-1][k][0] = dp[i][0][0] = 0dp[-1][k][1] = dp[i][0][1] = -infinity状态转移方程：dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i])读者可能会问，这个数组索引是 -1 怎么编程表示出来呢，负无穷怎么表示呢？这都是细节问题，有很多方法实现。现在完整的框架已经完成，下面开始具体化。以上是labuladong讲解的状态方程变化，我觉得讲的很不错，至少我从不知道状态方程是什么，到已经理解了。下面的题解部分，我有修改成python，并且根据自己能更好理解的方式进行了修改，labuladong的题解是采用了空间复杂度O(1)的方法，使用固定的两个变量来转换。而我下面的方式是dp[i][0]这种方式，这种方式我写的时候思路更清晰。但是这种方式的空间复杂度貌似是O(n)的。三、秒杀题目第一题，k = 1直接套状态转移方程，根据 base case，可以做一些化简：123dp[i][1][0] &#x3D; max(dp[i-1][1][0], dp[i-1][1][1] + prices[i])dp[i][1][1] &#x3D; max(dp[i-1][1][1], dp[i-1][0][0] - prices[i]) &#x3D; max(dp[i-1][1][1], -prices[i])解释：k = 0 的 base case，所以 dp[i-1][0][0] = 0。现在发现 k 都是 1，不会改变，即 k 对状态转移已经没有影响了。可以进行进一步化简去掉所有 k：12dp[i][0] &#x3D; max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] &#x3D; max(dp[i-1][1], -prices[i])直接写出代码：123456n = len(prices)dp = [[]]for i in range(n): dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i]) dp[i][1] = max(dp[i-1][1], -prices[i])return dp[n - 1][0]显然 i = 0 时 dp[i-1] 是不合法的。这是因为我们没有对 i 的 base case 进行处理。可以这样处理：1234567891011121314151617for i in range(n): if i - 1 == -1: dp[i][0] = 0; # 解释： # dp[i][0] = max(dp[-1][0], dp[-1][1] + prices[i]) # = max(0, -infinity + prices[i]) = 0 dp[i][1] = -prices[i] #解释： # dp[i][1] = max(dp[-1][1], dp[-1][0] - prices[i]) # = max(-infinity, 0 - prices[i]) # = -prices[i] continue dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] + prices[i]) dp[i][1] = Math.max(dp[i-1][1], -prices[i])return dp[n - 1][0]第一题就解决了，但是这样处理 base case 很麻烦，而且注意一下状态转移方程，新状态只和相邻的一个状态有关，其实不用整个 dp 数组，只需要一个变量储存相邻的那个状态就足够了，这样可以把空间复杂度降到 O(1):12345678910111213// k == 1def maxProfit(self, prices): n = len(prices) // base case: dp[-1][0] = 0, dp[-1][1] = -infinity dp_i_0 = 0 dp_i_1 = float('-inf'); for (int i = 0; i &lt; n; i++) &#123; // dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i]) dp_i_0 = Math.max(dp_i_0, dp_i_1 + prices[i]); // dp[i][1] = max(dp[i-1][1], -prices[i]) dp_i_1 = Math.max(dp_i_1, -prices[i]); return dp_i_0;两种方式都是一样的，不过这种编程方法简洁很多。但是如果没有前面状态转移方程的引导，是肯定看不懂的。后续的题目，我主要写这种空间复杂度 O(1) 的解法。没有使用变量的python代码：12345678910111213141516class Solution(object): def maxProfit(self, prices): \"\"\" :type prices: List[int] :rtype: int \"\"\" n = len(prices) if n&lt;=1: return 0 dp = [[None, None] for _ in range(n)] # base case: dp[0][0] = 0 dp[0][1] = -prices[0] for i in range(1, n): dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i]) dp[i][1] = max(dp[i-1][1], -prices[i]) return dp[-1][0]第二题，k = +infinity每天都有三种动作：买入（buy）、卖出（sell）、无操作（rest）。因为不限制交易次数，因此交易次数这个因素不影响题目，不必考虑。DP Table 是二维的，两个维度分别是天数（0,1,…,n-1）和是否持有股票（1 表持有，0 表不持有）。状态转移方程Case 1，今天我没有股票，有两种可能：**昨天我手上就没有股票，今天不做任何操作（rest）；昨天我手上有一只股票，今天按照时价卖掉了（sell），收获了一笔钱Case 2，今天持有一只股票，有两种可能：昨天我手上就有这只股票，今天不做任何操作（rest）；昨天我没有股票，今天按照时价买入一只（sell），花掉了一笔钱综上，第 i 天的状态转移方程为：12dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] = max(dp[i-1][1], dp[i-1][0] - prices[i])注意上面的转移方程只是对某一天而言的，要求出整个 DP Table 的状态，需要对 i 进行遍历。边界状态观察状态转移方程，第 i 天的状态只由第 i-1 天状态推导而来，因此边界状态只需要定义 i=0（也就是第一天）即可：12dp[0][0] = 0 # 第一天没有股票，说明没买没卖，获利为0dp[0][1] = -prices[0] # 第一天持有股票，说明买入了，花掉一笔钱代码12345678910111213141516171819class Solution: def maxProfit(self, prices): n = len(prices) if n&lt;=1: return 0 # dp table dp = [[None, None] for _ in range(n)] \"\"\" 边界条件，初始条件 第 i 天的状态只由第 i-1 天状态推导而来，因此边界状态只需要定义 i=0（也就是第一天） \"\"\" dp[0][0] = 0 dp[0][1] = -prices[0] # for i in range(1, n): dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i]) dp[i][1] = max(dp[i-1][1], dp[i-1][0] - prices[i]) return dp[-1][0]# 返回最后一天且手上没有股票时的获利情况第三题，k = +infinity with cooldown题解：这道题的在 买卖股票的最佳时机 II 的基础上添加了冷冻期的要求，即每次 sell 之后要等一天才能继续交易。状态转移方程要做修改，如果第 i 天选择买入股票，状态要从第 i-2 的转移，而不是 i-1 (因为第 i-1 天是冷冻期)。另外，由于状态转移方程中出现了 dp[i-2] 推导 dp[i-1]，因此状态边界除了考虑 i=0 天，还要加上 i=1 天的状态。Solution 如下代码：123456789101112131415161718192021222324class Solution(object): def maxProfit(self, prices): \"\"\" :type prices: List[int] :rtype: int \"\"\" n = len(prices) if n &lt;= 1: return 0 dp = [[None, None] for _ in range(n)] \"\"\" 如果第 i 天选择买入股票，状态要从第 i-2 的转移，而不是 i-1 (因为第 i-1 天是冷冻期)。 另外，由于状态转移方程中出现了 dp[i-2] 推导 dp[i-1]，因此状态边界除了考虑 i=0 天，还要加上 i=1 天的状态。 \"\"\" dp[0][0] = 0 dp[0][1] = -prices[0] dp[1][0] = max(dp[0][0], dp[0][1] + prices[1]) dp[1][1] = max(dp[0][1], dp[0][0] - prices[1]) for i in range(2, n): dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i]) dp[i][1] = max(dp[i-1][1], dp[i-2][0]-prices[i]) return dp[-1][0]第四题，k = +infinity with fee每次交易要支付手续费，只要把手续费从利润中减去即可。改写方程：1234dp[i][0] &#x3D; max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] &#x3D; max(dp[i-1][1], dp[i-1][0] - prices[i] - fee)解释：相当于买入股票的价格升高了。在第一个式子里减也是一样的，相当于卖出股票的价格减小了。直接翻译成代码：12345678910111213class Solution: def maxProfit(self, prices: List[int], fee: int) -&gt; int: n = len(prices) if n&lt;=1: return 0 dp = [[None, None] for _ in range(n)] dp[0][0] = 0 dp[0][1] = -prices[0] for i in range(1, n): dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i]-fee) # 卖出股票时注意要缴手续费 dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i]) return dp[-1][0]第五题，k = 2题目约定最多交易次数 k=2，因此交易次数必须作为一个新的维度考虑进 DP Table 里，也就是说，这道题需要三维 DP 来解决。三个维度分别为：天数 i（i=0,1,…,n-1），买入股票的次数 j（j=1,2,…,k）和是否持有股票（1 表持有，0 表不持有）. 特别注意买入股票的次数为 j 时，其实隐含了另一个信息，就是卖出股票的次数为 j-1 或 j 次。状态转移方程，这里还是比较难懂，读了几遍，我的理解是，下面第一行代码，右边表示的是，昨天持有股票，但是今天没有持有，所以卖掉，卖掉没有影响k的次数。但是第二行代码的右边，昨天没有持有股票而今天有持有，则是今天🈶购买股票，昨天没有购买的情况下，则对应的k为k-11234dp[i][j][0] = max(dp[i-1][j][0], dp[i-1][j][1]+prices[i]) # 右边:今天卖了昨天持有的股票，所以两天买入股票的次数都是jdp[i][j][1] = max(dp[i-1][j][1], dp[i-1][j-1][0]-prices[i]) # 右边:昨天没有持股而今天买入一只，故昨天买入的次数是j-1注意上面的转移方程只是穷举了第三个维度，要求出整个 DP Table 的状态，需要对 i 和 j 进行遍历。边界状态观察状态转移方程知，边界状态需要考虑两个方面：i=0 和 j=012345678# j=0 for i in range(n): dp[i][0][0] = 0 # 没有买入过股票，且手头没有持股，则获取的利润为0 dp[i][0][1] = -float('inf') # 没有买入过股票，不可能持股，用利润负无穷表示这种不可能# i=0for j in range(1, k+1): # 前面j=0已经赋值了，这里j从1开始 dp[0][k][0] = 0 dp[0][k][1] = -prices[0]特别注意，上述两轮边界定义有交集——dp[0][0][0] 和 dp[0][0][1] ，后者会得到不同的结果，应以 j=0 时赋值结果为准。1234567891011121314151617181920class Solution: def maxProfit(self, prices: List[int]) -&gt; int: n = len(prices) if n&lt;=1: return 0 # dp table dp = [[[None, None] for _ in range(3)] for _ in range(n)] \"\"\"边界条件，分别为i=0, k=0\"\"\" for i in range(n): dp[i][0][0] = 0 dp[i][0][1] = float('-inf') for k in range(3): dp[0][k][0] = 0 dp[0][k][1] = -prices[0] # 这里注意：i=0， k=0在上面已经计算过了。所以这里的下标从1开始 for i in range(1,n): for k in range(1, 3): dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]) dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) return dp[-1][-1][0]第六题，k = any integer**有了上一题 k = 2 的铺垫，这题应该和上一题的第一个解法没啥区别。但是出现了一个超内存的错误，原来是传入的 k 值会非常大，dp 数组太大了。现在想想，交易次数 k 最多有多大呢？一次交易由买入和卖出构成，至少需要两天。所以说有效的限制 k 应该不超过 n/2，如果超过，就没有约束作用了，相当于 k = +infinity。这种情况是之前解决过的。直接把之前的代码重用：123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): def maxProfit(self, k, prices): \"\"\" :type k: int :type prices: List[int] :rtype: int \"\"\" n = len(prices) if n&lt;=1: return 0 \"\"\" 正常交易的情况下，完成一次交易最少也需要两天的时间。所以有效的交易次数应该小于等于 n//2 如果大于 n//2 ，则此种情况退化为可交易任意次的情况 \"\"\" if k &gt; n // 2: dp_1 = [[None, None] for _ in range(n)] # # 边界条件 dp_1[0][0] = 0 dp_1[0][1] = -prices[0] for i in range(1, n): dp_1[i][0] = max(dp_1[i - 1][0], dp_1[i - 1][1] + prices[i]) dp_1[i][1] = max(dp_1[i - 1][1], dp_1[i - 1][0] - prices[i]) return dp_1[-1][0] else: dp_2 = [[[None, None] for _ in range(k+1)] for _ in range(n)] \"\"\"边界条件，分别为i=0, k=0\"\"\" for i in range(n): dp_2[i][0][0] = 0 dp_2[i][0][1] = float('-inf') for k in range(k+1): dp_2[0][k][0] = 0 dp_2[0][k][1] = -prices[0] for i in range(1, n): for k in range(1, k+1): dp_2[i][k][0] = max(dp_2[i-1][k][0], dp_2[i-1][k][1] + prices[i]) dp_2[i][k][1] = max(dp_2[i-1][k][1], dp_2[i-1][k-1][0] - prices[i]) return dp_2[-1][-1][0]至此，6 道题目通过一个状态转移方程全部解决。四、最后总结本文给大家讲了如何通过状态转移的方法解决复杂的问题，用一个状态转移方程秒杀了 6 道股票买卖问题，现在想想，其实也不算难对吧？这已经属于动态规划问题中较困难的了。关键就在于列举出所有可能的「状态」，然后想想怎么穷举更新这些「状态」。一般用一个多维 dp 数组储存这些状态，从 base case 开始向后推进，推进到最后的状态，就是我们想要的答案。想想这个过程，你是不是有点理解「动态规划」这个名词的意义了呢？","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"cpeixin.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"Flink Allowed Lateness","slug":"Flink-Allowed-Lateness","date":"2019-08-01T14:36:27.000Z","updated":"2020-09-06T09:03:11.595Z","comments":true,"path":"2019/08/01/Flink-Allowed-Lateness/","link":"","permalink":"cpeixin.cn/2019/08/01/Flink-Allowed-Lateness/","excerpt":"","text":"在之前的博文中，我们讲到了Flink WaterMark机制来针对乱序数据的处理，并且设置了一个数值较小的延迟时间maxOutOfOrderness，通常这个允许的延迟时间是我们根据具体业务数据观察而来的，例如通过观察，我司的游戏数据大概会延迟2～3秒到达，所以针对我司的业务，maxOutOfOrderness可以设置为2秒。但是针对一些特殊的情况，游戏网关会有阻塞，发送出来的数据会有较大的延迟情况，例如某些数据会迟到20分钟到达，并且数据不能被抛弃，如果这时我们把maxOutOfOrderness设置为20分钟，那显然是不合适的，每个窗口的周期会变得超长，并且在内存中缓存了太多的数据。针对这种情况，Flink给出的解决方案是Allowed Lateness在allowedLateness()中会设置一个Time值，主要是为了等待迟到的数据，在一定时间范围内，如果属于该窗口的数据到来，仍会进行计算，后面会对计算方式仔细说明注意：该方法只针对于基于event-time的窗口，如果是基于processing-time，并且指定了非零的time值则会抛出异常默认情况下，允许的延迟设置为 0。也就是说，到达水印后的元素将被丢弃。获取最新数据作为侧面输出使用Flink的侧面输出流功能，您可以获取最近被丢弃的数据流。首先，您需要指定要sideOutputLateData(OutputTag)在窗口流上使用的较晚数据。然后，您可以根据窗口化操作的结果获取侧面输出流，获取到的侧输出流，你可以对历史数据进行更新操作：123456789101112val lateOutputTag = OutputTag[T](\"late-data\")val input: DataStream[T] = ...val result = input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .allowedLateness(&lt;time&gt;) .sideOutputLateData(lateOutputTag) .&lt;windowed transformation&gt;(&lt;window function&gt;)val lateStream = result.getSideOutput(lateOutputTag)下面给出一个完整的实例，需求背景，每5秒统计一次近10秒时间内，用户在各个游戏的得分列表，乱序数据允许2秒的延迟，对于晚期数据到达，allowedLateness允许20秒的延迟，并且对晚期数据进行收集，后续和直接对接数据库更新。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package data_stream.watermarkimport org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractorimport org.apache.flink.streaming.api.scala.function.&#123;ProcessWindowFunction, WindowFunction&#125;import org.apache.flink.streaming.api.scala.&#123;DataStream, OutputTag, StreamExecutionEnvironment, _&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.api.windowing.windows.TimeWindowimport org.apache.flink.util.Collectorimport scala.collection.mutable.ListBufferobject watermark_allowedLateness &#123; case class GameData(user_id: String, game_id: String, game_time: Long, game_score: Int) def main(args: Array[String]): Unit = &#123; val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) env.getConfig.setAutoWatermarkInterval(100L) //watermark周期 val socketStream: DataStream[String] = env.socketTextStream(\"localhost\", 8888) val gameStream: DataStream[GameData] = socketStream.map((line: String) =&gt; &#123; val array_data: Array[String] = line.split(\",\") GameData(array_data(0), array_data(1), array_data(2).toLong, array_data(3).toInt) &#125;) .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[GameData](Time.seconds(2)) &#123; override def extractTimestamp(element: GameData) = &#123; element.game_time &#125; &#125;) var gameLateData = new OutputTag[GameData](\"late\") val windowStream: DataStream[(String, List[Int])] = gameStream .keyBy((_: GameData).user_id) .timeWindow(Time.seconds(10), Time.seconds(5)) // 数据延迟超过2秒，交给allowedLateness来处理 .allowedLateness(Time.seconds(20)) .sideOutputLateData(gameLateData) // 接下里会发生三种情况： // 1.没有延迟或者延迟小于2秒的数据，watermark保证窗口的触发，正常进入窗口中计算 // 2.数据延迟在2秒-20秒之间的数据，watermark+allowedLateness机制，watermark &lt; window_end_time + allowedLateness_time时触发窗口 // 3.数据延迟大于20秒的数据，则输入到侧输出流处理sideOutputLateData //第一种方法 // .process(new ProcessWindowFunction[GameData, (String, List[Int]), String, TimeWindow] &#123; // override def process(key: String, context: Context, elements: Iterable[GameData], out: Collector[(String, List[Int])]): Unit = &#123; // val scoreList: ListBuffer[Int] = ListBuffer[Int]() // val scoreiterator: Iterator[GameData] = elements.iterator // while (scoreiterator.hasNext) &#123; // val data: GameData = scoreiterator.next() // scoreList += data.game_score // &#125; // out.collect((key, scoreList.toList)) // &#125; // &#125;) //第二种方法 .apply(new WindowFunction[GameData, (String, List[Int]), String, TimeWindow] &#123; override def apply(key: String, window: TimeWindow, input: Iterable[GameData], out: Collector[(String, List[Int])]): Unit = &#123; val scoreList: ListBuffer[Int] = ListBuffer[Int]() val scoreiterator: Iterator[GameData] = input.iterator while (scoreiterator.hasNext) &#123; val data: GameData = scoreiterator.next() scoreList += data.game_score &#125; println(s\"$&#123;window.getStart&#125;=====$&#123;window.getEnd&#125;\") out.collect((key, scoreList.toList)) &#125; &#125;) windowStream.print(\"window data\") val late: DataStream[GameData] = windowStream.getSideOutput(gameLateData) late.print(\"迟到的数据:\") env.execute(this.getClass.getName) &#125;&#125;输入测试数据：12345678910111213141516171819202122232425262728293031user_001,g1,1591016001000,100user_001,g2,1591016002000,200user_001,g4,1591016004000,400user_001,g1,1591016005000,500user_001,g2,1591016006000,600user_001,g2,1591016007000,700user_001,g4,1591016009000,400user_001,g1,1591016010000,100user_001,g2,1591016011000,1100user_001,g4,1591016014000,1400user_001,g8,1591016008000,800（迟到没超20秒，触发）user_001,g8,1591016003000,300（迟到没超20秒，触发）user_001,g17,1591016017000,1700user_001,g43,1591016043000,4300user_001,g22,1591016022000,2200user_001,g21,1591016021000,2100user_001,g21,1591016019000,1900user_001,g1,1591016001000,100（迟到超20秒，没有触发）输出结果：1234567891011121314151617181920212223242526271591015995000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016005000window data&gt; (user_001,List(100, 200, 400))1591016000000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016010000window data&gt; (user_001,List(100, 200, 400, 500, 600, 700, 400))1591016000000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016010000window data&gt; (user_001,List(100, 200, 400, 500, 600, 700, 400, 800))1591016000000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016010000window data&gt; (user_001,List(100, 200, 400, 500, 600, 700, 400, 800, 300))1591015995000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016005000window data&gt; (user_001,List(100, 200, 400, 300))1591016005000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016015000window data&gt; (user_001,List(500, 600, 700, 400, 100, 1100, 1400, 800))1591016010000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016020000window data&gt; (user_001,List(100, 1100, 1400, 1700))1591016015000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016025000window data&gt; (user_001,List(1700))1591016020000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016030000window data&gt; (user_001,List(2200))1591016015000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016025000window data&gt; (user_001,List(1700, 2200))1591016020000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016030000window data&gt; (user_001,List(2200, 2100))1591016015000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016025000window data&gt; (user_001,List(1700, 2200, 2100))1591016015000&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;1591016025000window data&gt; (user_001,List(1700, 2200, 2100, 1900))迟到的数据:&gt; GameData(user_001,g1,1591016001000,100)","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink DataStream - Event Time","slug":"Flink-DataStream-Event-Time","date":"2019-07-25T10:09:11.000Z","updated":"2020-09-06T09:02:14.916Z","comments":true,"path":"2019/07/25/Flink-DataStream-Event-Time/","link":"","permalink":"cpeixin.cn/2019/07/25/Flink-DataStream-Event-Time/","excerpt":"","text":"Event Time / Processing Time / Ingestion TimeFlink 在流式传输程序中支持不同的_时间_概念。处理时间(Processing Time)：处理时间是指执行相应操作的机器的系统时间。当流式程序按处理时间运行时，所有基于时间的操作（如时间窗口）都将使用运行相应操作员的计算机的系统时钟。每小时处理时间窗口将包括系统时钟指示整小时的时间之间到达特定操作员的所有记录。例如，如果应用程序在9:15 am开始运行，则第一个每小时处理时间窗口将包括在9:15 am和10:00 am之间处理的事件，下一个窗口将包括在10:00 am和11:00 am之间处理的事件，依此类推。处理时间是最简单的时间概念，不需要流和机器之间的协调。它提供了最佳的性能和最低的延迟。但是，在分布式和异步环境中，处理时间不能提供确定性，因为它容易受到记录到达系统（例如从消息队列）到达系统的速度，记录在系统内部操作员之间流动的速度的影响。 以及中断（计划的或其他方式）。事件时间(Event time)：事件时间是每个事件在其生产设备上发生的时间。该时间通常在它们进入Flink之前嵌入到记录中，并且 可以从每个记录中提取_事件时间戳_。在事件时间中，时间的进度取决于数据，而不取决于任何挂钟。事件时间程序必须指定如何生成“ _事件时间水印”_，这是一种表示事件时间进展的机制。在理想情况下，事件时间处理将产生完全一致且确定的结果，而不管事件何时到达或它们的顺序如何。但是，除非已知事件是按时间戳（按时间戳）到达的，否则事件时间处理会在等待无序事件时产生一定的延迟。由于只能等待有限的时间，因此这限制了确定性事件时间应用程序的可用性。假设所有数据都已到达，事件时间操作将按预期方式运行，即使在处理无序或迟到的事件或重新处理历史数据时，也会产生正确且一致的结果。例如，每小时事件时间窗口将包含所有带有落入该小时事件时间戳的记录，无论它们到达的顺序或处理的时间。请注意，有时当事件时间程序实时处理实时数据时，它们将使用一些_处理时间_操作，以确保它们及时进行。摄取时间(Ingestion Time)：摄取时间是事件进入Flink的时间。在源操作员处，每个记录都将源的当前时间作为时间戳记，并且基于时间的操作（如时间窗口）引用该时间戳记。_摄取时间_从概念上讲介于_事件时间和处理时间之间_。与_处理时间_相比 ，它稍微贵一些，但结果却更可预测。由于 _摄取时间_使用稳定的时间戳（在源处分配了一次），因此对记录的不同窗口操作将引用相同的时间戳，而在_处理时间中，_每个窗口操作员都可以将记录分配给不同的窗口（基于本地系统时钟和任何运输延误）。与_事件时间_相比，_提取时间_程序无法处理任何乱序事件或迟到的数据，但是程序不必指定如何生成_水印_。在内部，_摄取时间与事件时间_非常相似，但是具有自动时间戳分配和自动水印生成功能。设置时间特征（Setting a Time Characteristic）Flink DataStream程序的第一部分通常设置基准_时间特征_。该设置定义了数据流源的行为方式（例如，是否分配时间戳），以及诸如的窗口操作应使用什么时间概念KeyedStream.timeWindow(Time.seconds(30))。以下示例显示了一个Flink程序，该程序在每小时的时间窗口中汇总事件。窗口的行为与时间特征相适应。1234567891011val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)// alternatively:// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))stream .keyBy( _.getUser ) .timeWindow(Time.hours(1)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...)请注意，为了在_事件时间中_运行此示例，程序需要使用直接为数据定义事件时间并自己发出水印的源，或者程序必须在源之后注入_Timestamp Assigner＆Watermark Generator_。这些功能描述了如何访问事件时间戳，以及事件流呈现出何种程度的乱序。以下部分描述了_时间戳和水印_背后的一般机制。有关如何在Flink DataStream API中使用时间戳分配和水印生成的指南，请参阅 生成时间戳/水印。现实世界中的时间是不一致的，在 flink 中被划分为事件时间，提取时间，处理时间三种。如果以 EventTime 为基准来定义时间窗口那将形成 EventTimeWindow,要求消息本身就应该携带 EventTime 如果以 IngesingtTime 为基准来定义时间窗口那将形成 IngestingTimeWindow,以 source 的 systemTime 为准。 如果以 ProcessingTime 基准来定义时间窗口那将形成 ProcessingTimeWindow，以 operator 的 systemTime 为准。以上三种选择是需要根据不同业务需求进行选择的，例如，在数据计算的过程中，需要考虑到网络延迟的原因，那么就应该选择setStreamTimeCharacteristic(TimeCharacteristic.EventTime)，相反，不需要考虑延迟原因的话，只考虑处理数据的时间，则可以选择另两个。Event Time and Watermarks支持事件时间的_流处理器需要一种测量事件时间进度的方法。例如，当事件时间超过一个小时结束时，需要通知构建每小时窗口的窗口操作员，以便该操作员可以关闭正在进行的窗口。_事件时间_可以独立于_处理时间_（由挂钟测量）进行。例如，在一个程序中，操作员的当前_事件时间_可能会稍微落后于_处理时间 （考虑到事件接收的延迟），而两者均以相同的速度进行。另一方面，另一个流媒体程序可以通过快速转发已经在Kafka主题（或另一个消息队列）中缓存的一些历史数据来在数周的事件时间内进行处理，而处理时间仅为几秒钟。Flink中衡量事件时间进度的机制是水印。水印作为数据流的一部分流动，并带有时间戳t。甲_水印（T）_宣布事件时间达到时间 吨该流，这意味着应该有从该流没有更多的元素与时间戳_T” &lt;= T_（即，具有时间戳的事件较旧的或等于水印）。下图显示了带有（逻辑）时间戳记的事件流，以及串联的水印。在此示例中，事件是按顺序排列的（相对于其时间戳），这意味着水印只是流中的周期性标记。水印对于_乱序_流至关重要，如下图所示，其中事件不是按其时间戳排序的。通常，水印是一种声明，即到流中的那个点，直到某个时间戳的所有事件都应该到达。一旦水印到达操作员，操作员就可以将其内部_事件时钟_提前到水印的值。请注意，事件时间是由新创建的一个（或多个）流元素从产生它们的事件或触发了创建这些元素的水印中继承的。并行流中的水印(Watermarks in Parallel Streams)水印在源函数处或源函数之后直接生成。源函数的每个并行子任务通常独立生成其水印。这些水印定义了该特定并行源处的事件时间。随着水印在流式传输程序中的流动，它们会提前到达其到达的运营商的事件时间。每当操作员提前其事件时间时，都会为其后续操作员在下游生成新的水印。一些运算符消耗多个输入流；例如，并集，或遵循_keyBy（…）或partition（…）_函数的运算符。该操作员的当前事件时间是其输入流的事件时间中的最小值。随着其输入流更新其事件时间，操作员也将更新。下图显示了流过并行流的事件和水印的示例，操作员跟踪事件时间。请注意，Kafka源支持按分区添加水印，您可以在此处阅读更多信息。后期元素某些元素可能会违反水印条件，这意味着即使在发生_水印（t）_之后，也会出现更多时间戳为_t’&lt;= t的_元素。实际上，在许多现实世界的设置中，某些元素可以任意延迟，从而无法指定某个事件时间戳记的所有元素都将发生的时间。此外，即使可以限制延迟，通常也不希望将水印延迟太多，因为这会导致事件时间窗的评估延迟过多。由于这个原因，流式传输程序可能会明确期望某些_后期_元素。延迟元素是在系统的事件时间时钟（由水印指示）已经经过延迟元素时间戳的时间之后到达的元素。有关如何在事件时间窗口中使用延迟元素的更多信息，请参见允许延迟。闲置来源当前，使用纯事件时间水印生成器，如果没有要处理的元素，则水印将无法进行。这意味着在输入数据存在间隙的情况下，事件时间将不会继续进行，例如不会触发窗口操作符，因此现有窗口将无法生成任何输出数据。为了避免这种情况，可以使用周期性的水印分配器，这些分配器不仅基于元素时间戳进行分配。一个示例解决方案可能是一个分配器，该分配器在一段时间内未观察到新事件之后切换为使用当前处理时间作为时间基础。可以使用将源标记为空闲SourceFunction.SourceContext#markAsTemporarilyIdle。有关详细信息，请参考此方法的Javadoc以及StreamStatus。调试水印请参阅“ 调试Windows和事件时间”部分以在运行时调试水印。运营商如何处理水印通常，要求操作员在将给定水印转发到下游之前对其进行完全处理。例如， WindowOperator将首先评估应触发哪个窗口，只有在产生了所有由水印触发的输出之后，水印本身才会被发送到下游。换句话说，由于水印的出现而产生的所有元素将在水印之前发出。相同的规则适用于TwoInputStreamOperator。但是，在这种情况下，操作员的当前水印被定义为其两个输入的最小值。生成时间戳/水印分配时间戳带有时间戳和水印的源函数时间戳记分配器/水印生成器每个Kafka分区的时间戳本节与在事件时间运行的程序有关。有关事件时间_， _处理时间和摄取时间的简介_，请参阅事件时间的简介。为了处理**_事件时间，流式传输程序需要相应地设置时间特征**。12val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)分配时间戳为了使用_事件时间_，Flink需要知道事件的_时间戳_，这意味着流中的每个元素都需要_分配_其事件时间戳。这通常是通过从元素的某个字段访问/提取时间戳来完成的。时间戳分配与生成水印齐头并进，水印告诉系统事件时间的进展。有两种分配时间戳和生成水印的方法：直接在数据流源中通过时间戳分配器/水印生成器：在Flink中，时间戳分配器还定义要发送的水印注意自1970年1月1日T00：00：00Z的Java时代以来，时间戳和水印都指定为毫秒。带有时间戳和水印的源函数流源可以将时间戳直接分配给它们产生的元素，并且它们还可以发出水印。完成此操作后，无需时间戳分配器。请注意，如果使用时间戳分配器，则源所提供的任何时间戳和水印都将被覆盖。要将时间戳直接分配给源中的元素，源必须使用上的collectWithTimestamp(...) 方法SourceContext。要生成水印，源必须调用该emitWatermark(Watermark)函数。下面是一个简单的示例_（非检查点）_源，该源分配时间戳并生成水印：12345678910override def run(ctx: SourceContext[MyType]): Unit &#x3D; &#123; while (&#x2F;* condition *&#x2F;) &#123; val next: MyType &#x3D; getNext() ctx.collectWithTimestamp(next, next.eventTimestamp) if (next.hasWatermarkTime) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime)) &#125; &#125;&#125;时间戳记分配器/水印生成器时间戳记分配器获取流并产生带有时间戳记的元素和水印的新流。如果原始流已经具有时间戳和/或水印，则时间戳分配器将覆盖它们。时间戳记分配器通常在数据源之后立即指定，但并非严格要求这样做。例如，一种常见的模式是在时间戳分配器之前解析（_MapFunction_）和筛选器（_FilterFunction_）。无论如何，都需要在事件时间的第一个操作（例如第一个窗口操作）之前指定时间戳分配器。作为一种特殊情况，当使用Kafka作为流作业的源时，Flink允许在源（或使用者）本身内部指定时间戳分配器/水印发射器。有关如何执行此操作的更多信息，请参见 Kafka Connector文档。注意：本节的其余部分介绍了程序员为了创建自己的时间戳提取器/水印发射器而必须实现的主要接口。要查看Flink附带的预实现提取器，请参阅“ 预定义时间戳提取器/水印发射器”页面。爪哇斯卡拉12345678910111213141516val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter())val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())withTimestampsAndWatermarks .keyBy( _.getGroup ) .timeWindow(Time.seconds(10)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...)**带有定期水印AssignerWithPeriodicWatermarks 分配时间戳并定期生成水印（可能取决于流元素，或仅基于处理时间）。通过定义生成水印的间隔（每n毫秒） ExecutionConfig.setAutoWatermarkInterval(...)。分配器的getCurrentWatermark()方法每次都会被调用，如果返回的水印非空且大于前一个水印，则将发出新的水印。在这里，我们显示了两个使用定期水印生成的时间戳分配器的简单示例。请注意，Flink随附与以下所示BoundedOutOfOrdernessTimestampExtractor类似的内容BoundedOutOfOrdernessGenerator，您可以在此处阅读有关内容。12345678910111213141516171819202122232425262728293031323334353637383940/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxOutOfOrderness = 3500L // 3.5 seconds var currentMaxTimestamp: Long = _ override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; val timestamp = element.getCreationTime() currentMaxTimestamp = max(timestamp, currentMaxTimestamp) timestamp &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound new Watermark(currentMaxTimestamp - maxOutOfOrderness) &#125;&#125;/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxTimeLag = 5000L // 5 seconds override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current time minus the maximum time lag new Watermark(System.currentTimeMillis() - maxTimeLag) &#125;&#125;**带标点的水印要在特定事件表明可能会生成新的水印时生成水印，请使用 AssignerWithPunctuatedWatermarks。对于此类，Flink将首先调用该extractTimestamp(...)方法为元素分配时间戳，然后立即checkAndGetNextWatermark(...)在该元素上调用该 方法。该checkAndGetNextWatermark(...)方法会传递该方法中分配的时间戳extractTimestamp(...) ，并可以决定是否要生成水印。每当该checkAndGetNextWatermark(...) 方法返回一个非空水印，并且该水印大于最新的先前水印时，就会发出新的水印。爪哇斯卡拉12345678910class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123; if (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null &#125;&#125;_注意：_可以在每个事件上生成水印。但是，由于每个水印都会在下游引起一些计算，因此过多的水印会降低性能。每个Kafka分区的时间戳当使用Apache Kafka作为数据源时，每个Kafka分区可能都有一个简单的事件时间模式（时间戳升序或有界乱序）。但是，在使用来自Kafka的流时，通常会并行使用多个分区，从而交错插入分区中的事件并破坏每个分区的模式（这是Kafka的客户客户端工作方式所固有的）。在这种情况下，您可以使用Flink的Kafka分区感知水印生成。使用该功能，将在Kafka使用者内部针对每个Kafka分区生成水印，并且按与合并水印在流shuffle上相同的方式合并每个分区的水印。例如，如果事件时间戳严格按照每个Kafka分区递增，则使用递增时间戳水印生成器生成按分区的水印 将产生完美的整体水印。下图显示了如何使用按kafka分区的水印生成，以及在这种情况下水印如何通过流数据流传播。123456val kafkaSource = new FlinkKafkaConsumer09[MyType](\"myTopic\", schema, props)kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123; def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp&#125;)val stream: DataStream[MyType] = env.addSource(kafkaSource)预定义的时间戳提取器/水印发射器如时间戳和水印处理中所述，Flink提供了抽象，允许程序员分配自己的时间戳并发出自己的水印。更具体地说，根据使用情况，可以通过实现AssignerWithPeriodicWatermarks和AssignerWithPunctuatedWatermarks接口之一来实现。简而言之，第一个将定期发出水印，而第二个则根据传入记录的某些属性发出水印，例如，每当流中遇到特殊元素时。为了进一步简化此类任务的编程工作，Flink附带了一些预先实现的时间戳分配器。本节提供了它们的列表。除了开箱即用的功能外，它们的实现还可以作为自定义实现的示例。时间戳递增的分配者_定期_生成水印的最简单的特殊情况是给定源任务看到的时间戳以升序出现的情况。在这种情况下，当前时间戳始终可以充当水印，因为没有更早的时间戳会到达。请注意，_每个并行数据源任务_只需要增加时间戳记即可。例如，如果在一个特定的设置中，一个并行数据源实例读取一个Kafka分区，则只需要在每个Kafka分区内将时间戳记递增。每当对并行流进行混洗，合并，连接或合并时，Flink的水印合并机制都会生成正确的水印。12val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )**分配器允许固定的延迟时间周期性水印生成的另一个示例是水印在流中看到的最大（事件时间）时间戳落后固定时间量的情况。这种情况包括预先知道流中可能遇到的最大延迟的场景，例如，当创建包含时间戳的元素的自定义源时，该时间戳在固定的时间段内传播以进行测试。对于这些情况，Flink提供了BoundedOutOfOrdernessTimestampExtractor，将用作参数maxOutOfOrderness，即在计算给定窗口的最终结果时允许元素延迟到被忽略之前的最长时间。延迟对应于的结果t - t_w，其中t元素的（事件时间）时间戳和t_w前一个水印的时间戳。如果lateness &gt; 0那么该元素将被认为是较晚的元素，默认情况下，在为其相应窗口计算作业结果时将其忽略。有关 使用延迟元素的更多信息，请参见有关允许延迟的文档。12val stream: DataStream[MyEvent] &#x3D; ...val withTimestampsAndWatermarks &#x3D; stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Kafka副本机制详解","slug":"Kafka副本机制详解","date":"2019-07-25T08:10:25.000Z","updated":"2020-09-02T08:11:39.144Z","comments":true,"path":"2019/07/25/Kafka副本机制详解/","link":"","permalink":"cpeixin.cn/2019/07/25/Kafka%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Apache Kafka 的副本机制。所谓的副本机制（Replication），也可以称之为备份机制，通常是指分布式系统在多台网络互联的机器上保存有相同的数据拷贝。副本机制有什么好处呢？提供数据冗余。即使系统部分组件失效，系统依然能够继续运转，因而增加了整体可用性以及数据持久性。提供高伸缩性。支持横向扩展，能够通过增加机器的方式来提升读性能，进而提高读操作吞吐量。改善数据局部性。允许将数据放入与用户地理位置相近的地方，从而降低系统延时。这些优点都是在分布式系统教科书中最常被提及的，但是有些遗憾的是，对于 Apache Kafka 而言，目前只能享受到副本机制带来的第 1 个好处，也就是提供数据冗余实现高可用性和高持久性。我会在这一讲后面的内容中，详细解释 Kafka 没能提供第 2 点和第 3 点好处的原因。不过即便如此，副本机制依然是 Kafka 设计架构的核心所在，它也是 Kafka 确保系统高可用和消息高持久性的重要基石。副本定义在讨论具体的副本机制之前，我们先花一点时间明确一下副本的含义。我们之前谈到过，Kafka 是有主题概念的，而每个主题又进一步划分成若干个分区。副本的概念实际上是在分区层级下定义的，每个分区配置有若干个副本。所谓副本（Replica），本质就是一个只能追加写消息的提交日志。根据 Kafka 副本机制的定义，同一个分区下的所有副本保存有相同的消息序列，这些副本分散保存在不同的 Broker 上，从而能够对抗部分 Broker 宕机带来的数据不可用。在实际生产环境中，每台 Broker 都可能保存有各个主题下不同分区的不同副本，因此，单个 Broker 上存有成百上千个副本的现象是非常正常的。接下来我们来看一张图，它展示的是一个有 3 台 Broker 的 Kafka 集群上的副本分布情况。从这张图中，我们可以看到，主题 1 分区 0 的 3 个副本分散在 3 台 Broker 上，其他主题分区的副本也都散落在不同的 Broker 上，从而实现数据冗余。副本角色既然分区下能够配置多个副本，而且这些副本的内容还要一致，那么很自然的一个问题就是：我们该如何确保副本中所有的数据都是一致的呢？特别是对 Kafka 而言，当生产者发送消息到某个主题后，消息是如何同步到对应的所有副本中的呢？针对这个问题，最常见的解决方案就是采用基于领导者（Leader-based）的副本机制。Apache Kafka 就是这样的设计。基于领导者的副本机制的工作原理如下图所示，我来简单解释一下这张图里面的内容。第一，在 Kafka 中，副本分成两类：领导者副本（Leader Replica）和追随者副本（Follower Replica）。每个分区在创建时都要选举一个副本，称为领导者副本，其余的副本自动称为追随者副本。第二，Kafka 的副本机制比其他分布式系统要更严格一些。在 Kafka 中，追随者副本是不对外提供服务的。这就是说，任何一个追随者副本都不能响应消费者和生产者的读写请求。所有的请求都必须由领导者副本来处理，或者说，所有的读写请求都必须发往领导者副本所在的 Broker，由该 Broker 负责处理。追随者副本不处理客户端请求，它唯一的任务就是从领导者副本异步拉取消息，并写入到自己的提交日志中，从而实现与领导者副本的同步。第三，当领导者副本挂掉了，或者说领导者副本所在的 Broker 宕机时，Kafka 依托于 ZooKeeper 提供的监控功能能够实时感知到，并立即开启新一轮的领导者选举，从追随者副本中选一个作为新的领导者。老 Leader 副本重启回来后，只能作为追随者副本加入到集群中。你一定要特别注意上面的第二点，即追随者副本是不对外提供服务的。还记得刚刚我们谈到副本机制的好处时，说过 Kafka 没能提供读操作横向扩展以及改善局部性吗？具体的原因就在于此。对于客户端用户而言，Kafka 的追随者副本没有任何作用，它既不能像 MySQL 那样帮助领导者副本“抗读”，也不能实现将某些副本放到离客户端近的地方来改善数据局部性。既然如此，Kafka 为什么要这样设计呢？其实这种副本机制有两个方面的好处。1.方便实现“Read-your-writes”。所谓 Read-your-writes，顾名思义就是，当你使用生产者 API 向 Kafka 成功写入消息后，马上使用消费者 API 去读取刚才生产的消息。举个例子，比如你平时发微博时，你发完一条微博，肯定是希望能立即看到的，这就是典型的 Read-your-writes 场景。如果允许追随者副本对外提供服务，由于副本同步是异步的，因此有可能出现追随者副本还没有从领导者副本那里拉取到最新的消息，从而使得客户端看不到最新写入的消息。2.方便实现单调读（Monotonic Reads）。什么是单调读呢？就是对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。如果允许追随者副本提供读服务，那么假设当前有 2 个追随者副本 F1 和 F2，它们异步地拉取领导者副本数据。倘若 F1 拉取了 Leader 的最新消息而 F2 还未及时拉取，那么，此时如果有一个消费者先从 F1 读取消息之后又从 F2 拉取消息，它可能会看到这样的现象：第一次消费时看到的最新消息在第二次消费时不见了，这就不是单调读一致性。但是，如果所有的读请求都是由 Leader 来处理，那么 Kafka 就很容易实现单调读一致性。In-sync Replicas（ISR）我们刚刚反复说过，追随者副本不提供服务，只是定期地异步拉取领导者副本中的数据而已。既然是异步的，就存在着不可能与 Leader 实时同步的风险。在探讨如何正确应对这种风险之前，我们必须要精确地知道同步的含义是什么。或者说，Kafka 要明确地告诉我们，追随者副本到底在什么条件下才算与 Leader 同步。基于这个想法，Kafka 引入了** In-sync Replicas，也就是所谓的 ISR 副本集合。ISR 中的副本都是与 Leader 同步的副本，相反，不在 ISR 中的追随者副本就被认为是与 Leader 不同步的。那么，到底什么副本能够进入到 ISR 中呢？我们首先要明确的是，Leader 副本天然就在 ISR 中。也就是说，ISR 不只是追随者副本集合，它必然包括 Leader 副本。甚至在某些情况下，ISR 只有 Leader 这一个副本。另外，能够进入到 ISR 的追随者副本要满足一定的条件。 至于是什么条件，我先卖个关子，我们先来一起看看下面这张图。图中有 3 个副本：1 个领导者副本和 2 个追随者副本。Leader 副本当前写入了 10 条消息，Follower1 副本同步了其中的 6 条消息，而 Follower2 副本只同步了其中的 3 条消息。现在，请你思考一下，对于这 2 个追随者副本，你觉得哪个追随者副本与 Leader 不同步？答案是，要根据具体情况来定。换成英文，就是那句著名的“It depends”。看上去好像 Follower2 的消息数比 Leader 少了很多，它是最有可能与 Leader 不同步的。的确是这样的，但仅仅是可能。事实上，这张图中的 2 个 Follower 副本都有可能与 Leader 不同步，但也都有可能与 Leader 同步。也就是说，Kafka 判断 Follower 是否与 Leader 同步的标准，不是看相差的消息数，而是另有“玄机”。这个标准就是 Broker 端参数 replica.lag.time.max.ms 参数值。这个参数的含义是 Follower 副本能够落后 Leader 副本的最长时间间隔，当前默认值是 10 秒。这就是说，只要一个 Follower 副本落后 Leader 副本的时间不连续超过 10 秒，那么 Kafka 就认为该 Follower 副本与 Leader 是同步的，即使此时 Follower 副本中保存的消息明显少于 Leader 副本中的消息。我们在前面说过，Follower 副本唯一的工作就是不断地从 Leader 副本拉取消息，然后写入到自己的提交日志中。如果这个同步过程的速度持续慢于 Leader 副本的消息写入速度，那么在 replica.lag.time.max.ms 时间后，此 Follower 副本就会被认为是与 Leader 副本不同步的，因此不能再放入 ISR 中。此时，Kafka 会自动收缩 ISR 集合，将该副本“踢出”ISR。值得注意的是，倘若该副本后面慢慢地追上了 Leader 的进度，那么它是能够重新被加回 ISR 的。这也表明，ISR 是一个动态调整的集合**，而非静态不变的。Unclean 领导者选举（Unclean Leader Election）既然 ISR 是可以动态调整的，那么自然就可以出现这样的情形：ISR 为空。因为 Leader 副本天然就在 ISR 中，如果 ISR 为空了，就说明 Leader 副本也“挂掉”了，Kafka 需要重新选举一个新的 Leader。可是 ISR 是空，此时该怎么选举新 Leader 呢？Kafka 把所有不在 ISR 中的存活副本都称为非同步副本。通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程称为** Unclean 领导者选举。Broker 端参数unclean.leader.election.enable 控制是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean 领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。如果你听说过 CAP 理论的话，你一定知道，一个分布式系统通常只能同时满足一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）中的两个**。显然，在这个问题上，Kafka 赋予你选择 C 或 A 的权利。你可以根据你的实际业务场景决定是否开启 Unclean 领导者选举。不过，我强烈建议你不要开启它，毕竟我们还可以通过其他的方式来提升高可用性。如果为了这点儿高可用性的改善，牺牲了数据一致性，那就非常不值当了。扩展到目前为止，我反复强调了 Follower 副本不对外提供服务这件事情。有意思的是，社区最近正在考虑是否要打破这个限制，即允许 Follower 副本处理客户端消费者发来的请求。社区主要的考量是，这能够用于改善云上数据的局部性，更好地服务地理位置相近的客户。如果允许 Follower 副本对外提供读服务，你觉得应该如何避免或缓解因 Follower 副本与 Leader 副本不同步而导致的数据不一致的情形？Q&amp;A:Q:LEO和HW这两个概念？A:一个分区有3个副本，一个leader，2个follower。producer向leader写了10条消息，follower1从leader处拷贝了5条消息，follower2从leader处拷贝了3条消息，那么leader副本的LEO就是10，HW=3；follower1副本的LEO是5。Q:producer生产消息ack=all的时候，消息是怎么保证到follower的A:通过HW机制。leader处的HW要等所有follower LEO都越过了才会前移Q:Broker 端参数 replica.lag.time.max.ms 参数值落后 Leader 副本的最长时间间隔，默认是10秒，怎么计算的？A:follower从leader拿到消息后会更新一个名为_lastCaughtUpTimeMs的字段。每当要检查follower是否out of ISR时就会用当前时间减去这个字段值去和replica.lag.time.max.ms 比较Q: 选举leader机制A:目前选举leader的算法很简单，一般是选择AR中第一个处在ISR集合的副本为leader。比如AR的副本顺序是[1,2,3]，ISR是[2,3]，那么副本2就是leader","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"Flink DataStream 时间语义","slug":"Flink-DataStream-时间语义","date":"2019-07-22T09:44:42.000Z","updated":"2020-09-06T09:02:30.911Z","comments":true,"path":"2019/07/22/Flink-DataStream-时间语义/","link":"","permalink":"cpeixin.cn/2019/07/22/Flink-DataStream-%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89/","excerpt":"","text":"Event Time - Processing Time - Ingestion TimeFlink 在流式传输程序中支持不同的_时间_概念。处理时间(Processing Time)：简单的理解，就是数据在窗口中，被处理执行的时间。下面是官方的释义：处理时间是指执行相应操作的机器的系统时间。当流式程序按处理时间运行时，所有基于时间的操作（如时间窗口）都将使用运行相应操作员的计算机的系统时钟。每小时处理时间窗口将包括系统时钟指示整小时的时间之间到达特定操作员的所有记录。例如，如果应用程序在9:15 am开始运行，则第一个每小时处理时间窗口将包括在9:15 am和10:00 am之间处理的事件，下一个窗口将包括在10:00 am和11:00 am之间处理的事件，依此类推。处理时间是最简单的时间概念，不需要流和机器之间的协调。它提供了最佳的性能和最低的延迟。但是，在分布式和异步环境中，处理时间不能提供确定性，因为它容易受到记录到达系统（例如从消息队列）到达系统的速度，记录在系统内部操作员之间流动的速度的影响。 以及中断（计划的或其他方式）。事件时间(Event time)：简单的理解，就是数据在产生的时间，例如用户A在x时间，点击了B商品，此时所产生的数据的时间时间就是x。下面是官方释义：事件时间是每个事件在其生产设备上发生的时间。该时间通常在它们进入Flink之前嵌入到记录中，并且 可以从每个记录中提取_事件时间戳_。在事件时间中，时间的进度取决于数据，而不取决于任何挂钟。事件时间程序必须指定如何生成“ _事件时间水印”_，这是一种表示事件时间进展的机制。在理想情况下，事件时间处理将产生完全一致且确定的结果，而不管事件何时到达或它们的顺序如何。但是，除非已知事件是按时间戳（按时间戳）到达的，否则事件时间处理会在等待无序事件时产生一定的延迟。由于只能等待有限的时间，因此这限制了确定性事件时间应用程序的可用性。假设所有数据都已到达，事件时间操作将按预期方式运行，即使在处理无序或迟到的事件或重新处理历史数据时，也会产生正确且一致的结果。例如，每小时事件时间窗口将包含所有带有落入该小时事件时间戳的记录，无论它们到达的顺序或处理的时间。请注意，有时当事件时间程序实时处理实时数据时，它们将使用一些_处理时间_操作，以确保它们及时进行。摄入时间(Ingestion Time)：摄取时间是事件进入Flink的时间。在源操作员处，每个记录都将源的当前时间作为时间戳记，并且基于时间的操作（如时间窗口）引用该时间戳记。_摄取时间_从概念上讲介于_事件时间和处理时间之间_。与_处理时间_相比 ，它稍微贵一些，但结果却更可预测。由于 _摄取时间_使用稳定的时间戳（在源处分配了一次），因此对记录的不同窗口操作将引用相同的时间戳，而在_处理时间中，_每个窗口操作员都可以将记录分配给不同的窗口（基于本地系统时钟和任何运输延误）。与_事件时间_相比，_提取时间_程序无法处理任何乱序事件或迟到的数据，但是程序不必指定如何生成_水印_。在内部，_摄取时间与事件时间_非常相似，但是具有自动时间戳分配和自动水印生成功能。在实际工作中，Ingestion Time使用的场景较少。并且是不设置时间语义是，默认使用process time设置时间特征（Setting a Time Characteristic）Flink DataStream程序的第一部分通常设置基准_时间特征_。该设置定义了数据流源的行为方式（例如，是否分配时间戳），以及诸如的窗口操作应使用什么时间概念KeyedStream.timeWindow(Time.seconds(30))。以下示例显示了一个Flink程序，该程序在每小时的时间窗口中汇总事件。窗口的行为与时间特征相适应。123456789101112val env = StreamExecutionEnvironment.getExecutionEnvironment//默认为process time，所以如果选择ProcessingTime，也可以不进行此步的设置env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))stream .keyBy( _.getUser ) .timeWindow(Time.hours(1)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...)请注意，为了在事件时间中运行此示例，程序需要使用直接为数据定义事件时间并自己发出水印的源，或者程序必须在源之后注入Timestamp Assigner＆Watermark Generator。这些功能描述了如何访问事件时间戳，以及事件流呈现出何种程度的乱序。现实世界中的时间是不一致的，在 flink 中被划分为事件时间，摄入时间，处理时间三种。如果以 EventTime 为基准来定义时间窗口那将形成 EventTimeWindow,*要求消息本身就应该携带 EventTime *如果以 IngesingtTime 为基准来定义时间窗口那将形成 IngestingTimeWindow,以 source 的 systemTime 为准。 如果以 ProcessingTime 基准来定义时间窗口那将形成 ProcessingTimeWindow，以 operator 的 systemTime 为准。以上三种选择是需要根据不同业务需求进行选择的，例如，在数据计算的过程中，需要考虑到网络延迟的原因，那么就应该选择setStreamTimeCharacteristic(TimeCharacteristic.EventTime)，相反，不需要考虑延迟原因的话，只考虑处理数据的时间，则可以选择另两个。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink对接Kafka","slug":"Flink对接Kafka","date":"2019-07-09T10:25:43.000Z","updated":"2020-10-12T15:20:29.855Z","comments":true,"path":"2019/07/09/Flink对接Kafka/","link":"","permalink":"cpeixin.cn/2019/07/09/Flink%E5%AF%B9%E6%8E%A5Kafka/","excerpt":"","text":"在实时计算的场景下，绝大多数的数据源都是消息系统，而 Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；同时也讲了 Flink 作为生产者像 Kafka 写入数据的方式和代码实现。这一课时我们将从以下几个方面介绍 Flink 消费 Kafka 中的数据方式和源码实现。Flink 如何消费 KafkaFlink 在和 Kafka 对接的过程中，跟 Kafka 的版本是强相关的。上一课时也提到了，我们在使用 Kafka 连接器时需要引用相对应的 Jar 包依赖，对于某些连接器比如 Kafka 是有版本要求的，一定要去官方网站找到对应的依赖版本。我们本地的 Kafka 版本是 2.1.0，所以需要对应的类是 FlinkKafkaConsumer。首先需要在 pom.xml 中引入 jar 包依赖：复制12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;下面将对 Flink 消费 Kafka 数据的方式进行分类讲解。消费单个 Topic12345678910111213141516171819202122232425262728public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); env.enableCheckpointing(5000); Properties properties = new Properties(); properties.setProperty(\"bootstrap.servers\", \"127.0.0.1:9092\"); // 如果你是0.8版本的Kafka，需要配置 //properties.setProperty(\"zookeeper.connect\", \"localhost:2181\"); //设置消费组 properties.setProperty(\"group.id\", \"group_test\"); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(\"test\", new SimpleStringSchema(), properties); //设置从最早的ffset消费 consumer.setStartFromEarliest(); //还可以手动指定相应的 topic, partition，offset,然后从指定好的位置开始消费 //HashMap&lt;KafkaTopicPartition, Long&gt; map = new HashMap&lt;&gt;(); //map.put(new KafkaTopicPartition(\"test\", 1), 10240L); //假如partition有多个，可以指定每个partition的消费位置 //map.put(new KafkaTopicPartition(\"test\", 2), 10560L); //然后各个partition从指定位置消费 //consumer.setStartFromSpecificOffsets(map); env.addSource(consumer).flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; System.out.println(value); &#125; &#125;); env.execute(\"start consumer...\");&#125;在设置消费 Kafka 中的数据时，可以显示地指定从某个 Topic 的每一个 Partition 中进行消费。消费多个 Topic我们的业务中会有这样的情况，同样的数据根据类型不同发送到了不同的 Topic 中，比如线上的订单数据根据来源不同分别发往移动端和 PC 端两个 Topic 中。但是我们不想把同样的代码复制一份，需重新指定一个 Topic 进行消费，这时候应该怎么办呢？12345678910111213Properties properties &#x3D; new Properties();properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);&#x2F;&#x2F; 如果你是0.8版本的Kafka，需要配置&#x2F;&#x2F;properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);&#x2F;&#x2F;设置消费组properties.setProperty(&quot;group.id&quot;, &quot;group_test&quot;);FlinkKafkaConsumer&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties);ArrayList&lt;String&gt; topics &#x3D; new ArrayList&lt;&gt;(); topics.add(&quot;test_A&quot;); topics.add(&quot;test_B&quot;); &#x2F;&#x2F; 传入一个 list，完美解决了这个问题 FlinkKafkaConsumer&lt;Tuple2&lt;String, String&gt;&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(topics, new SimpleStringSchema(), properties);...我们可以传入一个 list 来解决消费多个 Topic 的问题，如果用户需要区分两个 Topic 中的数据，那么需要在发往 Kafka 中数据新增一个字段，用来区分来源。消息序列化我们在上述消费 Kafka 消息时，都默认指定了消息的序列化方式，即 SimpleStringSchema。这里需要注意的是，在我们使用 SimpleStringSchema 的时候，返回的结果中只有原数据，没有 topic、parition 等信息，这时候可以自定义序列化的方式来实现自定义返回数据的结构。1234567891011121314151617181920212223public class CustomDeSerializationSchema implements KafkaDeserializationSchema&lt;ConsumerRecord&lt;String, String&gt;&gt; &#123; &#x2F;&#x2F;是否表示流的最后一条元素,设置为false，表示数据会源源不断地到来 @Override public boolean isEndOfStream(ConsumerRecord&lt;String, String&gt; nextElement) &#123; return false; &#125; &#x2F;&#x2F;这里返回一个ConsumerRecord&lt;String,String&gt;类型的数据，除了原数据还包括topic，offset，partition等信息 @Override public ConsumerRecord&lt;String, String&gt; deserialize(ConsumerRecord&lt;byte[], byte[]&gt; record) throws Exception &#123; return new ConsumerRecord&lt;String, String&gt;( record.topic(), record.partition(), record.offset(), new String(record.key()), new String(record.value()) ); &#125; &#x2F;&#x2F;指定数据的输入类型 @Override public TypeInformation&lt;ConsumerRecord&lt;String, String&gt;&gt; getProducedType() &#123; return TypeInformation.of(new TypeHint&lt;ConsumerRecord&lt;String, String&gt;&gt;()&#123;&#125;); &#125;&#125;这里自定义了 CustomDeSerializationSchema 信息，就可以直接使用了。Parition 和 Topic 动态发现在很多场景下，随着业务的扩展，我们需要对 Kafka 的分区进行扩展，为了防止新增的分区没有被及时发现导致数据丢失，消费者必须要感知 Partition 的动态变化，可以使用 FlinkKafkaConsumer 的动态分区发现实现。我们只需要指定下面的配置，即可打开动态分区发现功能：每隔 10ms 会动态获取 Topic 的元数据，对于新增的 Partition 会自动从最早的位点开始消费数据。1properties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, &quot;10&quot;);如果业务场景需要我们动态地发现 Topic，可以指定 Topic 的正则表达式：1FlinkKafkaConsumer&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(Pattern.compile(&quot;^test_([A-Za-z0-9]*)$&quot;), new SimpleStringSchema(), properties);Flink 消费 Kafka 设置 offset 的方法Flink 消费 Kafka 需要指定消费的 offset，也就是偏移量。Flink 读取 Kafka 的消息有五种消费方式：指定 Topic 和 Partition从最早位点开始消费从指定时间点开始消费从最新的数据开始消费从上次消费位点开始消费复制123456789101112131415161718192021222324&#x2F;*** Flink从指定的topic和parition中指定的offset开始*&#x2F;Map&lt;KafkaTopicPartition, Long&gt; offsets &#x3D; new HashedMap();offsets.put(new KafkaTopicPartition(&quot;test&quot;, 0), 10000L);offsets.put(new KafkaTopicPartition(&quot;test&quot;, 1), 20000L);offsets.put(new KafkaTopicPartition(&quot;test&quot;, 2), 30000L);consumer.setStartFromSpecificOffsets(offsets);&#x2F;*** Flink从topic中最早的offset消费*&#x2F;consumer.setStartFromEarliest();&#x2F;*** Flink从topic中指定的时间点开始消费*&#x2F;consumer.setStartFromTimestamp(1559801580000l);&#x2F;*** Flink从topic中最新的数据开始消费*&#x2F;consumer.setStartFromLatest();&#x2F;*** Flink从topic中指定的group上次消费的位置开始消费，所以必须配置group.id参数*&#x2F;consumer.setStartFromGroupOffsets();源码解析从上面的类图可以看出，FlinkKafkaConsumer 继承了 FlinkKafkaConsumerBase，而 FlinkKafkaConsumerBase 最终是对 SourceFunction 进行了实现。整体的流程：FlinkKafkaConsumer 首先创建了 KafkaFetcher 对象，然后 KafkaFetcher 创建了 KafkaConsumerThread 和 Handover，KafkaConsumerThread 负责直接从 Kafka 中读取 msg，并交给 Handover，然后 Handover 将 msg 传递给 KafkaFetcher.emitRecord 将消息发出。因为 FlinkKafkaConsumerBase 实现了 RichFunction 接口，所以当程序启动的时候，会首先调用 FlinkKafkaConsumerBase.open 方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136public void open(Configuration configuration) throws Exception &#123; &#x2F;&#x2F; 指定offset的提交方式 this.offsetCommitMode &#x3D; OffsetCommitModes.fromConfiguration( getIsAutoCommitEnabled(), enableCommitOnCheckpoints, ((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled()); &#x2F;&#x2F; 创建分区发现器 this.partitionDiscoverer &#x3D; createPartitionDiscoverer( topicsDescriptor, getRuntimeContext().getIndexOfThisSubtask(), getRuntimeContext().getNumberOfParallelSubtasks()); this.partitionDiscoverer.open(); subscribedPartitionsToStartOffsets &#x3D; new HashMap&lt;&gt;(); final List&lt;KafkaTopicPartition&gt; allPartitions &#x3D; partitionDiscoverer.discoverPartitions(); if (restoredState !&#x3D; null) &#123; for (KafkaTopicPartition partition : allPartitions) &#123; if (!restoredState.containsKey(partition)) &#123; restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET); &#125; &#125; for (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123; if (!restoredFromOldState) &#123; if (KafkaTopicPartitionAssigner.assign( restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks()) &#x3D;&#x3D; getRuntimeContext().getIndexOfThisSubtask())&#123; subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue()); &#125; &#125; else &#123; subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue()); &#125; &#125; if (filterRestoredPartitionsWithCurrentTopicsDescriptor) &#123; subscribedPartitionsToStartOffsets.entrySet().removeIf(entry -&gt; &#123; if (!topicsDescriptor.isMatchingTopic(entry.getKey().getTopic())) &#123; LOG.warn( &quot;&#123;&#125; is removed from subscribed partitions since it is no longer associated with topics descriptor of current execution.&quot;, entry.getKey()); return true; &#125; return false; &#125;); &#125; LOG.info(&quot;Consumer subtask &#123;&#125; will start reading &#123;&#125; partitions with offsets in restored state: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets); &#125; else &#123; switch (startupMode) &#123; case SPECIFIC_OFFSETS: if (specificStartupOffsets &#x3D;&#x3D; null) &#123; throw new IllegalStateException( &quot;Startup mode for the consumer set to &quot; + StartupMode.SPECIFIC_OFFSETS + &quot;, but no specific offsets were specified.&quot;); &#125; for (KafkaTopicPartition seedPartition : allPartitions) &#123; Long specificOffset &#x3D; specificStartupOffsets.get(seedPartition); if (specificOffset !&#x3D; null) &#123; subscribedPartitionsToStartOffsets.put(seedPartition, specificOffset - 1); &#125; else &#123; subscribedPartitionsToStartOffsets.put(seedPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET); &#125; &#125; break; case TIMESTAMP: if (startupOffsetsTimestamp &#x3D;&#x3D; null) &#123; throw new IllegalStateException( &quot;Startup mode for the consumer set to &quot; + StartupMode.TIMESTAMP + &quot;, but no startup timestamp was specified.&quot;); &#125; for (Map.Entry&lt;KafkaTopicPartition, Long&gt; partitionToOffset : fetchOffsetsWithTimestamp(allPartitions, startupOffsetsTimestamp).entrySet()) &#123; subscribedPartitionsToStartOffsets.put( partitionToOffset.getKey(), (partitionToOffset.getValue() &#x3D;&#x3D; null) KafkaTopicPartitionStateSentinel.LATEST_OFFSET : partitionToOffset.getValue() - 1); &#125; break; default: for (KafkaTopicPartition seedPartition : allPartitions) &#123; subscribedPartitionsToStartOffsets.put(seedPartition, startupMode.getStateSentinel()); &#125; &#125; if (!subscribedPartitionsToStartOffsets.isEmpty()) &#123; switch (startupMode) &#123; case EARLIEST: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the earliest offsets: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()); break; case LATEST: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the latest offsets: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()); break; case TIMESTAMP: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from timestamp &#123;&#125;: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), startupOffsetsTimestamp, subscribedPartitionsToStartOffsets.keySet()); break; case SPECIFIC_OFFSETS: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the specified startup offsets &#123;&#125;: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), specificStartupOffsets, subscribedPartitionsToStartOffsets.keySet()); List&lt;KafkaTopicPartition&gt; partitionsDefaultedToGroupOffsets &#x3D; new ArrayList&lt;&gt;(subscribedPartitionsToStartOffsets.size()); for (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123; if (subscribedPartition.getValue() &#x3D;&#x3D; KafkaTopicPartitionStateSentinel.GROUP_OFFSET) &#123; partitionsDefaultedToGroupOffsets.add(subscribedPartition.getKey()); &#125; &#125; if (partitionsDefaultedToGroupOffsets.size() &gt; 0) &#123; LOG.warn(&quot;Consumer subtask &#123;&#125; cannot find offsets for the following &#123;&#125; partitions in the specified startup offsets: &#123;&#125;&quot; + &quot;; their startup offsets will be defaulted to their committed group offsets in Kafka.&quot;, getRuntimeContext().getIndexOfThisSubtask(), partitionsDefaultedToGroupOffsets.size(), partitionsDefaultedToGroupOffsets); &#125; break; case GROUP_OFFSETS: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the committed group offsets in Kafka: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()); &#125; &#125; else &#123; LOG.info(&quot;Consumer subtask &#123;&#125; initially has no partitions to read from.&quot;, getRuntimeContext().getIndexOfThisSubtask()); &#125; &#125;&#125;对 Kafka 中的 Topic 和 Partition 的数据进行读取的核心逻辑都在 run 方法中：123456789101112131415161718192021222324252627282930313233343536373839404142public void run(SourceContext&lt;T&gt; sourceContext) throws Exception &#123; if (subscribedPartitionsToStartOffsets &#x3D;&#x3D; null) &#123; throw new Exception(&quot;The partitions were not set for the consumer&quot;); &#125; this.successfulCommits &#x3D; this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER); this.failedCommits &#x3D; this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER); final int subtaskIndex &#x3D; this.getRuntimeContext().getIndexOfThisSubtask(); this.offsetCommitCallback &#x3D; new KafkaCommitCallback() &#123; @Override public void onSuccess() &#123; successfulCommits.inc(); &#125; @Override public void onException(Throwable cause) &#123; LOG.warn(String.format(&quot;Consumer subtask %d failed async Kafka commit.&quot;, subtaskIndex), cause); failedCommits.inc(); &#125; &#125;; if (subscribedPartitionsToStartOffsets.isEmpty()) &#123; sourceContext.markAsTemporarilyIdle(); &#125; LOG.info(&quot;Consumer subtask &#123;&#125; creating fetcher with offsets &#123;&#125;.&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets); this.kafkaFetcher &#x3D; createFetcher( sourceContext, subscribedPartitionsToStartOffsets, periodicWatermarkAssigner, punctuatedWatermarkAssigner, (StreamingRuntimeContext) getRuntimeContext(), offsetCommitMode, getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP), useMetrics); if (!running) &#123; return; &#125; if (discoveryIntervalMillis &#x3D;&#x3D; PARTITION_DISCOVERY_DISABLED) &#123; kafkaFetcher.runFetchLoop(); &#125; else &#123; runWithPartitionDiscovery(); &#125;&#125;Flink 消费 Kafka 数据代码上面介绍了 Flink 消费 Kafka 的方式，以及消息序列化的方式，同时介绍了分区和 Topic 的动态发现方法，那么回到我们的项目中来，消费 Kafka 数据的完整代码如下：12345678910111213141516171819202122public class KafkaConsumer &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment(); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); env.enableCheckpointing(5000); Properties properties &#x3D; new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); &#x2F;&#x2F;设置消费组 properties.setProperty(&quot;group.id&quot;, &quot;group_test&quot;); properties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, &quot;10&quot;); FlinkKafkaConsumer&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties); &#x2F;&#x2F;设置从最早的ffset消费 consumer.setStartFromEarliest(); env.addSource(consumer).flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; System.out.println(value); &#125; &#125;); env.execute(&quot;start consumer...&quot;); &#125;&#125;我们可以直接右键运行代码，在控制台中可以看到数据的正常打印，如下图所示：通过代码可知，我们之前发往 Kafka 的消息被完整地打印出来了。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 编程模型","slug":"Flink-编程模型","date":"2019-07-07T07:32:00.000Z","updated":"2020-09-06T09:05:04.898Z","comments":true,"path":"2019/07/07/Flink-编程模型/","link":"","permalink":"cpeixin.cn/2019/07/07/Flink-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"抽象级别Flink提供了不同级别的抽象来开发流/批处理应用程序。最低级别的抽象仅提供状态流。它 通过Process Function嵌入到DataStream API中。它允许用户自由地处理一个或多个流中的事件，并使用一致的容错_状态_。此外，用户可以注册事件时间和处理时间回调，从而允许程序实现复杂的计算。实际上，大多数应用程序不需要上述低级抽象，而是针对Core API进行编程， 例如DataStream API（有界/无界流）和DataSet API （有界数据集）。这些流畅的API为数据处理提供了通用的构建块，例如各种形式的用户指定的转换，联接，聚合，窗口，状态等。这些API中处理的数据类型以相应编程语言中的类表示。低级_Process Function_与_DataStream API_集成在一起，从而使得仅对某些操作进行低级抽象成为可能。该_数据集API_提供的有限数据集的其他原语，如循环/迭代。该Table API是为中心的声明性DSL 表，其可被动态地改变的表（表示流时）。该Table API遵循（扩展）关系模型：表有一个模式连接（类似于在关系数据库中的表）和API提供可比的操作，如select, project, join, group-by, aggregate等表API程序以声明的方式定义应该进行哪些逻辑运算，_而 不确切地定义_运算的代码外观_。尽管Table API可以通过各种类型的用户定义函数进行扩展，但它的表达性不如_Core API。_，但使用起来更简洁（无需编写代码）。此外，Table API程序还经过优化程序，该优化程序在执行之前应用优化规则。可以在表和_DataStream / DataSet_之间无缝转换，从而允许程序将_Table API_以及_DataStream 和DataSet API混合使用。Flink提供的最高级别的抽象是SQL。这种抽象在语义和表达方式上均类似于_Table API_，但是将程序表示为SQL查询表达式。在SQL抽象与表API SQL查询紧密地相互作用，并且可以在中定义的表执行_表API_。程序和数据流Flink程序的基本构建部分是流和转换。（请注意，Flink的DataSet API中使用的DataSet也是内部流）从概念上讲，流是数据记录流（可能永无止境），而转换_是将一个或多个流作为一个操作的操作。输入，并产生一个或多个输出流。执行时，Flink程序将映射到由流和转换运算符组成的流数据流。每个数据流都以一个或多个源开始，并以一个或多个接收器结束。数据流类似于任意**有向无环图_（DAG）**。尽管可以通过_迭代_构造来允许特殊形式的循环 ，但是在大多数情况下，为简单起见，我们将对其进行介绍。程序中的转换与数据流中的运算符之间通常存在一一对应的关系。但是，有时，一个转换可能包含多个转换运算符。源和接收器记录在流连接器和批处理连接器文档中。转换记录在DataStream运算符和DataSet转换中。回到顶部并行数据流Flink中的程序本质上是并行的和分布式的。在执行期间，一个流具有一个或多个流分区，并且每个_运算符_具有一个或多个运算符子任务。操作员子任务彼此独立，并在不同的线程中执行，并且可能在不同的机器或容器上执行。_operator subtask_的数量是该特定操作员的并行性。流的并行性始终是其生产运营商的并行性。同一程序的不同运算符可能具有不同的并行度。流可以_按一对一_（或_转发_）模式或_重新分配_模式在两个运算符之间传输数据：一对一的流（例如，上图中的_Source_和_map（）_运算符之间）保留元素的分区和排序。这意味着_map（）_运算符的subtask [1] 将以与_Source_运算符的subtask [1]产生的相同顺序看到相同的元素。重新分配流（如上面的map（）和keyBy（） / window（）_之间以及 _keyBy（） / window（）和Sink之间_）会更改流的分区。每个_operator subtask都_将数据发送到不同的目标子任务，具体取决于所选的转换。实例是 _keyBy（） _（其重新分区通过散列键），_broadcast() 或者 rebalance() _（其重新分区随机地）。在_重新分配_交换中，元素之间的顺序仅保留在每对发送和接收子任务中（例如_map（）的 subtask [1]和map（）的 subtask [2]_keyBy / window_）。因此，在此示例中，保留了每个键内的顺序，但是并行性确实引入了不确定性，即不同键的聚合结果到达接收器的顺序。有关配置和控制并行性的详细信息，请参见并行执行文档。视窗汇总事件（例如，计数，总和）在流上的工作方式与批处理中的不同。例如，不可能计算流中的所有元素，因为流通常是无限的（无界）。相反，流上的聚合（计数，总和等）由窗口确定范围，例如_“过去5分钟内的计数”或“最近100个元素的总和”_。Windows可以是_时间驱动的_（例如：每30秒）或_数据驱动的_（例如：每100个元素）。通常可以区分不同类型的窗口，例如_滚动窗口_（无重叠）， _滑动窗口_（有重叠）和_会话窗口_（由不活动的间隙打断）。可以在此博客文章中找到更多窗口示例。更多详细信息在docs窗口中。时间在流式传输程序中引用时间（例如，定义窗口）时，可以引用不同的时间概念：事件时间是创建事件的时间。通常用事件中的时间戳记来描述，例如由生产传感器或生产服务附加。Flink通过时间戳分配器访问事件时间戳。接收时间是事件在源操作员进入Flink数据流的时间。处理时间是每个执行基于时间的操作的操作员的本地时间。有关如何处理时间的更多详细信息，请参见事件时间文档。有状态的操作尽管数据流中的许多操作一次仅查看一个_事件_（例如事件解析器），但某些操作会记住多个事件的信息（例如窗口运算符）。这些操作称为有状态。有状态操作的状态以可以被认为是嵌入式键/值存储的方式维护。严格将状态与有状态运算符读取的流一起进行分区和分发。因此，只有在_keyBy（）_函数之后，才可以在_键控流_上访问键/值状态，并且仅限于与当前事件的键关联的值。对齐流键和状态键可确保所有状态更新都是本地操作，从而确保了一致性而没有事务开销。这种对齐方式还允许Flink重新分配状态并透明地调整流分区。有关更多信息，请参阅关于state的文档。容错检查点Flink通过结合stream replay and checkpointing.来实现容错。检查点与每个输入流中的特定点以及每个运算符的对应状态有关。通过恢复operators的状态并从检查点开始重放事件，可以从检查点恢复流数据流，同时保持一致性_（一次处理语义）_。检查点间隔是在执行过程中权衡容错开销与恢复时间（需要重播的事件数）的一种方法。容错内部的描述提供了有关Flink如何管理检查点和相关主题的更多信息。有关启用和配置检查点的详细信息，请参见checkpointing API文档。串流处理Flink执行批处理程序，这是流程序的特例，其中流是有界的（元素数量有限）。甲_数据集_在内部视为数据流。因此，以上概念以同样的方式适用于批处理程序，也适用于流式程序，但有少量例外：批处理程序的容错功能不使用检查点。通过完全重播流来进行恢复。这是可能的，因为输入是有界的。这将成本更多地推向了恢复，但由于避免了检查点，因此使常规处理的成本降低了。DataSet API中的状态操作使用简化的内存中/核外数据结构，而不是键/值索引。DataSet API引入了特殊的同步（基于超步）迭代，仅在有限流上才有可能。有关详细信息，请查看迭代文档。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink CEP编程","slug":"Flink-CEP编程","date":"2019-07-06T08:43:09.000Z","updated":"2020-09-06T08:58:16.144Z","comments":true,"path":"2019/07/06/Flink-CEP编程/","link":"","permalink":"cpeixin.cn/2019/07/06/Flink-CEP%E7%BC%96%E7%A8%8B/","excerpt":"","text":"背景Complex Event Processing（CEP）是 Flink 提供的一个非常亮眼的功能，关于 CEP 的解释我们引用维基百科中的一段话：CEP, is event processing that combines data from multiple sources to infer events or patterns that suggest more complicated circumstances. The goal of complex event processing is to identify meaningful events (such as opportunities or threats) and respond to them as quickly as possible.在我们的实际生产中，随着数据的实时性要求越来越高，实时数据的量也在不断膨胀，在某些业务场景中需要根据连续的实时数据，发现其中有价值的那些事件。说到底，Flink 的 CEP 到底解决了什么样的问题呢？比如，我们需要在大量的订单交易中发现那些虚假交易，在网站的访问日志中寻找那些使用脚本或者工具“爆破”登录的用户，或者在快递运输中发现那些滞留很久没有签收的包裹等。如果你对 CEP 的理论基础非常感兴趣，推荐一篇论文“Efﬁcient Pattern Matching over Event Streams”。Flink 对 CEP 的支持非常友好，并且支持复杂度非常高的模式匹配，其吞吐和延迟都令人满意。程序结构Flink CEP 的程序结构主要分为两个步骤：定义模式匹配结果我们在官网中可以找到一个 Flink 提供的案例：12345678910111213141516171819202122232425262728293031323334DataStream&lt;Event&gt; input &#x3D; ...Pattern&lt;Event, ?&gt; pattern &#x3D; Pattern.&lt;Event&gt;begin(&quot;start&quot;).where( new SimpleCondition&lt;Event&gt;() &#123; @Override public boolean filter(Event event) &#123; return event.getId() &#x3D;&#x3D; 42; &#125; &#125; ).next(&quot;middle&quot;).subtype(SubEvent.class).where( new SimpleCondition&lt;SubEvent&gt;() &#123; @Override public boolean filter(SubEvent subEvent) &#123; return subEvent.getVolume() &gt;&#x3D; 10.0; &#125; &#125; ).followedBy(&quot;end&quot;).where( new SimpleCondition&lt;Event&gt;() &#123; @Override public boolean filter(Event event) &#123; return event.getName().equals(&quot;end&quot;); &#125; &#125; );PatternStream&lt;Event&gt; patternStream &#x3D; CEP.pattern(input, pattern);DataStream&lt;Alert&gt; result &#x3D; patternStream.process( new PatternProcessFunction&lt;Event, Alert&gt;() &#123; @Override public void processMatch( Map&lt;String, List&lt;Event&gt;&gt; pattern, Context ctx, Collector&lt;Alert&gt; out) throws Exception &#123; out.collect(createAlertFrom(pattern)); &#125; &#125;);在这个案例中可以看到程序结构分别是：第一步，定义一个模式 Pattern，在这里定义了一个这样的模式，即在所有接收到的事件中匹配那些以 id 等于 42 的事件，然后匹配 volume 大于 10.0 的事件，继续匹配一个 name 等于 end 的事件；第二步，匹配模式并且发出报警，根据定义的 pattern 在输入流上进行匹配，一旦命中我们的模式，就发出一个报警。模式定义Flink 支持了非常丰富的模式定义，这些模式也是我们实现复杂业务逻辑的基础。我们把支持的模式简单做了以下分类，完整的模式定义 API 支持可以参考官网资料。简单模式联合模式匹配后的忽略模式源码解析我们在上面的官网案例中可以发现，Flink CEP 的整个过程是：从一个 Source 作为输入经过一个 Pattern 算子转换为 PatternStream经过 select/process 算子转换为 DataStream我们来看一下 select 和 process 算子都做了什么？可以看到最终的逻辑都是在 PatternStream 这个类中进行的。1234567public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; process( final PatternProcessFunction&lt;T, R&gt; patternProcessFunction, final TypeInformation&lt;R&gt; outTypeInfo) &#123; return builder.build( outTypeInfo, builder.clean(patternProcessFunction));&#125;最终经过 PatternStreamBuilder 的 build 方法生成了一个 SingleOutputStreamOperator，这个类继承了 DataStream。最终的处理计算逻辑其实都封装在了 CepOperator 这个类中，而在 CepOperator 这个类中的 processElement 方法则是对每一条数据的处理逻辑。同时由于 CepOperator 实现了 Triggerable 接口，所以会执行定时器。所有核心的处理逻辑都在 updateNFA 这个方法中。入口在这里：1234567private void processEvent(NFAState nfaState, IN event, long timestamp) throws Exception &#123; try (SharedBufferAccessor&lt;IN&gt; sharedBufferAccessor &#x3D; partialMatches.getAccessor()) &#123; Collection&lt;Map&lt;String, List&lt;IN&gt;&gt;&gt; patterns &#x3D; nfa.process(sharedBufferAccessor, nfaState, event, timestamp, afterMatchSkipStrategy, cepTimerService); processMatchedSequences(patterns, timestamp); &#125;&#125;NFA 的全称为 非确定有限自动机，NFA 中包含了模式匹配中的各个状态和状态间的转换。在 NFA 这个类中的核心方法是：process 和 advanceTime，这两个方法的实现有些复杂，总体来说可以归纳为，每当一条新来的数据进入状态机都会驱动整个状态机进行状态转换。实战案例我们模拟电商网站用户搜索的数据来作为数据的输入源，然后查找其中重复搜索某一个商品的人，并且发送一条告警消息。代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main(String[] args) throws Exception&#123; final StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStreamSource source &#x3D; env.fromElements( &#x2F;&#x2F;浏览记录 Tuple3.of(&quot;Marry&quot;, &quot;外套&quot;, 1L), Tuple3.of(&quot;Marry&quot;, &quot;帽子&quot;,1L), Tuple3.of(&quot;Marry&quot;, &quot;帽子&quot;,2L), Tuple3.of(&quot;Marry&quot;, &quot;帽子&quot;,3L), Tuple3.of(&quot;Ming&quot;, &quot;衣服&quot;,1L), Tuple3.of(&quot;Marry&quot;, &quot;鞋子&quot;,1L), Tuple3.of(&quot;Marry&quot;, &quot;鞋子&quot;,2L), Tuple3.of(&quot;LiLei&quot;, &quot;帽子&quot;,1L), Tuple3.of(&quot;LiLei&quot;, &quot;帽子&quot;,2L), Tuple3.of(&quot;LiLei&quot;, &quot;帽子&quot;,3L) ); &#x2F;&#x2F;定义Pattern,寻找连续搜索帽子的用户 Pattern&lt;Tuple3&lt;String, String, Long&gt;, Tuple3&lt;String, String, Long&gt;&gt; pattern &#x3D; Pattern .&lt;Tuple3&lt;String, String, Long&gt;&gt;begin(&quot;start&quot;) .where(new SimpleCondition&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123; @Override public boolean filter(Tuple3&lt;String, String, Long&gt; value) throws Exception &#123; return value.f1.equals(&quot;帽子&quot;); &#125; &#125;) &#x2F;&#x2F;.timesOrMore(3); .next(&quot;middle&quot;) .where(new SimpleCondition&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123; @Override public boolean filter(Tuple3&lt;String, String, Long&gt; value) throws Exception &#123; return value.f1.equals(&quot;帽子&quot;); &#125; &#125;); KeyedStream keyedStream &#x3D; source.keyBy(0); PatternStream patternStream &#x3D; CEP.pattern(keyedStream, pattern); SingleOutputStreamOperator matchStream &#x3D; patternStream.select(new PatternSelectFunction&lt;Tuple3&lt;String, String, Long&gt;, String&gt;() &#123; @Override public String select(Map&lt;String, List&lt;Tuple3&lt;String, String, Long&gt;&gt;&gt; pattern) throws Exception &#123; List&lt;Tuple3&lt;String, String, Long&gt;&gt; middle &#x3D; pattern.get(&quot;middle&quot;); return middle.get(0).f0 + &quot;:&quot; + middle.get(0).f2 + &quot;:&quot; + &quot;连续搜索两次帽子!&quot;; &#125; &#125;); matchStream.printToErr(); env.execute(&quot;execute cep&quot;);&#125;上述代码的逻辑我们可以分解如下。首先定义一个数据源，模拟了一些用户的搜索数据，然后定义了自己的 Pattern。这个模式的特点就是连续两次搜索商品“帽子”，然后进行匹配，发现匹配后输出一条提示信息，直接打印在控制台上。可以看到，提示信息已经打印在了控制台上。总结Flink CEP 的支持和实现，并且模拟了一下简单的电商搜索场景来实现对连续搜索某一个商品的提示，关于模式匹配还有更加复杂的应用，比如识别网站的用户的爆破登录、运维监控等，建议你结合源码和官网进行深入的学习。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink Exactly-once 实现原理解析","slug":"Apache-Flink结合Kafka构建端到端的Exactly-Once处理","date":"2019-07-03T17:06:47.000Z","updated":"2020-09-06T17:32:19.300Z","comments":true,"path":"2019/07/04/Apache-Flink结合Kafka构建端到端的Exactly-Once处理/","link":"","permalink":"cpeixin.cn/2019/07/04/Apache-Flink%E7%BB%93%E5%90%88Kafka%E6%9E%84%E5%BB%BA%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84Exactly-Once%E5%A4%84%E7%90%86/","excerpt":"","text":"背景通常情况下，流式计算系统都会为用户提供指定数据处理的可靠模式功能，用来表明在实际生产运行中会对数据处理做哪些保障。一般来说，流处理引擎通常为用户的应用程序提供三种数据处理语义：最多一次、至少一次和精确一次。最多一次（At-most-Once）：这种语义理解起来很简单，用户的数据只会被处理一次，不管成功还是失败，不会重试也不会重发。至少一次（At-least-Once）：这种语义下，系统会保证数据或事件至少被处理一次。如果中间发生错误或者丢失，那么会从源头重新发送一条然后进入处理系统，所以同一个事件或者消息会被处理多次。精确一次（Exactly-Once）：表示每一条数据只会被精确地处理一次，不多也不少。Exactly-Once 是 Flink、Spark 等流处理系统的核心特性之一，这种语义会保证每一条消息只被流处理系统处理一次。“精确一次” 语义是 Flink 1.4.0 版本引入的一个重要特性，而且，Flink 号称支持“端到端的精确一次”语义。在这里我们解释一下“端到端（End to End）的精确一次”，它指的是 Flink 应用从 Source 端开始到 Sink 端结束，数据必须经过的起始点和结束点。Flink 自身是无法保证外部系统“精确一次”语义的，所以 Flink 若要实现所谓“端到端（End to End）的精确一次”的要求，那么外部系统必须支持“精确一次”语义；然后借助 Flink 提供的分布式快照和两阶段提交才能实现。分布式快照机制我们在之前的课程中讲解过 Flink 的容错机制，Flink 提供了失败恢复的容错机制，而这个容错机制的核心就是持续创建分布式数据流的快照来实现。同 Spark 相比，Spark 仅仅是针对 Driver 的故障恢复 Checkpoint。而 Flink 的快照可以到算子级别，并且对全局数据也可以做快照。Flink 的分布式快照受到 Chandy-Lamport 分布式快照算法启发，同时进行了量身定做，有兴趣的同学可以搜一下。BarrierFlink 分布式快照的核心元素之一是 Barrier（数据栅栏），我们也可以把 Barrier 简单地理解成一个标记，该标记是严格有序的，并且随着数据流往下流动。每个 Barrier 都带有自己的 ID，Barrier 极其轻量，并不会干扰正常的数据处理。如上图所示，假如我们有一个从左向右流动的数据流，Flink 会依次生成 snapshot 1、 snapshot 2、snapshot 3……Flink 中有一个专门的“协调者”负责收集每个 snapshot 的位置信息，这个“协调者”也是高可用的。Barrier 会随着正常数据继续往下流动，每当遇到一个算子，算子会插入一个标识，这个标识的插入时间是上游所有的输入流都接收到 snapshot n。与此同时，当我们的 sink 算子接收到所有上游流发送的 Barrier 时，那么就表明这一批数据处理完毕，Flink 会向“协调者”发送确认消息，表明当前的 snapshot n 完成了。当所有的 sink 算子都确认这批数据成功处理后，那么本次的 snapshot 被标识为完成。这里就会有一个问题，因为 Flink 运行在分布式环境中，一个 operator 的上游会有很多流，每个流的 barrier n 到达的时间不一致怎么办？这里 Flink 采取的措施是：快流等慢流。拿上图的 barrier n 来说，其中一个流到的早，其他的流到的比较晚。当第一个 barrier n到来后，当前的 operator 会继续等待其他流的 barrier n。直到所有的barrier n 到来后，operator 才会把所有的数据向下发送。异步和增量按照上面我们介绍的机制，每次在把快照存储到我们的状态后端时，如果是同步进行就会阻塞正常任务，从而引入延迟。因此 Flink 在做快照存储时，可采用异步方式。此外，由于 checkpoint 是一个全局状态，用户保存的状态可能非常大，多数达 G 或者 T 级别。在这种情况下，checkpoint 的创建会非常慢，而且执行时占用的资源也比较多，因此 Flink 提出了增量快照的概念。也就是说，每次都是进行的全量 checkpoint，是基于上次进行更新的。两阶段提交上面我们讲解了基于 checkpoint 的快照操作，快照机制能够保证作业出现 fail-over 后可以从最新的快照进行恢复，即分布式快照机制可以保证 Flink 系统内部的“精确一次”处理。但是我们在实际生产系统中，Flink 会对接各种各样的外部系统，比如 Kafka、HDFS 等，一旦 Flink 作业出现失败，作业会重新消费旧数据，这时候就会出现重新消费的情况，也就是重复消费。针对这种情况，Flink 1.4 版本引入了一个很重要的功能：两阶段提交，也就是 TwoPhaseCommitSinkFunction。两阶段搭配特定的 source 和 sink（特别是 0.11 版本 Kafka）使得“精确一次处理语义”成为可能。在 Flink 中两阶段提交的实现方法被封装到了 TwoPhaseCommitSinkFunction 这个抽象类中，我们只需要实现其中的beginTransaction、preCommit、commit、abort 四个方法就可以实现“精确一次”的处理语义，实现的方式我们可以在官网中查到：beginTransaction，在开启事务之前，我们在目标文件系统的临时目录中创建一个临时文件，后面在处理数据时将数据写入此文件；preCommit，在预提交阶段，刷写（flush）文件，然后关闭文件，之后就不能写入到文件了，我们还将为属于下一个检查点的任何后续写入启动新事务；commit，在提交阶段，我们将预提交的文件原子性移动到真正的目标目录中，请注意，这会增加输出数据可见性的延迟；abort，在中止阶段，我们删除临时文件。Flink-Kafka Exactly-once如上图所示，我们用 Kafka-Flink-Kafka 这个案例来介绍一下实现“端到端精确一次”语义的过程，整个过程包括：从 Kafka 读取数据窗口聚合操作将数据写回 Kafka整个过程可以总结为下面四个阶段：一旦 Flink 开始做 checkpoint 操作，那么就会进入 pre-commit 阶段，同时 Flink JobManager 会将检查点 Barrier 注入数据流中 ；当所有的 barrier 在算子中成功进行一遍传递，并完成快照后，则 pre-commit 阶段完成；等所有的算子完成“预提交”，就会发起一个“提交”动作，但是任何一个“预提交”失败都会导致 Flink 回滚到最近的 checkpoint；pre-commit 完成，必须要确保 commit 也要成功，上图中的 Sink Operators 和 Kafka Sink 会共同来保证。现状目前 Flink 支持的精确一次 Source 列表如下表所示，你可以使用对应的 connector 来实现对应的语义要求：如果你需要实现真正的“端到端精确一次语义”，则需要 sink 的配合。目前 Flink 支持的列表如下表所示：总结由于强大的异步快照机制和两阶段提交，Flink 实现了“端到端的精确一次语义”，在特定的业务场景下十分重要，我们在进行业务开发需要语义保证时，要十分熟悉目前 Flink 支持的语义特性。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"一文搞懂Flink内部的Exactly Once和At Least Once","slug":"一文搞懂Flink内部的Exactly-Once和At-Least-Once","date":"2019-07-03T13:24:51.000Z","updated":"2020-07-03T13:26:50.945Z","comments":true,"path":"2019/07/03/一文搞懂Flink内部的Exactly-Once和At-Least-Once/","link":"","permalink":"cpeixin.cn/2019/07/03/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82Flink%E5%86%85%E9%83%A8%E7%9A%84Exactly-Once%E5%92%8CAt-Least-Once/","excerpt":"","text":"大纲介绍CheckPoint如何保障Flink任务的高可用CheckPoint中的状态简介如何实现全域一致的分布式快照？什么是barrier？什么是barrier对齐？为什么barrier对齐就是Exactly Once？为什么barrier不对齐就是 At Least Once？Flink简介Apache Flink® - Stateful Computations over Data StreamsApache Flink® - 数据流上的有状态计算Flink 1.8 DocumentState &amp; Fault Tolerance有状态函数和运算符在各个元素/事件的处理中存储数据（状态数据可以修改和查询，可以自己维护，根据自己的业务场景，保存历史数据或者中间结果到状态中）例如：当应用程序搜索某些事件模式时，状态将存储到目前为止遇到的事件序列。在每分钟/小时/天聚合事件时，状态保存待处理的聚合。当在数据点流上训练机器学习模型时，状态保持模型参数的当前版本。当需要管理历史数据时，状态允许有效访问过去发生的事件。什么是状态？无状态计算的例子比如：我们只是进行一个字符串拼接，输入 a，输出 a_666,输入b，输出 b_666输出的结果跟之前的状态没关系，符合幂等性。幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用有状态计算的例子计算pv、uv输出的结果跟之前的状态有关系，不符合幂等性，访问多次，pv会增加Flink的CheckPoint功能简介Flink CheckPoint 的存在就是为了解决flink任务failover掉之后，能够正常恢复任务。那CheckPoint具体做了哪些功能，为什么任务挂掉之后，通过CheckPoint能使得任务恢复呢？CheckPoint是通过给程序快照的方式使得将历史某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。问题来了，快照是什么鬼？能吃吗？SnapShot翻译为快照，指将程序中某些信息存一份，后期可以用来恢复。对于一个Flink任务来讲，快照里面到底保存着什么信息呢？晦涩难懂的概念怎么办？当然用案例来代替咯，用案例让大家理解快照里面到底存什么信息。选一个大家都比较清楚的指标，app的pv，flink该怎么统计呢？我们从Kafka读取到一条条的日志，从日志中解析出app_id，然后将统计的结果放到内存中一个Map集合，app_id做为key，对应的pv做为value，每次只需要将相应app_id 的pv值+1后put到Map中即可flink的Source task记录了当前消费到kafka test topic的所有partition的offset，为了方便理解CheckPoint的作用，这里先用一个partition进行讲解，假设名为 “test”的 topic只有一个partition0例：（0，1000）,表示0号partition目前消费到offset为1000的数据flink的pv task记录了当前计算的各app的pv值，为了方便讲解，我这里有两个app：app1、app2例：（app1，50000）（app2，10000），表示app1当前pv值为50000 ，表示app2当前pv值为10000，每来一条数据，只需要确定相应app_id，将相应的value值+1后put到map中即可该案例中，CheckPoint到底记录了什么信息呢？记录的其实就是第n次CheckPoint消费的offset信息和各app的pv值信息，记录一下发生CheckPoint当前的状态信息，并将该状态信息保存到相应的状态后端。（注：状态后端是保存状态的地方，决定状态如何保存，如何保障状态高可用，我们只需要知道，我们能从状态后端拿到offset信息和pv信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过checkpoint来恢复我们的应用程序）chk-100：{offset：（0，1000）pv：（app1，50000）（app2，10000）}该状态信息表示第100次CheckPoint的时候， partition 0 offset消费到了1000，pv统计结果为（app1，50000）（app2，10000）任务挂了，如何恢复？假如我们设置了三分钟进行一次CheckPoint，保存了上述所说的 chk-100 的CheckPoint状态后，过了十秒钟，offset已经消费到 （0，1100），pv统计结果变成了（app1，50080）（app2，10020），但是突然任务挂了，怎么办？莫慌，其实很简单，flink只需要从最近一次成功的CheckPoint保存的offset（0，1000）处接着消费即可，当然pv值也要按照状态里的pv值（app1，50000）（app2，10000）进行累加，不能从（app1，50080）（app2，10020）处进行累加，因为 partition 0 offset消费到 1000时，pv统计结果为（app1，50000）（app2，10000）当然如果你想从offset （0，1100）pv（app1，50080）（app2，10020）这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来。我们能做的最高效方式就是从最近一次成功的CheckPoint处恢复，也就是我一直所说的chk-100,以上讲解，基本就是CheckPoint承担的工作，描述的场景比较简单疑问，计算pv的task在一直运行，它怎么知道什么时候去做这个快照？或者说计算pv的task怎么保障它自己计算的pv值（app1，50000）（app2，10000）就是offset（0，1000）那一刻的统计结果呢？flink是在数据中加了一个叫做barrier的东西（barrier中文翻译：栅栏），下图中红圈处就是两个barrierbarrier从Source Task处生成，一直流到Sink Task，期间所有的Task只要碰到barrier，就会触发自身进行快照CheckPoint , barrier n-1处做的快照就是指Job从开始处理到 barrier n-1所有的状态数据, barrier n 处做的快照就是指从Job开始到处理到 barrier n所有的状态数据对应到pv案例中就是，Source Task接收到JobManager的编号为chk-100的CheckPoint触发请求后，发现自己恰好接收到kafka offset（0，1000）处的数据，所以会往offset（0，1000）数据之后offset（0，1001）数据之前安插一个barrier，然后自己开始做快照，也就是将offset（0，1000）保存到状态后端chk-100中。然后barrier接着往下游发送，当统计pv的task接收到barrier后，也会暂停处理数据，将自己内存中保存的pv信息（app1，50000）（app2，10000）保存到状态后端chk-100中。OK，flink大概就是通过这个原理来保存快照的。统计pv的task接收到barrier，就意味着barrier之前的数据都处理了，所以说，不会出现丢数据的情况barrier的作用就是为了把数据区分开，CheckPoint过程中有一个同步做快照的环节不能处理barrier之后的数据，为什么呢？如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据结合案例来讲就是，统计pv的task想对（app1，50000）（app2，10000）做快照，但是如果数据还在处理，可能快照还没保存下来，状态已经变成了（app1，50001）（app2，10001），快照就不准确了，就不能保障Exactly Once了总结流式计算中状态交互简易场景精确一次的容错方法，周期性地对消费offset和统计的状态信息或统计结果进行快照消费到X位置的时候，将X对应的状态保存下来消费到Y位置的时候，将Y对应的状态保存下来多并行度、多Operator情况下，CheckPoint过程分布式状态容错面临的问题与挑战如何确保状态拥有精确一次的容错保证？如何在分布式场景下替多个拥有本地状态的算子产生一个全域一致的快照？如何在不中断运算的前提下产生快照？多并行度、多Operator实例的情况下，如何做全域一致的快照所有的Operator运行过程中遇到barrier后，都对自身的状态进行一次快照，保存到相应状态后端对应到pv案例：有的Operator计算的app1的pv，有的Operator计算的app2的pv，当他们碰到barrier时，都需要将目前统计的pv信息快照到状态后端多Operator状态恢复具体怎么做这个快照呢？利用之前所讲的barrier策略JobManager向Source Task发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrierSource Task自身做快照，并保存到状态后端Source Task将barrier跟数据流一块往下游发送当下游的Operator实例接收到CheckPoint barrier后，对自身做快照多并行度快照详图上述图中，有4个带状态的Operator实例，相应的状态后端就可以想象成填4个格子。整个CheckPoint 的过程可以当做Operator实例填自己格子的过程，Operator实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单的认为一次完整的CheckPoint做完了上面只是快照的过程，整个CheckPoint执行过程如下1、JobManager端的 CheckPointCoordinator向 所有SourceTask发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier2、当task收到所有的barrier后，向自己的下游继续传递barrier，然后自身执行快照，并将自己的状态异步写入到持久化存储中增量CheckPoint只是把最新的一部分更新写入到外部存储为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照3、当task完成备份后，会将备份数据的地址（state handle）通知给JobManager的CheckPointCoordinator如果CheckPoint的持续时长超过 了CheckPoint设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次CheckPoint失败，会把这次CheckPoint产生的所有 状态数据全部删除4、 最后 CheckPoint Coordinator 会把整个 StateHandle 封装成 completed CheckPoint Meta，写入到hdfsbarrier对齐什么是barrier对齐？一旦Operator从输入流接收到CheckPoint barrier n，它就不能处理来自该流的任何数据记录，直到它从其他所有输入接收到barrier n为止。否则，它会混合属于快照n的记录和属于快照n + 1的记录接收到barrier n的流暂时被搁置。从这些流接收的记录不会被处理，而是放入输入缓冲区。上图中第2个图，虽然数字流对应的barrier已经到达了，但是barrier之后的1、2、3这些数据只能放到buffer中，等待字母流的barrier到达一旦最后所有输入流都接收到barrier n，Operator就会把缓冲区中pending 的输出数据发出去，然后把CheckPoint barrier n接着往下游发送这里还会对自身进行快照，之后，Operator将继续处理来自所有输入流的记录，在处理来自流的记录之前先处理来自输入缓冲区的记录什么是barrier不对齐？上述图2中，当还有其他输入流的barrier还没有到达时，会把已到达的barrier之后的数据1、2、3搁置在缓冲区，等待其他流的barrier到达后才能处理barrier不对齐就是指当还有其他流的barrier还没到达时，为了不影响性能，也不用理会，直接处理barrier之后的数据。等到所有流的barrier的都到达后，就可以对该Operator做CheckPoint了为什么要进行barrier对齐？不对齐到底行不行？答：Exactly Once时必须barrier对齐，如果barrier不对齐就变成了At Least Once后面的部分主要证明这句话CheckPoint的目的就是为了保存快照，如果不对齐，那么在chk-100快照之前，已经处理了一些chk-100 对应的offset之后的数据，当程序从chk-100恢复任务时，chk-100对应的offset之后的数据还会被处理一次，所以就出现了重复消费。如果听不懂没关系，后面有案例让您懂结合pv案例来看，之前的案例为了简单，描述的kafka的topic只有1个partition，这里为了讲述barrier对齐，所以topic有2个partittion结合业务，先介绍一下上述所有算子在业务中的功能Source的kafka的Consumer，从kakfa中读取数据到flink应用中TaskA中的map将读取到的一条kafka日志转换为我们需要统计的app_idkeyBy 按照app_id进行keyBy，相同的app_id 会分到下游TaskB的同一个实例中TaskB的map在状态中查出该app_id 对应的pv值，然后+1，存储到状态中利用Sink将统计的pv值写入到外部存储介质中我们从kafka的两个partition消费数据，TaskA和TaskB都有两个并行度，所以总共flink有4个Operator实例，这里我们称之为 TaskA0、TaskA1、TaskB0、TaskB1假设已经成功做了99次CheckPoint，这里详细解释第100次CheckPoint过程JobManager内部有个定时调度，假如现在10点00分00秒到了第100次CheckPoint的时间了，JobManager的CheckPointCoordinator进程会向所有的Source Task发送CheckPointTrigger，也就是向TaskA0、TaskA1发送CheckPointTriggerTaskA0、TaskA1接收到CheckPointTrigger，会往数据流中安插barrier，将barrier发送到下游，在自己的状态中记录barrier安插的offset位置，然后自身做快照，将offset信息保存到状态后端这里假如TaskA0消费的partition0的offset为10000，TaskA1消费的partition1的offset为10005。那么状态中会保存 (0，10000)(1，10005)，表示0号partition消费到了offset为10000的位置，1号partition消费到了offset为10005的位置然后TaskA的map和keyBy算子中并没有状态，所以不需要进行快照接着数据和barrier都向下游TaskB发送，相同的app_id 会发送到相同的TaskB实例上，这里假设有两个app：app0和app1，经过keyBy后，假设app0分到了TaskB0上，app1分到了TaskB1上。基于上面描述，TaskA0和TaskA1中的所有app0的数据都发送到TaskB0上，所有app1的数据都发送到TaskB1上现在我们假设TaskB0做CheckPoint的时候barrier对齐了，TaskB1做CheckPoint的时候barrier不对齐，当然不能这么配置，我就是举这么个例子，带大家分析一下barrier对不对齐到底对统计结果有什么影响？上面说了chk-100的这次CheckPoint，offset位置为(0，10000)(1，10005)，TaskB0使用barrier对齐，也就是说TaskB0不会处理barrier之后的数据，所以TaskB0在chk-100快照的时候，状态后端保存的app0的pv数据是从程序开始启动到kafka offset位置为(0，10000)(1，10005)的所有数据计算出来的pv值，一条不多（没处理barrier之后，所以不会重复），一条不少(barrier之前的所有数据都处理了，所以不会丢失)，假如保存的状态信息为(app0，8000)表示消费到(0，10000)(1，10005)offset的时候，app0的pv值为8000TaskB1使用的barrier不对齐，假如TaskA0由于服务器的CPU或者网络等其他波动，导致TaskA0处理数据较慢，而TaskA1很稳定，所以处理数据比较快。导致的结果就是TaskB1先接收到了TaskA1的barrier，由于配置的barrier不对齐，所以TaskB1会接着处理TaskA1 barrier之后的数据，过了2秒后，TaskB1接收到了TaskA0的barrier，于是对状态中存储的app1的pv值开始做CheckPoint 快照，保存的状态信息为(app1，12050)，但是我们知道这个(app1，12050)实际上多处理了2秒TaskA1发来的barrier之后的数据，也就是kafka topic对应的partition1 offset 10005之后的数据，app1真实的pv数据肯定要小于这个12050，partition1的offset保存的offset虽然是10005，但是我们实际上可能已经处理到了offset 10200的数据，假设就是处理到了10200虽然状态保存的pv值偏高了，但是不能说明重复处理，因为我的TaskA1并没有再次去消费partition1的offset 10005~10200的数据，所以相当于也没有重复消费，只是展示的结果更实时了分析到这里，我们先梳理一下我们的状态保存了什么：chk-100offset：(0，10000)(1，10005)pv：(app0，8000) (app1，12050)接着程序在继续运行，过了10秒，由于某个服务器挂了，导致我们的四个Operator实例有一个Operator挂了，所以Flink会从最近一次的状态恢复，也就是我们刚刚详细讲的chk-100处恢复，那具体是怎么恢复的呢？Flink 同样会起四个Operator实例，我还称他们是 TaskA0、TaskA1、TaskB0、TaskB1。四个Operator会从状态后端读取保存的状态信息。从offset：(0，10000)(1，10005) 开始消费，并且基于 pv：(app0，8000) (app1，12050)值进行累加统计然后你就应该会发现这个app1的pv值12050实际上已经包含了partition1的offset 1000510200的数据，所以partition1从offset 10005恢复任务时，partition1的offset 1000510200的数据被消费了两次TaskB1设置的barrier不对齐，所以CheckPoint chk-100对应的状态中多消费了barrier之后的一些数据（TaskA1发送），重启后是从chk-100保存的offset恢复，这就是所说的At Least Once由于上面说TaskB0设置的barrier对齐，所以app0不会出现重复消费，因为app0没有消费offset：(0，10000)(1，10005) 之后的数据，也就是所谓的Exactly Once看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么barrier对齐就是Exactly Once，为什么barrier不对齐就是 At Least Once到底什么时候会出现barrier对齐？首先设置了Flink的CheckPoint语义是：Exactly OnceOperator实例必须有多个输入流才会出现barrier对齐- 对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以，必须有多个流才叫对齐。barrier对齐其实也就是上游多个流配合使得数据对齐的过程 - 言外之意：如果Operator实例只有一个输入流，就根本不存在barrier对齐，自己跟自己默认永远都是对齐的Q&amp;A第一种场景计算PV，kafka只有一个partition，精确一次，至少一次就没有区别？答：如果只有一个partition，对应flink任务的Source Task并行度只能是1，确实没有区别，不会有至少一次的存在了，肯定是精确一次。因为只有barrier不对齐才会有可能重复处理，这里并行度都已经为1，默认就是对齐的，只有当上游有多个并行度的时候，多个并行度发到下游的barrier才需要对齐，单并行度不会出现barrier不对齐，所以必然精确一次。其实还是要理解barrier对齐就是Exactly Once不会重复消费，barrier不对齐就是 At Least Once可能重复消费，这里只有单个并行度根本不会存在barrier不对齐，所以不会存在至少一次语义为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照；这一步，如果向下发送barrier后，自己同步快照慢怎么办？下游已经同步好了，自己还没？答: 可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游快照的更及时了，我只要保障下游把barrier之前的数据都处理了，并且不处理barrier之后的数据，然后做快照，那么下游也同样支持精确一次。这个问题你不要从全局思考，你单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意一点，如果有一个Operator 的CheckPoint失败了或者因为CheckPoint超时也会导致失败，那么JobManager会认为整个CheckPoint失败。失败的CheckPoint是不能用来恢复任务的，必须所有的算子的CheckPoint都成功，那么这次CheckPoint才能认为是成功的，才能用来恢复任务我程序中Flink的CheckPoint语义设置了 Exactly Once，但是我的mysql中看到数据重复了？程序中设置了1分钟1次CheckPoint，但是5秒向mysql写一次数据，并commit答：Flink要求end to end的精确一次都必须实现TwoPhaseCommitSinkFunction。如果你的chk-100成功了，过了30秒，由于5秒commit一次，所以实际上已经写入了6批数据进入mysql，但是突然程序挂了，从chk100处恢复，这样的话，之前提交的6批数据就会重复写入，所以出现了重复消费。Flink的精确一次有两种情况，一个是Flink内部的精确一次，一个是端对端的精确一次，这个博客所描述的都是关于Flink内部去的精确一次，我后期再发一个博客详细介绍一下Flink端对端的精确一次如何实现转载自：https://www.jianshu.com/p/8d6569361999","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"flink","slug":"flink","permalink":"cpeixin.cn/tags/flink/"}]},{"title":"Kafka 无消息丢失配置","slug":"Kafka-无消息丢失配置","date":"2019-06-27T06:35:15.000Z","updated":"2020-09-02T06:36:12.136Z","comments":true,"path":"2019/06/27/Kafka-无消息丢失配置/","link":"","permalink":"cpeixin.cn/2019/06/27/Kafka-%E6%97%A0%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一直以来，很多人对于 Kafka 丢失消息这件事情都有着自己的理解，因而也就有着自己的解决之道。在讨论具体的应对方法之前，我觉得我们首先要明确，在 Kafka 的世界里什么才算是消息丢失，或者说 Kafka 在什么情况下能保证消息不丢失。这点非常关键，因为很多时候我们容易混淆责任的边界，如果搞不清楚事情由谁负责，自然也就不知道由谁来出解决方案了。那 Kafka 到底在什么情况下才能保证消息不丢失呢？一句话概括，Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。这句话里面有两个核心要素，我们一一来看。第一个核心要素是“已提交的消息”。什么是已提交的消息？当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为“已提交”消息了。那为什么是若干个 Broker 呢？这取决于你对“已提交”的定义。你可以选择只要有一个 Broker 成功保存该消息就算是已提交，也可以是令所有 Broker 都成功保存该消息才算是已提交。不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。第二个核心要素就是“有限度的持久化保证”，也就是说 Kafka 不可能保证在任何情况下都做到不丢失消息。举个极端点的例子，如果地球都不存在了，Kafka 还能保存任何消息吗？显然不能！倘若这种情况下你依然还想要 Kafka 不丢消息，那么只能在别的星球部署 Kafka Broker 服务器了。现在你应该能够稍微体会出这里的“有限度”的含义了吧，其实就是说 Kafka 不丢消息是有前提条件的。假如你的消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N 个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。总结一下，Kafka 是能做到不丢失消息的，只不过这些消息必须是已提交的消息，而且还要满足一定的条件。当然，说明这件事并不是要为 Kafka 推卸责任，而是为了在出现该类问题时我们能够明确责任边界。“消息丢失”案例案例 1：生产者程序丢失数据Producer 程序丢失消息，这应该算是被抱怨最多的数据丢失场景了。我来描述一个场景：你写了一个 Producer 应用向 Kafka 发送消息，最后发现 Kafka 没有保存，于是大骂：“Kafka 真烂，消息发送居然都能丢失，而且还不告诉我？！”如果你有过这样的经历，那么请先消消气，我们来分析下可能的原因。目前 Kafka Producer 是异步发送消息的，也就是说如果你调用的是 producer.send(msg) 这个 API，那么它通常会立即返回，但此时你不能认为消息发送已成功完成。这种发送方式有个有趣的名字，叫“fire and forget”，翻译一下就是“发射后不管”。这个术语原本属于导弹制导领域，后来被借鉴到计算机领域中，它的意思是，执行完一个操作后不去管它的结果是否成功。调用 producer.send(msg) 就属于典型的“fire and forget”，因此如果出现消息丢失，我们是无法知晓的。这个发送方式挺不靠谱吧，不过有些公司真的就是在使用这个 API 发送消息。如果用这个方式，可能会有哪些因素导致消息没有发送成功呢？其实原因有很多，例如网络抖动，导致消息压根就没有发送到 Broker 端；或者消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等。这么来看，让 Kafka“背锅”就有点冤枉它了。就像前面说过的，Kafka 不认为消息是已提交的，因此也就没有 Kafka 丢失消息这一说了。不过，就算不是 Kafka 的“锅”，我们也要解决这个问题吧。实际上，解决此问题的方法非常简单：Producer 永远要使用带有回调通知的发送 API，也就是说不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。不要小瞧这里的 callback（回调），它能准确地告诉你消息是否真的提交成功了。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。举例来说，如果是因为那些瞬时错误，那么仅仅让 Producer 重试就可以了；如果是消息不合格造成的，那么可以调整消息格式后再次发送。总之，处理发送失败的责任在 Producer 端而非 Broker 端。你可能会问，发送失败真的没可能是由 Broker 端的问题造成的吗？当然可能！如果你所有的 Broker 都宕机了，那么无论 Producer 端怎么重试都会失败的，此时你要做的是赶快处理 Broker 端的问题。但之前说的核心论据在这里依然是成立的：Kafka 依然不认为这条消息属于已提交消息，故对它不做任何持久化保证。案例 2：消费者程序丢失数据Consumer 端丢失数据主要体现在 Consumer 端要消费的消息不见了。Consumer 程序有个“位移”的概念，表示的是这个 Consumer 当前消费到的 Topic 分区的位置。下面这张图来自于官网，它清晰地展示了 Consumer 端的位移数据。比如对于 Consumer A 而言，它当前的位移值就是 9；Consumer B 的位移值是 11。这里的“位移”类似于我们看书时使用的书签，它会标记我们当前阅读了多少页，下次翻书的时候我们能直接跳到书签页继续阅读。正确使用书签有两个步骤：第一步是读书，第二步是更新书签页。如果这两步的顺序颠倒了，就可能出现这样的场景：当前的书签页是第 90 页，我先将书签放到第 100 页上，之后开始读书。当阅读到第 95 页时，我临时有事中止了阅读。那么问题来了，当我下次直接跳到书签页阅读时，我就丢失了第 96～99 页的内容，即这些消息就丢失了。同理，Kafka 中 Consumer 端的消息丢失就是这么一回事。要对抗这种消息丢失，办法很简单：维持先消费消息（阅读），再更新位移（书签）的顺序即可。这样就能最大限度地保证消息不丢失。当然，这种处理方式可能带来的问题是消息的重复处理，类似于同一页书被读了很多遍，但这不属于消息丢失的情形。在专栏后面的内容中，我会跟你分享如何应对重复消费的问题。除了上面所说的场景，其实还存在一种比较隐蔽的消息丢失场景。我们依然以看书为例。假设你花钱从网上租借了一本共有 10 章内容的电子书，该电子书的有效阅读时间是 1 天，过期后该电子书就无法打开，但如果在 1 天之内你完成阅读就退还租金。为了加快阅读速度，你把书中的 10 个章节分别委托给你的 10 个朋友，请他们帮你阅读，并拜托他们告诉你主旨大意。当电子书临近过期时，这 10 个人告诉你说他们读完了自己所负责的那个章节的内容，于是你放心地把该书还了回去。不料，在这 10 个人向你描述主旨大意时，你突然发现有一个人对你撒了谎，他并没有看完他负责的那个章节。那么很显然，你无法知道那一章的内容了。对于 Kafka 而言，这就好比 Consumer 程序从 Kafka 获取到消息后开启了多个线程异步处理消息，而 Consumer 程序自动地向前更新位移。假如其中某个线程运行失败了，它负责的消息没有被成功处理，但位移已经被更新了，因此这条消息对于 Consumer 而言实际上是丢失了。这里的关键在于 Consumer 自动提交位移，与你没有确认书籍内容被全部读完就将书归还类似，你没有真正地确认消息是否真的被消费就“盲目”地更新了位移。这个问题的解决方案也很简单：如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移。在这里我要提醒你一下，单个 Consumer 程序使用多线程来消费消息说起来容易，写成代码却异常困难，因为你很难正确地处理位移的更新，也就是说避免无消费消息丢失很简单，但极易出现消息被消费了多次的情况。最佳实践看完这两个案例之后，我来分享一下 Kafka 无消息丢失的配置，每一个其实都能对应上面提到的问题。不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用带有回调通知的 send 方法。设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries &gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。设置 replication.factor &gt;= 3。这也是 Broker 端的参数。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。设置 min.insync.replicas &gt; 1。这依然是 Broker 端参数，控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。确保 replication.factor &gt; min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的。Q&amp;AQ:kafka是在落地刷盘之后，同步副本成功后，才能会被消费吗？A:有可能在落盘之前就被消费了。能否被消费不是看是否flush到磁盘，而是看leader副本的高水位是否越过了该条消息Q:producer发送消息后，broker成功写入消息了，但是ack因为网络问题没有到达producer，生产者可能会重试发送这条消息。这种问题如何避免重复消费呢A:使用幂等producer","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"Kafka生产者消息分区原理","slug":"Kafka生产者消息分区原理","date":"2019-06-20T04:07:59.000Z","updated":"2020-09-02T04:09:31.634Z","comments":true,"path":"2019/06/20/Kafka生产者消息分区原理/","link":"","permalink":"cpeixin.cn/2019/06/20/Kafka%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E5%88%86%E5%8C%BA%E5%8E%9F%E7%90%86/","excerpt":"","text":"为什么分区？Kafka 的消息组织方式实际上是三级结构：主题 - 分区 - 消息。主题下的每条消息只会保存在某一个分区中，而不会在多个分区中被保存多份。你觉得为什么 Kafka 要做这样的设计？为什么使用分区的概念而不是直接使用多个主题呢？其实分区的作用就是提供负载均衡的能力，或者说对数据进行分区的主要原因，就是为了实现系统的高伸缩性（Scalability）。不同的分区能够被放置到不同节点的机器上，而数据的读写操作也都是针对分区这个粒度而进行的，这样每个节点的机器都能独立地执行各自分区的读写请求处理。并且，我们还可以通过添加新的节点机器来增加整体系统的吞吐量。实际上分区的概念以及分区数据库早在 1980 年就已经有大牛们在做了，比如那时候有个叫 Teradata 的数据库就引入了分区的概念。值得注意的是，不同的分布式系统对分区的叫法也不尽相同。比如在 Kafka 中叫分区，在 MongoDB 和 Elasticsearch 中就叫分片 Shard，而在 HBase 中则叫 Region，在 Cassandra 中又被称作 vnode。从表面看起来它们实现原理可能不尽相同，但对底层分区（Partitioning）的整体思想却从未改变。除了提供负载均衡这种最核心的功能之外，利用分区也可以实现其他一些业务级别的需求，比如实现业务级别的消息顺序的问题都有哪些分区策略？下面我们说说 Kafka 生产者的分区策略。所谓分区策略是决定生产者将消息发送到哪个分区的算法。轮询策略也称 Round-robin 策略，即顺序分配。比如一个主题下有 3 个分区，那么第一条消息被发送到分区 0，第二条被发送到分区 1，第三条被发送到分区 2，以此类推。当生产第 4 条消息时又会重新开始，即将其分配到分区 0，就像下面这张图展示的那样轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上，故默认情况下它是最合理的分区策略，也是我们最常用的分区策略之一。随机策略也称 Randomness 策略。所谓随机就是我们随意地将消息放置到任意一个分区上，如下面这张图所示。本质上看随机策略也是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于轮询策略，所以如果追求数据的均匀分布，还是使用轮询策略比较好。事实上，随机策略是老版本生产者使用的分区策略，在新版本中已经改为轮询了。按key保序策略Kafka 允许为每条消息定义消息键，简称为 Key。这个 Key 的作用非常大，它可以是一个有着明确业务含义的字符串，比如客户代码、部门编号或是业务 ID 等；也可以用来表征消息元数据。特别是在 Kafka 不支持时间戳的年代，在一些场景中，工程师们都是直接将消息创建时间封装进 Key 里面的。一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略，如下图所示。小结今天我们讨论了 Kafka 生产者消息分区的机制以及常见的几种分区策略。切记分区是实现负载均衡以及高吞吐量的关键，故在生产者这一端就要仔细盘算合适的分区策略，避免造成消息数据的“倾斜”，使得某些分区成为性能瓶颈，这样极易引发下游数据消费的性能下降。Q&amp;AQ:在消息重试的时候，分区策略会重新再计算一次吗？比如一开始选择到5号分区，但是5号分区有问题导致重试，重试的时候可以重试发送到别的分区上吗？A:不会的。消息重试只是简单地将消息重新发送到之前的分区Q: **要实现分区数据有序性，一定是要单分区才可以实现么？A: 使用key+多分区也可以实现。反正保证同一批因果依赖的消息分到一个分区就可以Q:Kafka支持事务消息吗？A:0.11开始支持事务了。嗯，并没有所谓的事务消息，不过倒是有事务标记消息（transaciton marker）。事务Consumer靠它来判断消息的可见性——即什么消息属于已提交事务的消息，事务consumer能够读取。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"Flink核心概念","slug":"Flink核心概念","date":"2019-06-17T07:32:00.000Z","updated":"2020-10-08T13:30:46.230Z","comments":true,"path":"2019/06/17/Flink核心概念/","link":"","permalink":"cpeixin.cn/2019/06/17/Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"","text":"Flink Application ClusterFlink Application Cluster 是一个专用的 Flink Cluster，它仅用于执行单个 Flink Job。Flink Cluster的生命周期与 Flink Job的生命周期绑定在一起。以前，Flink Application Cluster 也称为_job mode_的 Flink Cluster。和 Flink Session Cluster 作对比。Flink Cluster一般情况下，Flink 集群是由一个 Flink Master 和一个或多个 Flink TaskManager 进程组成的分布式系统。EventEvent 是对应用程序建模的域的状态更改的声明。它可以同时为流或批处理应用程序的 input 和 output，也可以单独是 input 或者 output 中的一种。Event 是特殊类型的 Record。ExecutionGraph见 Physical Graph。FunctionFunction 是由用户实现的，并封装了 Flink 程序的应用程序逻辑。大多数 Function 都由相应的 Operator 封装。InstanceInstance 常用于描述运行时的特定类型(通常是 Operator 或者 Function)的一个具体实例。由于 Apache Flink 主要是用 Java 编写的，所以，这与 Java 中的 Instance 或 Object 的定义相对应。在 Apache Flink 的上下文中，parallel instance 也常用于强调同一 Operator 或者 Function 的多个 instance 以并行的方式运行。Flink JobFlink Job 代表运行时的 Flink 程序。Flink Job 可以提交到长时间运行的 Flink Session Cluster，也可以作为独立的 Flink Application Cluster 启动。JobGraph见 Logical Graph。Flink JobManagerJobManager 是在 Flink Master 运行中的组件之一。JobManager 负责监督单个作业 Task 的执行。以前，整个 Flink Master 都叫做 JobManager。Logical GraphLogical Graph 是一种描述流处理程序的高阶逻辑有向图。节点是Operator，边代表输入/输出关系、数据流和数据集中的之一。Managed StateManaged State 描述了已在框架中注册的应用程序的托管状态。对于托管状态，Apache Flink 会负责持久化和重伸缩等事宜。Flink MasterFlink Master 是 Flink Cluster 的主节点。它包含三个不同的组件：Flink Resource Manager、Flink Dispatcher、运行每个 Flink Job 的 Flink JobManager。OperatorLogical Graph 的节点。算子执行某种操作，该操作通常由 Function 执行。Source 和 Sink 是数据输入和数据输出的特殊算子。Operator Chain算子链由两个或多个连续的 Operator 组成，两者之间没有任何的重新分区。同一算子链内的算子可以彼此直接传递 record，而无需通过序列化或 Flink 的网络栈。Partition分区是整个数据流或数据集的独立子集。通过将每个 Record 分配给一个或多个分区，来把数据流或数据集划分为多个分区。在运行期间，Task 会消费数据流或数据集的分区。改变数据流或数据集分区方式的转换通常称为重分区。Physical GraphPhysical graph 是一个在分布式运行时，把 Logical Graph 转换为可执行的结果。节点是 Task，边表示数据流或数据集的输入/输出关系或 partition。RecordRecord 是数据集或数据流的组成元素。Operator 和 Function接收 record 作为输入，并将 record 作为输出发出。Flink Session Cluster长时间运行的 Flink Cluster，它可以接受多个 Flink Job 的执行。此 Flink Cluster 的生命周期不受任何 Flink Job 生命周期的约束限制。以前，Flink Session Cluster 也称为 session mode 的 Flink Cluster，和 Flink Application Cluster 相对应。State Backend对于流处理程序，Flink Job 的 State Backend 决定了其 state 是如何存储在每个 TaskManager 上的（ TaskManager 的 Java 堆栈或嵌入式 RocksDB），以及它在 checkpoint 时的写入位置（ Flink Master 的 Java 堆或者 Filesystem）。Sub-TaskSub-Task 是负责处理数据流 Partition 的 Task。”Sub-Task”强调的是同一个 Operator 或者 Operator Chain 具有多个并行的 Task 。TaskTask 是 Physical Graph 的节点。它是基本的工作单元，由 Flink 的 runtime 来执行。Task 正好封装了一个 Operator 或者 Operator Chain 的 _parallel instance_。Flink TaskManagerTaskManager 是 Flink Cluster 的工作进程。Task 被调度到 TaskManager 上执行。TaskManager 相互通信，只为在后续的 Task 之间交换数据。TransformationTransformation 应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。Transformation 可能会在每个记录的基础上更改数据流或数据集，但也可以只更改其分区或执行聚合。虽然 Operator 和 Function 是 Flink API 的“物理”部分，但 Transformation 只是一个 API 概念。具体来说，大多数（但不是全部）Transformation 是由某些 Operator 实现的。分布式缓存熟悉 Hadoop 的你应该知道，分布式缓存最初的思想诞生于 Hadoop 框架，Hadoop 会将一些数据或者文件缓存在 HDFS 上，在分布式环境中让所有的计算节点调用同一个配置文件。在 Flink 中，Flink 框架开发者们同样将这个特性进行了实现。Flink 提供的分布式缓存类型 Hadoop，目的是为了在分布式环境中让每一个 TaskManager 节点保存一份相同的数据或者文件，当前计算节点的 task 就像读取本地文件一样拉取这些配置。分布式缓存在我们实际生产环境中最广泛的一个应用，就是在进行表与表 Join 操作时，如果一个表很大，另一个表很小，那么我们就可以把较小的表进行缓存，在每个 TaskManager 都保存一份，然后进行 Join 操作。那么我们应该怎样使用 Flink 的分布式缓存呢？举例如下：12345678910111213141516171819202122232425262728public static void main(String[] args) throws Exception &#123;final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.registerCachedFile(\"/Users/wangzhiwu/WorkSpace/quickstart/distributedcache.txt\", \"distributedCache\"); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试 DataSource&lt;String&gt; data = env.fromElements(\"Linea\", \"Lineb\", \"Linec\", \"Lined\"); DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用该缓存文件 File myFile = getRuntimeContext().getDistributedCache().getFile(\"distributedCache\"); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; this.dataList.add(line); System.err.println(\"分布式缓存为:\" + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println(\"使用datalist：\" + dataList + \"-------\" +value); //业务逻辑 return dataList +\"：\" + value; &#125; &#125;); result.printToErr(); &#125;从上面的例子中可以看出，使用分布式缓存有两个步骤。第一步：首先需要在 env 环境中注册一个文件，该文件可以来源于本地，也可以来源于 HDFS ，并且为该文件取一个名字。第二步：在使用分布式缓存时，可根据注册的名字直接获取。可以看到，在上述案例中，我们把一个本地的 distributedcache.txt 文件注册为 distributedCache，在下面的 map 算子中直接通过这个名字将缓存文件进行读取并且进行了处理。我们直接运行该程序，在控制台可以看到如下输出：在使用分布式缓存时也需要注意一些问题，需要我们缓存的文件在任务运行期间最好是只读状态，否则会造成数据的一致性问题。另外，缓存的文件和数据不宜过大，否则会影响 Task 的执行速度，在极端情况下会造成 OOM。故障恢复和重启策略自动故障恢复是 Flink 提供的一个强大的功能，在实际运行环境中，我们会遇到各种各样的问题从而导致应用挂掉，比如我们经常遇到的非法数据、网络抖动等。Flink 提供了强大的可配置故障恢复和重启策略来进行自动恢复。故障恢复我们在上一课时中介绍过 Flink 的配置文件，其中有一个参数 jobmanager.execution.failover-strategy: region。Flink 支持了不同级别的故障恢复策略，jobmanager.execution.failover-strategy 的可配置项有两种：full 和 region。当我们配置的故障恢复策略为 full 时，集群中的 Task 发生故障，那么该任务的所有 Task 都会发生重启。而在实际生产环境中，我们的大作业可能有几百个 Task，出现一次异常如果进行整个任务重启，那么经常会导致长时间任务不能正常工作，导致数据延迟。但是事实上，我们可能只是集群中某一个或几个 Task 发生了故障，只需要重启有问题的一部分即可，这就是 Flink **基于 Region 的局部重启策略。在这个策略下，Flink 会把我们的任务分成不同的 Region，当某一个 Task 发生故障时，Flink 会计算需要故障恢复的最小 Region。Flink 在判断需要重启的 Region 时，采用了以下的判断逻辑：发生错误的 Task 所在的 Region 需要重启；如果当前 Region 的依赖数据出现损坏或者部分丢失，那么生产数据的 Region 也需要重启；为了保证数据一致性，当前 Region 的下游 Region 也需要重启。重启策略Flink 提供了多种类型和级别的重启策略，常用的重启策略包括：固定延迟重启策略模式失败率重启策略模式无重启策略模式Flink 在判断使用的哪种重启策略时做了默认约定，如果用户配置了 checkpoint，但没有设置重启策略，那么会按照固定延迟重启策略模式进行重启；如果用户没有配置 checkpoint，那么默认不会重启。下面我们分别对这三种模式进行详细讲解。无重启策略模式在这种情况下，如果我们的作业发生错误，任务会直接退出。我们可以在 flink-conf.yaml 中配置：1restart-strategy: none也可以在程序中使用代码指定：12final ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart());固定延迟重启策略模式固定延迟重启策略会通过在 flink-conf.yaml 中设置如下配置参数，来启用此策略：1restart-strategy: fixed-delay固定延迟重启策略模式需要指定两个参数，首先 Flink 会根据用户配置的重试次数进行重试，每次重试之间根据配置的时间间隔进行重试，如下表所示：举个例子，假如我们需要任务重试 3 次，每次重试间隔 5 秒，那么需要进行一下配置：12restart-strategy.fixed-delay.attempts: 3restart-strategy.fixed-delay.delay: 5 s当前我们也可以在代码中进行设置：1234env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, &#x2F;&#x2F; 重启次数 Time.of(5, TimeUnit.SECONDS) &#x2F;&#x2F; 时间间隔));失败率重启策略模式**首先我们在 flink-conf.yaml 中指定如下配置：1restart-strategy: failure-rate这种重启模式需要指定三个参数，如下表所示。失败率重启策略在 Job 失败后会重启，但是超过失败率后，Job 会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。这种策略的配置理解较为困难，我们举个例子，假如 5 分钟内若失败了 3 次，则认为该任务失败，每次失败的重试间隔为 5 秒。那么我们的配置应该是：123restart-strategy.failure-rate.max-failures-per-interval: 3restart-strategy.failure-rate.failure-rate-interval: 5 minrestart-strategy.failure-rate.delay: 5 s当然，也可以在代码中直接指定：12345env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, &#x2F;&#x2F; 每个时间间隔的最大故障次数 Time.of(5, TimeUnit.MINUTES), &#x2F;&#x2F; 测量故障率的时间间隔 Time.of(5, TimeUnit.SECONDS) &#x2F;&#x2F; 每次任务失败时间间隔));最后，需要注意的是，在实际生产环境中由于每个任务的负载和资源消耗不一样，我们推荐在代码中指定每个任务的重试机制和重启策略。并行度并行度是 Flink 执行任务的核心概念之一，它被定义为在分布式运行环境中我们的一个算子任务被切分成了多少个子任务并行执行。我们提高任务的并行度（Parallelism）在很大程度上可以大大提高任务运行速度。一般情况下，我们可以通过四种级别来设置任务的并行度。算子级别在代码中可以调用 setParallelism 方法来设置每一个算子的并行度。例如：1234DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts &#x3D; text.flatMap(new LineSplitter()) .groupBy(0) .sum(1).setParallelism(1);事实上，Flink 的每个算子都可以单独设置并行度。这也是我们最推荐的一种方式，可以针对每个算子进行任务的调优。执行环境级别我们在创建 Flink 的上下文时可以显示的调用 env.setParallelism() 方法，来设置当前执行环境的并行度，这个配置会对当前任务的所有算子、Source、Sink 生效。当然你还可以在算子级别设置并行度来覆盖这个设置。12final ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();env.setParallelism(5);提交任务级别用户在提交任务时，可以显示的指定 -p 参数来设置任务的并行度，例如：1.&#x2F;bin&#x2F;flink run -p 10 WordCount.jar系统配置级别我们在上一课时中提到了 flink-conf.yaml 中的一个配置：parallelism.default，该配置即是在系统层面设置所有执行环境的并行度配置。整体上讲，这四种级别的配置生效优先级如下：算子级别 &gt; 执行环境级别 &gt; 提交任务级别 &gt; 系统配置级别。在这里，要特别提一下 Flink 中的 Slot 概念。我们知道，Flink 中的 TaskManager 是执行任务的节点，那么在每一个 TaskManager 里，还会有“槽位”，也就是 Slot。Slot 个数代表的是每一个 TaskManager 的并发执行能力。假如我们指定 taskmanager.numberOfTaskSlots:3，即每个 taskManager 有 3 个 Slot ，那么整个集群就有 3 * taskManager 的个数多的槽位。这些槽位就是我们整个集群所拥有的所有执行任务的资源。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink核心概念","slug":"Flink-名词解释","date":"2019-06-17T07:32:00.000Z","updated":"2020-09-06T07:03:42.923Z","comments":true,"path":"2019/06/17/Flink-名词解释/","link":"","permalink":"cpeixin.cn/2019/06/17/Flink-%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/","excerpt":"","text":"Flink Application ClusterFlink Application Cluster 是一个专用的 Flink Cluster，它仅用于执行单个 Flink Job。Flink Cluster的生命周期与 Flink Job的生命周期绑定在一起。以前，Flink Application Cluster 也称为_job mode_的 Flink Cluster。和 Flink Session Cluster 作对比。Flink Cluster一般情况下，Flink 集群是由一个 Flink Master 和一个或多个 Flink TaskManager 进程组成的分布式系统。EventEvent 是对应用程序建模的域的状态更改的声明。它可以同时为流或批处理应用程序的 input 和 output，也可以单独是 input 或者 output 中的一种。Event 是特殊类型的 Record。ExecutionGraph见 Physical Graph。FunctionFunction 是由用户实现的，并封装了 Flink 程序的应用程序逻辑。大多数 Function 都由相应的 Operator 封装。InstanceInstance 常用于描述运行时的特定类型(通常是 Operator 或者 Function)的一个具体实例。由于 Apache Flink 主要是用 Java 编写的，所以，这与 Java 中的 Instance 或 Object 的定义相对应。在 Apache Flink 的上下文中，parallel instance 也常用于强调同一 Operator 或者 Function 的多个 instance 以并行的方式运行。Flink JobFlink Job 代表运行时的 Flink 程序。Flink Job 可以提交到长时间运行的 Flink Session Cluster，也可以作为独立的 Flink Application Cluster 启动。JobGraph见 Logical Graph。Flink JobManagerJobManager 是在 Flink Master 运行中的组件之一。JobManager 负责监督单个作业 Task 的执行。以前，整个 Flink Master 都叫做 JobManager。Logical GraphLogical Graph 是一种描述流处理程序的高阶逻辑有向图。节点是Operator，边代表输入/输出关系、数据流和数据集中的之一。Managed StateManaged State 描述了已在框架中注册的应用程序的托管状态。对于托管状态，Apache Flink 会负责持久化和重伸缩等事宜。Flink MasterFlink Master 是 Flink Cluster 的主节点。它包含三个不同的组件：Flink Resource Manager、Flink Dispatcher、运行每个 Flink Job 的 Flink JobManager。OperatorLogical Graph 的节点。算子执行某种操作，该操作通常由 Function 执行。Source 和 Sink 是数据输入和数据输出的特殊算子。Operator Chain算子链由两个或多个连续的 Operator 组成，两者之间没有任何的重新分区。同一算子链内的算子可以彼此直接传递 record，而无需通过序列化或 Flink 的网络栈。Partition分区是整个数据流或数据集的独立子集。通过将每个 Record 分配给一个或多个分区，来把数据流或数据集划分为多个分区。在运行期间，Task 会消费数据流或数据集的分区。改变数据流或数据集分区方式的转换通常称为重分区。Physical GraphPhysical graph 是一个在分布式运行时，把 Logical Graph 转换为可执行的结果。节点是 Task，边表示数据流或数据集的输入/输出关系或 partition。RecordRecord 是数据集或数据流的组成元素。Operator 和 Function接收 record 作为输入，并将 record 作为输出发出。Flink Session Cluster长时间运行的 Flink Cluster，它可以接受多个 Flink Job 的执行。此 Flink Cluster 的生命周期不受任何 Flink Job 生命周期的约束限制。以前，Flink Session Cluster 也称为 session mode 的 Flink Cluster，和 Flink Application Cluster 相对应。State Backend对于流处理程序，Flink Job 的 State Backend 决定了其 state 是如何存储在每个 TaskManager 上的（ TaskManager 的 Java 堆栈或嵌入式 RocksDB），以及它在 checkpoint 时的写入位置（ Flink Master 的 Java 堆或者 Filesystem）。Sub-TaskSub-Task 是负责处理数据流 Partition 的 Task。”Sub-Task”强调的是同一个 Operator 或者 Operator Chain 具有多个并行的 Task 。TaskTask 是 Physical Graph 的节点。它是基本的工作单元，由 Flink 的 runtime 来执行。Task 正好封装了一个 Operator 或者 Operator Chain 的 _parallel instance_。Flink TaskManagerTaskManager 是 Flink Cluster 的工作进程。Task 被调度到 TaskManager 上执行。TaskManager 相互通信，只为在后续的 Task 之间交换数据。TransformationTransformation 应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。Transformation 可能会在每个记录的基础上更改数据流或数据集，但也可以只更改其分区或执行聚合。虽然 Operator 和 Function 是 Flink API 的“物理”部分，但 Transformation 只是一个 API 概念。具体来说，大多数（但不是全部）Transformation 是由某些 Operator 实现的。分布式缓存熟悉 Hadoop 的你应该知道，分布式缓存最初的思想诞生于 Hadoop 框架，Hadoop 会将一些数据或者文件缓存在 HDFS 上，在分布式环境中让所有的计算节点调用同一个配置文件。在 Flink 中，Flink 框架开发者们同样将这个特性进行了实现。Flink 提供的分布式缓存类型 Hadoop，目的是为了在分布式环境中让每一个 TaskManager 节点保存一份相同的数据或者文件，当前计算节点的 task 就像读取本地文件一样拉取这些配置。分布式缓存在我们实际生产环境中最广泛的一个应用，就是在进行表与表 Join 操作时，如果一个表很大，另一个表很小，那么我们就可以把较小的表进行缓存，在每个 TaskManager 都保存一份，然后进行 Join 操作。那么我们应该怎样使用 Flink 的分布式缓存呢？举例如下：12345678910111213141516171819202122232425262728public static void main(String[] args) throws Exception &#123;final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.registerCachedFile(\"/Users/wangzhiwu/WorkSpace/quickstart/distributedcache.txt\", \"distributedCache\"); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试 DataSource&lt;String&gt; data = env.fromElements(\"Linea\", \"Lineb\", \"Linec\", \"Lined\"); DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用该缓存文件 File myFile = getRuntimeContext().getDistributedCache().getFile(\"distributedCache\"); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; this.dataList.add(line); System.err.println(\"分布式缓存为:\" + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println(\"使用datalist：\" + dataList + \"-------\" +value); //业务逻辑 return dataList +\"：\" + value; &#125; &#125;); result.printToErr(); &#125;从上面的例子中可以看出，使用分布式缓存有两个步骤。第一步：首先需要在 env 环境中注册一个文件，该文件可以来源于本地，也可以来源于 HDFS ，并且为该文件取一个名字。第二步：在使用分布式缓存时，可根据注册的名字直接获取。可以看到，在上述案例中，我们把一个本地的 distributedcache.txt 文件注册为 distributedCache，在下面的 map 算子中直接通过这个名字将缓存文件进行读取并且进行了处理。我们直接运行该程序，在控制台可以看到如下输出：在使用分布式缓存时也需要注意一些问题，需要我们缓存的文件在任务运行期间最好是只读状态，否则会造成数据的一致性问题。另外，缓存的文件和数据不宜过大，否则会影响 Task 的执行速度，在极端情况下会造成 OOM。故障恢复和重启策略自动故障恢复是 Flink 提供的一个强大的功能，在实际运行环境中，我们会遇到各种各样的问题从而导致应用挂掉，比如我们经常遇到的非法数据、网络抖动等。Flink 提供了强大的可配置故障恢复和重启策略来进行自动恢复。故障恢复我们在上一课时中介绍过 Flink 的配置文件，其中有一个参数 jobmanager.execution.failover-strategy: region。Flink 支持了不同级别的故障恢复策略，jobmanager.execution.failover-strategy 的可配置项有两种：full 和 region。当我们配置的故障恢复策略为 full 时，集群中的 Task 发生故障，那么该任务的所有 Task 都会发生重启。而在实际生产环境中，我们的大作业可能有几百个 Task，出现一次异常如果进行整个任务重启，那么经常会导致长时间任务不能正常工作，导致数据延迟。但是事实上，我们可能只是集群中某一个或几个 Task 发生了故障，只需要重启有问题的一部分即可，这就是 Flink **基于 Region 的局部重启策略。在这个策略下，Flink 会把我们的任务分成不同的 Region，当某一个 Task 发生故障时，Flink 会计算需要故障恢复的最小 Region。Flink 在判断需要重启的 Region 时，采用了以下的判断逻辑：发生错误的 Task 所在的 Region 需要重启；如果当前 Region 的依赖数据出现损坏或者部分丢失，那么生产数据的 Region 也需要重启；为了保证数据一致性，当前 Region 的下游 Region 也需要重启。重启策略Flink 提供了多种类型和级别的重启策略，常用的重启策略包括：固定延迟重启策略模式失败率重启策略模式无重启策略模式Flink 在判断使用的哪种重启策略时做了默认约定，如果用户配置了 checkpoint，但没有设置重启策略，那么会按照固定延迟重启策略模式进行重启；如果用户没有配置 checkpoint，那么默认不会重启。下面我们分别对这三种模式进行详细讲解。无重启策略模式在这种情况下，如果我们的作业发生错误，任务会直接退出。我们可以在 flink-conf.yaml 中配置：1restart-strategy: none也可以在程序中使用代码指定：12final ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart());固定延迟重启策略模式固定延迟重启策略会通过在 flink-conf.yaml 中设置如下配置参数，来启用此策略：1restart-strategy: fixed-delay固定延迟重启策略模式需要指定两个参数，首先 Flink 会根据用户配置的重试次数进行重试，每次重试之间根据配置的时间间隔进行重试，如下表所示：举个例子，假如我们需要任务重试 3 次，每次重试间隔 5 秒，那么需要进行一下配置：12restart-strategy.fixed-delay.attempts: 3restart-strategy.fixed-delay.delay: 5 s当前我们也可以在代码中进行设置：1234env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, &#x2F;&#x2F; 重启次数 Time.of(5, TimeUnit.SECONDS) &#x2F;&#x2F; 时间间隔));失败率重启策略模式**首先我们在 flink-conf.yaml 中指定如下配置：1restart-strategy: failure-rate这种重启模式需要指定三个参数，如下表所示。失败率重启策略在 Job 失败后会重启，但是超过失败率后，Job 会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。这种策略的配置理解较为困难，我们举个例子，假如 5 分钟内若失败了 3 次，则认为该任务失败，每次失败的重试间隔为 5 秒。那么我们的配置应该是：123restart-strategy.failure-rate.max-failures-per-interval: 3restart-strategy.failure-rate.failure-rate-interval: 5 minrestart-strategy.failure-rate.delay: 5 s当然，也可以在代码中直接指定：12345env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, &#x2F;&#x2F; 每个时间间隔的最大故障次数 Time.of(5, TimeUnit.MINUTES), &#x2F;&#x2F; 测量故障率的时间间隔 Time.of(5, TimeUnit.SECONDS) &#x2F;&#x2F; 每次任务失败时间间隔));最后，需要注意的是，在实际生产环境中由于每个任务的负载和资源消耗不一样，我们推荐在代码中指定每个任务的重试机制和重启策略。并行度并行度是 Flink 执行任务的核心概念之一，它被定义为在分布式运行环境中我们的一个算子任务被切分成了多少个子任务并行执行。我们提高任务的并行度（Parallelism）在很大程度上可以大大提高任务运行速度。一般情况下，我们可以通过四种级别来设置任务的并行度。算子级别在代码中可以调用 setParallelism 方法来设置每一个算子的并行度。例如：1234DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts &#x3D; text.flatMap(new LineSplitter()) .groupBy(0) .sum(1).setParallelism(1);事实上，Flink 的每个算子都可以单独设置并行度。这也是我们最推荐的一种方式，可以针对每个算子进行任务的调优。执行环境级别我们在创建 Flink 的上下文时可以显示的调用 env.setParallelism() 方法，来设置当前执行环境的并行度，这个配置会对当前任务的所有算子、Source、Sink 生效。当然你还可以在算子级别设置并行度来覆盖这个设置。12final ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();env.setParallelism(5);提交任务级别用户在提交任务时，可以显示的指定 -p 参数来设置任务的并行度，例如：1.&#x2F;bin&#x2F;flink run -p 10 WordCount.jar系统配置级别我们在上一课时中提到了 flink-conf.yaml 中的一个配置：parallelism.default，该配置即是在系统层面设置所有执行环境的并行度配置。整体上讲，这四种级别的配置生效优先级如下：算子级别 &gt; 执行环境级别 &gt; 提交任务级别 &gt; 系统配置级别。在这里，要特别提一下 Flink 中的 Slot 概念。我们知道，Flink 中的 TaskManager 是执行任务的节点，那么在每一个 TaskManager 里，还会有“槽位”，也就是 Slot。Slot 个数代表的是每一个 TaskManager 的并发执行能力。假如我们指定 taskmanager.numberOfTaskSlots:3，即每个 taskManager 有 3 个 Slot ，那么整个集群就有 3 * taskManager 的个数多的槽位。这些槽位就是我们整个集群所拥有的所有执行任务的资源。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink Table API  window操作","slug":"Flink-Table-API-window操作","date":"2019-06-15T09:13:04.000Z","updated":"2020-09-06T01:31:09.801Z","comments":true,"path":"2019/06/15/Flink-Table-API-window操作/","link":"","permalink":"cpeixin.cn/2019/06/15/Flink-Table-API-window%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Table API是用于流和批处理的统一的关系API。Table API查询可以直接在批处理或流输入上运行而无需修改。Table API是SQL语言的超集，是专门为与Apache Flink配合使用而设计的。Table API是用于Scala和Java的语言集成的API。Table API查询不是将查询指定为SQL，而是以Java或Scala中的语言嵌入样式定义，并具有IDE支持，例如自动完成和语法验证。Table API与Flink的SQL集成共享其API的许多概念和部分。看一下Common Concepts＆API，了解如何注册表或创建Table对象。“ 流概念”页面讨论了流的特定概念，例如动态表和时间属性。下面我们将进行Table API在流式处理中的操作，在实际的工作中，Flink也是大多数场景都是在流计算中使用。Group Windows“Group”窗口根据时间或行计数间隔将组行聚合为有限的组，并每组评估一次聚合函数。对于批处理表，窗口是按时间间隔对记录进行分组的便捷快捷方式。Windows是使用该window(w: GroupWindow)子句定义的，并且需要使用该as子句指定的别名。为了按窗口对表进行分组，必须在groupBy(…)子句中像常规分组属性一样引用窗口别名。以下示例显示如何在表上定义窗口聚合。1234val table = input .window([w: GroupWindow] as 'w) // define window with alias w .groupBy('w) // group the table by window w .select('b.sum) // aggregate在流环境中，如果窗口聚合除对窗口以外的一个或多个属性进行分组，则只能并行计算窗口聚合，即，groupBy(…)子句引用了窗口别名和至少一个其他属性。groupBy(…)子句仅引用一个窗口别名（如在上面的例子）只能由一个单一的，非平行的任务进行评估。以下示例显示如何使用其他分组属性定义窗口聚合。1234val table = input .window([w: GroupWindow] as 'w) // define window with alias w .groupBy('w, 'a) // group the table by attribute a and window w .select('a, 'b.sum) // aggregate窗口属性诸如开始，结束，或一个时间窗口的rowtime时间戳可以在选择语句被添加为窗口别名作为的一个属性w.start，w.end以及w.rowtime分别。窗口开始和行时间时间戳是包含窗口的上下边界。相反，窗口结束时间戳是唯一的窗口上边界。例如，从下午2点开始的30分钟滚动窗口将具有14:00:00.000开始时间戳记，14:29:59.999行时间时间戳记和14:30:00.000结束时间戳记。1234val table = input .window([w: GroupWindow] as 'w) // define window with alias w .groupBy('w, 'a) // group the table by attribute a and window w .select('a, 'w.start, 'w.end, 'w.rowtime, 'b.count) // aggregate and add window start, end, and rowtime timestamps该Window参数定义行如何映射到窗口。Window不是用户可以实现的接口。相反，Table API提供了一组Window具有特定语义的预定义类，这些预定义类被转换为基础DataStream或DataSet操作。支持的窗口定义在下面列出。Tumbling Windows滚动窗口将行分配给固定长度的非重叠连续窗口。例如，5分钟的滚动窗口以5分钟为间隔对行进行分组。可以在事件时间，处理时间或行数上定义滚动窗口。滚动窗口是使用以下Tumble类定义的：方法描述over将窗口的长度定义为时间或行计数间隔。on用于分组（时间间隔）或排序（行计数）的时间属性。对于批查询，它可以是任何Long或Timestamp属性。对于流查询，它必须是声明的event-time或processing-time time属性。as为窗口分配别名。别名用于在以下groupBy()子句中引用窗口，并可以选择在子句中选择窗口属性，例如窗口开始，结束或行时间时间戳select()。123456&#x2F;&#x2F; Tumbling Event-time Window.window(Tumble over 10.minutes on &#39;rowtime as &#39;w)&#x2F;&#x2F; Tumbling Processing-time Window (assuming a processing-time attribute &quot;proctime&quot;).window(Tumble over 10.minutes on &#39;proctime as &#39;w)&#x2F;&#x2F; Tumbling Row-count Window (assuming a processing-time attribute &quot;proctime&quot;).window(Tumble over 10.rows on &#39;proctime as &#39;w)Sliding Windows滑动窗口的大小固定，并以指定的滑动间隔滑动。如果滑动间隔小于窗口大小，则滑动窗口重叠。因此，可以将行分配给多个窗口。例如，一个15分钟大小的滑动窗口和5分钟的滑动间隔将每行分配给3个15分钟大小的不同窗口，这些窗口以5分钟的间隔进行评估。可以在事件时间，处理时间或行数上定义滑动窗口。滑动窗口是通过使用以下Slide类来定义的：方法描述over将窗口的长度定义为时间或行计数间隔。every将幻灯片间隔定义为时间间隔或行计数间隔。滑动间隔必须与尺寸间隔具有相同的类型。on用于分组（时间间隔）或排序（行计数）的时间属性。对于批查询，它可以是任何Long或Timestamp属性。对于流查询，它必须是声明的event-time或processing-time time属性。as为窗口分配别名。别名用于在以下groupBy()子句中引用窗口，并可以选择在子句中选择窗口属性，例如窗口开始，结束或行时间时间戳select()。123456// Sliding Event-time Window.window(Slide over 10.minutes every 5.minutes on 'rowtime as 'w)// Sliding Processing-time window (assuming a processing-time attribute \"proctime\").window(Slide over 10.minutes every 5.minutes on 'proctime as 'w)// Sliding Row-count window (assuming a processing-time attribute \"proctime\").window(Slide over 10.rows every 5.rows on 'proctime as 'w)Session Windows会话窗口没有固定的大小，但是其边界由不活动的时间间隔定义，即，如果在定义的间隔时间段内未出现任何事件，则会话窗口关闭。例如，间隔30分钟的会话窗口在30分钟不活动后观察到一行时开始（否则该行将被添加到现有窗口），如果在30分钟内未添加任何行，则关闭该窗口。会话窗口可以在事件时间或处理时间工作。通过使用以下Session类定义会话窗口：方法描述withGap将两个窗口之间的间隔定义为时间间隔。on用于分组（时间间隔）或排序（行计数）的时间属性。对于批查询，它可以是任何Long或Timestamp属性。对于流查询，它必须是声明的event-time或processing-time time属性。as为窗口分配别名。别名用于在以下groupBy()子句中引用窗口，并可以选择在子句中选择窗口属性，例如窗口开始，结束或行时间时间戳select()。1234// Session Event-time Window.window(Session withGap 10.minutes on 'rowtime as 'w)// Session Processing-time Window (assuming a processing-time attribute \"proctime\").window(Session withGap 10.minutes on 'proctime as 'w)Over Windows窗口聚合是标准SQL（OVER子句）中已知的，并在SELECT查询的子句中定义。与在GROUP BY子句中指定的组窗口不同，在窗口上方不会折叠行。取而代之的是，在窗口聚合中，为每个输入行在其相邻行的范围内计算一个聚合。窗口是使用window(w: OverWindow)子句（over_window(OverWindow)在Python API中使用）定义的，并通过select()方法中的别名进行引用。以下示例显示如何在表上定义窗口聚合123val table = input .window([w: OverWindow] as 'w) // define over window with alias w .select('a, 'b.sum over 'w, 'c.min over 'w) // aggregate over the over window wOverWindow在限定范围内的数据对其进行聚合计算。OverWindow不是用户可以实现的接口。相反，Table API提供了Over用于配置上方窗口属性的类。可以在事件时间或处理时间以及指定为时间间隔或行计数的范围上定义窗口上方。受支持的over窗口定义作为Over（和其他类）上的方法公开，并在下面列出：方法需要描述partitionBy可选的在一个或多个属性上定义输入的分区。每个分区都经过单独排序，并且聚合函数分别应用于每个分区。注意：在流环境中，仅当窗口包含partition by子句时，才可以并行计算整个窗口聚合。如果没有partitionBy(...)流，则由单个非并行任务处理。orderBy需要定义每个分区内的行顺序，从而定义将聚合函数应用于行的顺序。注意：对于流查询，它必须是声明的event-time或processing-time time属性。当前，仅支持单个sort属性。preceding可选的定义窗口中包含的并在当前行之前的行的间隔。该间隔可以指定为时间间隔或行计数间隔。用时间间隔的大小（例如，10.minutes时间间隔或10.rows行计数间隔）指定窗口边界。窗口上的无边界是使用常量指定的，即UNBOUNDED_RANGE用于时间间隔或UNBOUNDED_ROW用于行计数间隔。Windows上的无边界从分区的第一行开始。如果preceding条款被省略，UNBOUNDED_RANGE并且CURRENT_RANGE被用作默认preceding和following用于该窗口。following可选的定义窗口中包含并紧随当前行的行的窗口间隔。该间隔必须与前面的间隔（时间或行计数）以相同的单位指定。目前，不支持具有当前行之后的行的窗口。相反，您可以指定两个常量之一：- CURRENT_ROW 将窗口的上限设置为当前行。- CURRENT_RANGE 将窗口的上限设置为当前行的排序键，即，与当前行具有相同排序键的所有行都包含在窗口中。如果following省略该子句，则时间间隔窗口CURRENT_RANGE的上限定义为，行计数间隔窗口的上限定义为CURRENT_ROW。as需要为上方窗口分配别名。别名用于引用以下select()子句中的over窗口。注意：**当前，select()必须在相同的窗口内计算同一调用中的所有聚合函数。Unbounded Over Windows123456789// Unbounded Event-time over window (assuming an event-time attribute \"rowtime\").window(Over partitionBy 'a orderBy 'rowtime preceding UNBOUNDED_RANGE as 'w)// Unbounded Processing-time over window (assuming a processing-time attribute \"proctime\").window(Over partitionBy 'a orderBy 'proctime preceding UNBOUNDED_RANGE as 'w)// Unbounded Event-time Row-count over window (assuming an event-time attribute \"rowtime\").window(Over partitionBy 'a orderBy 'rowtime preceding UNBOUNDED_ROW as 'w) // Unbounded Processing-time Row-count over window (assuming a processing-time attribute \"proctime\").window(Over partitionBy 'a orderBy 'proctime preceding UNBOUNDED_ROW as 'w)bounded Over Windows123456789// Bounded Event-time over window (assuming an event-time attribute \"rowtime\").window(Over partitionBy 'a orderBy 'rowtime preceding 1.minutes as 'w)// Bounded Processing-time over window (assuming a processing-time attribute \"proctime\").window(Over partitionBy 'a orderBy 'proctime preceding 1.minutes as 'w)// Bounded Event-time Row-count over window (assuming an event-time attribute \"rowtime\").window(Over partitionBy 'a orderBy 'rowtime preceding 10.rows as 'w) // Bounded Processing-time Row-count over window (assuming a processing-time attribute \"proctime\").window(Over partitionBy 'a orderBy 'proctime preceding 10.rows as 'w)基于行的操作基于行的操作生成具有多列的输出。算子描述MapBatch\\Streaming使用用户定义的标量函数或内置标量函数执行映射操作。如果输出类型是复合类型，则输出将被展平。```class MyMapFunction extends ScalarFunction {def eval(a: String): Row = {Row.of(a, “pre-“ + a)}override def getResultType(signature: Array[Class[_]]): TypeInformation[_] =Types.ROW(Types.STRING, Types.STRING)}val func = new MyMapFunction()val table = input.map(func(‘c)).as(‘a, ‘b)12 || **FlatMap**&lt;br &#x2F;&gt;Batch\\Streaming | 使用表函数执行flatMap操作。class MyFlatMapFunction extends TableFunction[Row] {def eval(str: String): Unit = {if (str.contains(“#”)) {str.split(“#”).foreach({ s =&gt;val row = new Row(2)row.setField(0, s)row.setField(1, s.length)collect(row)})}}override def getResultType: TypeInformation[Row] = {Types.ROW(Types.STRING, Types.INT)}}val func = new MyFlatMapFunctionval table = input.flatMap(func(‘c)).as(‘a, ‘b)123 || **Aggregate**&lt;br &#x2F;&gt;Batch\\Streaming&lt;br &#x2F;&gt;**Result Updating** | 使用聚合函数执行聚合操作。您必须使用select语句关闭“聚合”，并且select语句不支持聚合功能。如果输出类型是复合类型，则聚合的输出将被展平。&lt;br &#x2F;&gt;case class MyMinMaxAcc(var min: Int, var max: Int)&lt;br &#x2F;&gt;class MyMinMax extends AggregateFunction[Row, MyMinMaxAcc] &#123;&lt;br &#x2F;&gt; def accumulate(acc: MyMinMaxAcc, value: Int): Unit &#x3D; &#123;&lt;br &#x2F;&gt; if (value &lt; acc.min) &#123;&lt;br &#x2F;&gt; acc.min &#x3D; value&lt;br &#x2F;&gt; &#125;&lt;br &#x2F;&gt; if (value &gt; acc.max) &#123;&lt;br &#x2F;&gt; acc.max &#x3D; value&lt;br &#x2F;&gt; &#125;&lt;br &#x2F;&gt; &#125;&lt;br &#x2F;&gt; override def createAccumulator(): MyMinMaxAcc &#x3D; MyMinMaxAcc(0, 0)&lt;br &#x2F;&gt; &lt;br &#x2F;&gt; def resetAccumulator(acc: MyMinMaxAcc): Unit &#x3D; &#123;&lt;br &#x2F;&gt; acc.min &#x3D; 0&lt;br &#x2F;&gt; acc.max &#x3D; 0&lt;br &#x2F;&gt; &#125;&lt;br &#x2F;&gt; override def getValue(acc: MyMinMaxAcc): Row &#x3D; &#123;&lt;br &#x2F;&gt; Row.of(Integer.valueOf(acc.min), Integer.valueOf(acc.max))&lt;br &#x2F;&gt; &#125;&lt;br &#x2F;&gt; override def getResultType: TypeInformation[Row] &#x3D; &#123;&lt;br &#x2F;&gt; new RowTypeInfo(Types.INT, Types.INT)&lt;br &#x2F;&gt; &#125;&lt;br &#x2F;&gt;&#125;&lt;br &#x2F;&gt;val myAggFunc &#x3D; new MyMinMax&lt;br &#x2F;&gt;val table &#x3D; input&lt;br &#x2F;&gt; .groupBy(&#39;key)&lt;br &#x2F;&gt; .aggregate(myAggFunc(&#39;a) as (&#39;x, &#39;y))&lt;br &#x2F;&gt; .select(&#39;key, &#39;x, &#39;y) || **组窗口聚合**&lt;br &#x2F;&gt;Batch\\Streaming | 在[组窗口](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;table&#x2F;tableApi.html#group-windows)和可能的一个或多个分组键上对表进行分组和聚集。您必须使用select语句关闭“聚合”。并且select语句不支持“ *”或聚合函数。val myAggFunc = new MyMinMaxval table = input.window(Tumble over 5.minutes on ‘rowtime as ‘w) // define window.groupBy(‘key, ‘w) // group by key and window.aggregate(myAggFunc(‘a) as (‘x, ‘y)).select(‘key, ‘x, ‘y, ‘w.start, ‘w.end) // access window properties and aggregate results1234 || **FlatAggregate**Streaming&lt;br &#x2F;&gt;**Result Updating** | 类似于**GroupBy聚合**。使用以下运行表聚合运算符将分组键上的行分组，以逐行聚合行。与AggregateFunction的区别在于TableAggregateFunction可以为一个组返回0个或更多记录。您必须使用select语句关闭“ flatAggregate”。并且select语句不支持聚合函数。&lt;br &#x2F;&gt;除了使用&#96;emitValue&#96;输出结果，还可以使用&#96;emitUpdateWithRetract&#96;方法。与不同&#96;emitValue&#96;，&#96;emitUpdateWithRetract&#96;用于发出已更新的值。此方法以缩回模式增量输出数据，即，一旦有更新，我们必须先缩回旧记录，然后再发送新的更新记录。如果在表聚合函数中定义了这两种方法，则该&#96;emitUpdateWithRetract&#96;方法将优先于该&#96;emitValue&#96;方法使用，因为这被认为比该方法更有效，&#96;emitValue&#96;因为它可以递增地输出值。有关详细信息，请[参见表聚合函数](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;table&#x2F;functions&#x2F;udfs.html#table-aggregation-functions)。import java.lang.{Integer =&gt; JInteger}import org.apache.flink.table.api.Typesimport org.apache.flink.table.functions.TableAggregateFunction/*** Accumulator for top2.*/class Top2Accum {var first: JInteger = _var second: JInteger = _}/*** The top2 user-defined table aggregate function.*/class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] {override def createAccumulator(): Top2Accum = {val acc = new Top2Accumacc.first = Int.MinValueacc.second = Int.MinValueacc}def accumulate(acc: Top2Accum, v: Int) {if (v &gt; acc.first) {acc.second = acc.firstacc.first = v} else if (v &gt; acc.second) {acc.second = v}}def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = {val iter = its.iterator()while (iter.hasNext) {val top2 = iter.next()accumulate(acc, top2.first)accumulate(acc, top2.second)}}def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = {// emit the value and rankif (acc.first != Int.MinValue) {out.collect(JTuple2.of(acc.first, 1))}if (acc.second != Int.MinValue) {out.collect(JTuple2.of(acc.second, 2))}}}val top2 = new Top2val orders: Table = tableEnv.from(“Orders”)val result = orders.groupBy(‘key).flatAggregate(top2(‘a) as (‘v, ‘rank)).select(‘key, ‘v, ‘rank)12**注意：**对于流式查询，根据聚合类型和不同分组关键字的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。有关详细信息，请参见[查询配置](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;table&#x2F;streaming&#x2F;query_configuration.html)。 || **Group Window FlatAggregate**&lt;br &#x2F;&gt;**Streaming** | 在[组窗口](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;table&#x2F;tableApi.html#group-windows)和可能的一个或多个分组键上对表进行分组和聚集。您必须使用select语句关闭“ flatAggregate”。并且select语句不支持聚合函数。val top2 = new Top2val orders: Table = tableEnv.from(“Orders”)val result = orders.window(Tumble over 5.minutes on ‘rowtime as ‘w) // define window.groupBy(‘a, ‘w) // group by key and window.flatAggregate(top2(‘b) as (‘v, ‘rank)).select(‘a, w.start, ‘w.end, ‘w.rowtime, ‘v, ‘rank) // access window properties and aggregate results1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 |&lt;br &#x2F;&gt;&lt;br &#x2F;&gt;下面是根据上面所讲到的操作，所写的小实例，每五秒统计一次每个用户目前的游戏得分情况&#96;&#96;&#96;scalapackage tableAPIimport org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractorimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.table.api.&#123;EnvironmentSettings, Slide, Table, Tumble&#125;import org.apache.flink.table.api.scala.StreamTableEnvironmentimport org.apache.flink.types.Rowimport org.apache.flink.streaming.api.scala._object table_stream_window &#123; case class GameData(user_id: String, game_id: String, game_time: Long, game_score: Int) def main(args: Array[String]): Unit &#x3D; &#123; val stream_env: StreamExecutionEnvironment &#x3D; StreamExecutionEnvironment.getExecutionEnvironment stream_env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val settings: EnvironmentSettings &#x3D; EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build() val table_env: StreamTableEnvironment &#x3D; StreamTableEnvironment.create(stream_env, settings) stream_env.setParallelism(1) val socketStream: DataStream[String] &#x3D; stream_env.socketTextStream(&quot;localhost&quot;, 8888) val gameStream: DataStream[GameData] &#x3D; socketStream.map((line: String) &#x3D;&gt; &#123; val array_data: Array[String] &#x3D; line.split(&quot;,&quot;) GameData(array_data(0), array_data(1), array_data(2).toLong, array_data(3).toInt) &#125;).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[GameData](Time.seconds(3))&#123; override def extractTimestamp(element: GameData): Long &#x3D; &#123; element.game_time &#125; &#125;) &#x2F;&#x2F;分组聚合操作 import org.apache.flink.table.api.scala._ &#x2F;&#x2F;创建动态Table,并且指定event time val game_table: Table &#x3D; table_env.fromDataStream(gameStream,&#39;user_id,&#39;game_id,&#39;game_time.rowtime,&#39;game_score) &#x2F;&#x2F;开启窗口,滑动窗口&#x2F;&#x2F; game_table.window(Slide.over(&quot;10.second&quot;).every(&quot;5.second&quot;).on(&quot;game_time&quot;).as(&quot;window&quot;)) &#x2F;&#x2F;开启窗口,滚动窗口&#x2F;&#x2F; val game_score_sum: Table &#x3D; game_table.window(Tumble.over(&quot;5.second&quot;).on(&quot;game_time&quot;).as(&quot;window&quot;)) val game_score_sum: Table &#x3D; game_table.window(Tumble over 5.second on &#39;game_time as &#39;window) .groupBy(&#39;window, &#39;user_id) &#x2F;&#x2F;必须指定窗口字段 .select(&#39;user_id, &#39;game_score.sum, &#39;window.start, &#39;window.end) table_env .toRetractStream[Row](game_score_sum) .filter((_: (Boolean, Row))._1&#x3D;&#x3D;true) .print(&quot;user_sum_score&quot;) table_env.execute(&quot;user_sum_score——job&quot;) &#125;&#125;内置函数我们在讲解 Flink Table &amp; SQL 所支持的常用算子前，需要说明一点，Flink 自从 0.9 版本开始支持 Table &amp; SQL 功能一直处于完善开发中，且在不断进行迭代。我们在官网中也可以看到这样的提示：Please note that the Table API and SQL are not yet feature complete and are being actively developed. Not all operations are supported by every combination of [Table API, SQL] and [stream, batch] input.Flink Table &amp; SQL 的开发一直在进行中，并没有支持所有场景下的计算逻辑。从我个人实践角度来讲，在使用原生的 Flink Table &amp; SQL 时，务必查询官网当前版本对 Table &amp; SQL 的支持程度，尽量选择场景明确，逻辑不是极其复杂的场景。常用算子目前 Flink SQL 支持的语法主要如下：复制123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566query: values | &#123; select | selectWithoutFrom | query UNION [ ALL ] query | query EXCEPT query | query INTERSECT query &#125; [ ORDER BY orderItem [, orderItem ]* ] [ LIMIT &#123; count | ALL &#125; ] [ OFFSET start &#123; ROW | ROWS &#125; ] [ FETCH &#123; FIRST | NEXT &#125; [ count ] &#123; ROW | ROWS &#125; ONLY]orderItem: expression [ ASC | DESC ]select: SELECT [ ALL | DISTINCT ] &#123; * | projectItem [, projectItem ]* &#125; FROM tableExpression [ WHERE booleanExpression ] [ GROUP BY &#123; groupItem [, groupItem ]* &#125; ] [ HAVING booleanExpression ] [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]selectWithoutFrom: SELECT [ ALL | DISTINCT ] &#123; * | projectItem [, projectItem ]* &#125;projectItem: expression [ [ AS ] columnAlias ] | tableAlias . *tableExpression: tableReference [, tableReference ]* | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]joinCondition: ON booleanExpression | USING &#39;(&#39; column [, column ]* &#39;)&#39;tableReference: tablePrimary [ matchRecognize ] [ [ AS ] alias [ &#39;(&#39; columnAlias [, columnAlias ]* &#39;)&#39; ] ]tablePrimary: [ TABLE ] [ [ catalogName . ] schemaName . ] tableName | LATERAL TABLE &#39;(&#39; functionName &#39;(&#39; expression [, expression ]* &#39;)&#39; &#39;)&#39; | UNNEST &#39;(&#39; expression &#39;)&#39;values: VALUES expression [, expression ]*groupItem: expression | &#39;(&#39; &#39;)&#39; | &#39;(&#39; expression [, expression ]* &#39;)&#39; | CUBE &#39;(&#39; expression [, expression ]* &#39;)&#39; | ROLLUP &#39;(&#39; expression [, expression ]* &#39;)&#39; | GROUPING SETS &#39;(&#39; groupItem [, groupItem ]* &#39;)&#39;windowRef: windowName | windowSpecwindowSpec: [ windowName ] &#39;(&#39; [ ORDER BY orderItem [, orderItem ]* ] [ PARTITION BY expression [, expression ]* ] [ RANGE numericOrIntervalExpression &#123;PRECEDING&#125; | ROWS numericExpression &#123;PRECEDING&#125; ] &#39;)&#39;...可以看到 Flink SQL 和传统的 SQL 一样，支持了包含查询、连接、聚合等场景，另外还支持了包括窗口、排序等场景。下面我就以最常用的算子来做详细的讲解。SELECT/AS/WHERE**SELECT、WHERE 和传统 SQL 用法一样，用于筛选和过滤数据，同时适用于 DataStream 和 DataSet。12SELECT * FROM Table;SELECT name，age FROM Table;当然我们也可以在 WHERE 条件中使用 =、&lt;、&gt;、&lt;&gt;、&gt;=、&lt;=，以及 AND、OR 等表达式的组合：12345SELECT name，age FROM Table where name LIKE &#39;%小明%&#39;;SELECT * FROM Table WHERE age &#x3D; 20;SELECT name, ageFROM TableWHERE name IN (SELECT name FROM Table2)GROUP BY / DISTINCT/HAVINGGROUP BY 用于进行分组操作，DISTINCT 用于结果去重。HAVING 和传统 SQL 一样，可以用来在聚合函数之后进行筛选。1234SELECT DISTINCT name FROM Table;SELECT name, SUM(score) as TotalScore FROM Table GROUP BY name;SELECT name, SUM(score) as TotalScore FROM Table GROUP BY name HAVINGSUM(score) &gt; 300;JOINJOIN 可以用于把来自两个表的数据联合起来形成结果表，目前 Flink 的 Join 只支持等值连接。Flink 支持的 JOIN 类型包括：1234JOIN - INNER JOINLEFT JOIN - LEFT OUTER JOINRIGHT JOIN - RIGHT OUTER JOINFULL JOIN - FULL OUTER JOIN例如，用用户表和商品表进行关联：123456SELECT *FROM User LEFT JOIN Product ON User.name &#x3D; Product.buyerSELECT *FROM User RIGHT JOIN Product ON User.name &#x3D; Product.buyerSELECT *FROM User FULL OUTER JOIN Product ON User.name &#x3D; Product.buyerLEFT JOIN、RIGHT JOIN 、FULL JOIN 相与我们传统 SQL 中含义一样。WINDOW**根据窗口数据划分的不同，目前 Apache Flink 有如下 3 种：滚动窗口，窗口数据有固定的大小，窗口中的数据不会叠加；滑动窗口，窗口数据有固定大小，并且有生成间隔；会话窗口，窗口数据没有固定的大小，根据用户传入的参数进行划分，窗口数据无叠加；滚动窗口滚动窗口的特点是：有固定大小、窗口中的数据不会重叠，如下图所示：滚动窗口的语法：复制123456789SELECT [gk], [TUMBLE_START(timeCol, size)], [TUMBLE_END(timeCol, size)], agg1(col1), ... aggn(colN)FROM Tab1GROUP BY [gk], TUMBLE(timeCol, size)举例说明，我们需要计算每个用户每天的订单数量：1SELECT user, TUMBLE_START(timeLine, INTERVAL &#39;1&#39; DAY) as winStart, SUM(amount) FROM Orders GROUP BY TUMBLE(timeLine, INTERVAL &#39;1&#39; DAY), user;其中，TUMBLE_START 和 TUMBLE_END 代表窗口的开始时间和窗口的结束时间，TUMBLE (timeLine, INTERVAL ‘1’ DAY) 中的 timeLine 代表时间字段所在的列，INTERVAL ‘1’ DAY 表示时间间隔为一天。滑动窗口**滑动窗口有固定的大小，与滚动窗口不同的是滑动窗口可以通过 slide 参数控制滑动窗口的创建频率。需要注意的是，多个滑动窗口可能会发生数据重叠，具体语义如下：滑动窗口的语法与滚动窗口相比，只多了一个 slide 参数：123456789SELECT [gk], [HOP_START(timeCol, slide, size)] , [HOP_END(timeCol, slide, size)], agg1(col1), ... aggN(colN) FROM Tab1GROUP BY [gk], HOP(timeCol, slide, size)例如，我们要每间隔一小时计算一次过去 24 小时内每个商品的销量：1SELECT product, SUM(amount) FROM Orders GROUP BY HOP(rowtime, INTERVAL &#39;1&#39; HOUR, INTERVAL &#39;1&#39; DAY), product上述案例中的 INTERVAL ‘1’ HOUR 代表滑动窗口生成的时间间隔。会话窗口**会话窗口定义了一个非活动时间，假如在指定的时间间隔内没有出现事件或消息，则会话窗口关闭。会话窗口的语法如下：123456789SELECT [gk], SESSION_START(timeCol, gap) AS winStart, SESSION_END(timeCol, gap) AS winEnd, agg1(col1), ... aggn(colN)FROM Tab1GROUP BY [gk], SESSION(timeCol, gap)举例，我们需要计算每个用户过去 1 小时内的订单量：1SELECT user, SESSION_START(rowtime, INTERVAL &#39;1&#39; HOUR) AS sStart, SESSION_ROWTIME(rowtime, INTERVAL &#39;1&#39; HOUR) AS sEnd, SUM(amount) FROM Orders GROUP BY SESSION(rowtime, INTERVAL &#39;1&#39; HOUR), user内置函数Flink 中还有大量的内置函数，我们可以直接使用，将内置函数分类如下：比较函数逻辑函数算术函数字符串处理函数时间函数比较函数逻辑函数算术函数字符串处理函数时间函数","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"python爬虫 - 动态爬取","slug":"python爬虫之动态爬取","date":"2019-06-12T15:26:15.000Z","updated":"2020-04-04T17:11:17.062Z","comments":true,"path":"2019/06/12/python爬虫之动态爬取/","link":"","permalink":"cpeixin.cn/2019/06/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/","excerpt":"","text":"我们的目的是抓取拉勾网Python分类下全国到目前为止展示出来的所有招聘信息，首先在浏览器点击进去看看吧。如果你足够小心或者网速比较慢，那么你会发现，在点击Python分类之后跳到的新页面上，招聘信息出现时间是晚于页面框架出现时间的。到这里，我们几乎可以肯定，招聘信息并不在页面HTML源码中，我们可以通过按下”command+option+u”(在Windows和Linux上的快捷键是”ctrl+u”)来查看网页源码，果然在源码中没有出现页面展示的招聘信息。到这一步，我看到的大多数教程都会教，使用什么什么库，如何如何模拟浏览器环境，通过怎样怎样的方式完成网页的渲染，然后得到里面的信息…永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。那么我们怎么处理呢？经验是，这样的情况，大多是是浏览器会在请求和解析HTML之后，根据js的“指示”再发送一次请求，得到页面展示的内容，然后通过js渲染之后展示到界面。好消息是，这样的请求往往得到的内容是json格式的，所以我们非但不会加重爬虫的任务，反而可能会省去解析HTML的功夫。那个，继续打开Chrome的开发者工具，当我们点击“下一页”之后，浏览器发送了如下请求：注意观察”positionAjax.json”这个请求，它的Type是”xhr”，全称叫做”XMLHttpRequest”，XMLHttpRequest对象可以在不向服务器提交整个页面的情况下，实现局部更新网页。那么，现在它的可能性最大了，我们单击它之后好好观察观察吧：点击之后我们在右下角发现了如上详情，其中几个tab的内容表示：Headers：请求和响应的详细信息Preview：响应体格式化之后的显示Response：响应体原始内容Cookies：CookiesTiming：时间开销通过对内容的观察，返回的确实是一个json字符串，内容包括本页每一个招聘信息，到这里至少我们已经清楚了，确实不需要解析HTML就可以拿到拉钩招聘的信息了。那么，请求该如何模拟呢？我们切换到Headers这一栏，留意三个地方：上面的截图展示了这次请求的请求方式、请求地址等信息。上面的截图展示了这次请求的请求头，一般来讲，其中我们需要关注的是Cookie / Host / Origin / Referer / User-Agent / X-Requested-With等参数。上面这张截图展示了这次请求的提交数据，根据观察，kd表示我们查询的关键字，pn表示当前页码。那么，我们的爬虫需要做的事情，就是按照页码不断向这个接口发送请求，并解析其中的json内容，将我们需要的值存储下来就好了。这里有两个问题：什么时候结束，以及如何的到json中有价值的内容。我们回过头重新观察一下返回的json，格式化之后的层级关系如下：很容易发现，content下的hasNextPage即为是否存在下一页，而content下的result是一个list，其中的每项则是一条招聘信息。在Python中，json字符串到对象的映射可以通过json这个库完成：1234import jsonjson_obj = json.loads(\"&#123;'key': 'value'&#125;\") # 字符串到对象json_str = json.dumps(json_obj) # 对象到字符串json字符串的”[ ]“映射到Python的类型是list，”{ }”映射到Python则是dict。到这里，分析过程已经完全结束，可以愉快的写代码啦。具体代码这里不再给出，希望你可以自己独立完成，如果在编写过程中存在问题，可以联系我获取帮助。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"Flink Table API & SQL","slug":"Flink-Table-API-SQL","date":"2019-06-11T06:24:45.000Z","updated":"2020-09-06T01:23:00.138Z","comments":true,"path":"2019/06/11/Flink-Table-API-SQL/","link":"","permalink":"cpeixin.cn/2019/06/11/Flink-Table-API-SQL/","excerpt":"","text":"简介Apache Flink具有两个有关联的API：Table API和SQL，用于统一流和批处理（批流一体）。Table API是由Scala和Java语言集成的查询API，它允许以非常直观的方式，组合来自关系运算符（例如filter，join）的查询。Flink的SQL支持基于实现SQL标准的Apache Calcite。无论输入是批处理输入（DataSet）还是流输入（DataStream），在两个接口中指定的查询都具有相同的语义并指定相同的结果。Table API和SQL接口以及Flink的DataStream和DataSet API紧密集成在一起。您可以轻松地在所有API和基于API的库之间切换。例如，您可以使用CEP库从DataStream中提取模式，然后再使用Table API分析模式，或者您可以在预处理程序上运行Gelly图算法之前，使用SQL查询扫描，过滤和聚合批处理表。请注意，Table API和SQL尚未完成功能，正在积极开发中。[Table API，SQL]和[stream，batch]输入的每种组合都不支持所有操作。**程序依赖从Flink 1.9开始，Flink提供了两种不同的计划程序实现来评估Table＆SQL API程序：Blink计划程序和Flink 1.9之前可用的旧计划程序。计划人员负责将关系运算符转换为可执行的，优化的Flink作业。两位计划者都带有不同的优化规则和运行时类。它们在支持的功能方面也可能有所不同。注意**对于生产用例，建议使用Flink 1.9之前的旧计划器。所有Table API和SQL组件都捆绑在flink-table或flink-table-blinkMaven构件中。以下依赖关系与大多数项目有关：flink-table-common：用于通过自定义功能，格式等扩展表生态系统的通用模块。flink-table-api-java：适用于使用Java编程语言的纯表程序的Table＆SQL API（处于开发初期，不建议使用！）。flink-table-api-scala：使用Scala编程语言的纯表程序的Table＆SQL API（处于开发初期，不建议使用！）。flink-table-api-java-bridge：使用Java编程语言支持带有DataStream / DataSet API的Table＆SQL API。flink-table-api-scala-bridge：使用Scala编程语言支持带有DataStream / DataSet API的Table＆SQL API。flink-table-planner：表程序计划程序和运行时。这是1.9版本之前Flink的唯一计划者。仍然是推荐的一种。flink-table-planner-blink：新的Blink计划器。flink-table-runtime-blink：新的Blink运行时。flink-table-uber：将上述API模块以及旧的计划器打包到大多数Table＆SQL API用例的分发中。默认情况下，超级JAR文件flink-table-*.jar位于/libFlink版本的目录中。flink-table-uber-blink：将上述API模块以及特定于Blink的模块打包到大多数Table＆SQL API用例的分发中。默认情况下，超级JAR文件flink-table-blink-*.jar位于/libFlink版本的目录中。有关如何在表程序中的新旧Blink规划器之间进行切换的更多信息，请参见通用API页面。表程序依赖性根据目标编程语言，您需要将Java或Scala API添加到项目中，以便使用Table API和SQL定义管道：1234567891011121314&lt;!-- Either... --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;&lt;!-- or... --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;此外，如果要在IDE中本地运行Table API和SQL程序，则必须添加以下一组模块之一，具体取决于要使用的计划程序：1234567891011121314&lt;!-- Either... (for the old planner that was available before Flink 1.9) --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;&lt;!-- or.. (for the new Blink planner) --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;在内部，表生态系统的一部分在Scala中实现。因此，请确保为批处理和流应用程序都添加以下依赖项：123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;扩展依赖如果要实现与Kafka或一组用户定义的函数进行交互的自定义格式，则以下依赖关系就足够了，并且可以用于SQL Client的JAR文件：123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-common&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10.0&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;当前，该模块包括以下扩展点：SerializationSchemaFactoryDeserializationSchemaFactoryScalarFunctionTableFunctionAggregateFunction两个Planner之间主要的不同Blink将批处理作业视为流的特殊情况。因此，还不支持Table和DataSet之间的转换，并且批处理作业不会转换成DateSet程序，则还是和DataStream程序一样。Blink planner 不支持BatchTableSource，而是使用StreamTableSource 代替。Blink planner 程序仅支持全新的Catalog，不支持ExternalCatalog，ExternalCatalog不推荐使用。使用FilterableTableSource在**old planner和Blink planner中是不相容的。old planner会将PlannerExpressions 按下FilterableTableSource，而Blink planner**的计划程序将Expressions 按下。基于字符串的键值配置选项（有关详细信息，请参阅有关配置的文档）仅用于Blink计划器。两个planner实现（CalciteConfig）PlannerConfig不同。Blink planner会将多个接收器优化为一个DAG（仅在上支持TableEnvironment，而不在上支持StreamTableEnvironment）。old planner将始终将每个接收器优化为一个新的DAG，其中所有DAG彼此独立。old planner程序现在不支持目录统计信息，而Blink计划程序则支持。Table API &amp; SQL程序框架用于批处理和流式传输的所有Table API和SQL程序都遵循相同的模式。以下代码示例显示了Table API和SQL程序的通用结构。12345678910111213141516171819// create a TableEnvironment for specific planner batch or streamingval tableEnv = ... // see \"Create a TableEnvironment\" section// create a TabletableEnv.connect(...).createTemporaryTable(\"table1\")// register an output TabletableEnv.connect(...).createTemporaryTable(\"outputTable\")// create a Table from a Table API queryval tapiResult = tableEnv.from(\"table1\").select(...)// create a Table from a SQL queryval sqlResult = tableEnv.sqlQuery(\"SELECT ... FROM table1 ...\")// emit a Table API result Table to a TableSink, same for SQL resulttapiResult.insertInto(\"outputTable\")// executetableEnv.execute(\"scala_job\")创建TableEnvironment这TableEnvironment是Table API和SQL集成的中心概念。它负责：在内部catalog中注册Table注册catalog加载可插拔模块执行SQL查询注册用户定义的UDF函数将DataStream或DataSet转换为Table持有对ExecutionEnvironment或的引用StreamExecutionEnvironment一张表始终绑定到特定的TableEnvironment。不可能在同一查询中组合不同TableEnvironments的表，例如，将它们join或union。以下是四种不同类型的定义：12345678910111213141516171819202122232425262728293031323334353637// **********************// FLINK STREAMING QUERY// **********************import org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.table.api.EnvironmentSettingsimport org.apache.flink.table.api.scala.StreamTableEnvironmentval fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build()val fsEnv = StreamExecutionEnvironment.getExecutionEnvironmentval fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings)// or val fsTableEnv = TableEnvironment.create(fsSettings)// ******************// FLINK BATCH QUERY// ******************import org.apache.flink.api.scala.ExecutionEnvironmentimport org.apache.flink.table.api.scala.BatchTableEnvironmentval fbEnv = ExecutionEnvironment.getExecutionEnvironmentval fbTableEnv = BatchTableEnvironment.create(fbEnv)// **********************// BLINK STREAMING QUERY// **********************import org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.table.api.EnvironmentSettingsimport org.apache.flink.table.api.scala.StreamTableEnvironmentval bsEnv = StreamExecutionEnvironment.getExecutionEnvironmentval bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()val bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)// or val bsTableEnv = TableEnvironment.create(bsSettings)// ******************// BLINK BATCH QUERY// ******************import org.apache.flink.table.api.&#123;EnvironmentSettings, TableEnvironment&#125;val bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build()val bbTableEnv = TableEnvironment.create(bbSettings)注意：如果/lib目录中只有一个计划器jar ，则可以使用useAnyPlanner（use_any_planner对于python）创建specific EnvironmentSettings。在Catalog中创建表TableEnvironment维护使用标识符创建的表目录的映射。每个标识符由3部分组成：目录名称，数据库名称和对象名称。如果未指定目录或数据库，则将使用当前默认值（请参阅表标识符扩展部分中的示例）。表可以是虚拟（VIEWS）视图或常规（TABLES）。VIEWS可以从现有Table对象创建，通常是Table API或SQL查询的结果。TABLES描述外部数据，例如文件，数据库表或消息队列。临时表与永久表表可以是临时的，并与单个Flink会话的生命周期相关，也可以是永久的，并且在多个Flink会话和群集中可见。永久表需要一个目录（例如Hive Metastore）来维护有关表的元数据。创建永久表后，连接到目录的任何Flink会话都可以看到该表，并且该表将一直存在，直到明确删除该表为止。另一方面，临时表始终存储在内存中，并且仅在它们在其中创建的Flink会话期间存在。这些表对其他会话不可见。它们未绑定到任何目录或数据库，但可以在一个目录或数据库的名称空间中创建。如果删除了临时表的相应数据库，则不会删除这些临时表。创建TableVirtual TablesTableAPI对象对应于VIEW（虚拟表）是一个SQL术语。它封装了逻辑查询计划。可以在目录中创建它，如下所示：12345678// get a TableEnvironmentval tableEnv = ... // see \"Create a TableEnvironment\" section// table is the result of a simple projection query val projTable: Table = tableEnv.from(\"X\").select(...)// register the Table projTable as table \"projectedTable\"tableEnv.createTemporaryView(\"projectedTable\", projTable)注意： Table对象类似于VIEW关系数据库系统中的对象，即，定义的查询Table未经过优化，但是当另一个查询引用已注册时将内联Table。如果多个查询引用同一个已注册的查询Table，则将为每个引用查询内联该查询并执行多次，即，Table将不会共享已注册的结果。Connector Tables也可以从关系数据库中创建一个TABLE 。连接器描述了存储表数据的外部系统。可以在此处声明诸如Apacha Kafka之类的存储系统或常规文件系统。123456tableEnvironment .connect(...) .withFormat(...) .withSchema(...) .inAppendMode() .createTemporaryTable(\"MyTable\")查询表Table APITable API是用于Scala和Java的语言集成查询API。与SQL相反，查询未指定为SQL字符串，而是以编程语言算子逐步构成。API基于Table代表表（流或批处理）的类，并提供应用关系操作的方法。这些方法返回一个新Table对象，该对象表示在input上应用关系运算的结果Table。一些关系操作由多个方法调用组成，例如table.groupBy(…).select()，其中groupBy(…)指定的分组table和select(…)对分组的投影table。该Table API文档介绍了支持流媒体和批次表中的所有表API操作。以下示例显示了一个简单的Table API聚合查询：1234567891011121314// get a TableEnvironmentval tableEnv = ... // see \"Create a TableEnvironment\" section// register Orders table// scan registered Orders tableval orders = tableEnv.from(\"Orders\")// compute revenue for all customers from Franceval revenue = orders .filter('cCountry === \"FRANCE\") .groupBy('cID, 'cName) .select('cID, 'cName, 'revenue.sum AS 'revSum)// emit or convert Table// execute query注意：** Scala Table API使用Scala符号，这些符号以单个记号（’）开头来引用的属性Table。Table API使用Scala隐式。确保导入，org.apache.flink.api.scala._然后org.apache.flink.table.api.scala._使用Scala隐式转换。SQLFlink的SQL集成基于实现SQL标准的Apache Calcite。SQL查询被指定为常规字符串。该SQL文件描述弗林克的流媒体和批量表的SQL支持。以下示例说明如何指定查询并以返回结果Table。123456789101112// get a TableEnvironmentval tableEnv = ... // see \"Create a TableEnvironment\" section// register Orders table// compute revenue for all customers from Franceval revenue = tableEnv.sqlQuery(\"\"\" |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = 'FRANCE' |GROUP BY cID, cName \"\"\".stripMargin)// emit or convert Table// execute query下面的示例演示如何指定将查询结果插入已注册表的更新查询。12345678910111213// get a TableEnvironmentval tableEnv = ... // see \"Create a TableEnvironment\" section// register \"Orders\" table// register \"RevenueFrance\" output table// compute revenue for all customers from France and emit to \"RevenueFrance\"tableEnv.sqlUpdate(\"\"\" |INSERT INTO RevenueFrance |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = 'FRANCE' |GROUP BY cID, cName \"\"\".stripMargin)// execute query混合Table API和SQL表API和SQL查询可以轻松混合，因为它们都返回Table对象：可以在TableSQL查询返回的对象上定义Table API 查询。可以通过在Table API查询的结果上定义SQL查询，方法是在中注册结果表，TableEnvironment然后在FROMSQL查询的子句中引用该表。与DataStream和DataSet API集成计划中的两个planner都可以与DataStreamAPI 集成。只有old planner程序可以与DataSet API集成，在批量处理情况下的Blink planner不能与两者结合使用。 注意：下面DataSet讨论的API仅与批量使用的old planner程序有关。Table API和SQL查询可以轻松地与DataStream和DataSet程序集成并嵌入其中。例如，可以查询外部表（例如从RDBMS），进行一些预处理，例如过滤，投影，聚合或与元数据联接，然后使用DataStream或DataSet API（以及在这些API之上构建的任何库，例如CEP或Gelly）。相反，也可以将Table API或SQL查询应用于DataStream或DataSet程序的结果。Scala的隐式转换scala Table API功能的隐式转换DataSet，DataStream以及Table类。org.apache.flink.table.api.scala._除了导入org.apache.flink.api.scala._Scala DataStream API 的包外，还可以通过导入包来启用这些转换。从DataStream或DataSet创建视图一个DataStream或DataSet可在注册TableEnvironment为视图。结果视图的模式取决于注册的DataStream或的数据类型DataSet。请检查有关将数据类型映射到表模式的部分，以获取详细信息。注意：从DataStream或创建的DataSet视图只能注册为临时视图。1234567891011// get TableEnvironment // registration of a DataSet is equivalentval tableEnv: StreamTableEnvironment = ... // see \"Create a TableEnvironment\" sectionval stream: DataStream[(Long, String)] = ...// register the DataStream as View \"myTable\" with fields \"f0\", \"f1\"tableEnv.createTemporaryView(\"myTable\", stream)// register the DataStream as View \"myTable2\" with fields \"myLong\", \"myString\"tableEnv.createTemporaryView(\"myTable2\", stream, 'myLong, 'myString)将DataStream或DataSet转换为表还可以将DataStream或DataSetin直接转换为aTable。如果要在Table API查询中使用Table，这将很方便。12345678// get TableEnvironment// registration of a DataSet is equivalentval tableEnv = ... // see \"Create a TableEnvironment\" sectionval stream: DataStream[(Long, String)] = ...// convert the DataStream into a Table with default fields '_1, '_2val table1: Table = tableEnv.fromDataStream(stream)// convert the DataStream into a Table with fields 'myLong, 'myStringval table2: Table = tableEnv.fromDataStream(stream, 'myLong, 'myString)将表转换为DataStream或DataSetTable可以转换为DataStream或DataSet。这样，可以在Table API或SQL查询的结果上运行自定义DataStream或DataSet程序。将Table转换为DataStreamor时DataSet，您需要指定结果DataStreamor DataSet的数据类型，即，将Table转换为的数据类型。最方便的转换类型通常是Row。以下列表概述了不同选项的功能：Row：字段按位置，任意数量的字段，对null值的支持，没有类型安全的访问进行映射。POJO：字段按名称映射（POJO字段必须命名为Table字段），任意数量的字段，支持null值，类型安全访问。案例类：字段按位置映射，不支持null值，类型安全访问。元组：按位置映射字段，限制为22（Scala）或25（Java）字段，不支持null值，类型安全访问。原子类型：Table必须具有单个字段，不支持null值，类型安全访问。将Table转换为Stream对Table数据进行查询，数据流将动态更新，随着新记录到达查询的输入流，它会发生变化。因此，将DataStream这种动态查询转换内容，需要对表的更新进行编码。有两种将Table转化成DataStream的模式：Append Mode: 仅仅在对Table进行Insert动态操作的情况下才能使用此模式，即仅追加的情况，并且以前的结果不会更新。Retract Mode: 始终可以使用此模式。它使用标志进行编码INSERT和DELETE更改boolean。123456789101112131415161718// get TableEnvironment. // registration of a DataSet is equivalentval tableEnv: StreamTableEnvironment = ... // see \"Create a TableEnvironment\" section// Table with two fields (String name, Integer age)val table: Table = ...// convert the Table into an append DataStream of Rowval dsRow: DataStream[Row] = tableEnv.toAppendStream[Row](table)// convert the Table into an append DataStream of Tuple2[String, Int]val dsTuple: DataStream[(String, Int)] dsTuple = tableEnv.toAppendStream[(String, Int)](table)// convert the Table into a retract DataStream of Row.// A retract stream of type X is a DataStream[(Boolean, X)]. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE.val retractStream: DataStream[(Boolean, Row)] = tableEnv.toRetractStream[Row](table)注意：有关动态表及其属性的详细讨论，请参见“ 动态表”文档。将Table转换为DataSetTable转换为DataSet如下：123456789101112// get TableEnvironment // registration of a DataSet is equivalentval tableEnv = BatchTableEnvironment.create(env)// Table with two fields (String name, Integer age)val table: Table = ...// convert the Table into a DataSet of Rowval dsRow: DataSet[Row] = tableEnv.toDataSet[Row](table)// convert the Table into a DataSet of Tuple2[String, Int]val dsTuple: DataSet[(String, Int)] = tableEnv.toDataSet[(String, Int)](table)**Table API DataStream演示**12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package tableAPIimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.table.api.&#123;EnvironmentSettings, Table&#125;import org.apache.flink.table.api.scala.StreamTableEnvironmentimport org.apache.flink.streaming.api.scala._import org.apache.flink.types.Rowobject table_stream &#123; case class GameData(user_id: String, game_id: String, game_time: Long, game_score: Int) def main(args: Array[String]): Unit = &#123; val stream_env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val settings: EnvironmentSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build() val table_env: StreamTableEnvironment = StreamTableEnvironment.create(stream_env, settings)// stream_env.setParallelism(1) val socketStream: DataStream[String] = stream_env.socketTextStream(\"localhost\", 8888) val gameStream: DataStream[GameData] = socketStream.map((line: String) =&gt; &#123; val array_data: Array[String] = line.split(\",\") GameData(array_data(0), array_data(1), array_data(2).toLong, array_data(3).toInt) &#125;) // 打印表结构 // table_env.registerDataStream(\"t_game_detail\", gameStream) // val t_game_detail: Table = table_env.scan(\"t_game_detail\") // t_game_detail.printSchema() //查询,去重操作 //这里需注意，使用去重操作时，不能使用toAppendStream。 //val t_game_detail: Table = table_env.fromDataStream(gameStream).select(\"user_id\").distinct() //过滤操作 //val t_game_detail: Table = table_env.fromDataStream(gameStream).filter(\"game_score&gt;500\") //分组聚合操作 import org.apache.flink.table.api.scala._ val t_game_detail: Table = table_env.fromDataStream(gameStream).groupBy('user_id).select('user_id, 'user_id.count as 'count)//.select('user_id, 'game_score.sum as 'sum_game_score) //.groupBy('user_id).select('user_id, 'game_score.avg as 'avg_game_score) // 这里如果对原始数据没有做字段的添加，修改，删除等，toAppendStream的类型可以为case class类型 // 如果对字段进行修改了，可以设置为Row类型，会简化操作 val result: DataStream[(Boolean, Row)] = table_env.toRetractStream[Row](t_game_detail).filter((_: (Boolean, Row))._1 == true) result.print(\"t_game_detail\") stream_env.execute(\"stream table\") &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink DataStream API","slug":"Flink-DataStream-API","date":"2019-06-09T14:44:42.000Z","updated":"2020-09-06T01:05:06.206Z","comments":true,"path":"2019/06/09/Flink-DataStream-API/","link":"","permalink":"cpeixin.cn/2019/06/09/Flink-DataStream-API/","excerpt":"","text":"Flink中的DataStream程序是常规程序，可对数据流实施转换（例如，过滤，更新状态，定义窗口，聚合）。最初从各种来源（例如，消息队列，套接字流，文件）创建数据流。结果通过接收器返回，接收器可以例如将数据写入文件或标准输出（例如命令行终端）。Flink程序可以在各种上下文中运行，独立运行或嵌入其他程序中。执行可以在本地JVM或许多计算机的群集中进行。范例程序以下程序是流式窗口单词计数应用程序的一个完整的工作示例，该应用程序在5秒的窗口中对来自Web套接字的单词进行计数。您可以复制并粘贴代码以在本地运行。123456789101112131415import org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Timeobject WindowWordCount &#123; def main(args: Array[String]) &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val text = env.socketTextStream(\"localhost\", 9999) val counts = text.flatMap &#123; _.toLowerCase.split(\"\\\\W+\") filter &#123; _.nonEmpty &#125; &#125; .map &#123; (_, 1) &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1) counts.print() env.execute(\"Window Stream WordCount\") &#125;&#125;要运行示例程序，请首先从终端使用netcat启动输入流：1nc -lk 9999只需输入一些单词，然后按回车键即可获得一个新单词。这些将作为单词计数程序的输入。如果您想看到计数大于1，请在5秒钟内一次又一次地键入相同的单词（如果您不能快速键入☺，则将窗口大小从5秒钟增加）。数据源源是程序读取其输入的位置。您可以使用将源附加到程序StreamExecutionEnvironment.addSource(sourceFunction)。Flink附带了许多预先实现的源函数，但是您始终可以通过实现SourceFunction for非并行源，实现ParallelSourceFunction接口或扩展 RichParallelSourceFunctionfor并行源来编写自己的自定义源。可从以下位置访问几个预定义的流源StreamExecutionEnvironment：基于文件readTextFile(path)- TextInputFormat逐行读取文本文件，即符合规范的文件，并将其作为字符串返回。readFile(fileInputFormat, path) -根据指定的文件输入格式读取（一次）文件。readFile(fileInputFormat, path, watchType, interval, pathFilter)-这是前两个内部调用的方法。它path根据给定的读取文件fileInputFormat。根据提供的内容watchType，此源可以定期（每intervalms）监视路径中的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或处理一次路径中当前的数据并退出（FileProcessingMode.PROCESS_ONCE）。使用pathFilter，用户可以进一步从文件中排除文件。实施：在后台，Flink将文件读取过程分为两个子任务，即目录监视和数据读取。这些子任务中的每一个都是由单独的实体实现的。监视由单个非并行（并行度= 1）任务实现，而读取由并行运行的多个任务执行。后者的并行性等于作业并行性。单个监视任务的作用是扫描目录（根据定期扫描或仅扫描一次watchType），找到要处理的文件，将它们分成多个部分，并将这些拆分分配给下游阅读器。读者将是阅读实际数据的人。每个拆分只能由一个阅读器读取，而阅读器可以一一阅读多个拆分。重要笔记：如果将watchType设置为FileProcessingMode.PROCESS_CONTINUOUSLY，则在修改文件时，将完全重新处理其内容。这可能会破坏“完全一次”的语义，因为在文件末尾附加数据将导致重新处理其所有内容。如果将watchType设置为FileProcessingMode.PROCESS_ONCE，则源将扫描路径一次并退出，而无需等待读取器完成文件内容的读取。当然，读者将继续阅读，直到读取了所有文件内容。关闭源将导致在该点之后没有更多检查点。这可能导致节点故障后恢复速度变慢，因为作业将从上一个检查点恢复读取。基于套接字socketTextStream-从套接字读取。元素可以由定界符分隔。 ### 基于集合fromCollection(Seq)-从Java Java.util.Collection创建数据流。集合中的所有元素必须具有相同的类型。fromCollection(Iterator)-从迭代器创建数据流。该类指定迭代器返回的元素的数据类型。fromElements(elements: _*)-从给定的对象序列创建数据流。所有对象必须具有相同的类型。fromParallelCollection(SplittableIterator)-从迭代器并行创建数据流。该类指定迭代器返回的元素的数据类型。generateSequence(from, to) -并行生成给定间隔中的数字序列。 ### 自定义addSource-附加新的源功能。例如，要阅读Apache Kafka，可以使用 addSource(new FlinkKafkaConsumer08&lt;&gt;(…))。有关更多详细信息，请参见连接器。Flink 连接 kafka实例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package data_streamimport java.util.Propertiesimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.flink.api.common.serialization.SimpleStringSchemaimport org.apache.flink.streaming.api.scala._import org.apache.flink.runtime.state.filesystem.FsStateBackendimport org.apache.flink.streaming.api.CheckpointingModeimport org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.streaming.connectors.kafka.&#123;FlinkKafkaConsumer, FlinkKafkaConsumerBase&#125;object wordcount_stream &#123; private val KAFKA_TOPIC: String = \"weibo_keyword\" def main(args: Array[String]) &#123; val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"kafka_consumer\") val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // exactly-once 语义保证整个应用内端到端的数据一致性 env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 开启检查点并指定检查点时间间隔为5s env.enableCheckpointing(5000) // checkpoint every 5000 msecs // 设置StateBackend，并指定状态数据存储位置 env.setStateBackend(new FsStateBackend(\"file:///Users/cpeixin/IdeaProjects/code_warehouse/data/KafkaSource\")) val dataSource: FlinkKafkaConsumerBase[String] = new FlinkKafkaConsumer( KAFKA_TOPIC, new SimpleStringSchema(), properties)// .setStartFromEarliest() // 指定从最早offset开始消费 .setStartFromLatest() // 指定从最新offset开始消费 env.addSource(dataSource) .flatMap(get_value(_: String).split(\",\")) .map((_: String, 1)) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1) .print()// .setParallelism(1) 设置并行度 env.execute(\"Flink Streaming—————KafkaSource\") &#125; def get_value(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;数据接收器数据接收器使用DataStream，并将其转发到文件，套接字，外部系统或打印它们。Flink带有各种内置的输出格式，这些格式封装在DataStream的操作后面：writeAsText()/ TextOutputFormat-将元素按行写为字符串。通过调用每个元素的toString（）方法获得字符串。writeAsCsv(…)/ CsvOutputFormat-将元组写为逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的toString（）方法。print()/ printToErr() - 在标准输出/标准错误流上打印每个元素的toString（）值。可选地，可以提供前缀（msg），该前缀在输出之前。这可以帮助区分不同的打印调用。如果并行度大于1，则输出之前还将带有产生输出的任务的标识符。writeUsingOutputFormat()/ FileOutputFormat-自定义文件输出的方法和基类。支持自定义对象到字节的转换。writeToSocket -根据以下内容将元素写入套接字 SerializationSchemaaddSink-调用自定义接收器功能。Flink捆绑有连接到其他系统（例如Apache Kafka）的连接器，这些连接器实现为接收器功能。请注意，上的write*()方法DataStream主要用于调试目的。它们不参与Flink的检查点，这意味着这些功能通常具有至少一次的语义。刷新到目标系统的数据取决于OutputFormat的实现。这意味着并非所有发送到OutputFormat的元素都立即显示在目标系统中。同样，在失败的情况下，这些记录可能会丢失。为了将流可靠，准确地一次传输到文件系统中，请使用flink-connector-filesystem。同样，通过该.addSink(…)方法的自定义实现可以参与Flink一次精确语义的检查点。执行参数在StreamExecutionEnvironment包含了ExecutionConfig允许用于运行组工作的具体配置值。请参考执行配置 以获取大多数参数的说明。这些参数专门与DataStream API有关：setAutoWatermarkInterval(long milliseconds)：设置自动水印发射的间隔。您可以使用获取当前值long getAutoWatermarkInterval()容错能力状态和检查点描述了如何启用和配置Flink的检查点机制。控制延迟默认情况下，元素不会在网络上一对一传输（这会导致不必要的网络通信），但是会进行缓冲。缓冲区的大小（实际上是在计算机之间传输的）可以在Flink配置文件中设置。尽管此方法可以优化吞吐量，但是当传入流不够快时，它可能会导致延迟问题。为了控制吞吐量和延迟，您可以env.setBufferTimeout(timeoutMillis)在执行环境（或各个运算符）上使用来设置缓冲区填充的最大等待时间。在此时间之后，即使缓冲区未满，也会自动发送缓冲区。此超时的默认值为100毫秒。用法:123val env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironmentenv.setBufferTimeout(timeoutMillis)env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis)为了最大化吞吐量，请设置set setBufferTimeout(-1)来消除超时，并且仅在缓冲区已满时才刷新它们。为了使延迟最小化，请将超时设置为接近0的值（例如5或10 ms）。应避免将缓冲区超时设置为0，因为它可能导致严重的性能下降。调试在分布式群集中运行流式程序之前，最好确保已实现的算法按预期工作。因此，实施数据分析程序通常是检查结果，调试和改进的增量过程。Flink提供的功能可通过在IDE中支持本地调试，注入测试数据和收集结果数据来大大简化数据分析程序的开发过程。本节提供一些提示，说明如何简化Flink程序的开发。本地执行环境A LocalStreamEnvironment在创建该JVM的同一JVM进程内启动Flink系统。如果从IDE启动LocalEnvironment，则可以在代码中设置断点并轻松调试程序。创建并按如下方式使用LocalEnvironment：1234val env = StreamExecutionEnvironment.createLocalEnvironment()val lines = env.addSource(/* some source */)// build your programenv.execute()收集数据源Flink提供了由Java集合支持的特殊数据源，以简化测试。一旦测试了程序，就可以轻松地将源和接收器替换为可读取/写入外部系统的源和接收器。收集数据源可以按如下方式使用：123456789val env = StreamExecutionEnvironment.createLocalEnvironment()// Create a DataStream from a list of elementsval myInts = env.fromElements(1, 2, 3, 4, 5)// Create a DataStream from any Collectionval data: Seq[(String, Int)] = ...val myTuples = env.fromCollection(data)// Create a DataStream from an Iteratorval longIt: Iterator[Long] = ...val myLongs = env.fromCollection(longIt)注意：当前，集合数据源要求数据类型和迭代器实现 Serializable。此外，收集数据源不能并行执行（并行度= 1）。迭代器数据接收器Flink还提供接收器以收集DataStream结果以进行测试和调试。可以如下使用：1234import org.apache.flink.streaming.experimental.DataStreamUtilsimport scala.collection.JavaConverters.asScalaIteratorConverterval myResult: DataStream[(String, Int)] = ...val myOutput: Iterator[(String, Int)] = DataStreamUtils.collect(myResult.javaStream).asScala注意： flink-streaming-contrib模块已从Flink 1.5.0中删除。已移入flink-streaming-java和flink-streaming-scala。DataStream APIMapMap 接受一个元素作为输入，并且根据开发者自定义的逻辑处理后输出。123456789101112131415161718class StreamingDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment(); &#x2F;&#x2F;获取数据源 DataStreamSource&lt;MyStreamingSource.Item&gt; items &#x3D; env.addSource(new MyStreamingSource()).setParallelism(1); &#x2F;&#x2F;Map SingleOutputStreamOperator&lt;Object&gt; mapItems &#x3D; items.map(new MapFunction&lt;MyStreamingSource.Item, Object&gt;() &#123; @Override public Object map(MyStreamingSource.Item item) throws Exception &#123; return item.getName(); &#125; &#125;); &#x2F;&#x2F;打印结果 mapItems.print().setParallelism(1); String jobName &#x3D; &quot;user defined streaming source&quot;; env.execute(jobName); &#125;&#125;我们只取出每个 Item 的 name 字段进行打印。注意，Map 算子是最常用的算子之一，官网中的表述是对一个 DataStream 进行映射，每次进行转换都会调用 MapFunction 函数。从源 DataStream 到目标 DataStream 的转换过程中，返回的是 SingleOutputStreamOperator。当然了，我们也可以在重写的 map 函数中使用 lambda 表达式。123SingleOutputStreamOperator&lt;Object&gt; mapItems &#x3D; items.map( item -&gt; item.getName());甚至，还可以自定义自己的 Map 函数。通过重写 MapFunction 或 RichMapFunction 来自定义自己的 map 函数。123456789101112131415161718class StreamingDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment(); &#x2F;&#x2F;获取数据源 DataStreamSource&lt;MyStreamingSource.Item&gt; items &#x3D; env.addSource(new MyStreamingSource()).setParallelism(1); SingleOutputStreamOperator&lt;String&gt; mapItems &#x3D; items.map(new MyMapFunction()); &#x2F;&#x2F;打印结果 mapItems.print().setParallelism(1); String jobName &#x3D; &quot;user defined streaming source&quot;; env.execute(jobName); &#125; static class MyMapFunction extends RichMapFunction&lt;MyStreamingSource.Item,String&gt; &#123; @Override public String map(MyStreamingSource.Item item) throws Exception &#123; return item.getName(); &#125; &#125;&#125;此外，在 RichMapFunction 中还提供了 open、close 等函数方法，重写这些方法还能实现更为复杂的功能，比如获取累加器、计数器等。FlatMapFlatMap 接受一个元素，返回零到多个元素。FlatMap 和 Map 有些类似，但是当返回值是列表的时候，FlatMap 会将列表“平铺”，也就是以单个元素的形式进行输出。1234567SingleOutputStreamOperator&lt;Object&gt; flatMapItems &#x3D; items.flatMap(new FlatMapFunction&lt;MyStreamingSource.Item, Object&gt;() &#123; @Override public void flatMap(MyStreamingSource.Item item, Collector&lt;Object&gt; collector) throws Exception &#123; String name &#x3D; item.getName(); collector.collect(name); &#125;&#125;);上面的程序会把名字逐个输出。我们也可以在 FlatMap 中实现更为复杂的逻辑，比如过滤掉一些我们不需要的数据等。Filter顾名思义，Fliter 的意思就是过滤掉不需要的数据，每个元素都会被 filter 函数处理，如果 filter 函数返回 true 则保留，否则丢弃。例如，我们只保留 id 为偶数的那些 item。123456SingleOutputStreamOperator&lt;MyStreamingSource.Item&gt; filterItems &#x3D; items.filter(new FilterFunction&lt;MyStreamingSource.Item&gt;() &#123; @Override public boolean filter(MyStreamingSource.Item item) throws Exception &#123; return item.getId() % 2 &#x3D;&#x3D; 0; &#125;&#125;);当然，我们也可以在 filter 中使用 lambda 表达式：123SingleOutputStreamOperator&lt;MyStreamingSource.Item&gt; filterItems &#x3D; items.filter( item -&gt; item.getId() % 2 &#x3D;&#x3D; 0);KeyBy在介绍 KeyBy 函数之前，需要你理解一个概念：KeyedStream。 在实际业务中，我们经常会需要根据数据的某种属性或者单纯某个字段进行分组，然后对不同的组进行不同的处理。举个例子，当我们需要描述一个用户画像时，则需要根据用户的不同行为事件进行加权；再比如，我们在监控双十一的交易大盘时，则需要按照商品的品类进行分组，分别计算销售额。我们在使用 KeyBy 函数时会把 DataStream 转换成为 KeyedStream，事实上 KeyedStream 继承了 DataStream，KeyedStream 中的元素会根据用户传入的参数进行分组。我们在第 02 课时中讲解的 WordCount 程序，曾经使用过 KeyBy：12345678910111213&#x2F;&#x2F; 将接收的数据进行拆分，分组，窗口计算并且进行聚合输出 DataStream&lt;WordWithCount&gt; windowCounts &#x3D; text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(&quot;\\\\s&quot;)) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds ....在生产环境中使用 KeyBy 函数时要十分注意！该函数会把数据按照用户指定的 key 进行分组，那么相同分组的数据会被分发到一个 subtask 上进行处理，在大数据量和 key 分布不均匀的时非常容易出现数据倾斜和反压，导致任务失败。常见的解决方式是把所有数据加上随机前后缀，这些我们会在后面的课时中进行深入讲解。AggregationsAggregations 为聚合函数的总称，常见的聚合函数包括但不限于 sum、max、min 等。Aggregations 也需要指定一个 key 进行聚合，官网给出了几个常见的例子：12345678910keyedStream.sum(0);keyedStream.sum(&quot;key&quot;);keyedStream.min(0);keyedStream.min(&quot;key&quot;);keyedStream.max(0);keyedStream.max(&quot;key&quot;);keyedStream.minBy(0);keyedStream.minBy(&quot;key&quot;);keyedStream.maxBy(0);keyedStream.maxBy(&quot;key&quot;);在上面的这几个函数中，max、min、sum 会分别返回最大值、最小值和汇总值；而 minBy 和 maxBy 则会把最小或者最大的元素全部返回。我们拿 max 和 maxBy 举例说明：12345678910111213141516StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();&#x2F;&#x2F;获取数据源List data &#x3D; new ArrayList&lt;Tuple3&lt;Integer,Integer,Integer&gt;&gt;();data.add(new Tuple3&lt;&gt;(0,1,0));data.add(new Tuple3&lt;&gt;(0,1,1));data.add(new Tuple3&lt;&gt;(0,2,2));data.add(new Tuple3&lt;&gt;(0,1,3));data.add(new Tuple3&lt;&gt;(1,2,5));data.add(new Tuple3&lt;&gt;(1,2,9));data.add(new Tuple3&lt;&gt;(1,2,11));data.add(new Tuple3&lt;&gt;(1,2,13));DataStreamSource&lt;MyStreamingSource.Item&gt; items &#x3D; env.fromCollection(data);items.keyBy(0).max(2).printToErr();&#x2F;&#x2F;打印结果String jobName &#x3D; &quot;user defined streaming source&quot;;env.execute(jobName);我们直接运行程序，会发现奇怪的一幕：从上图中可以看到，我们希望按照 Tuple3 的第一个元素进行聚合，并且按照第三个元素取最大值。结果如我们所料，的确是按照第三个元素大小依次进行的打印，但是结果却出现了一个这样的元素 (0,1,2)，这在我们的源数据中并不存在。我们在 Flink 官网中的文档可以发现：The difference between min and minBy is that min returns the minimum value, whereas minBy returns the element that has the minimum value in this field (same for max and maxBy).文档中说：min 和 minBy 的区别在于，min 会返回我们制定字段的最大值，minBy 会返回对应的元素（max 和 maxBy 同理）。网上很多资料也这么写：min 和 minBy 的区别在于 min 返回最小的值，而 minBy 返回最小值的key，严格来说这是不正确的。min 和 minBy 都会返回整个元素，只是 min 会根据用户指定的字段取最小值，并且把这个值保存在对应的位置，而对于其他的字段，并不能保证其数值正确。max 和 maxBy 同理。事实上，对于 Aggregations 函数，Flink 帮助我们封装了状态数据，这些状态数据不会被清理，所以在实际生产环境中应该尽量避免在一个无限流上使用 Aggregations。而且，对于同一个 keyedStream ，只能调用一次 Aggregation 函数。ReduceReduce 函数的原理是，会在每一个分组的 keyedStream 上生效，它会按照用户自定义的聚合逻辑进行分组聚合。例如：1234567891011121314151617181920List data &#x3D; new ArrayList&lt;Tuple3&lt;Integer,Integer,Integer&gt;&gt;();data.add(new Tuple3&lt;&gt;(0,1,0));data.add(new Tuple3&lt;&gt;(0,1,1));data.add(new Tuple3&lt;&gt;(0,2,2));data.add(new Tuple3&lt;&gt;(0,1,3));data.add(new Tuple3&lt;&gt;(1,2,5));data.add(new Tuple3&lt;&gt;(1,2,9));data.add(new Tuple3&lt;&gt;(1,2,11));data.add(new Tuple3&lt;&gt;(1,2,13));DataStreamSource&lt;Tuple3&lt;Integer,Integer,Integer&gt;&gt; items &#x3D; env.fromCollection(data);&#x2F;&#x2F;items.keyBy(0).max(2).printToErr();SingleOutputStreamOperator&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; reduce &#x3D; items.keyBy(0).reduce(new ReduceFunction&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt;() &#123; @Override public Tuple3&lt;Integer,Integer,Integer&gt; reduce(Tuple3&lt;Integer, Integer, Integer&gt; t1, Tuple3&lt;Integer, Integer, Integer&gt; t2) throws Exception &#123; Tuple3&lt;Integer,Integer,Integer&gt; newTuple &#x3D; new Tuple3&lt;&gt;(); newTuple.setFields(0,0,(Integer)t1.getField(2) + (Integer) t2.getField(2)); return newTuple; &#125;&#125;);reduce.printToErr().setParallelism(1);我们对下面的元素按照第一个元素进行分组，第三个元素分别求和，并且把第一个和第二个元素都置为 0：12345678data.add(new Tuple3&lt;&gt;(0,1,0));data.add(new Tuple3&lt;&gt;(0,1,1));data.add(new Tuple3&lt;&gt;(0,2,2));data.add(new Tuple3&lt;&gt;(0,1,3));data.add(new Tuple3&lt;&gt;(1,2,5));data.add(new Tuple3&lt;&gt;(1,2,9));data.add(new Tuple3&lt;&gt;(1,2,11));data.add(new Tuple3&lt;&gt;(1,2,13));那么最终会得到：(0,0,6) 和 (0,0,38)。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 应用场景","slug":"Flink-应用场景","date":"2019-06-08T09:00:58.000Z","updated":"2020-09-05T17:12:49.142Z","comments":true,"path":"2019/06/08/Flink-应用场景/","link":"","permalink":"cpeixin.cn/2019/06/08/Flink-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"Apache Flink 功能强大，支持开发和运行多种不同种类的应用程序。它的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。Flink 不仅可以运行在包括 YARN、 Mesos、Kubernetes 在内的多种资源管理框架上，还支持在Standalone独立部署。在启用高可用选项的情况下，它不存在单点失效问题。事实证明，Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。接下来我们将介绍 Flink 常见的几类应用并给出相关实例链接。事件驱动型应用数据分析应用数据管道应用事件驱动型应用什么是事件驱动型应用？事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。事件驱动型应用是在计算存储分离的传统应用基础上进化而来。在传统架构中，应用需要读写远程事务型数据库。相反，事件驱动型应用是基于状态化流处理来完成。在该设计中，数据和计算不会分离，应用只需访问本地（内存或磁盘）即可获取数据。系统容错性的实现依赖于定期向远程持久化存储写入 checkpoint。下图描述了传统应用和事件驱动型应用架构的区别。事件驱动型应用的优势？事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少。Flink 如何支持事件驱动型应用？高效的状态管理，Flink 自带的 State Backend 可以很好的存储中间状态信息；丰富的窗口支持，Flink 支持包含滚动窗口、滑动窗口及其他窗口；多种时间语义，Flink 支持 Event Time、Processing Time 和 Ingestion Time；不同级别的容错，Flink 支持 At Least Once 或 Exactly Once 容错级别。内置的 ProcessFunction 支持细粒度时间控制，方便实现一些高级业务逻辑。同时，Flink 还拥有一个复杂事件处理（CEP）类库，可以用来检测数据流中的模式。Flink 中针对事件驱动应用的明星特性当属 savepoint。Savepoint 是一个一致性的状态映像，它可以用来初始化任意状态兼容的应用。在完成一次 savepoint 后，即可放心对应用升级或扩容，还可以启动多个版本的应用来完成 A/B 测试。典型的事件驱动型应用实例反欺诈异常检测基于规则的报警业务流程监控（社交网络）Web 应用数据分析应用什么是数据分析应用？数据分析任务需要从原始数据中提取有价值的信息和指标。传统的分析方式通常是利用批查询，或将事件记录下来并基于此有限数据集构建应用来完成。为了得到最新数据的分析结果，必须先将它们加入分析数据集并重新执行查询或运行应用，随后将结果写入存储系统或生成报告。借助一些先进的流处理引擎，还可以实时地进行数据分析。和传统模式下读取有限数据集不同，流式查询或应用会接入实时事件流，并随着事件消费持续产生和更新结果。这些结果数据可能会写入外部数据库系统或以内部状态的形式维护。仪表展示应用可以相应地从外部数据库读取数据或直接查询应用的内部状态。如下图所示，Apache Flink 同时支持流式及批量分析应用。流式分析应用的优势？和批量分析相比，由于流式分析省掉了周期性的数据导入和查询过程，因此从事件中获取指标的延迟更低。不仅如此，批量查询必须处理那些由定期导入和输入有界性导致的人工数据边界，而流式查询则无须考虑该问题。另一方面，流式分析会简化应用抽象。批量查询的流水线通常由多个独立部件组成，需要周期性地调度提取数据和执行查询。如此复杂的流水线操作起来并不容易，一旦某个组件出错将会影响流水线的后续步骤。而流式分析应用整体运行在 Flink 之类的高端流处理系统之上，涵盖了从数据接入到连续结果计算的所有步骤，因此可以依赖底层引擎提供的故障恢复机制。Flink 如何支持数据分析类应用？Flink 为持续流式分析和批量分析都提供了良好的支持。具体而言，它内置了一个符合 ANSI 标准的 SQL 接口，将批、流查询的语义统一起来。无论是在记录事件的静态数据集上还是实时事件流上，相同 SQL 查询都会得到一致的结果。同时 Flink 还支持丰富的用户自定义函数，允许在 SQL 中执行定制化代码。如果还需进一步定制逻辑，可以利用 Flink DataStream API 和 DataSet API 进行更低层次的控制。此外，Flink 的 Gelly 库为基于批量数据集的大规模高性能图分析提供了算法和构建模块支持。典型的数据分析应用实例电信网络质量监控移动应用中的产品更新及实验评估分析消费者技术中的实时数据即席分析大规模图分析数据管道应用什么是数据管道？提取-转换-加载（ETL）是一种在存储系统之间进行数据转换和迁移的常用方法。ETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库。数据管道和 ETL 作业的用途相似，都可以转换、丰富数据，并将其从某个存储系统移动到另一个。但数据管道是以持续流模式运行，而非周期性触发。因此它支持从一个不断生成数据的源头读取记录，并将它们以低延迟移动到终点。例如：数据管道可以用来监控文件系统目录中的新文件，并将其数据写入事件日志；另一个应用可能会将事件流物化到数据库或增量构建和优化查询索引。下图描述了周期性 ETL 作业和持续数据管道的差异。数据管道的优势？和周期性 ETL 作业相比，持续数据管道可以明显降低将数据移动到目的端的延迟。此外，由于它能够持续消费和发送数据，因此用途更广，支持用例更多。Flink 如何支持数据管道应用？很多常见的数据转换和增强操作可以利用 Flink 的 SQL 接口（或 Table API）及用户自定义函数解决。如果数据管道有更高级的需求，可以选择更通用的 DataStream API 来实现。Flink 为多种数据存储系统（如：Kafka、Kinesis、Elasticsearch、JDBC数据库系统等）内置了连接器。同时它还提供了文件系统的连续型数据源及数据汇，可用来监控目录变化和以时间分区的方式写入文件。典型的数据管道应用实例电子商务中的实时查询索引构建电子商务中的持续 ETL丰富的APIFlink 自身提供了不同级别的抽象来支持我们开发流式或者批量处理程序，上图描述了 Flink 支持的 4 种不同级别的抽象。对于我们开发者来说，大多数应用程序不需要上图中的最低级别的 Low-level 抽象，而是针对 Core API 编程， 比如 DataStream API（有界/无界流）和 DataSet API （有界数据集）。这些流畅的 API 提供了用于数据处理的通用构建块，比如各种形式用户指定的转换、连接、聚合、窗口、状态等。Table API 和 SQL 是 Flink 提供的更为高级的 API 操作，Flink SQL 是 Flink 实时计算为简化计算模型，降低用户使用实时计算门槛而设计的一套符合标准 SQL 语义的开发语言。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 运行架构","slug":"Flink-运行架构","date":"2019-06-07T07:32:00.000Z","updated":"2020-09-05T16:43:49.300Z","comments":true,"path":"2019/06/07/Flink-运行架构/","link":"","permalink":"cpeixin.cn/2019/06/07/Flink-%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/","excerpt":"","text":"客户端提交流程Session Mode部署启动session：1.&#x2F;bin&#x2F;yarn-session.sh -tm 1028 -s 2运行样例程序：123hdfs dfs -put wordcount_demo.txt &#x2F;data&#x2F;.&#x2F;bin&#x2F;flink run .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar --input hdfs:&#x2F;&#x2F;localhost:8020&#x2F;data&#x2F;wordcount_demo.txt --out hdfs:&#x2F;&#x2F;localhost:8020&#x2F;data&#x2F;wordcount_demo_out.txt停止集群服务：方式一：1echo &quot;stop&quot; | .&#x2F;bin&#x2F;yarn-session.sh -id application_1599311264123_0002方式二：123yarn application listyarn application kill application_1599311264123_0001**Per-Job Mode直接使用命令提交程序1.&#x2F;bin&#x2F;flink run -m yarn-cluster .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar任务提交流程2.解析命令参数项并初始化，启动指定运行模式如果是per-job运行模式将根据命令行参数指定的Job主类创建job graph；如果可以从命令行参数(-yid )或YARN properties临时文件(${java.io.tmpdir}/.yarn-properties-${user.name})中获取应用ID，向指定的应用提交Job；否则当命令行参数中包含 -d（表示detached模式）和 -m yarn-cluster（表示指定YARN集群模式），启动per-job运行模式；否则当命令行参数项不包含 -yq（表示查询YARN集群可用资源）时，启动session运行模式；3.获取YARN集群信息、新应用ID并启动运行前检查；**通过YarnClient向YARN ResourceManager请求创建一个新应用（YARN RM收到创建应用请求后生成新应用ID和container申请的资源上限后返回），并且获取YARN Slave节点报告（YARN RM返回全部slave节点的ID、状态、rack、http地址、总资源、已使用资源等信息）；运行前检查：(1) 简单验证YARN集群能否访问；(2) 最大node资源能否满足flink JobManager/TaskManager vcores资源申请需求；(3) 指定queue是否存在(不存在也只是打印WARN信息，后续向YARN提交时排除异常并退出)；(4)当预期应用申请的Container资源会超出YARN资源限制时抛出异常并退出；(5) 当预期应用申请不能被满足时（例如总资源超出YARN集群可用资源总量、Container申请资源超出NM可用资源最大值等）提供一些参考信息。4.将应用配置(flink-conf.yaml、logback.xml、log4j.properties)和相关文件(flink jars、ship files、user jars、job graph等)上传至分布式存储(例如HDFS)的应用暂存目录(/user/${user.name}/.flink/)；5.准备应用提交上下文(ApplicationSubmissionContext，包括应用的名称、类型、队列、标签等信息和应用Master的container的环境变量、classpath、资源大小等)，注册处理部署失败的shutdown hook（清理应用对应的HDFS目录），然后通过YarnClient向YARN RM提交应用；6.循环等待直到应用状态为RUNNING，包含两个阶段：循环等待应用提交成功（SUBMITTED）：默认每隔200ms通过YarnClient获取应用报告，如果应用状态不是NEW和NEW_SAVING则认为提交成功并退出循环，每循环10次会将当前的应用状态输出至日志：”Application submission is not finished, submitted application is still in “，提交成功后输出日志：”Submitted application “循环等待应用正常运行（RUNNING）：每隔250ms通过YarnClient获取应用报告，每轮循环也会将当前的应用状态输出至日志：”Deploying cluster, current state “。应用状态成功变为RUNNING后将输出日志”YARN application has been deployed successfully.” 并退出循环，如果等到的是非预期状态如FAILED/FINISHED/KILLED,就会在输出YARN返回的诊断信息（”The YARN application unexpectedly switched to state during deployment. Diagnostics from YARN: …”）之后抛出异常并退出。Flink Cluster启动流程1.YARN RM中的ClientRMService（为普通用户提供的RPC服务组件，处理来自客户端的各种RPC请求，比如查询YARN集群信息，提交、终止应用等）接收到应用提交请求，简单校验后将请求转交RMAppManager（YARN RM内部管理应用生命周期的组件）；2.RMAppManager根据应用提交上下文内容创建初始状态为NEW的应用，将应用状态持久化到RM状态存储服务（例如ZooKeeper集群，RM状态存储服务用来保证RM重启、HA切换或发生故障后集群应用能够正常恢复，后续流程中的涉及状态存储时不再赘述），应用状态变为NEW_SAVING；3.应用状态存储完成后，应用状态变为SUBMITTED；RMAppManager开始向ResourceScheduler（YARN RM可拔插资源调度器，YARN自带三种调度器FifoScheduler/FairScheduler/CapacityScheduler，其中CapacityScheduler支持功能最多使用最广泛，FifoScheduler功能最简单基本不可用，今年社区已明确不再继续支持FairScheduler，建议已有用户迁至CapacityScheduler）提交应用，如果无法正常提交（例如队列不存在、不是叶子队列、队列已停用、超出队列最大应用数限制等）则抛出拒绝该应用，应用状态先变为FINAL_SAVING触发应用状态存储流程并在完成后变为FAILED；如果提交成功，应用状态变为ACCEPTED；4.开始创建应用运行实例(ApplicationAttempt，由于一次运行实例中最重要的组件是ApplicationMaster，下文简称AM，它的状态代表了ApplicationAttempt的当前状态，所以ApplicationAttempt实际也代表了AM)，初始状态为NEW；5.初始化应用运行实例信息，并向ApplicationMasterService（AM&amp;RM协议接口服务，处理来自AM的请求，主要包括注册和心跳）注册，应用实例状态变为SUBMITTED；6.RMAppManager维护的应用实例开始初始化AM资源申请信息并重新校验队列，然后向ResourceScheduler申请AM Container（Container是YARN中资源的抽象，包含了内存、CPU等多维度资源），应用实例状态变为ACCEPTED；7.ResourceScheduler会根据优先级（队列/应用/请求每个维度都有优先级配置）从根队列开始层层递进，先后选择当前优先级最高的子队列、应用直至具体某个请求，然后结合集群资源分布等情况作出分配决策，AM Container分配成功后，应用实例状态变为ALLOCATED_SAVING，并触发应用实例状态存储流程，存储成功后应用实例状态变为ALLOCATED；8.RMAppManager维护的应用实例开始通知ApplicationMasterLauncher（AM生命周期管理服务，负责启动或清理AM container）启动AM container，ApplicationMasterLauncher与YARN NodeManager（下文简称YARN NM，与YARN RM保持通信，负责管理单个节点上的全部资源、Container生命周期、附属服务等，监控节点健康状况和Container资源使用）建立通信并请求启动AM container；9.ContainerManager（YARN NM核心组件，管理所有Container的生命周期）接收到AM container启动请求，YARN NM开始校验Container Token及资源文件，创建应用实例和Container实例并存储至本地，结果返回后应用实例状态变为LAUNCHED；10.ResourceLocalizationService（资源本地化服务，负责Container所需资源的本地化。它能够按照描述从HDFS上下载Container所需的文件资源，并尽量将它们分摊到各个磁盘上以防止出现访问热点）初始化各种服务组件、创建工作目录、从HDFS下载运行所需的各种资源至Container工作目录（路径为: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache//）；11.ContainersLauncher（负责container的具体操作，包括启动、重启、恢复和清理等）将待运行Container所需的环境变量和运行命令写到Container工作目录下的launch_container.sh脚本中，然后运行该脚本启动Container；12.Container进程加载并运行ClusterEntrypoint**(Flink JobManager入口类，每种集群部署模式和应用运行模式都有相应的实现，例如在YARN集群部署模式下，per-job应用运行模式实现类是YarnJobClusterEntrypoint，session应用运行模式实现类是YarnSessionClusterEntrypoint)，首先初始化相关运行环境：输出各软件版本及运行环境信息、命令行参数项、classpath等信息；注册处理各种SIGNAL的handler:记录到日志注册JVM关闭保障的shutdown hook：避免JVM退出时被其他shutdown hook阻塞打印YARN运行环境信息：用户名从运行目录中加载flink conf初始化文件系统创建并启动各类内部服务（包括RpcService、HAService、BlobServer、HeartbeatServices、MetricRegistry、ExecutionGraphStore等）将RPC address和port更新到flink conf配置13.启动ResourceManager（Flink资源管理核心组件，包含YarnResourceManager和SlotManager两个子组件，YarnResourceManager负责外部资源管理，与YARN RM建立通信并保持心跳，申请或释放TaskManager资源，注销应用等；SlotManager则负责内部资源管理，维护全部Slot信息和状态）及相关服务，创建异步AMRMClient，开始注册AM，注册成功后每隔一段时间（心跳间隔配置项：${yarn.heartbeat.interval}，默认5s）向YARN RM发送心跳来发送资源更新请求和接受资源变更结果。YARN RM内部该应用和应用运行实例的状态都变为RUNNING，并通知AMLivelinessMonitor服务监控AM是否存活状态，当心跳超过一定时间（默认10分钟）触发AM failover流程；14.启动**Dispatcher（负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager）及相关服务（包括REST endpoint等），在per-job运行模式下，Dispatcher将直接从Container工作目录加载JobGraph文件；在session运行模式下，Dispatcher将在接收客户端提交的Job（_通过BlockServer接收job graph文件）后再进行后续流程；15.根据JobGraph启动JobManager（负责作业调度、管理Job和Task的生命周期），构建 ExecutionGraph（JobGraph的并行化版本，调度层最核心的数据结构）；16.JobManager开始执行ExecutionGraph，向ResourceManager申请资源；17.ResourceManager将资源请求加入等待请求队列，并通过心跳向YARN RM申请新的Container资源来启动TaskManager进程；后续流程如果有空闲Slot资源，SlotManager将其分配给等待请求队列中匹配的请求，不用再通过18. YarnResourceManager申请新的TaskManager；18.YARN ApplicationMasterService接收到资源请求后，解析出新的资源请求并更新应用请求信息；19.YARN ResourceScheduler成功为该应用分配资源后更新应用信息，ApplicationMasterService接收到Flink JobManager的下一次心跳时返回新分配资源信息；20.Flink ResourceManager接收到新分配的Container资源后，准备好TaskManager启动上下文（ContainerLauncherContext，生成TaskManager配置并上传至分布式存储，配置其他依赖和环境变量等），然后向YARN NM申请启动TaskManager进程，YARN NM启动Container的流程与AM Container启动流程基本类似，区别在于应用实例在NM上已存在并未RUNNING状态时则跳过应用实例初始化流程，这里不再赘述；21.TaskManager进程加载并运行YarnTaskExecutorRunner（Flink TaskManager入口类），初始化流程完成后启动TaskExecutor（负责执行Task相关操作）；22.TaskExecutor启动后先向ResourceManager注册，成功后再向SlotManager汇报自己的Slot资源与状态；SlotManager接收到Slot空闲资源后主动触发Slot分配，从等待请求队列中选出合适的资源请求后，向TaskManager请求该Slot资源23.TaskManager收到请求后检查该Slot是否可分配（不存在则返回异常信息）、Job是否已注册（没有则先注册再分配Slot），检查通过后将Slot分配给JobManager；24.JobManager检查Slot分配是否重复**，通过后通知Execution执行部署task流程，向TaskExecutor提交task；TaskExecutor启动新的线程运行Task。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink集群部署模式","slug":"Flink集群部署模式","date":"2019-06-06T15:09:30.000Z","updated":"2020-09-05T16:43:45.665Z","comments":true,"path":"2019/06/06/Flink集群部署模式/","link":"","permalink":"cpeixin.cn/2019/06/06/Flink%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"Session ModeSession模式整个集群资源共享，提升整个集群资源利用率。不需要为每一个Job生成一套新的集群。Per-JobApplication Mode（更新！！！ Flink 1.11版本才开始支持）","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink 基础","slug":"Flink-基础","date":"2019-06-05T14:44:42.000Z","updated":"2020-09-06T07:04:35.327Z","comments":true,"path":"2019/06/05/Flink-基础/","link":"","permalink":"cpeixin.cn/2019/06/05/Flink-%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Flink 程序是实现了分布式集合转换（例如过滤、映射、更新状态、join、分组、定义窗口、聚合）的规范化程序。集合初始创建自 source（例如读取文件、kafka 主题，或本地内存中的集合）。结果通过 sink 返回，例如，它可以将数据写入（分布式）文件，或标准输出（例如命令行终端）。Flink 程序可以在多种环境中运行，独立运行或嵌入到其他程序中。可以在本地 JVM 中执行，也可以在多台机器的集群上执行。针对有界和无界两种数据 source 类型，你可以使用 DataSet API 来编写批处理程序或使用 DataStream API 来编写流处理程序。本篇指南将介绍这两种 API 通用的基本概念，关于使用 API 编写程序的具体信息请查阅 流处理指南 和 批处理指南。请注意：** 当展示如何使用 API 的实际示例时我们使用 StreamingExecutionEnvironment 和 DataStream API。对于批处理，将他们替换为 ExecutionEnvironment 和 DataSet API 即可，概念是完全相同的。DataSet 和 DataStreamFlink 用特有的 DataSet 和 DataStream 类来表示程序中的数据。你可以将他们视为可能包含重复项的不可变数据集合。对于 DataSet，数据是有限的，而对于 DataStream，元素的数量可以是无限的。这些集合与标准的 Java 集合有一些关键的区别。首先它们是不可变的，也就是说它们一旦被创建你就不能添加或删除元素了。你也不能简单地检查它们内部的元素。在 Flink 程序中，集合最初通过添加数据 source 来创建，通过使用诸如 map、filter 等 API 方法对数据 source 进行转换从而派生新的集合。剖析一个 Flink 程序Flink 程序看起来像是转换数据集合的规范化程序。每个程序由一些基本的部分组成：获取执行环境，加载/创建初始数据，指定对数据的转换操作，指定计算结果存放的位置，触发程序执行我们现在将概述每个步骤，详细信息请参阅相应章节。请注意所有 Scala DataSet API 可以在这个包 org.apache.flink.api.scala 中找到，同时，所有 Scala DataStream API 可以在这个包 org.apache.flink.streaming.api.scala 中找到。StreamExecutionEnvironment 是所有 Flink 程序的基础。你可以使用它的这些静态方法获取：123getExecutionEnvironment()createLocalEnvironment()createRemoteEnvironment(host: String, port: Int, jarFiles: String*)通常你只需要使用 getExecutionEnvironment()，因为它会根据上下文环境完成正确的工作，如果你在 IDE 中执行程序或者作为标准的 Java 程序来执行，它会创建你的本机执行环境。如果你将程序封装成 JAR 包，然后通过命令行调用，Flink 集群管理器会执行你的 main 方法并且 getExecutionEnvironment() 会返回在集群上执行程序的执行环境。针对不同的数据 source，执行环境有若干不同的读取文件的方法：你可以逐行读取 CSV 文件，或者使用完全自定义的输入格式。要将文本文件作为一系列行读取，你可以使用：12val env = StreamExecutionEnvironment.getExecutionEnvironment()val text: DataStream[String] = env.readTextFile(\"file:///path/to/file\")如此你会得到一个 DataStream 然后对其应用转换操作从而创建新的派生 DataStream。通过调用 DataStream 的转换函数来进行转换。下面是一个映射转换的实例：12val input: DataSet[String] &#x3D; ...val mapped &#x3D; input.map &#123; x &#x3D;&gt; x.toInt &#125;这会通过把原始数据集合的每个字符串转换为一个整数创建一个新的 DataStream。一旦你得到了包含最终结果的 DataStream，就可以通过创建 sink 将其写入外部系统。如下是一些创建 sink 的示例：12writeAsText(path: String)print()当设定好整个程序以后你需要调用 StreamExecutionEnvironment 的 execute() 方法触发程序执行。至于在你的本机触发还是提交到集群运行取决于 ExecutionEnvironment 的类型。execute() 方法返回 JobExecutionResult，它包括执行耗时和一个累加器的结果。如果你不需要等待作业的结束，只是想要触发程序执行，你可以调用 StreamExecutionEnvironment 的 executeAsync() 方法。这个方法将返回一个 JobClient 对象，通过 JobClient 能够与程序对应的作业进行交互。作为例子，这里介绍通过 executeAsync() 实现与 execute() 相同行为的方法。12final JobClient jobClient = env.executeAsync();final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult(userClassloader).get();有关流数据的 source 和 sink 以及有关 DataStream 支持的转换操作的详细信息请参阅流处理指南。有关批数据的 source 和 sink 以及有关 DataSet 支持的转换操作的详细信息请参阅批处理指南。延迟计算无论在本地还是集群执行，所有的 Flink 程序都是延迟执行的：当程序的 main 方法被执行时，并不立即执行数据的加载和转换，而是创建每个操作并将其加入到程序的执行计划中。当执行环境调用 execute() 方法显式地触发执行的时候才真正执行各个操作。延迟计算允许你构建复杂的程序，Flink 将其作为整体计划单元来执行。指定键一些转换操作（join, coGroup, keyBy, groupBy）要求在元素集合上定义键。另外一些转换操作 （Reduce, GroupReduce, Aggregate, Windows）允许在应用这些转换之前将数据按键分组。如下对 DataSet 分组1234DataSet&lt;...&gt; input = // [...]DataSet&lt;...&gt; reduced = input .groupBy(/*在这里定义键*/) .reduceGroup(/*一些处理操作*/);如下对 DataStream 指定键1234DataStream&lt;...&gt; input = // [...]DataStream&lt;...&gt; windowed = input .keyBy(/*在这里定义键*/) .window(/*指定窗口*/);Flink 的数据模型不是基于键值对的。因此你不需要将数据集类型物理地打包到键和值中。键都是“虚拟的”：它们的功能是指导分组算子用哪些数据来分组。请注意：*下面的讨论中我们将以 *DataStream** 和 **keyby** 为例。 对于 DataSet API 你只需要用 **DataSet** 和 **groupBy** 替换即可。**为 Tuple 定义键最简单的方式是按照 Tuple 的一个或多个字段进行分组：12val input: DataStream[(Int, String, Long)] = // [...]val keyed = input.keyBy(0)按照第一个字段（整型字段）对 Tuple 分组。12val input: DataSet[(Int, String, Long)] = // [...]val grouped = input.groupBy(0,1)这里我们用第一个字段和第二个字段组成的组合键对 Tuple 分组对于嵌套 Tuple 请注意： 如果你的 DataStream 是嵌套 Tuple，例如：1DataStream&lt;Tuple3&lt;Tuple2&lt;Integer, Float&gt;,String,Long&gt;&gt; ds;指定 keyBy(0) 将导致系统使用整个 Tuple2 作为键（一个整数和一个浮点数）。 如果你想“进入”到 Tuple2 的内部，你必须使用如下所述的字段表达式键。使用字段表达式定义键可以使用基于字符串的字段表达式来引用嵌套字段，并定义用于分组、排序、join 或 coGrouping 的键。字段表达式可以很容易地选取复合（嵌套）类型中的字段，例如 Tuple 和 POJO 类型。下例中，我们有一个包含“word”和“count”两个字段的 POJO：WC。要用 word 字段分组，我们只需要把它的名字传给 keyBy() 函数即可。12345678910// 普通的 POJO（简单的 Java 对象）class WC(var word: String, var count: Int) &#123; def this() &#123; this(\"\", 0L) &#125;&#125;val words: DataStream[WC] = // [...]val wordCounts = words.keyBy(\"word\").window(/*指定窗口*/)// 或者，代码少一点的 case classcase class WC(word: String, count: Int)val words: DataStream[WC] = // [...]val wordCounts = words.keyBy(\"word\").window(/*指定窗口*/)字段表达式语法**：根据字段名称选择 POJO 的字段。例如 “user” 就是指 POJO 类型的“user”字段。根据 1 开始的字段名称或 0 开始的字段索引选择 Tuple 的字段。例如 “_1” 和 “5” 分别指 Java Tuple 类型的第一个和第六个字段。可以选择 POJO 和 Tuple 的嵌套字段。 例如，一个 POJO 类型有一个“user”字段还是一个 POJO 类型，那么 “user.zip” 即指这个“user”字段的“zip”字段。任意嵌套和混合的 POJO 和 Tuple都是支持的，例如 “_2.user.zip” 或 “user._4.1.zip”。可以使用 &quot;*&quot; 通配符表达式选择完整的类型。这也适用于非 Tuple 或 POJO 类型。字段表达式示例**:12345678910class WC(var complex: ComplexNestedClass, var count: Int) &#123; def this() &#123; this(null, 0) &#125;&#125;class ComplexNestedClass( var someNumber: Int, someFloat: Float, word: (Long, Long, String), hadoopCitizen: IntWritable) &#123; def this() &#123; this(0, 0, (0, 0, \"\"), new IntWritable(0)) &#125;&#125;这些字段表达式对于以上代码示例都是合法的：&quot;count&quot;：WC 类的 count 字段。&quot;complex&quot;：递归选择 POJO 类型 ComplexNestedClass 的 complex 字段的全部字段。&quot;complex.word._3&quot;：选择嵌套 Tuple3 类型的最后一个字段。&quot;complex.hadoopCitizen&quot;：选择 hadoop 的 IntWritable 类型。使用键选择器函数定义键定义键的另一种方法是“键选择器”函数。键选择器函数将单个元素作为输入并返回元素的键。键可以是任意类型，并且可以由确定性计算得出。下例展示了一个简单返回对象字段的键选择器函数：1234&#x2F;&#x2F; 普通的 case classcase class WC(word: String, count: Int)val words: DataStream[WC] &#x3D; &#x2F;&#x2F; [...]val keyed &#x3D; words.keyBy( _.word )指定转换函数大多数转换操作需要用户定义函数。本节列举了指定它们的不同方法。Lambda 函数正如前面的例子中所见，所有操作都接受 lambda 函数来描述操作：12val data: DataSet[String] = // [...]data.filter &#123; _.startsWith(\"http://\") &#125;1234val data: DataSet[Int] = // [...]data.reduce &#123; (i1,i2) =&gt; i1 + i2 &#125;// 或者data.reduce &#123; _ + _ &#125;富函数所有需要用户定义函数的转换操作都可以将富函数作为参数。例如，对于1data.map &#123; x =&gt; x.toInt &#125;你可以替换成123class MyMapFunction extends RichMapFunction[String, Int] &#123; def map(in: String):Int = &#123; in.toInt &#125;&#125;;并像往常一样将函数传递给 map 转换操作：1data.map(new MyMapFunction())富函数也可以被定义为匿名类：123data.map (new RichMapFunction[String, Int] &#123; def map(in: String):Int &#x3D; &#123; in.toInt &#125;&#125;)富函数为用户定义函数（map、reduce 等）额外提供了 4 个方法： open、close、getRuntimeContext 和 setRuntimeContext。这些方法有助于向函数传参（请参阅 向函数传递参数）、 创建和终止本地状态、访问广播变量（请参阅 广播变量）、访问诸如累加器和计数器等运行时信息（请参阅 累加器和计数器）和迭代信息（请参阅 迭代）。支持的数据类型Flink 对于 DataSet 或 DataStream 中可以包含的元素类型做了一些限制。这么做是为了使系统能够分析类型以确定有效的执行策略。有七种不同的数据类型：Java Tuple 和 Scala Case ClassJava POJO基本数据类型常规的类值Hadoop Writable特殊类型Tuple 和 Case ClassScala Case Class（以及作为 Case Class 的特例的 Scala Tuple）是复合类型，包含固定数量的各种类型的字段。Tuple 的字段从 1 开始索引。例如 _1 指第一个字段。Case Class 字段用名称索引。123456789case class WordCount(word: String, count: Int)val input = env.fromElements( WordCount(\"hello\", 1), WordCount(\"world\", 2)) // Case Class 数据集input.keyBy(\"word\")// 以字段表达式“word”为键val input2 = env.fromElements((\"hello\", 1), (\"world\", 2)) // Tuple2 数据集input2.keyBy(0, 1) // 以第 0 和第 1 个字段为键POJOFlink 将满足如下条件的 Java 和 Scala 的类作为特殊的 POJO 数据类型处理：类必须是公有的。它必须有一个公有的无参构造器（默认构造器）。所有的字段要么是公有的要么必须可以通过 getter 和 setter 函数访问。例如一个名为 foo 的字段，它的 getter 和 setter 方法必须命名为 getFoo() 和 setFoo()。字段的类型必须被已注册的序列化程序所支持。POJO 通常用 PojoTypeInfo 表示，并使用 PojoSerializer（Kryo 作为可配置的备用序列化器）序列化。 例外情况是 POJO 是 Avro 类型（Avro 指定的记录）或作为“Avro 反射类型”生成时。 在这种情况下POJO 由 AvroTypeInfo 表示，并且由 AvroSerializer 序列化。 如果需要，你可以注册自己的序列化器；更多信息请参阅 序列化。Flink 分析 POJO 类型的结构，也就是说，它会推断出 POJO 的字段。因此，POJO 类型比常规类型更易于使用。此外，Flink 可以比一般类型更高效地处理 POJO。下例展示了一个拥有两个公有字段的简单 POJO。123456789class WordWithCount(var word: String, var count: Int) &#123; def this() &#123; this(null, -1) &#125;&#125;val input = env.fromElements( new WordWithCount(\"hello\", 1), new WordWithCount(\"world\", 2)) // Case Class 数据集input.keyBy(\"word\")// 以字段表达式“word”为键基本数据类型Flink 支持所有 Java 和 Scala 的基本数据类型如 Integer、 String、和 Double。常规的类Flink 支持大部分 Java 和 Scala 的类（API 和自定义）。 除了包含无法序列化的字段的类，如文件指针，I / O流或其他本地资源。遵循 Java Beans 约定的类通常可以很好地工作。Flink 对于所有未识别为 POJO 类型的类（请参阅上面对于的 POJO 要求）都作为常规类处理。 Flink 将这些数据类型视为黑盒，并且无法访问其内容（为了诸如高效排序等目的）。常规类使用 Kryo 序列化框架进行序列化和反序列化。值值 类型手工描述其序列化和反序列化。它们不是通过通用序列化框架，而是通过实现org.apache.flinktypes.Value 接口的 read 和 write 方法来为这些操作提供自定义编码。当通用序列化效率非常低时，使用值类型是合理的。例如，用数组实现稀疏向量。已知数组大部分元素为零，就可以对非零元素使用特殊编码，而通用序列化只会简单地将所有数组元素都写入。org.apache.flinktypes.CopyableValue 接口以类似的方式支持内部手工克隆逻辑。Flink 有与基本数据类型对应的预定义值类型。（ByteValue、 ShortValue、 IntValue、LongValue、 FloatValue、DoubleValue、 StringValue、CharValue、 BooleanValue）。这些值类型充当基本数据类型的可变变体：它们的值可以改变，允许程序员重用对象并减轻垃圾回收器的压力。Hadoop Writable可以使用实现了 org.apache.hadoop.Writable 接口的类型。它们会使用 write() 和 readFields() 方法中定义的序列化逻辑。特殊类型可以使用特殊类型，包括 Scala 的 Either、Option 和 Try。 Java API 有对 Either 的自定义实现。 类似于 Scala 的 Either，它表示一个具有 Left 或 Right 两种可能类型的值。 Either 可用于错误处理或需要输出两种不同类型记录的算子。类型擦除和类型推断_Java 编译器在编译后抛弃了大量泛型类型信息。这在 Java 中被称作 _类型擦除_。它意味着在运行时，对象的实例已经不知道它的泛型类型了。例如 DataStream&lt;String&gt; 和 DataStream&lt;Long&gt; 的实例在 JVM 看来是一样的。Flink 在准备程序执行时（程序的 main 方法被调用时）需要类型信息。Flink Java API 尝试重建以各种方式丢弃的类型信息，并将其显式存储在数据集和算子中。你可以通过 DataStream.getType() 获取数据类型。此方法返回 TypeInformation 的一个实例，这是 Flink 内部表示类型的方式。类型推断有其局限性，在某些情况下需要程序员的“配合”。 这方面的示例是从集合创建数据集的方法，例如 ExecutionEnvironment.fromCollection()，你可以在这里传递一个描述类型的参数。 像MapFunction&lt;I, O&gt; 这样的泛型函数同样可能需要额外的类型信息。可以通过输入格式和函数实现 ResultTypeQueryable 接口，以明确告知 API 其返回类型。 被调函数的_输入类型_通常可以通过先前操作的结果类型来推断。累加器和计数器累加器简单地由 加法操作 和 最终累加结果构成，可在作业结束后使用。最简单的累加器是一个 计数器：你可以使用 Accumulator.add(V value) 方法递增它。作业结束时 Flink 会合计（合并）所有的部分结果并发送给客户端。累加器在 debug 或者你想快速了解数据的时候非常有用。Flink 目前有如下 内置累加器。它们每一个都实现了 Accumulator 接口。IntCounter, LongCounter 和 DoubleCounter： 有关使用计数器的示例，请参见下文。Histogram: 离散数量桶的直方图实现。在内部，它只是一个从整数到整数的映射。你可以用它计算值的分布，例如一个词频统计程序中每行词频的分布。如何使用累加器：**首先你必须在要使用它的用户定义转换函数中创建累加器对象（下例为计数器）。1private IntCounter numLines = new IntCounter();其次，你必须注册累加器对象，通常在富函数的 open() 方法中。在这里你还可以定义名称。1getRuntimeContext().addAccumulator(\"num-lines\", this.numLines);你现在可以在算子函数中的任何位置使用累加器，包括 open() 和 close() 方法。1this.numLines.add(1);总体结果将存储在 JobExecutionResult 对象中，该对象是从执行环境的 execute() 方法返回的 （目前这仅在执行等待作业完成时才有效）。1myJobExecutionResult.getAccumulatorResult(&quot;num-lines&quot;)每个作业的所有累加器共享一个命名空间。 这样你就可以在作业的不同算子函数中使用相同的累加器。Flink 会在内部合并所有同名累加器。关于累加器和迭代请注意： 目前，累加器的结果只有在整个作业结束以后才可用。我们还计划实现在下一次迭代中使前一次迭代的结果可用。你可以使用 Aggregators 计算每次迭代的统计信息，并根据这些信息确定迭代何时终止。自定义累加器：要实现你自己的累加器，只需编写累加器接口的实现即可。如果你认为 Flink 应该提供你的自定义累加器，请创建 pull request。你可以选择实现 Accumulator 或者 SimpleAccumulator。Accumulator&lt;V,R&gt; 最灵活：它为要递增的值定义类型 V，为最终结果定义类型 R。例如对于 histogram，V 是数字而 R 是 histogram。SimpleAccumulator 则适用于两个类型相同的情况，例如计数器。Scala API扩展为了在Scala和Java API之间保持相当程度的一致性，用于批处理和流传输的标准API省略了一些允许在Scala中进行高水平表达的功能。如果您想_享受完整的Scala体验_，则可以选择加入通过隐式转换来增强Scala API的扩展。要使用所有可用的扩展，您只需import为DataSet API 添加一个简单的1import org.apache.flink.api.scala.extensions._或DataStream API1import org.apache.flink.streaming.api.scala.extensions._另外，您也可以导入单个扩展_一个点菜_只使用那些你喜欢。通常，DataSet和DataStream API都不接受匿名模式匹配函数来解构元组，案例类或集合，如下所示：123456val data: DataSet[(Int, String, Double)] = // [...]data.map &#123; case (id, name, temperature) =&gt; // [...] // The previous line causes the following compilation error: // \"The argument types of an anonymous function must be fully known. (SLS 8.5)\"&#125;此扩展在DataSet和DataStream Scala API中引入了新方法，这些新方法在扩展API中具有一对一的对应关系。这些委托方法确实支持匿名模式匹配功能。DataSet APIMethodOriginalExamplemapWithmap (DataSet)```data.mapWith {case (_, value) =&gt; value.toString}12 || **mapPartitionWith** | **mapPartition (DataSet)** |data.mapPartitionWith {case head #:: _ =&gt; head}12 || **flatMapWith** | **flatMap (DataSet)** |data.flatMapWith {case (_, name, visitTimes) =&gt; visitTimes.map(name -&gt; _)}12 || **filterWith** | **filter (DataSet)** |data.filterWith {case Train(_, isOnTime) =&gt; isOnTime}12 || **reduceWith** | **reduce (DataSet, GroupedDataSet)** |data.reduceWith {case ((, amount1), (, amount2)) =&gt; amount1 + amount2}12 || **reduceGroupWith** | **reduceGroup (GroupedDataSet)** |data.reduceGroupWith {case id #:: value #:: _ =&gt; id -&gt; value}12 || **groupingBy** | **groupBy (DataSet)** |data.groupingBy {case (id, _, _) =&gt; id}12 || **sortGroupWith** | **sortGroup (GroupedDataSet)** |grouped.sortGroupWith(Order.ASCENDING) {case House(_, value) =&gt; value}12 || **combineGroupWith** | **combineGroup (GroupedDataSet)** |grouped.combineGroupWith {case header #:: amounts =&gt; amounts.sum}12 || **projecting** | **apply (JoinDataSet, CrossDataSet)** |data1.join(data2).whereClause(case (pk, _) =&gt; pk).isEqualTo(case (_, fk) =&gt; fk).projecting {case ((pk, tx), (products, fk)) =&gt; tx -&gt; products}data1.cross(data2).projecting {case ((a, ), (, b) =&gt; a -&gt; b}12 || **projecting** | **apply (CoGroupDataSet)** |data1.coGroup(data2).whereClause(case (pk, _) =&gt; pk).isEqualTo(case (_, fk) =&gt; fk).projecting {case (head1 #:: _, head2 #:: _) =&gt; head1 -&gt; head2}}123456789 |&lt;a name&#x3D;&quot;datastream-api&quot;&gt;&lt;&#x2F;a&gt;#### &lt;a name&#x3D;&quot;n1Myp&quot;&gt;&lt;&#x2F;a&gt;#### DataStream API| Method | Original | Example || :--- | :--- | :---: || **mapWith** | **map (DataStream)** |data.mapWith {case (_, value) =&gt; value.toString}12 || **flatMapWith** | **flatMap (DataStream)** |data.flatMapWith {case (_, name, visits) =&gt; visits.map(name -&gt; _)}12 || **filterWith** | **filter (DataStream)** |data.filterWith {case Train(_, isOnTime) =&gt; isOnTime}12 || **keyingBy** | **keyBy (DataStream)** |data.keyingBy {case (id, _, _) =&gt; id}12 || **mapWith** | **map (ConnectedDataStream)** |data.mapWith(map1 = case (_, value) =&gt; value.toString,map2 = case (_, _, value, _) =&gt; value + 1)12 || **flatMapWith** | **flatMap (ConnectedDataStream)** |data.flatMapWith(flatMap1 = case (_, json) =&gt; parse(json),flatMap2 = case (_, _, json, _) =&gt; parse(json))12 || **keyingBy** | **keyBy (ConnectedDataStream)** |data.keyingBy(key1 = case (_, timestamp) =&gt; timestamp,key2 = case (id, _, _) =&gt; id)12 || **reduceWith** | **reduce (KeyedStream, WindowedStream)** |data.reduceWith {case ((, sum1), (, sum2) =&gt; sum1 + sum2}12 || **foldWith** | **fold (KeyedStream, WindowedStream)** |data.foldWith(User(bought = 0)) {case (User(b), (_, items)) =&gt; User(b + items.size)}12 || **applyWith** | **apply (WindowedStream)** |data.applyWith(0)(foldFunction = case (sum, amount) =&gt; sum + amountwindowFunction = case (k, w, sum) =&gt; // […])12 || **projecting** | **apply (JoinedStream)** |data1.join(data2).whereClause(case (pk, _) =&gt; pk).isEqualTo(case (_, fk) =&gt; fk).projecting {case ((pk, tx), (products, fk)) =&gt; tx -&gt; products}1234567 |&lt;br &#x2F;&gt;有关每种方法的语义的更多信息，请参考 [DataSet](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;batch&#x2F;index.html)和[DataStream](https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;dev&#x2F;datastream_api.html) API文档。&lt;br &#x2F;&gt;&lt;br &#x2F;&gt;要专门使用此扩展，可以添加以下内容&#96;import&#96;：&#96;&#96;&#96;scalaimport org.apache.flink.api.scala.extensions.acceptPartialFunctions用于数据集扩展和1import org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions以下代码片段显示了如何一起使用这些扩展方法（与DataSet API一起使用）的最小示例：12345678910111213141516171819object Main &#123; import org.apache.flink.api.scala.extensions._ case class Point(x: Double, y: Double) def main(args: Array[String]): Unit = &#123; val env = ExecutionEnvironment.getExecutionEnvironment val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6)) ds.filterWith &#123; case Point(x, _) =&gt; x &gt; 1 &#125;.reduceWith &#123; case (Point(x1, y1), (Point(x2, y2))) =&gt; Point(x1 + y1, x2 + y2) &#125;.mapWith &#123; case Point(x, y) =&gt; (x, y) &#125;.flatMapWith &#123; case (x, y) =&gt; Seq(\"x\" -&gt; x, \"y\" -&gt; y) &#125;.groupingBy &#123; case (id, value) =&gt; id &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Flink集群架构","slug":"Flink集群架构","date":"2019-06-05T09:45:00.000Z","updated":"2020-09-05T16:43:40.508Z","comments":true,"path":"2019/06/05/Flink集群架构/","link":"","permalink":"cpeixin.cn/2019/06/05/Flink%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84/","excerpt":"","text":"ClientClient虽然不是运行时（runtime）和作业执行时的一部分，但它是被用作准备和提交 dataflow 到 JobManager 的。提交完成之后，客户端可以断开连接，也可以保持连接来接收进度报告。客户端既可以作为触发执行的 Java / Scala 程序的一部分，也可以在命令行进程中运行./bin/flink runJobManagersJobManagers （也称为 masters_）协调分布式计算。它们负责调度任务、协调 checkpoints、协调故障恢复等。每个 Job 至少会有一个 JobManager。高可用部署下会有多个 JobManagers，其中一个作为 _leader_，其余处于 _standby 状态。TaskManagersTaskManagers（也称为 _workers_）执行 dataflow 中的 _tasks_（准确来说是 subtasks ），并且缓存和交换数据 _streams_。每个 Job 至少会有一个 TaskManager。JobGraphJobGraph是在用户提交Jar后，在Client端内部生成的，注意下图streamGraph -&gt; JobGraphTask Slots每个 worker（TaskManager）都是一个 JVM 进程_，并且可以在不同的线程中执行一个或多个 subtasks。为了控制 worker 接收 task 的数量，worker 拥有所谓的 task slots （至少一个）。每个 _task slots 代表 TaskManager 的一份固定资源子集。例如，具有三个 slots 的 TaskManager 会将其管理的内存资源分成三等份给每个 slot。 划分资源意味着 subtask 之间不会竞争资源，但是也意味着它们只拥有固定的资源。注意这里并没有 CPU 隔离，当前 slots 之间只是划分任务的内存资源。通过调整 slot 的数量，用户可以决定 subtasks 的隔离方式。每个 TaskManager 有一个 slot 意味着每组 task 在一个单独的 JVM 中运行（例如，在一个单独的容器中启动）。拥有多个 slots 意味着多个 subtasks 共享同一个 JVM。 Tasks 在同一个 JVM 中共享 TCP 连接（通过多路复用技术）和心跳信息（heartbeat messages）。它们还可能共享数据集和数据结构，从而降低每个 task 的开销。默认情况下，Flink 允许 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，只要它们来自同一个 job。因此，一个 slot 可能会负责这个 job 的整个管道（pipeline）。允许 slot sharing 有两个好处：Flink 集群需要与 job 中使用的最高并行度一样多的 slots。这样不需要计算作业总共包含多少个 tasks（具有不同并行度）。更好的资源利用率。在没有 slot sharing 的情况下，简单的 subtasks（_source/map()_）将会占用和复杂的 subtasks （_window_）一样多的资源。通过 slot sharing，将示例中的并行度从 2 增加到 6 可以充分利用 slot 的资源，同时确保繁重的 subtask 在 TaskManagers 之间公平地获取资源。APIs 还包含了 resource group 机制，它可以用来防止不必要的 slot sharing。根据经验，合理的 slots 数量应该和 CPU 核数相同。在使用超线程（hyper-threading）时，每个 slot 将会占用 2 个或更多的硬件线程上下文（hardware thread contexts）。State Backendskey/values 索引存储的数据结构取决于 state backend 的选择。一类 state backend 将数据存储在内存的哈希映射中另一类 state backend 使用 RocksDB 作为键/值存储。除了定义保存状态（state）的数据结构之外， state backend 还实现了获取键/值状态的时间点快照的逻辑，并将该快照存储为 checkpoint 的一部分。Savepoints用 Data Stream API 编写的程序可以从 savepoint 继续执行。Savepoints 允许在不丢失任何状态的情况下升级程序和 Flink 集群。Savepoints 是手动触发的 checkpoints，它依靠常规的 checkpoint 机制获取程序的快照并将其写入 state backend。在执行期间，程序会定期在 worker 节点上创建快照并生成 checkpoints。对于恢复，Flink 仅需要最后完成的 checkpoint，而一旦完成了新的 checkpoint，旧的就可以被丢弃。Savepoints 类似于这些定期的 checkpoints，除了它们是由用户触发并且在新的 checkpoint 完成时不会自动过期。你可以通过命令行 或在取消一个 job 时通过 REST API 来创建 Savepoints。最后这张图是比较完整的集群架构图","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"Spark 进行全方位性能调优","slug":"Spark-进行全方位性能调优","date":"2019-06-05T07:16:40.000Z","updated":"2020-09-05T07:25:26.586Z","comments":true,"path":"2019/06/05/Spark-进行全方位性能调优/","link":"","permalink":"cpeixin.cn/2019/06/05/Spark-%E8%BF%9B%E8%A1%8C%E5%85%A8%E6%96%B9%E4%BD%8D%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","excerpt":"","text":"让 Spark 作业在海量数据面前稳定且快速地运行，这就需要对 Spark 进行性能调优。调优 Spark 是一个持续的过程，随着你对 Spark、数据本身、业务场景愈发了解，调优的思路也会更加多样，这是一个持续累积的过程。能够有针对性地对 Spark 作业进行调优是一名有经验的大数据工程师的必备技能。本课时将会从硬件、资源管理平台与使用方式 3 个维度介绍如何对 Spark 进行性能调优。在介绍调优之前，我们先来看看如何查看 Spark 的作业日志。日志收集如果作业执行报错或者速度异常，通常需要查看 Spark 作业日志，Spark 日志通常是排错的唯一根据，更是作业调优的好帮手。查看日志的时候，需要注意的是 Spark 作业是一个分布式执行的过程，所以日志也是分布式的，联想到 Spark 的架构，Spark 的日志也分为两个级别：DriverExecutor一般来说，小错误通常可以从 Driver 日志中定位，但是复杂一点的问题，还是要从 Executor 的执行情况来判断。如果我们选取 yarn-client 的模式执行，日志会输出到客户端，我们直接查看即可，非常方便。我们可以用下面这种方式收集，如下：复制1234567nohup .&#x2F;bin&#x2F;spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\--executor-memory 20G \\--num-executors 50 \\&#x2F;path&#x2F;to&#x2F;examples.jar \\1000 &gt;&gt; o &amp;其中 nohup 和 &amp; 表示后台执行，&gt;&gt; o 表示将日志输出到文件 o 中。查看 Executor 的日志需要先将散落在各个节点（Container）的日志收集汇总成一个文件。以 YARN 平台为例：123456789101112yarn logs -applicationId application_1552880376963_0002 &gt;&gt; oapplication_1552880376963_0002 是 Spark 作业 id，当汇聚为一个文件后，我们就可以对其进行查看了。打开文件，我们发现这份日志是这样组织的：container_0-----------------------------------------------------WARN.....ERROR...........（日志内容） container_1-----------------------------------------------------......（日志内容）......这也非常好理解，本来就是从各个 Container 中收集并拼接的，但是这种方式给我们定位造成了一定障碍。阅读这样的日志，最重要的是找到最开始报错的那一句日志，因为一旦作业报错，几乎会造成所有 Container 报错，但大部分错误日志都对定位原因没有什么帮助。所以拿到这份日志要做的第一件事是利用时间戳和 ERROR 标记定位最初的错误日志。这种方式通常可以直接解决一半以上的报错问题。**硬件配置与资源管理平台构建 Spark 集群的硬件只需普通的商用 PC Server 即可，由于 Spark 作业对内存需求巨大，建议配置高性能 CPU、大内存的服务器，以下是建议配置：内存：256GCPU：Intel E5-2640v4硬盘：3T * 8该 CPU 是双路 6 核心，且具有超线程技术，所以一个 CPU 相当于有 2 * 6 * 2 = 24 核心。对于交换机的选择，通常，如果在生产环境使用，那么无论集群规模大小，都应该直接考虑万兆交换机，对于上千的集群，还需要多台交换机进行堆叠才能满足需求。Spark 基于资源管理平台运行，该平台对于 Spark 来说就像一个资源池一样，资源池的大小取决于每个物理节点有多少资源供资源管理平台调度。一般来说，每台节点应预留 20% 的资源保证操作系统与其他服务稳定运行，对于前面提到的机器配置，加入资源池的内存为 200G，CPU 为 20 核。假设使用 YARN 作为资源管理平台，相关配置如下：复制12yarn.nodemanager.resource.memory-mb &#x3D; 200Gyarn.nodemanager.resource.cpu-vcores &#x3D; 20假设 YARN 集群中有 10 个 NodeManager 节点，那么总共的资源池大小为 2000G、200 核。在 Spark 作业运行时，用户可以通过集群监控页面来查看集群 CPU 使用率，如果发现 CPU 使用率一直维持在偏低的水平，可以尝试将 yarn.nodemanager.resource.cpu-vcores 改大。内存与 CPU 资源设置应该维持一个固定的比例，如 1:5，这样在提交作业时，也按照这个比例来申请资源，可以提高集群整体资源利用率。一般来说，YARN 集群中会运行各种各样的作业，这样资源利用率会比较高，但是也经常造成 Spark 作业在需要时申请不到资源，这时可以采取 YARN 的新特性：基于标签的调度，在某些节点上打上相应的标签，来实现部分资源的隔离。这部分内容对于工程师与分析师来说，一般接触不到，属于大数据运维工程师职责的范畴，但是对于调优来说非常重要，有必要了解。参数调优与应用调优本课时主要从使用层面来介绍调优，其中会涉及参数调优、应用调优甚至代码调优。1. 提高作业并行度在作业并行程度不高的情况下，最有效的方式就是提高作业并行程度。在 Spark 作业划分中，一个 Executor 只能同时执行一个 Task ，一个计算任务的输入是一个分区（partition），所以改变并行程度只有一个办法，就是提高同时运行 Executor 的个数。通常集群的资源总量是一定的，这样 Executor 数量增加，必然会导致单个 Executor 所分得的资源减少，这样的话，在每个分区不变的情况下，有可能会引起性能方面的问题，所以，我们可以增大分区数来降低每个分区的大小，从而避免这个问题。RDD 一开始的分区数与该份数据在 HDFS 上的数据块数量一致，后面我们可以通过 coalesce 与 repartition 算子进行重分区，这其实改变的是 Map 端的分区数，如果想改变 Reduce 端的分区数，有两个办法，一个是修改配置 spark.default.parallelism，该配置设定所有 Reduce 端的分区数，会对所有 Shuffle 过程生效，此外还可以直接在算子中将分区数作为参数传入，绝大多数算子都有分区数参数的重载版本，如 groupByKey(600) 等。在 Shuffle 过程中，Shuffle 相关的算子会构建一个哈希表，Reduce 任务有时会因为这个表过大而造成内存溢出，这时就可以试着增大并行程度。2. 提高 Shuffle 性能Shuffle 是 Spark 作业中关键的一环，也是性能调优的重点，先来看看 Spark 参数中与 Shuffle 性能有关的有哪些：123spark.shuffle.file.bufferspark.reducer.maxSizeInFlightspark.shuffle.compress第 1 个配置是 Map 端输出的中间结果的缓冲区大小，默认 32K第 2 个配置是 Map 端输出的中间结果的文件大小，默认为 48M，该文件还会与其他文件进行合并。第 3 个配置是 Map 端输出是否开启压缩，默认开启。缓冲区当然越大，写入性能越高，所以有条件可以增大缓冲区大小，可以提升 Shuffle Write 的性能，但该参数实际消耗的内存为 C * spark.shuffle.file.buffer，其中 C 为执行该任务的核数。在 Shuffle Read 的过程中，Reduce Task 所在的 Executor 会按照 spark.reducer.maxSizeInFlight 的设置大小去拉取文件，这需要创建内存缓冲区来接收，在内存足够大的情况下，可以考虑提高 spark.reducer.maxSizeInFlight 的值来提升 Shuffle Read 的效率。spark.shuffle.compress 配置项默认为 true，表示会对 Map 端输出进行压缩。Spark Shuffle 会将中间结果写入到 spark.local.dir 配置的目录下，可以将该目录配置多路磁盘目录，以提升写入性能。3. 内存管理Spark 作业中内存主要有两个用途：计算和存储。计算是指在 Shuffle，连接，排序和聚合等操作中用于执行计算任务的内存，而存储指的是用于跨集群缓存和传播数据的内存。在 Spark 中，这两块共享一个统一的内存区域（M），如下图所示：用计算内存时，存储部分可以获取所有可用内存，反之亦然。如有必要，计算内存也可以将数据从存储区移出，但会在总存储内存使用量下降到特定阈值（R）时才执行。换句话说，R 决定了 M 内的一个分区，在这个分区中，数据不会被移出。由于实际情况的复杂性，存储区一般不会去占用计算区。这样设计是为了对那些不使用缓存的作业可以尽可能地使用全部内存；而需要使用缓存的作业也会有一个区域始终用来缓存数据，这样用户就可以不需要知道其背后复杂原理，自己根据实际内存需求来调节 M 与 R 的值，以达到最好效果。下面是决定 M 与 R 的两个配置：spark.memory.fraction，该配置表示 M 占 JVM 堆空间的比例，默认为 0.6，剩下 0.4 用于存储用户数据结构、Spark 中的内部元数据并防止在应对稀疏数据和异常大的数据时出现 OutOfMemory 的错误；spark.memory.storageFraction，该配置表示 R 占 M 的比例，默认为 0.5，这部分缓存的数据不会被移出。上面两个默认值基本满足绝大多数作业的使用，在特殊情况可以考虑设置 spark.memory.fraction 的值以适配 JVM 老年代的空间大小，默认 JVM 老年代在不经过设置的情况下占 JVM 的 2/3，所以这个值是合理的。Spark Executor 除了堆内存以外，还有非堆内存空间，这部分通过参数spark.yarn.executor.memoryoverhead 进行配置，最小为 384MB，默认为 Executor 内存的 10%。所以整个Executor JVM 消耗的内存为:1spark.yarn.executor.memoryoverhead + spark.executor.memory其中：12M &#x3D; spark.executor.memory * spark.memory.fractionR &#x3D; M * spark.memory.storageFraction此外，Spark 还有可能会用到堆外内存 O：1O &#x3D; spark.memory.offHeap.size所以整个 Spark 的内存管理布局如下图所示：用户需要知道每个部分的大小应如何调节，这样才能针对场景进行调优。这其实是 Spark 实现的一种比较简化且粗粒度的内存调节方案。如果用户想要更精细地调整内存的每个区域，就需要在参数中 spark.executor.extraClassPath 配置 Java 选项了，这种方式只针对富有经验的工程师，对于普通用户来说不太友好。4. 序列化序列化是以时间换空间的一种内存取舍方式，其根本原因还是内存比较吃紧，我们可以优先选择对象数组或者基本类型而不是那些集合类型来实现自己的数据结构，fastutil 包提供了与 Java 标准兼容的集合类型。除此之外，还应该避免使用大量小对象与指针嵌套的结构。我们可以考虑使用数据 ID 或者枚举对象来代替字符串键。如果内存小于 32GB，可以设置 Java 选项 -XX:+UseCompressedOops 来压缩指针为 4 字节，以上是需要用到序列化之前可以做的调优工作，以节省内存。对于大对象来说，可以使用 RDD 的 persist 算子并选取 MEMORY_ONLY_SER 级别进行存储，更好的方式则是以序列化的方式进行存储。这相当于用时间换空间，因为反序列化时会造成访问时间过慢，如果想用序列化的方式存储数据，推荐使用 Kyro 格式，它比原生的 Java 序列化框架性能优秀（官方介绍，性能提升 10 倍）。Spark 2.0 已经开始用 Kyro 序列化 shuffle 中传输的字符串等基础类型数据了。要想使用 Kyro 序列化库，要将需要序列化的类在 Kyro 中注册方可使用。使用步骤如下。编写一个注册器，实现 KyroRegister 接口，在 Kyro 中注册你需要使用的类：123456public static class YourKryoRegistrator implements KryoRegistrator &#123; public void registerClasses(Kryo kryo) &#123; &#x2F;&#x2F;&#x2F;&#x2F;在Kryo序列化库中注册自定义的类 kryo.register(YourClass.class, new FieldSerializer(kryo, YourClass.class)); &#125;&#125;设置序列化工具并配置注册器：123……spark.config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)spark.config(&quot;spark.kryo.registrator&quot;, YourKryoRegistrator.class.getName())5. JVM垃圾回收（GC）调优通常来说，那种只读取 RDD 一次，然后对其进行各种操作的作业不太会引起 GC 问题。当 Java 需要将老对象释放，为新对象腾出空间时，需要追踪所有 Java 对象，然后在其中找出没有使用的那些对象。GC 的成本与 Java 对象数量成正比，所以使用较少对象的数据结构会大大减轻 GC 压力，如直接使用整型数组，而不选用链表。通常在出现 GC 问题的时候，序列化缓存是首先应该尝试的方法。**由于执行计算任务需要的内存和缓存 RDD 的内存互相干扰，GC 也可能成为问题。这可以控制分配给 RDD 缓存空间来缓解这个问题。GC 调优的第 1 步是搞清楚 GC 的频率和花费的时间，这可以通过添加 Java 选项来完成：1-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps在 Spark 运行时，一旦发生 GC，就会被记录到日志里。为了进一步调优 JVM，先来看看 JVM 如何管理内存。Java 的堆空间被划分为 2 个区域：年轻代、老年代，顾名思义，年轻代会保存一些短生命周期对象，而老年代会保存长生命周期对象。年轻代又被划分为 3 个区域：一个 Eden 区，两个 Supervisor 区，如下图所示。简单来说，GC 过程是这样的：当 Eden 区被填满后，会触发 minor GC，然后 Eden 区和 Supvisor1 区还存活的对象被复制到 Supervisor2 区，如果某个对象太老或者 Supervisor2 区已满，则会将对象复制到老年代中，当老年代快满了，则会触发 full GC。在 Spark 中，GC 调优的目的是确保只有长生命周期的对象才会保存到老年代中，年轻代有充足的空间来存储短生命周期对象。这会有助于避免执行 full GC 来回收任务执行期间生成的临时对象，有以下几个办法：通过收集到的 GC 统计信息来检查是否有过多 GC，如果任务在完成之前多次触发 full GC，则意味着没有足够的内存可用于执行任务；如果 minor GC 次数过多，但并没有 major GC，可以为 Eden 区分配更多的内存来缓解，可以将 Eden 区大小预估为每个任务需要的内存空间，如果 Eden 区的大小为 E，则可以使用选项 -Xmn = 4 / 3 * E 来设置年轻代大小，增加的 1/3 为 Supervisor 区；如果通过收集到的 GC 统计信息，发现老年代快满了，可以通过 spark.memory.fraction 来减少用于缓存的内存空间；少缓存一点总比执行缓慢好，也可以考虑减少年轻代的大小，这可以通过设置 -Xmn 来实现，也可以设置 JVM 的 NewRatio 参数，该参数表示老年代与年轻代之间的比值，许多 JVM 默认为 2（2:1），这意味着老年代占堆空间的 2/3。该比例应该要大于 spark.memory.fraction 所设置的比例；在某些 GC 是瓶颈的情况下，可以通过 -XX:+UseG1GC 开启 G1GC，可以提高 GC 性能，对较大的堆，可能需要增加 G1 区大小，使用 -XX:G1HeapRegionSize=n 来进行设置；如果任务从 HDFS 读取数据，则可以以此来估计任务使用的内存量，解压缩块大小通常是 HDFS 数据块大小（假设为 256MB）的 23 倍，如果希望有足够 34 个任务内存空间，则 Eden 区大小为 4 * 3 * 256MB每当我们对 Java 选项做出调整后，要通过监控工具来查看 GC 花费的时间和频率是否有变化。可以通过在作业中设置 spark.executor.extraJavaOptions选项来指定执行程序的 GC 选项以及 JVM 内存各个区域的精确大小，但不能设置 JVM 堆大小，该项只能通过 –executor-memory 或者 spark.executor.memory 来进行设置。6. 将经常被使用的数据进行缓存如果某份数据经常会被使用，可以尝试用 cache 算子将其缓存，有时效果极好。7. 使用广播变量避免 Hash 连接操作在进行连接操作时，可以尝试将小表通过广播变量进行广播，从而避免 Shuffle，这种方式也被称为 Map 端连接。8. 聚合 filter 算子产生的大量小分区数据在使用 filter 算子后，通常数据会被打碎成很多个小分区，这会影响后面的执行操作，可以先对后面的数据用 coalesce 算子进行一次合并。9. 根据场景选用高性能算子很多算子都能达到相同的效果，但是性能差异却比较大，例如在聚合操作时，选择 reduceByKey 无疑比 groupByKey 更好；在 map 函数初始化性能消耗太大或者单条记录很大时，mapPartition 算子比 map 算子表现更好；在去重时，distinct 算子比 groupBy 算子表现更好。10. 数据倾斜数据倾斜是数据处理作业中的一个非常常见也是非常难以处理的一个问题。 正常情况下，数据通常都会出现数据倾斜的问题，只是情况轻重有别而已。数据倾斜的症状是大量数据集中到一个或者几个任务里，导致这几个任务会严重拖慢整个作业的执行速度，严重时甚至会导致整个作业执行失败。如下图所示：可以看到 Task A 处理了绝大多数数据，其他任务执行完成后，需要等待此任务执行完成，作业才算完成。对于这种情况，可以采取以下几种办法处理：过滤掉脏数据很多情况下，数据倾斜通常是由脏数据引起的，这个时候需要将脏数据过滤。提高作业的并行度这种方式从根本上仍然不能消除数据倾斜，只是尽可能地将数据分散到多个任务中去，这种方案只能提升作业的执行速度，但是不能解决数据倾斜的问题。广播变量可以将小表进行广播，避免了 Shuffle 的过程，这样就使计算相对均匀地分布在每个 Map 任务中，但是对于数据倾斜严重的情况，还是会出现作业执行缓慢的情况。将不均匀的数据进行单独处理在连接操作的时候，可以先从大表中将集中分布的连接键找出来，与小表单独处理，再与剩余数据连接的结果做合并。处理方法为：如果大表的数据存在数据倾斜，而小表不存在这种情况，可以将大表中存在倾斜的数据提取出来，并将小表中对应的数据提取出来，这时可以将小表中的数据扩充 n 倍，而大表中的每条数据则打上一个 n 以内的随机数作为新键，而小表中的数据则根据扩容批次作为新键，如下图所示：这种方式可以将倾斜的数据打散，从而避免数据倾斜。对于那种分组统计的任务，可以通过两阶段聚合的方案来解决，首先将数据打上一个随机的键值，并根据键的哈希值进行分发，将数据均匀的分散到多个任务中去，然后在每个任务中按照真实的键值做局部聚合，最后再按照真实的键值分发一次，得到最后的结果，如下图所示，这样，最后一次分发的数据已经是聚合过后的数据，就不会出现数据倾斜的情况。这种方法虽然能够解决数据倾斜的问题但只适合聚合计算的场景。小结对 Spark 作业进行调优。调优之前，看作业日志是基本功，这个没有什么捷径，只能多看。调优这个话题是一个很个性化的问题。对于离线计算任务，时间当然很重要，但不一定是最重要的。通常来说，对于实时处理，时间通常是唯一优化目标，如果执行时间有优化的空间，当然会不遗余力地进行优化。但是，对于离线计算任务，如 Spark 作业，作业执行时间并没有那么重要。通常，这类作业都是在夜深人静的晚上执行，1 小时与 90 分钟真的差异就那么大吗，不一定，所以对于离线计算作业来说，作业执行时间并不是最重要的，开发效率同样重要。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Flink 是什么？","slug":"Flink-是什么？","date":"2019-06-03T09:21:58.000Z","updated":"2020-09-06T07:04:15.048Z","comments":true,"path":"2019/06/03/Flink-是什么？/","link":"","permalink":"cpeixin.cn/2019/06/03/Flink-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"","text":"架构Apache Flink 是一个框架和分布式处理引擎，用于在_无边界和有边界_数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。接下来，我们来介绍一下 Flink 架构中的重要方面。处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。有界流 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。部署应用到任意地方Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。Flink 集成了所有常见的集群资源管理器，例如 Hadoop YARN、 Apache Mesos 和 Kubernetes，但同时也可以作为独立集群运行。Flink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。部署 Flink 应用程序时，Flink 会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。运行任意规模应用Flink 旨在任意规模上运行有状态流式应用。因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO。而且 Flink 很容易维护非常大的应用程序状态。其异步和增量的检查点算法对处理延迟产生最小的影响，同时保证精确一次状态的一致性。Flink 用户报告了其生产环境中一些令人印象深刻的扩展性数字处理每天处理数万亿的事件,应用维护几TB大小的状态, 和应用在数千个内核上运行。利用内存性能有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。应用Apache Flink 是一个针对无界和有界数据流进行有状态计算的框架。Flink 自底向上在不同的抽象级别提供了多种 API，并且针对常见的使用场景开发了专用的扩展库。流处理应用的基本组件可以由流处理框架构建和执行的应用程序类型是由框架对 流、状态、时间 的支持程度来决定的。在下文中，我们将对上述这些流处理应用的基本组件逐一进行描述，并对 Flink 处理它们的方法进行细致剖析。**流显而易见，（数据）流是流处理的基本要素。然而，流也拥有着多种特征。这些特征决定了流如何以及何时被处理。Flink 是一个能够处理任何类型数据流的强大处理框架。有界 和 无界 的数据流：流可以是无界的；也可以是有界的，例如固定大小的数据集。Flink 在无界的数据流处理上拥有诸多功能强大的特性，同时也针对有界的数据流开发了专用的高效算子。实时 和 历史记录 的数据流：所有的数据都是以流的方式产生，但用户通常会使用两种截然不同的方法处理数据。或是在数据生成时进行实时的处理；亦或是先将数据流持久化到存储系统中——例如文件系统或对象存储，然后再进行批处理。Flink 的应用能够同时支持处理实时以及历史记录数据流。状态只有在每一个单独的事件上进行转换操作的应用才不需要状态，换言之，每一个具有一定复杂度的流处理应用都是有状态的。任何运行基本业务逻辑的流处理应用都需要在一定时间内存储所接收的事件或中间结果，以供后续的某个时间点（例如收到下一个事件或者经过一段特定时间）进行访问并进行后续处理。应用状态是 Flink 中的一等公民，Flink 提供了许多状态管理相关的特性支持，其中包括：多种状态基础类型：Flink 为多种不同的数据结构提供了相对应的状态基础类型，例如原子值（value），列表（list）以及映射（map）。开发者可以基于处理函数对状态的访问方式，选择最高效、最适合的状态基础类型。插件化的State Backend：State Backend 负责管理应用程序状态，并在需要的时候进行 checkpoint。Flink 支持多种 state backend，可以将状态存在内存或者 RocksDB。RocksDB 是一种高效的嵌入式、持久化键值存储引擎。Flink 也支持插件式的自定义 state backend 进行状态存储。精确一次语义：Flink 的 checkpoint 和故障恢复算法保证了故障发生后应用状态的一致性。因此，Flink 能够在应用程序发生故障时，对应用程序透明，不造成正确性的影响。超大数据量状态：Flink 能够利用其异步以及增量式的 checkpoint 算法，存储数 TB 级别的应用状态。可弹性伸缩的应用：Flink 能够通过在更多或更少的工作节点上对状态进行重新分布，支持有状态应用的分布式的横向伸缩。时间时间是流处理应用另一个重要的组成部分。因为事件总是在特定时间点发生，所以大多数的事件流都拥有事件本身所固有的时间语义。进一步而言，许多常见的流计算都基于时间语义，例如窗口聚合、会话计算、模式检测和基于时间的 join。流处理的一个重要方面是应用程序如何衡量时间，即区分事件时间（event-time）和处理时间（processing-time）。Flink 提供了丰富的时间语义支持。事件时间模式：使用事件时间语义的流处理应用根据事件本身自带的时间戳进行结果的计算。因此，无论处理的是历史记录的事件还是实时的事件，事件时间模式的处理总能保证结果的准确性和一致性。Watermark 支持：Flink 引入了 watermark 的概念，用以衡量事件时间进展。Watermark 也是一种平衡处理延时和完整性的灵活机制。迟到数据处理：当以带有 watermark 的事件时间模式处理数据流时，在计算完成之后仍会有相关数据到达。这样的事件被称为迟到事件。Flink 提供了多种处理迟到数据的选项，例如将这些数据重定向到旁路输出（side output）或者更新之前完成计算的结果。处理时间模式：除了事件时间模式，Flink 还支持处理时间语义。处理时间模式根据处理引擎的机器时钟触发计算，一般适用于有着严格的低延迟需求，并且能够容忍近似结果的流处理应用。分层 APIFlink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。下文中，我们将简要描述每一种 API 及其应用，并提供相关的代码示例。ProcessFunctionProcessFunction 是 Flink 所提供的最具表达力的接口。ProcessFunction 可以处理一或两条输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件驱动应用所需要的基于单个事件的复杂业务逻辑。下面的代码示例展示了如何在 KeyedStream 上利用 KeyedProcessFunction 对标记为 START 和 END 的事件进行处理。当收到 START 事件时，处理函数会记录其时间戳，并且注册一个时长4小时的计时器。如果在计时器结束之前收到 END 事件，处理函数会计算其与上一个 START 事件的时间间隔，清空状态并将计算结果返回。否则，计时器结束，并清空状态。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 将相邻的 keyed START 和 END 事件相匹配并计算两者的时间间隔 * 输入数据为 Tuple2&lt;String, String&gt; 类型，第一个字段为 key 值， * 第二个字段标记 START 和 END 事件。 */public static class StartEndDuration extends KeyedProcessFunction&lt;String, Tuple2&lt;String, String&gt;, Tuple2&lt;String, Long&gt;&gt; &#123; private ValueState&lt;Long&gt; startTime; @Override public void open(Configuration conf) &#123; // obtain state handle startTime = getRuntimeContext() .getState(new ValueStateDescriptor&lt;Long&gt;(\"startTime\", Long.class)); &#125; /** Called for each processed event. */ @Override public void processElement( Tuple2&lt;String, String&gt; in, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; switch (in.f1) &#123; case \"START\": // set the start time if we receive a start event. startTime.update(ctx.timestamp()); // register a timer in four hours from the start event. ctx.timerService() .registerEventTimeTimer(ctx.timestamp() + 4 * 60 * 60 * 1000); break; case \"END\": // emit the duration between start and end event Long sTime = startTime.value(); if (sTime != null) &#123; out.collect(Tuple2.of(in.f0, ctx.timestamp() - sTime)); // clear the state startTime.clear(); &#125; default: // do nothing &#125; &#125; /** Called when a timer fires. */ @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) &#123; // Timeout interval exceeded. Cleaning up the state. startTime.clear(); &#125;&#125;这个例子充分展现了 KeyedProcessFunction 强大的表达力，也因此是一个实现相当复杂的接口。DataStream APIDataStream API 为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如map()、reduce()、aggregate() 等函数。你可以通过扩展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。下面的代码示例展示了如何捕获会话时间范围内所有的点击流事件，并对每一次会话的点击量进行计数。123456789101112131415161718// 网站点击 Click 的数据流DataStream&lt;Click&gt; clicks = ...DataStream&lt;Tuple2&lt;String, Long&gt;&gt; result = clicks // 将网站点击映射为 (userId, 1) 以便计数 .map( // 实现 MapFunction 接口定义函数 new MapFunction&lt;Click, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; map(Click click) &#123; return Tuple2.of(click.userId, 1L); &#125; &#125;) // 以 userId (field 0) 作为 key .keyBy(0) // 定义 30 分钟超时的会话窗口 .window(EventTimeSessionWindows.withGap(Time.minutes(30L))) // 对每个会话窗口的点击进行计数，使用 lambda 表达式定义 reduce 函数 .reduce((a, b) -&gt; Tuple2.of(a.f0, a.f1 + b.f1));SQL &amp; Table APIFlink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API 和 SQL 借助了 Apache Calcite 来进行查询的解析，校验以及优化。它们可以与 DataStream 和 DataSet API 无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。Flink 的关系型 API 旨在简化数据分析、数据流水线和 ETL 应用的定义。下面的代码示例展示了如何使用 SQL 语句查询捕获会话时间范围内所有的点击流事件，并对每一次会话的点击量进行计数。此示例与上述 DataStream API 中的示例有着相同的逻辑。123SELECT userId, COUNT(*)FROM clicksGROUP BY SESSION(clicktime, INTERVAL &#39;30&#39; MINUTE), userId库Flink 具有数个适用于常见数据处理应用场景的扩展库。这些库通常嵌入在 API 中，且并不完全独立于其它 API。它们也因此可以受益于 API 的所有特性，并与其他库集成。复杂事件处理(CEP)：模式检测是事件流处理中的一个非常常见的用例。Flink 的 CEP 库提供了 API，使用户能够以例如正则表达式或状态机的方式指定事件模式。CEP 库与 Flink 的 DataStream API 集成，以便在 DataStream 上评估模式。CEP 库的应用包括网络入侵检测，业务流程监控和欺诈检测。DataSet API：DataSet API 是 Flink 用于批处理应用程序的核心 API。DataSet API 所提供的基础算子包括map_、_reduce_、(outer) join_、_co-group_、_iterate_等。所有算子都有相应的算法和数据结构支持，对内存中的序列化数据进行操作。如果数据大小超过预留内存，则过量数据将存储到磁盘。Flink 的 DataSet API 的数据处理算法借鉴了传统数据库算法的实现，例如混合散列连接（hybrid hash-join）和外部归并排序（external merge-sort）。Gelly: Gelly 是一个可扩展的图形处理和分析库。Gelly 是在 DataSet API 之上实现的，并与 DataSet API 集成。因此，它能够受益于其可扩展且健壮的操作符。Gelly 提供了内置算法，如 label propagation、triangle enumeration 和 page rank 算法，也提供了一个简化自定义图算法实现的 Graph API。运维Apache Flink 是一个针对无界和有界数据流进行有状态计算的框架。由于许多流应用程序旨在以最短的停机时间连续运行，因此流处理器必须提供出色的故障恢复能力，以及在应用程序运行期间进行监控和维护的工具。Apache Flink 非常注重流数据处理的可运维性。因此在这一小节中，我们将详细介绍 Flink 的故障恢复机制，并介绍其管理和监控应用的功能。7 * 24小时稳定运行在分布式系统中，服务故障是常有的事，为了保证服务能够7*24小时稳定运行，像Flink这样的流处理器故障恢复机制是必须要有的。显然这就意味着，它(这类流处理器)不仅要能在服务出现故障时候能够重启服务，而且还要当故障发生时，保证能够持久化服务内部各个组件的当前状态，只有这样才能保证在故障恢复时候，服务能够继续正常运行，好像故障就没有发生过一样。Flink通过几下多种机制维护应用可持续运行及其一致性:检查点的一致性: Flink的故障恢复机制是通过建立分布式应用服务状态一致性检查点实现的，当有故障产生时，应用服务会重启后，再重新加载上一次成功备份的状态检查点信息。结合可重放的数据源，该特性可保证_精确一次（exactly-once）_的状态一致性。高效的检查点: 如果一个应用要维护一个TB级的状态信息，对此应用的状态建立检查点服务的资源开销是很高的，为了减小因检查点服务对应用的延迟性（SLAs服务等级协议）的影响，Flink采用异步及增量的方式构建检查点服务。端到端的精确一次: Flink 为某些特定的存储支持了事务型输出的功能，及时在发生故障的情况下，也能够保证精确一次的输出。集成多种集群管理服务: Flink已与多种集群管理服务紧密集成，如 Hadoop YARN, Mesos, 以及 Kubernetes。当集群中某个流程任务失败后，一个新的流程服务会自动启动并替代它继续执行。内置高可用服务: Flink内置了为解决单点故障问题的高可用性服务模块，此模块是基于Apache ZooKeeper 技术实现的，Apache ZooKeeper是一种可靠的、交互式的、分布式协调服务组件。Flink能够更方便地升级、迁移、暂停、恢复应用服务驱动关键业务服务的流应用是经常需要维护的。比如需要修复系统漏洞，改进功能，或开发新功能。然而升级一个有状态的流应用并不是简单的事情，因为在我们为了升级一个改进后版本而简单停止当前流应用并重启时，我们还不能丢失掉当前流应用的所处于的状态信息。而Flink的 Savepoint 服务就是为解决升级服务过程中记录流应用状态信息及其相关难题而产生的一种唯一的、强大的组件。一个 Savepoint，就是一个应用服务状态的一致性快照，因此其与checkpoint组件的很相似，但是与checkpoint相比，Savepoint 需要手动触发启动，而且当流应用服务停止时，它并不会自动删除。Savepoint 常被应用于启动一个已含有状态的流服务，并初始化其（备份时）状态。Savepoint 有以下特点：便于升级应用服务版本: Savepoint 常在应用版本升级时使用，当前应用的新版本更新升级时，可以根据上一个版本程序记录的 Savepoint 内的服务状态信息来重启服务。它也可能会使用更早的 Savepoint 还原点来重启服务，以便于修复由于有缺陷的程序版本导致的不正确的程序运行结果。方便集群服务移植: 通过使用 Savepoint，流服务应用可以自由的在不同集群中迁移部署。方便Flink版本升级: 通过使用 Savepoint，可以使应用服务在升级Flink时，更加安全便捷。增加应用并行服务的扩展性: Savepoint 也常在增加或减少应用服务集群的并行度时使用。便于A/B测试及假设分析场景对比结果: 通过把同一应用在使用不同版本的应用程序，基于同一个 Savepoint 还原点启动服务时，可以测试对比2个或多个版本程序的性能及服务质量。暂停和恢复服务: 一个应用服务可以在新建一个 Savepoint 后再停止服务，以便于后面任何时间点再根据这个实时刷新的 Savepoint 还原点进行恢复服务。归档服务: Savepoint 还提供还原点的归档服务，以便于用户能够指定时间点的 Savepoint 的服务数据进行重置应用服务的状态，进行恢复服务。监控和控制应用服务如其它应用服务一样，持续运行的流应用服务也需要监控及集成到一些基础设施资源管理服务中，例如一个组件的监控服务及日志服务等。监控服务有助于预测问题并提前做出反应，日志服务提供日志记录能够帮助追踪、调查、分析故障发生的根本原因。最后，便捷易用的访问控制应用服务运行的接口也是Flink的一个重要的亮点特征。Flink与许多常见的日志记录和监视服务集成得很好，并提供了一个REST API来控制应用服务和查询应用信息。具体表现如下：Web UI方式: Flink提供了一个web UI来观察、监视和调试正在运行的应用服务。并且还可以执行或取消组件或任务的执行。日志集成服务:Flink实现了流行的slf4j日志接口，并与日志框架log4j或logback集成。指标服务: Flink提供了一个复杂的度量系统来收集和报告系统和用户定义的度量指标信息。度量信息可以导出到多个报表组件服务，包括 JMX, Ganglia, Graphite, Prometheus, StatsD, Datadog, 和 Slf4j.标准的WEB REST API接口服务: Flink提供多种REST API接口，有提交新应用程序、获取正在运行的应用程序的Savepoint服务信息、取消应用服务等接口。REST API还提供元数据信息和已采集的运行中或完成后的应用服务的指标信息。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"}]},{"title":"【转载】谈谈流计算中的『Exactly Once』特性","slug":"【转载】谈谈流计算中的『Exactly-Once』特性","date":"2019-06-02T16:40:09.000Z","updated":"2020-07-01T16:41:41.011Z","comments":true,"path":"2019/06/03/【转载】谈谈流计算中的『Exactly-Once』特性/","link":"","permalink":"cpeixin.cn/2019/06/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E8%B0%88%E8%B0%88%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E3%80%8EExactly-Once%E3%80%8F%E7%89%B9%E6%80%A7/","excerpt":"","text":"本文翻译自 streaml.io 网站上的一篇博文：“Exactly once is NOT exactly the same” ，分析了流计算系统中常说的『Exactly Once』特性，主要观点是：『精确一次』并不保证是完全一样。主要内容如下：背景– 1.1. 最多一次（At-most-once）– 1.2. 至少一次（At-least-once）– 1.3. 精确一次（Exactly-once）『精确一次』是真正的『精确一次』吗?分布式快照与至少一次事件传递和重复数据删除的比较结论参考目前市面上使用较多的流计算系统有 Apache Storm，Apache Flink, Heron, Apache Kafka (Kafka Streams) 和 Apache Spark (Spark Streaming)。关于流计算系统有个被广泛讨论的特性是『exactly-once』语义，很多系统宣称已经支持了这一特性。但是，到底什么是『exactly-once』，怎么样才算是实现了『exactly-once』，人们存在很多误解和歧义。接下来我们做下分析。背景流处理（有时称为事件处理）可以简单地描述为是对无界数据或事件的连续处理。流或事件处理应用程序可以或多或少地被描述为有向图，并且通常被描述为有向无环图（DAG）。在这样的图中，每个边表示数据或事件流，每个顶点表示运算符，会使用程序中定义的逻辑处理来自相邻边的数据或事件。有两种特殊类型的顶点，通常称为 sources 和 sinks。sources读取外部数据/事件到应用程序中，而 sinks 通常会收集应用程序生成的结果。下图是流式应用程序的示例。A typical stream processing topology流处理引擎通常允许用户指定可靠性模式或处理语义，以指示它将为整个应用程序中的数据处理提供哪些保证。这些保证是有意义的，因为你始终会遇到由于网络，机器等可能导致数据丢失的故障。流处理引擎通常为应用程序提供了三种数据处理语义：最多一次、至少一次和精确一次。如下是对这些不同处理语义的宽松定义：最多一次（At-most-once）这本质上是一『尽力而为』的方法。保证数据或事件最多由应用程序中的所有算子处理一次。 这意味着如果数据在被流应用程序完全处理之前发生丢失，则不会进行其他重试或者重新发送。下图中的例子说明了这种情况。至少一次（At-least-once）应用程序中的所有算子都保证数据或事件至少被处理一次。这通常意味着如果事件在流应用程序完全处理之前丢失，则将从源头重放或重新传输事件。然而，由于事件是可以被重传的，因此一个事件有时会被处理多次，这就是所谓的至少一次。下图的例子描述了这种情况：第一个算子最初未能成功处理事件，然后在重试时成功，接着在第二次重试时也成功了，其实是没有必要的。精确一次（Exactly-once）即使是在各种故障的情况下，流应用程序中的所有算子都保证事件只会被『精确一次』的处理。（也有文章将 Exactly-once 翻译为：完全一次，恰好一次）通常使用两种流行的机制来实现『精确一次』处理语义。– 分布式快照 / 状态检查点– 至少一次事件传递和对重复数据去重实现『精确一次』的分布式快照/状态检查点方法受到 Chandy-Lamport 分布式快照算法的启发[1]。通过这种机制，流应用程序中每个算子的所有状态都会定期做 checkpoint。如果是在系统中的任何地方发生失败，每个算子的所有状态都回滚到最新的全局一致 checkpoint 点**。在回滚期间，将暂停所有处理。源也会重置为与最近 checkpoint 相对应的正确偏移量。整个流应用程序基本上是回到最近一次的一致状态，然后程序可以从该状态重新启动。下图描述了这种 checkpoint 机制的基础知识。在上图中，流应用程序在 T1 时间处正常工作，并且做了checkpoint。然而，在时间 T2，算子未能处理输入的数据。此时，S=4 的状态值已保存到持久存储器中，而状态值 S=12 保存在算子的内存中。为了修复这种差异，在时间 T3，处理程序将状态回滚到 S=4 并“重放”流中的每个连续状态直到最近，并处理每个数据。最终结果是有些数据已被处理了多次，但这没关系，因为无论执行了多少次回滚，结果状态都是相同的。另一种实现『精确一次』的方法是：在每个算子上实现至少一次事件传递和对重复数据去重。使用此方法的流处理引擎将重放失败事件，以便在事件进入算子中的用户定义逻辑之前，进一步尝试处理并移除每个算子的重复事件。此机制要求为每个算子维护一个事务日志，以跟踪它已处理的事件。利用这种机制的引擎有 Google 的 MillWheel[2] 和 Apache Kafka Streams。下图说明了这种机制的要点。『精确一次』是真正的『精确一次』吗?现在让我们重新审视『精确一次』处理语义真正对最终用户的保证。『精确一次』这个术语在描述正好处理一次时会让人产生误导。有些人可能认为『精确一次』描述了事件处理的保证，其中流中的每个事件只被处理一次。实际上，没有引擎能够保证正好只处理一次。在面对任意故障时，不可能保证每个算子中的用户定义逻辑在每个事件中只执行一次，因为用户代码被部分执行的可能性是永远存在的。考虑具有流处理运算符的场景，该运算符执行打印传入事件的 ID 的映射操作，然后返回事件不变。下面的伪代码说明了这个操作：1234Map (Event event) &#123; Print \"Event ID: \" + event.getId() Return event&#125;每个事件都有一个 GUID (全局惟一ID)。如果用户逻辑的精确执行一次得到保证，那么事件 ID 将只输出一次。但是，这是无法保证的，因为在用户定义的逻辑的执行过程中，随时都可能发生故障。引擎无法自行确定执行用户定义的处理逻辑的时间点。因此，不能保证任意用户定义的逻辑只执行一次。这也意味着，在用户定义的逻辑中实现的外部操作(如写数据库)也不能保证只执行一次。此类操作仍然需要以幂等的方式执行。那么，当引擎声明『精确一次』处理语义时，它们能保证什么呢？如果不能保证用户逻辑只执行一次，那么什么逻辑只执行一次？当引擎声明『精确一次』处理语义时，它们实际上是在说，它们可以保证引擎管理的状态更新只提交一次到持久的后端存储。上面描述的两种机制都使用持久的后端存储作为真实性的来源，可以保存每个算子的状态并自动向其提交更新。对于机制 1 (分布式快照 / 状态检查点)，此持久后端状态用于保存流应用程序的全局一致状态检查点(每个算子的检查点状态)。对于机制 2 (至少一次事件传递加上重复数据删除)，持久后端状态用于存储每个算子的状态以及每个算子的事务日志，该日志跟踪它已经完全处理的所有事件。提交状态或对作为真实来源的持久后端应用更新可以被描述为恰好发生一次。然而，如上所述，计算状态的更新 / 更改，即处理在事件上执行任意用户定义逻辑的事件，如果发生故障，则可能不止一次地发生。换句话说，事件的处理可以发生多次，但是该处理的效果只在持久后端状态存储中反映一次。因此，我们认为有效地描述这些处理语义最好的术语是『有效一次』（effectively once）。**分布式快照与至少一次事件传递和重复数据删除的比较从语义的角度来看，分布式快照和至少一次事件传递以及重复数据删除机制都提供了相同的保证。然而，由于两种机制之间的实现差异，存在显着的性能差异。机制 1（分布式快照 / 状态检查点）的性能开销是最小的，因为引擎实际上是往流应用程序中的所有算子一起发送常规事件和特殊事件，而状态检查点可以在后台异步执行。但是，对于大型流应用程序，故障可能会更频繁地发生，导致引擎需要暂停应用程序并回滚所有算子的状态，这反过来又会影响性能。流式应用程序越大，故障发生的可能性就越大，因此也越频繁，反过来，流式应用程序的性能受到的影响也就越大。然而，这种机制是非侵入性的，运行时需要的额外资源影响很小。机制 2（至少一次事件传递加重复数据删除）可能需要更多资源，尤其是存储。使用此机制，引擎需要能够跟踪每个算子实例已完全处理的每个元组，以执行重复数据删除，以及为每个事件执行重复数据删除本身。这意味着需要跟踪大量的数据，尤其是在流应用程序很大或者有许多应用程序在运行的情况下。执行重复数据删除的每个算子上的每个事件都会产生性能开销。但是，使用这种机制，流应用程序的性能不太可能受到应用程序大小的影响。对于机制 1，如果任何算子发生故障，则需要发生全局暂停和状态回滚；对于机制 2，失败的影响更加局部性。当在算子中发生故障时，可能尚未完全处理的事件仅从上游源重放/重传。性能影响与流应用程序中发生故障的位置是隔离的，并且对流应用程序中其他算子的性能几乎没有影响。从性能角度来看，这两种机制的优缺点如下。分布式快照 / 状态检查点的优缺点：优点：– 较小的性能和资源开缺点：– 对性能的影响较大– 拓扑越大，对性能的潜在影响越大至少一次事件传递以及重复数据删除机制的优缺点：优点： **– 故障对性能的影响是局部的– 故障的影响不一定会随着拓扑的大小而增加缺点：**– 可能需要大量的存储和基础设施来支持– 每个算子的每个事件的性能开销虽然从理论上讲，分布式快照和至少一次事件传递加重复数据删除机制之间存在差异，但两者都可以简化为至少一次处理加幂等性。对于这两种机制，当发生故障时(至少实现一次)，事件将被重放/重传，并且通过状态回滚或事件重复数据删除，算子在更新内部管理状态时本质上是幂等的。结论在这篇博客文章中，我希望能够让你相信『精确一次』这个词是非常具有误导性的。提供『精确一次』的处理语义实际上意味着流处理引擎管理的算子状态的不同更新只反映一次。『精确一次』并不能保证事件的处理，即任意用户定义逻辑的执行，只会发生一次。我们更喜欢用『有效一次』（effectively once）这个术语来表示这种保证，因为处理不一定保证只发生一次，但是对引擎管理的状态的影响只反映一次。两种流行的机制，分布式快照和重复数据删除，被用来实现精确/有效的一次性处理语义。这两种机制为消息处理和状态更新提供了相同的语义保证，但是在性能上存在差异。这篇文章并不是要让你相信任何一种机制都优于另一种，因为它们各有利弊。结","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"布隆过滤器-BloomFilter","slug":"布隆过滤器-BloomFilter","date":"2019-05-31T15:57:09.000Z","updated":"2020-07-31T15:59:50.613Z","comments":true,"path":"2019/05/31/布隆过滤器-BloomFilter/","link":"","permalink":"cpeixin.cn/2019/05/31/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-BloomFilter/","excerpt":"","text":"什么情况下需要布隆过滤器？先来看几个比较常见的例子字处理软件中，需要检查一个英语单词是否拼写正确在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上在网络爬虫里，一个网址是否被访问过yahoo, gmail等邮箱垃圾邮件过滤功能这几个例子有一个共同的特点： 如何判断一个元素是否存在一个集合中？常规思路数组链表树、平衡二叉树、TrieMap (红黑树)哈希表虽然上面描述的这几种数据结构配合常见的排序、二分搜索可以快速高效的处理绝大部分判断元素是否存在集合中的需求。但是当集合里面的元素数量足够大，如果有500万条记录甚至1亿条记录呢？这个时候常规的数据结构的问题就凸显出来了。数组、链表、树等数据结构会存储元素的内容，一旦数据量过大，消耗的内存也会呈现线性增长，最终达到瓶颈。有的同学可能会问，哈希表不是效率很高吗？查询效率可以达到O(1)。但是哈希表需要消耗的内存依然很高。使用哈希表存储一亿 个垃圾 email 地址的消耗？哈希表的做法：首先，哈希函数将一个email地址映射成8字节信息指纹；考虑到哈希表存储效率通常小于50%（哈希冲突）；因此消耗的内存：8 * 2 * 1亿 字节 = 1.6G 内存，普通计算机是无法提供如此大的内存。这个时候，布隆过滤器（Bloom Filter）就应运而生。在继续介绍布隆过滤器的原理时，先讲解下关于哈希函数的预备知识。哈希函数哈希函数的概念是：将任意大小的数据转换成特定大小的数据的函数，转换后的数据称为哈希值或哈希编码。下面是一幅示意图：可以明显的看到，原始数据经过哈希函数的映射后称为了一个个的哈希编码，数据得到压缩。哈希函数是实现哈希表和布隆过滤器的基础。布隆过滤器介绍巴顿.布隆于一九七零年提出一个很长的二进制向量 （位数组）一系列随机函数 (哈希)空间效率和查询效率高有一定的误判率（哈希表是精确匹配）布隆过滤器原理布隆过滤器（Bloom Filter）的核心实现是一个超大的位数组和几个哈希函数。假设位数组的长度为m，哈希函数的个数为k以上图为例，具体的操作流程：假设集合里面有3个元素{x, y, z}，哈希函数的个数为3。首先将位数组进行初始化，将里面每个位都设置位0。对于集合里面的每一个元素，将元素依次通过3个哈希函数进行映射，每次映射都会产生一个哈希值，这个值对应位数组上面的一个点，然后将位数组对应的位置标记为1。查询W元素是否存在集合中的时候，同样的方法将W通过哈希映射到位数组上的3个点。如果3个点的其中有一个点不为1，则可以判断该元素一定不存在集合中。反之，如果3个点都为1，则该元素可能存在集合中。注意：此处不能判断该元素是否一定存在集合中，可能存在一定的误判率。可以从图中可以看到：假设某个元素通过映射对应下标为4，5，6这3个点。虽然这3个点都为1，但是很明显这3个点是不同元素经过哈希得到的位置，因此这种情况说明元素虽然不在集合中，也可能对应的都是1，这是误判率存在的原因。布隆过滤器添加元素将要添加的元素给k个哈希函数得到对应于位数组上的k个位置将这k个位置设为1布隆过滤器查询元素将要查询的元素给k个哈希函数得到对应于位数组上的k个位置如果k个位置有一个为0，则肯定不在集合中如果k个位置全部为1，则可能在集合中布隆过滤器实现下面给出python的实现，使用murmurhash算法1234567891011121314151617181920212223from bitarray import bitarrayimport mmh3class BloomFilter: def __init__(self,size, hash_num): self.size = size self.hash_num = hash_num self.bit_array = bitarray(size) self.bit_array.serall(0) def add(self, data): for seed in range(self.hash_num): result = mmh3.hash(seed, data) self.bit_array[result] = 1 def lookup(self, data): for seed in range(self.hash_num): result = mmh3.hash(seed, data) if self.bit_array[result] == 0: return 'No this data' return 'Probably'","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"布隆过滤器","slug":"布隆过滤器","permalink":"cpeixin.cn/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"}]},{"title":"Kafka 重平衡流程解析","slug":"Kafka-重平衡流程解析","date":"2019-05-30T08:01:05.000Z","updated":"2020-07-11T08:02:32.879Z","comments":true,"path":"2019/05/30/Kafka-重平衡流程解析/","link":"","permalink":"cpeixin.cn/2019/05/30/Kafka-%E9%87%8D%E5%B9%B3%E8%A1%A1%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/","excerpt":"","text":"重平衡是什么？为什么要了解他？重平衡是什么Rebalance（重平衡 ）本质上是一种协议，规定了一个Consumer Group下的所有 Consumer 如何达成一致，来分配订阅Topic的每个分区。说简单点就是 给消费组每个消费者分配消费任务的过程。为什么要了解他？Rebalance是启动一个消费者组必经的过程，当然这不是最主要的，最主要的是，在消费的过程中，在某些情况下会导致这个过程再次发生，带来的后果就是整个集群暂时性的瘫痪，严重影响到Kafka的高可用发生重平衡的时机那么 Rebalance 会在什么时候发生呢？订阅主题数发生变化这种情况一般不会发生，如果发生，那也是因为我们的业务调整才会，所以这种基本要么不发生要么就是不可避免。主题分区发生变化这种情况发生会相对多一点，但是也有限，在部署Kafka集群前，我们就需要考虑到该集群的容量，以便来确定好分区数。虽然不一定一步到位，但是调整的次数应该是极其有限的，一般也可以选择在半夜低峰的时候进行调整，影响不大。消费端的消费者组成员变化基本上影响最大的就是这个原因了，为什么这么说呢？我们考虑下什么时候消费者组的成员会发生变化就能大概了解了。消费者处理消息超时，即如果消费者处理消费的消息的时间超过了Kafka集群配置的 max.poll.interval.ms 的值，那么该消费者将会自动离组心跳超时，如果消费者在指定的session.timeout.ms时间内没有汇报心跳，那么Kafka就会认为该消费已经dead了可以看出，消费端的消费者组成员变化一般都是由于异常引起的，所以其产生的 Rebalance 也是最难控制的。Kafka的心跳机制 与 RebalanceKafka的心跳机制 与 Rebalance 有什么关系呢？事实上，重平衡过程是靠消费者端的心跳线程（Heartbeat Thread）通知到其他消费者实例的每当消费者向其 coordinator 汇报心跳的时候，如果这个时候 coordinator 决定开启 Rebalance ，那么 coordinator 会将REBALANCE_IN_PROGRESS封装到心跳的响应中，当消费者接受到这个REBALANCE_IN_PROGRESS，他就知道需要开启新的一轮 Rebalance 了,所以heartbeat.interval.ms除了是设置心跳的间隔时间，其实也意味着 Rebalance 感知速度，心跳越快，那么 Rebalance 就能更快的被各个消费者感知。在 Kafka 0.10.1.0 版本之前，发送心跳请求是在消费者主线程完成的，也就是你写代码调用KafkaConsumer.poll方法的那个线程。这样做有诸多弊病，最大的问题在于，消息处理逻辑也是在这个线程中完成的。因此，一旦消息处理消耗了过长的时间，心跳请求将无法及时发到协调者那里，导致协调者“错误地”认为该消费者已“死”。自 0.10.1.0 版本开始，社区引入了一个单独的心跳线程来专门执行心跳请求发送，避免了这个问题。消费者组状态切换为什么要了解 消费者组状态 呢？这里主要是为了方便讲解 Rebalance 流程，所以你需要大概了解一下消费者组的状态切换，如下图其流转过程大概如下：一个消费者组最开始是Empty状态，当重平衡过程开启后，它会被置于PreparingRebalance状态 等待成员加入，成员都加入之后变更到CompletingRebalance状态等待分配方案，当coordinator分配完个消费者消费的分区后，最后就流转到Stable状态完成重平衡。当有新成员加入或已有成员退出时，消费者组的状态 从Stable直接跳到PreparingRebalance状态，此时，所有现存成员就必须重新申请加入组。当所有成员都退出组后，消费者组状态变更为Empty。Kafka定期自动删除过期位移的条件就是，组要处于Empty状态。因此，如果你的消费者组停掉了很长时间（超过7天），那么Kafka很可能就把该组的位移数据删除了。消费者端重平衡流程在消费者端，重平衡分为两个步骤：加入组。当组内成员加入组时，它会向 coordinator 发送JoinGroup请求。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，Coordinator 会从这些成员中选择一个担任这个消费者组的领导者。通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。特别注意的是：这里说的是消费者领导者。选出领导者之后，Coordinator 会把消费者组订阅信息封装进JoinGroup请求的 响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。如下图就是 JoinGroup 的全过程[图片上传中…(25-消费者组重平衡全流程解析.jpg-d67470-1567669412412-0)]JoinGroup 流程解析.jpg领导者消费者（Leader Consumer）分配方案。领导者向 Coordinator 发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向 Coordinator 发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让 Coordinator 接收分配方案，然后统一以 SyncGroup 响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。如下图：SyncGroup全流程解析.jpg消费者端重平衡流程 大概就这样了，下面我们再来看看：Broker端重平衡Broker端重平衡要剖析协调者端处理重平衡的全流程，我们必须要分几个场景来讨论。这几个场景分别是新成员加入组组成员主动离组组成员崩溃离组组成员提交位移。接下来，我们一个一个来讨论。新成员入组。新成员入组是指组处于Stable状态后，有新成员加入。如果是全新启动一个消费者组，Kafka是有一些自己的小优化的，流程上会有些许的不同。我们这里讨论的是，组稳定了之后有新成员加入的情形。当协调者收到新的JoinGroup请求后，它会通过心跳请求响应的方式通知组内现有的所有成员，强制它们开启新一轮的重平衡。具体的过程和之前的客户端重平衡流程是一样的。现在，我用一张时序图来说明协调者一端是如何处理新成员入组的。组成员主动离组。何谓主动离组？就是指消费者实例所在线程或进程调用close()方法主动通知协调者它要退出。这个场景就涉及到了第三类请求：LeaveGroup请求。协调者收到LeaveGroup请求后，依然会以心跳响应的方式通知其他成员，因此我就不再赘述了，还是直接用一张图来说明。组成员崩溃离组。崩溃离组是指消费者实例出现严重故障，突然宕机导致的离组。它和主动离组是有区别的，因为后者是主动发起的离组，协调者能马上感知并处理。但崩溃离组是被动的，协调者通常需要等待一段时间才能感知到，这段时间一般是由消费者端参数session.timeout.ms控制的。也就是说，Kafka一般不会超过session.timeout.ms就能感知到这个崩溃。当然，后面处理崩溃离组的流程与之前是一样的，我们来看看下面这张图。重平衡时协调者对组内成员提交位移的处理。正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。还是老办法，我们使用一张图来说明。总结其实不论哪种方式，都是差不多的流程，这里放开举例，最主要的还是为了更加清晰，如果发生类似的问题，可以很快的从上面这些可能入手。基本流程就是 Coordinator 感知到 消费者组的变化，然后在心跳的过程中发送重平衡信号通知各个消费者离组，然后消费者重新以 JoinGroup 方式加入 Coordinator，并选出Consumer Leader。当所有消费者加入 Coordinator，Consumer Leader会根据 Coordinator给予的分区信息给出分区方案。Coordinator 将该方案以 SyncGroup 的方式将该方案执行下去，通知各个消费者，这样就完成了一轮 重平衡了。转载来源：https://www.jianshu.com/p/6915e8c983bc","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"SHC：使用 Spark SQL 高效地读写 HBase","slug":"SHC：使用-Spark-SQL-高效地读写-HBase","date":"2019-05-16T02:27:59.000Z","updated":"2020-05-16T02:31:46.066Z","comments":true,"path":"2019/05/16/SHC：使用-Spark-SQL-高效地读写-HBase/","link":"","permalink":"cpeixin.cn/2019/05/16/SHC%EF%BC%9A%E4%BD%BF%E7%94%A8-Spark-SQL-%E9%AB%98%E6%95%88%E5%9C%B0%E8%AF%BB%E5%86%99-HBase/","excerpt":"","text":"Apache Spark 和 Apache HBase 是两个使用比较广泛的大数据组件。很多场景需要使用 Spark 分析/查询 HBase 中的数据，而目前 Spark 内置是支持很多数据源的，其中就包括了 HBase，但是内置的读取数据源还是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；TableInputFormat 中不支持 BulkGet；不能享受到 Spark SQL 内置的 catalyst 引擎的优化。从另一个方面来讲，如果先不谈关于性能的方面，那么你在写HBase的过程中，是不是步骤感觉很麻烦呢，Foreach每条数据Put到HBase中，往往写入逻辑部分的代码就要写很长很长的一段。基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。项目地址具体的引入方式，在之前的Spark SQL入门中有写到Catalog对于每个表，必须提供一个目录，其中包括行键和具有预定义列族的数据类型的列，并定义hbase列与表模式之间的映射。目录是用户定义的json格式。数据类型转换支持Java基本类型。将来，将支持其他数据类型，这些数据类型依赖于用户指定的Serdes。SHC支持三种内部Serdes：Avro，Phoenix和PrimitiveType。用户可以通过在目录中定义“ tableCoder”来指定要使用的serde。为此，请参考示例和单元测试。以Avro为例。用户定义的Serdes将负责将字节数组转换为Avro对象，而连接器将负责将Avro对象转换为催化剂支持的数据类型。用户定义新Serde时，需要使其“实现”特征’SHCDataType’。请注意，如果用户希望DataFrame仅处理字节数组，则可以指定二进制类型。然后，用户可以获得每个列为字节数组的催化剂行。用户可以使用定制的解串器进一步对它进行反序列化，或者直接在DataFrame的RDD上进行操作。数据局部性当Spark Worker节点与hbase区域服务器位于同一位置时，通过标识区域服务器位置并将执行程序与区域服务器一起定位来实现数据局部性。每个执行程序将仅对位于同一主机上的同一部分数据执行Scan / BulkGet。谓词下推该库使用HBase提供的现有标准HBase过滤器，并且不能在协处理器上运行。分区修剪通过从谓词中提取行键，我们将scan / BulkGet划分为多个非重叠区域，只有具有请求数据的区域服务器才会执行scan / BulkGet。当前，分区修剪是在行键的第一维上执行的。请注意，需要仔细定义WHERE条件。否则，结果扫描可能会包含一个比用户预期大的区域。例如，以下条件将导致完全扫描（rowkey1是行键的第一维，而column是常规的hbase列）。其中rowkey1&gt;“ abc” OR列=“ xyz”扫描和批量获取都通过指定WHERE子句向用户公开，例如，其中column&gt; x和column &lt;y用于扫描，而column = x用于get。所有操作都在执行程序中执行，而驱动程序仅构造这些操作。在内部，我们将它们转换为扫描或获取或两者结合，从而将Iterator [Row]返回至催化剂引擎。可创建的数据源该库支持从HBase读取/向HBase写入。应用用途下面说明了如何使用连接器的基本步骤。有关更多详细信息和高级用例（例如Avro和复合键支持），请参考存储库中的示例。定义了HBase目录123456789101112131415def catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"table1\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"cf4\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"cf5\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"cf6\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"cf7\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"cf8\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125; |&#125;\"\"\".stripMargin上面定义了一个名称为table1，行键为键，列数为（col1-col8）的HBase表的架构。请注意，行键还必须详细定义为具有特定cf（行键）的列（col0）。写入HBase表以填充数据1234sc.parallelize(data).toDF.write.options( Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; \"5\")) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .save()给定具有指定架构的DataFrame，上面将创建一个具有5个区域的HBase表并将该DataFrame保存在其中。请注意，如果未指定HBaseTableCatalog.newTable，则必须预先创建表。在HBase表的顶部执行DataFrame操作1234567def withCatalog(cat: String): DataFrame = &#123; sqlContext .read .options(Map(HBaseTableCatalog.tableCatalog-&gt;cat)) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .load()&#125;复杂查询12345678910val df = withCatalog(catalog)val s = df.filter((($\"col0\" &lt;= \"row050\" &amp;&amp; $\"col0\" &gt; \"row040\") || $\"col0\" === \"row005\" || $\"col0\" === \"row020\" || $\"col0\" === \"r20\" || $\"col0\" &lt;= \"row005\") &amp;&amp; ($\"col4\" === 1 || $\"col4\" === 42)) .select(\"col0\", \"col1\", \"col4\")s.showSQL支持12345// Load the dataframeval df = withCatalog(catalog)//SQL exampledf.createOrReplaceTempView(\"table\")sqlContext.sql(\"select count(col1) from table\").show ## 支持Avro模式 该连接器完全支持所有avro模式。用户可以在其目录中使用完整记录模式或部分字段模式作为数据类型（有关更多详细信息，请参阅[此处](https://github.com/hortonworks-spark/shc/wiki/2.-Native-Avro-Support)）。1234567891011121314151617181920val schema_array = s\"\"\"&#123;\"type\": \"array\", \"items\": [\"string\",\"null\"]&#125;\"\"\".stripMarginval schema_record = s\"\"\"&#123;\"namespace\": \"example.avro\", | \"type\": \"record\", \"name\": \"User\", | \"fields\": [ &#123;\"name\": \"name\", \"type\": \"string\"&#125;, | &#123;\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]&#125;, | &#123;\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]&#125; ] &#125;\"\"\".stripMarginval catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"htable\"&#125;, |\"rowkey\":\"key1\", |\"columns\":&#123; |\"col1\":&#123;\"cf\":\"rowkey\", \"col\":\"key1\", \"type\":\"double\"&#125;, |\"col2\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"avro\":\"schema_array\"&#125;, |\"col3\":&#123;\"cf\":\"cf1\", \"col\":\"col2\", \"avro\":\"schema_record\"&#125;, |\"col4\":&#123;\"cf\":\"cf1\", \"col\":\"col3\", \"type\":\"double\"&#125;, |\"col5\":&#123;\"cf\":\"cf1\", \"col\":\"col4\", \"type\":\"string\"&#125; |&#125; |&#125;\"\"\".stripMargin val df = sqlContext.read.options(Map(\"schema_array\"-&gt;schema_array,\"schema_record\"-&gt;schema_record, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").load()df.write.options(Map(\"schema_array\"-&gt;schema_array,\"schema_record\"-&gt;schema_record, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").save() ####12345678910111213141516171819202122val complex = s\"\"\"MAP&lt;int, struct&lt;varchar:string&gt;&gt;\"\"\"val schema = s\"\"\"&#123;\"namespace\": \"example.avro\", | \"type\": \"record\", \"name\": \"User\", | \"fields\": [ &#123;\"name\": \"name\", \"type\": \"string\"&#125;, | &#123;\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]&#125;, | &#123;\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]&#125; ] &#125;\"\"\".stripMarginval catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"htable\"&#125;, |\"rowkey\":\"key1:key2\", |\"columns\":&#123; |\"col1\":&#123;\"cf\":\"rowkey\", \"col\":\"key1\", \"type\":\"binary\"&#125;, |\"col2\":&#123;\"cf\":\"rowkey\", \"col\":\"key2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"avro\":\"schema1\"&#125;, |\"col4\":&#123;\"cf\":\"cf1\", \"col\":\"col2\", \"type\":\"string\"&#125;, |\"col5\":&#123;\"cf\":\"cf1\", \"col\":\"col3\", \"type\":\"double\", \"sedes\":\"org.apache.spark.sql.execution.datasources.hbase.DoubleSedes\"&#125;, |\"col6\":&#123;\"cf\":\"cf1\", \"col\":\"col4\", \"type\":\"$complex\"&#125; |&#125; |&#125;\"\"\".stripMargin val df = sqlContext.read.options(Map(\"schema1\"-&gt;schema, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").load()df.write.options(Map(\"schema1\"-&gt;schema, HBaseTableCatalog.tableCatalog-&gt;catalog)).format(\"org.apache.spark.sql.execution.datasources.hbase\").save()以上说明了我们的下一步，其中包括复合键支持，复杂数据类型，客户化Serde和Avro的支持。请注意，尽管所有主要部分都包含在当前代码库中，但现在可能无法运行。SHC查询优化SHC 主要使用下面的几种优化，使得 Spark 获取 HBase 的数据扫描范围得到减少，提高了数据读取的效率。将使用 Rowkey 的查询转换成 get 查询**我们都知道，HBase 中使用 Get 查询的效率是非常高的，所以如果查询的过滤条件是针对 RowKey 进行的，那么我们可以将它转换成 Get 查询。为了说明这点，我们使用下面的例子进行说明。假设我们定义好的 HBase catalog 如下：123456789101112131415val catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"iteblog\", \"tableCoder\":\"PrimitiveType\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"id\", \"type\":\"int\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"cf4\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"cf5\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"cf6\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"cf7\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"cf8\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125;|&#125;\"\"\".stripMargin那么如果有类似下面的查询12345val df = withCatalog(catalog)df.createOrReplaceTempView(\"iteblog_table\")sqlContext.sql(\"select * from iteblog_table where id = 1\")sqlContext.sql(\"select * from iteblog_table where id = 1 or id = 2\")sqlContext.sql(\"select * from iteblog_table where id in (1, 2)\")因为查询条件直接是针对 RowKey 进行的，所以这种情况直接可以转换成 Get 或者 BulkGet 请求的。第一个 SQL 查询过程类似于下面过程但是，如果碰到非 RowKey 的过滤，那么这种查询是需要扫描 HBase 的全表的。上面的查询在 shc 里面就是将 HBase 里面的所有数据拿到，然后传输到 Spark ，再通过 Spark 里面进行过滤，可见 shc 在这种情况下效率是很低下的。注意，上面的查询在 shc 返回的结果是错误的。具体原因是在将 id = 1 or col7 = ‘xxx’ 查询条件进行合并时，丢弃了所有的查找条件，相当于返回表的所有数据。定位到代码可以参见下面的123456def or[T](left: HRF[T], right: HRF[T])(implicit ordering: Ordering[T]): HRF[T] = &#123; val ranges = ScanRange.or(left.ranges, right.ranges) val typeFilter = TypedFilter.or(left.tf, right.tf) HRF(ranges, typeFilter, left.handled &amp;&amp; right.handled)&#125;同理，类似于下面的查询在 shc 里面其实都是全表扫描，并且将所有的数据返回到 Spark 层面上再进行一次过滤。123sqlContext.sql(\"select id, col6, col8 from iteblog_table where id = 1 or col7 &lt;= 'xxx'\")sqlContext.sql(\"select id, col6, col8 from iteblog_table where id = 1 or col7 &gt;= 'xxx'\")sqlContext.sql(\"select id, col6, col8 from iteblog_table where col7 = 'xxx'\")很显然，这种方式查询效率并不高，一种可行的方案是将算子下推到 HBase 层面，在 HBase 层面通过 SingleColumnValueFilter 过滤一部分数据，然后再返回到 Spark，这样可以节省很多数据的传输。组合 RowKey 的查询优化shc 还支持组合 RowKey 的方式来建表，具体如下：1234567891011121314151617def cat = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"iteblog\", \"tableCoder\":\"PrimitiveType\"&#125;, |\"rowkey\":\"key1:key2\", |\"columns\":&#123; |\"col00\":&#123;\"cf\":\"rowkey\", \"col\":\"key1\", \"type\":\"string\", \"length\":\"6\"&#125;, |\"col01\":&#123;\"cf\":\"rowkey\", \"col\":\"key2\", \"type\":\"int\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"cf4\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"cf5\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"cf6\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"cf7\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"cf8\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125; |&#125;\"\"\".stripMargin上面的 col00 和 col01 两列组合成一个 rowkey，并且 col00 排在前面，col01 排在后面。比如 col00 =’row002’，col01 = 2，那么组合的 rowkey 为 row002\\x00\\x00\\x00\\x02。那么在组合 Rowkey 的查询 shc 都有哪些优化呢？现在我们有如下查询1df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0\").show()根据上面的信息，RowKey 其实是由 col00 和 col01 组合而成的，那么上面的查询其实可以将 col00 和 col01 进行拼接，然后组合成一个 RowKey，然后上面的查询其实可以转换成一个 Get 查询。但是在 shc 里面，上面的查询是转换成一个 scan 和一个 get 查询的。scan 的 startRow 为 row000，endRow 为 row000\\xff\\xff\\xff\\xff；get 的 rowkey 为 row000\\xff\\xff\\xff\\xff，然后再将所有符合条件的数据返回，最后再在 Spark 层面上做一次过滤，得到最后查询的结果。因为 shc 里面组合键查询的代码还没完善，所以当前实现应该不是最终的。在 shc 里面下面两条 SQL 查询下沉到 HBase 的逻辑一致1df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 = 'row000'\").show()df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 = 'row000' and col01 = 0\").show()唯一区别是在 Spark 层面上的过滤。scan 查询优化如果我们的查询有 &lt; 或 &gt; 等查询过滤条件，比如下面的查询条件：1df.sqlContext.sql(\"select col00, col01, col1 from iteblog where col00 &gt; 'row000' and col00 &lt; 'row005'\").show()这个在 shc 里面转换成 HBase 的过滤为一条 get 和 一个 scan，具体为 get 的 Rowkey 为 row0005\\xff\\xff\\xff\\xff；scan 的 startRow 为 row000，endRow 为 row005\\xff\\xff\\xff\\xff，然后将查询的结果返回到 spark 层面上进行过滤。总体来说，shc 能在一定程度上对查询进行优化，避免了全表扫描。但是经过评测，shc 其实还有很多地方不够完善，算子下沉并没有下沉到 HBase 层面上进行。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark分区并行度决定机制","slug":"Spark分区并行度决定机制","date":"2019-04-26T12:43:19.000Z","updated":"2020-09-26T12:45:01.751Z","comments":true,"path":"2019/04/26/Spark分区并行度决定机制/","link":"","permalink":"cpeixin.cn/2019/04/26/Spark%E5%88%86%E5%8C%BA%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Spark分区并行度决定机制大家都知道Spark job中最小执行单位为task，合理设置Spark job每个stage的task数是决定性能好坏的重要因素之一，但是Spark自己确定最佳并行度的能力有限，这就要求我们在了解其中内在机制的前提下，去各种测试、计算等来最终确定最佳参数配比。Spark任务在执行时会将RDD划分为不同的stage，一个stage中task的数量跟最后一个RDD的分区数量相同。之前已经介绍过，stage划分的关键是宽依赖，而宽依赖往往伴随着shuffle操作。对于一个stage接收另一个stage的输入，这种操作通常都会有一个参数numPartitions来显示指定分区数。最典型的就是一些ByKey算子，比如groupByKey(numPartitions: Int)，但是这个分区数需要多次测试来确定合适的值。首先确定父RDD中的分区数（通过rdd.partitions().size()可以确定RDD的分区数），然后在此基础上增加分区数，多次调试直至在确定的资源任务能够平稳、安全的运行。对于没有父RDD的RDD，比如通过加载HDFS上的数据生成的RDD，它的分区数由InputFormat切分机制决定。通常就是一个HDFS block块对应一个分区，对于不可切分文件则一个文件对应一个分区。对于通过SparkContext的parallelize方法或者makeRDD生成的RDD分区数可以直接在方法中指定，如果未指定，则参考spark.default.parallelism的参数配置。下面是默认情况下确定defaultParallelism的源码：123override def defaultParallelism(): Int &#x3D; &#123; conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))&#125;通常，RDD的分区数与其所依赖的RDD的分区数相同，除非shuffle。但有几个特殊的算子：1.coalesce和repartition算子笔者先放两张关于该coalesce算子分别在RDD和DataSet中的源码图：（DataSet是Spark SQL中的分布式数据集，后边说到Spark时再细讲）通过coalesce源码分析，无论是在RDD中还是DataSet，默认情况下coalesce不会产生shuffle，此时通过coalesce创建的RDD分区数小于等于父RDD的分区数。笔者这里就不放repartition算子的源码了，分析起来也比较简单，图中我有所提示。但笔者建议，如下两种情况，请使用repartition算子：1）增加分区数repartition触发shuffle，shuffle的情况下可以增加分区数。coalesce默认不触发shuffle，即使调用该算子增加分区数，实际情况是分区数仍然是当前的分区数。2）极端情况减少分区数，比如将分区数减少为1调整分区数为1，此时数据处理上游stage并行度降，很影响性能。此时repartition的优势即不改变原来stage的并行度就体现出来了，在大数据量下，更为明显。但需要注意，因为repartition会触发shuffle，而要衡量好shuffle产生的代价和因为用repartition增加并行度带来的效益。2.union算子还是直接看源码：通过分析源码，RDD在调用union算子时，最终生成的RDD分区数分两种情况：1）union的RDD分区器已定义并且它们的分区器相同多个父RDD具有相同的分区器，union后产生的RDD的分区器与父RDD相同且分区数也相同。比如，n个RDD的分区器相同且是defined，分区数是m个。那么这n个RDD最终union生成的一个RDD的分区数仍是m，分区器也是相同的2）不满足第一种情况，则通过union生成的RDD的分区数为父RDD的分区数之和4.cartesian算子通过上述coalesce、repartition、union算子介绍和源码分析，很容易分析cartesian算子的源码。通过cartesian得到RDD分区数是其父RDD分区数的乘积。spark.default.parallelism谈Spark谈并行度上图是spark官网关于spark.default.parallelism参数说明：对于reduceByKey和join这些分布式shuffle算子操作，取决于它的父RDD中分区数的最大值对于没有父RDD的的算子，比如parallelize，依赖于集群管理器：本地模式：取决于本地机器的核数如果集群管理器是Mesos，则为8其他的：对比所有executor上总核数与2比较，哪个大是哪个当然上面这些都是默认值，如果我们自己设置了分区数，情况就会有所变化，直接看源码【查看org.apache.spark.Partitioner源码defaultPartitioner方法】你会发现，如果你使用reducebykey、groupByKey等这些带shuffle的算子，建议不要通过上述方法让程序内部去推测。完全可以通过传入一个确定的分区数或者自己实现一个分区器来做处理。当然这个确定的分区数也不是贸贸然设定的，需要结合你的业务场景根据实际情况来确定多少合适。比如shuffle时流经的数据量，这个就要结合分区数和shuffle总数据量来做适当调整，处理不好的结果极有可能导致数据倾斜等问题…在Spark SQL中，任务并行度参数则要参考spark.sql.shuffle.partitions，笔者这里先放一张图，详细的后面讲到Spark SQL时再细说：看下图在Spark流式计算中，通常将SparkStreaming和Kafka整合，这里又分两种情况：1.Receiver方式生成的微批RDD即BlockRDD，分区数就是block数receiver模式的并行度由spark.streaming.blockInterval决定，默认是200ms。receiver模式接收block.batch数据后会封装到RDD中，这里的block对应RDD中的partition。batchInterval一定的情况下：减少spark.streaming.Interval参数值，会增大DStream中的partition个数。建议spark.streaming.Interval最低不能低于50ms。2.Direct方式生成的微批RDD即kafkaRDD，分区数和kafka分区数一一对应Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。DirectKafkaInputDStream定期生成的RDD的类型是KafkaRDD。我们首先看看 KafkaRDD是如何划分分区的：它会根据从初始化时接收的offset信息参数，生成KafkaRDDPartition分区；每个分区对应着Kafka的一个topic partition 的一段数据，这段数据的信息OffsetRange表示， 它保存了数据的位置。通过源码分析可知：Partition的计算方法是为topic的每一个partition创建一个OffsetRange，所有的OffsetRange生成一个KafkaRDD。如何增大RDD的分区数，让每个partition处理的数据量增大？通过源码分析，可通过调小Kafka消息中Topic的分区数目；想要增加RDD的并行度，可通过调大Kafka消息中Topic的分区数目。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"shadowsock-vps搭建VPN","slug":"shadowsock-vps搭建VPN","date":"2019-04-19T15:26:15.000Z","updated":"2020-04-04T17:11:07.011Z","comments":true,"path":"2019/04/19/shadowsock-vps搭建VPN/","link":"","permalink":"cpeixin.cn/2019/04/19/shadowsock-vps%E6%90%AD%E5%BB%BAVPN/","excerpt":"","text":"前言还有10天左右就要回国了，由于职业的需要，对Google的依赖的越来越大的，那么回国后怎么才能‘科学上网’呢？之前在国内的时候，有使用过Lantern，稳定性和速度都还是不错了，可惜后来被和谐了。所以今天准备尝试搭建VPN，自己独立使用，一边搭建一边将过程记录下来。名词解释VPS: VPS（Virtual Private Server 虚拟专用服务器）技术，将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器 [1] 技术，和虚拟化技术 [2] 。在容器或虚拟机中，每个VPS都可分配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。VPS为使用者提供了管理配置的自由，可用于企业虚拟化，也可以用于IDC资源租用。VPN: VPN的学名叫虚拟专用网，洋文叫“Virtual Private Network”。维基百科的介绍在“这里”。本来这玩意儿主要是用于商业公司，为了让那些不在公司里的员工（比如出差在外的）能够方便地访问公司的内部网络。为了防止黑客冒充公司的员工，从外部访问公司的内部网络，VPN 软件都会提供强大的加密功能。而这个加密功能，也就让它顺便成为翻墙的利器。科学上网原理VPN浏览外网的原理使用 VPN 通常需要先安装客户端软件。当你运行 VPN 客户端，它会尝试联到 VPN 服务器（这点跟加密代理类似）。一旦和 VPN 服务器建立连接，VPN 客户端就会在你的系统中建立了一个虚拟局域网。而且，你的系统中也会多出一个虚拟网卡（在 Windows 下，可以用 ipconfig /all 命令，看到这多出来的网卡）。这样一来，你的系统中就有不止一块网卡。这就引出一个问题：那些访问网络的程序，它的数据流应该通过哪个网卡进出？为了解决此问题，VPN 客户端通常会修改你系统的路由表，让那些数据流，优先从虚拟的网卡进出。由于虚拟的网卡是通往 VPN 服务器的，当数据流到达 VPN 服务器之后，VPN 服务器再帮你把数据流转向到真正的目的地。前面说了，VPN 为了保证安全，都采用强加密的方式传输数据。这样一来，GFW 就无法分析你的网络数据流，进行敏感词过滤。所以，使用墙外的VPN服务器，无形中就能达到翻墙的效果。方案选择VPN是一个大类，其中有很多实现的方法，防火长城现在将 VPN 屏蔽的已经所剩无几，后来大家看到了SSH，使用SSH的sock5很稳定，但是特征也十分明显，防火长城可以对其直接进行定向干扰。而除了VPN，对于翻墙大家仍然有很多方法，比如Shadowsocks 、Lantern、VPNGate 等等，而实际上无论哪种方式，他们本身都需要一台服务器作为中间人进行消息传递。而VPS虚拟专用服务器就十分适合担当这个角色，并且由于VPS平时就作为商品在各类云服务器平台上售卖，自行购买并搭建相当方便，唯一需要的就是人们对于服务器的操作技术。而这次选择的方案是：VPS+Shadowsocks**Shadowsocks特点：省电，在电量查看里几乎看不到它的身影；支持开机自启动，且断网无影响，无需手动重连，方便网络不稳定或者3G&amp;Wi-Fi频繁切换的小伙伴；可使用自己的服务器，安全和速度的保证；支持区分国内外流量，传统VPN在翻出墙外后访问国内站点会变慢；可对应用设置单独代理，5.0之后的系统无需root。Shadowsocks 目前不容易被封杀主要是因为：建立在socks5协议之上，socks5是运用很广泛的协议，所以没办法直接封杀socks5协议使用socks5协议建立连接，而没有使用VPN中的服务端身份验证和密钥协商过程。而是在服务端和客户端直接写死密钥和加密算法。所以防火墙很难找到明显的特征，因为这就是个普通的socks5协议。Shadowsock搭建也比较简单，所以很多人自己架设VPS搭建，个人使用流量也很小，没法通过流量监控方式封杀。自定义加密方式和密钥。因为加密主要主要是防止被检测，所以要选择安全系数高的加密方式。之前RC4会很容易被破解，而导致被封杀。所以现在推荐使用AES加密。而在客户端和服务端自定义密钥，泄露的风险相对较小。所以如果是自己搭建的Shadosocks被封的概率很小，但是如果是第三方的Shadeowsocks，密码是server定的，你的数据很可能遭受到中间人攻击。开工购买vps首先我们需要购买一台境外的服务器，接着我们在这台云服务器里面安装代理服务，那么以后我们上网的时候就可以通过它来中转，轻松畅快的畅游全网了。购买VPS,我选择了vultr，大家用过都说好，购买的过程也很方便。第一步：选择离中国较近国家的服务器。第二步：选择服务器配置和系统这里，系统选择的是CentOS 7,配置的话，如果只是自己浏览网页的话，选择最低配置就好。其他的选项可以略过。第三步：支付和部署支付可以选择支付宝支付，非常方便。购买成功后，点击Server中的“+”号，来部署你刚刚选择的服务器。第四步：登陆服务器查看服务器详情 Server Details,根据提供的服务器信息，登陆服务器。我是使用Mac本身终端ssh到服务器上的，因为Mac上多数的SSH客户端要么收费，要么不好用，要么安装过程非常繁琐。1ssh -p 22 root@ip搭建shadowsocks服务器连接到你的 vultr 服务器之后，接下来就可以使用几个命令让你快速搭建一个属于自己的 ss 服务器：1yum install wget接着执行安装shadowsocks：1wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh获取 shadowsocks.sh 读取权限：1chmod +x shadowsocks.sh设置你的 ss 密码和端口号：1./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log接下来后就可以设置密码和端口号了密码和端口号可以使用默认的，也可以直接重新输入新的。选择加密方式设置完密码和端口号之后，我们选择加密方式，这里选择 7 ，使用aes-256-cfb的加密模式接着按任意键进行安装。安装ss完成后会给你显示你需要连接 vpn 的信息：搞定，将这些信息保存起来，那么这时候你就可以使用它们来科学上网啦。使用BBR加速上网安装 BBR1wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh获取读写权限1chmod +x bbr.sh启动BBR安装1./bbr.sh接着按任意键，开始安装，坐等一会。安装完成一会之后它会提示我们是否重新启动vps，我们输入 y 确定重启服务器。重新启动之后，输入：1lsmod | grep bbr如果看到 tcp_bbr 就说明 BBR 已经启动了。客户端进行连接windows使用Shadowsockswindows点击下载：Shadowsocks windows客户端打开 Shadowsocks 客户端，输入ip地址，密码，端口，和加密方式。接着点击确定，右下角会有个小飞机按钮，右键–&gt;启动代理。Android使用ShadowsocksAndroid点击下载：Shadowsocks Android客户端打开apk安装，接着打开APP，输入ip地址，密码，端口，和加密方式。即可科学上网。iPhone使用ShadowsocksiPhone要下载的app需要在appstore下载，但是需要用美区账号才能下载，而且这个APP需要钱。在这里提供一种解决方案，就是可以再搭建一个IPsec/L2TP VPN,专门给你的iPhone使用。Mac配置用的是Mac电脑，所以点击相关链接。东西都挂在github上，下载对应的zip文件，下载完成后安装并运行起来。点击图标，进入 服务器设置主要有四个地方要填，服务器的地址，端口号，加密方法，密码。服务器地址即为之前 Main controls选项中的IP地址。端口号、加密方法、密码必须与之前 Shadowsocks Server 中的信息一一匹配，否则会连接失败。设置完成后点击确定，然后服务器选择这个配置，默认选中PAC自动模式，确保Shadowsocks状态为On，这时候打开谷歌试试~接着就可以上外网了 😂","categories":[{"name":"工具","slug":"工具","permalink":"cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"shadowsock","slug":"shadowsock","permalink":"cpeixin.cn/tags/shadowsock/"}]},{"title":"Kafka 消费者详解","slug":"Kafka-消费者详解","date":"2019-04-10T15:09:38.000Z","updated":"2020-07-10T15:12:07.475Z","comments":true,"path":"2019/04/10/Kafka-消费者详解/","link":"","permalink":"cpeixin.cn/2019/04/10/Kafka-%E6%B6%88%E8%B4%B9%E8%80%85%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"一、消费者和消费者群组在 Kafka 中，消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。需要注意的是：同一个分区只能被同一个消费者群组里面的一个消费者读取，不可能存在同一个分区被同一个消费者群里多个消费者共同读取的情况，如图：可以看到即便消费者 Consumer5 空闲了，但是也不会去读取任何一个分区的数据，这同时也提醒我们在使用时应该合理设置消费者的数量，以免造成闲置和额外开销。二、分区再均衡因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。三、创建Kafka消费者在创建消费者的时候以下以下三个选项是必选的：bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；key.deserializer ：指定键的反序列化器；value.deserializer ：指定值的反序列化器。除此之外你还需要指明你需要想订阅的主题，可以使用如下两个 API :consumer.subscribe(Collectiontopics)：指明需要订阅的主题的集合；consumer.subscribe(Pattern pattern) ：使用正则来匹配需要订阅的集合。最后只需要通过轮询 API(poll) 向服务器定时请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，这使得开发者只需要关注从分区返回的数据，然后进行业务处理。 示例如下：123456789101112val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean))val topics = Array(\"weibo_keyword\")val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams))本篇文章的所有示例代码可以从 Github 上进行下载：kafka-basis三、 自动提交偏移量3.1 偏移量的重要性Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 ＿consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况：如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费；如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。3.2 自动提交偏移量Kafka 支持自动提交和手动提交偏移量两种方式。这里先介绍比较简单的自动提交：只需要将消费者的 enable.auto.commit 属性配置为 true 即可完成自动提交的配置。 此时每隔固定的时间，消费者就会把 poll() 方法接收到的最大偏移量进行提交，提交间隔由 auto.commit.interval.ms 属性进行配置，默认值是 5s。使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。四、手动提交偏移量用户可以通过将 enable.auto.commit 设为 false，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类：手动提交当前偏移量：即手动提交当前轮询的最大偏移量；手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。而按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。4.1 同步提交通过调用 consumer.commitSync() 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏移量。123456789while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; /*同步提交*/ consumer.commitSync();&#125;复制代码如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。4.2 异步提交异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下：123456789101112131415161718while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; /*异步提交并定义回调*/ consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.out.println(\"错误处理\"); offsets.forEach((x, y) -&gt; System.out.printf(\"topic = %s,partition = %d, offset = %s \\n\", x.topic(), x.partition(), y.offset())); &#125; &#125; &#125;);&#125;复制代码异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了，此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某些情况下，需要同时组合同步和异步两种提交方式。注：虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map&lt;TopicPartition, Integer&gt; offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。4.3 同步加异步提交下面这种情况，在正常的轮询中使用异步提交来保证吞吐量，但是因为在最后即将要关闭消费者了，所以此时需要用同步提交来保证最大限度的提交成功。1234567891011121314151617181920try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; &#x2F;&#x2F; 异步提交 consumer.commitAsync(); &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; &#x2F;&#x2F; 因为即将要关闭消费者，所以要用同步提交保证提交成功 consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125;复制代码4.4 提交特定偏移量在上面同步和异步提交的 API 中，实际上我们都没有对 commit 方法传递参数，此时默认提交的是当前轮询的最大偏移量，如果你需要提交特定的偏移量，可以调用它们的重载方法。12345&#x2F;*同步提交特定偏移量*&#x2F;commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) &#x2F;*异步提交特定偏移量*&#x2F; commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)复制代码需要注意的是，因为你可以订阅多个主题，所以 offsets 中必须要包含所有主题的每个分区的偏移量，示例代码如下：123456789101112131415161718try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#x2F;*记录每个主题的每个分区的偏移量*&#x2F; TopicPartition topicPartition &#x3D; new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata &#x3D; new OffsetAndMetadata(record.offset()+1, &quot;no metaData&quot;); &#x2F;*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*&#x2F; offsets.put(topicPartition, offsetAndMetadata); &#125; &#x2F;*提交特定偏移量*&#x2F; consumer.commitAsync(offsets, null); &#125;&#125; finally &#123; consumer.close();&#125;复制代码五、监听分区再均衡因为分区再均衡会导致分区与消费者的重新划分，有时候你可能希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 subscribe 的重载方法传入自定义的分区再均衡监听器。12345&#x2F;*订阅指定集合内的所有主题*&#x2F;subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#x2F;*使用正则匹配需要订阅的主题*&#x2F; subscribe(Pattern pattern, ConsumerRebalanceListener listener) 复制代码代码示例如下：123456789101112131415161718192021222324252627282930Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets &#x3D; new HashMap&lt;&gt;();consumer.subscribe(Collections.singletonList(topic), new ConsumerRebalanceListener() &#123; &#x2F;*该方法会在消费者停止读取消息之后，再均衡开始之前就调用*&#x2F; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; System.out.println(&quot;再均衡即将触发&quot;); &#x2F;&#x2F; 提交已经处理的偏移量 consumer.commitSync(offsets); &#125; &#x2F;*该方法会在重新分配分区之后，消费者开始读取消息之前被调用*&#x2F; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; &#125;&#125;);try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); TopicPartition topicPartition &#x3D; new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata &#x3D; new OffsetAndMetadata(record.offset() + 1, &quot;no metaData&quot;); &#x2F;*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*&#x2F; offsets.put(topicPartition, offsetAndMetadata); &#125; consumer.commitAsync(offsets, null); &#125;&#125; finally &#123; consumer.close();&#125;复制代码六 、退出轮询Kafka 提供了 consumer.wakeup() 方法用于退出轮询，它通过抛出 WakeupException 异常来跳出循环。需要注意的是，在退出线程时最好显示的调用 consumer.close() , 此时消费者会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡 ，而不需要等待会话超时。下面的示例代码为监听控制台输出，当输入 exit 时结束轮询，关闭消费者并退出程序：1234567891011121314151617181920212223242526272829303132&#x2F;*调用 wakeup 优雅的退出*&#x2F;final Thread mainThread &#x3D; Thread.currentThread();new Thread(() -&gt; &#123; Scanner sc &#x3D; new Scanner(System.in); while (sc.hasNext()) &#123; if (&quot;exit&quot;.equals(sc.next())) &#123; consumer.wakeup(); try &#123; &#x2F;*等待主线程完成提交偏移量、关闭消费者等操作*&#x2F; mainThread.join(); break; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;).start();try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; rd : records) &#123; System.out.printf(&quot;topic &#x3D; %s,partition &#x3D; %d, key &#x3D; %s, value &#x3D; %s, offset &#x3D; %d,\\n&quot;, rd.topic(), rd.partition(), rd.key(), rd.value(), rd.offset()); &#125; &#125;&#125; catch (WakeupException e) &#123; &#x2F;&#x2F;对于 wakeup() 调用引起的 WakeupException 异常可以不必处理&#125; finally &#123; consumer.close(); System.out.println(&quot;consumer 关闭&quot;);&#125;复制代码七、独立的消费者因为 Kafka 的设计目标是高吞吐和低延迟，所以在 Kafka 中，消费者通常都是从属于某个群组的，这是因为单个消费者的处理能力是有限的。但是某些时候你的需求可能很简单，比如可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，这个时候就不需要消费者群组和再均衡了， 只需要把主题或者分区分配给消费者，然后开始读取消息井提交偏移量即可。在这种情况下，就不需要订阅主题， 取而代之的是消费者为自己分配分区。 一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 分配分区的示例代码如下：12345678910111213141516171819List&lt;TopicPartition&gt; partitions &#x3D; new ArrayList&lt;&gt;();List&lt;PartitionInfo&gt; partitionInfos &#x3D; consumer.partitionsFor(topic);&#x2F;*可以指定读取哪些分区 如这里假设只读取主题的 0 分区*&#x2F;for (PartitionInfo partition : partitionInfos) &#123; if (partition.partition()&#x3D;&#x3D;0)&#123; partitions.add(new TopicPartition(partition.topic(), partition.partition())); &#125;&#125;&#x2F;&#x2F; 为消费者指定分区consumer.assign(partitions);while (true) &#123; ConsumerRecords&lt;Integer, String&gt; records &#x3D; consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;Integer, String&gt; record : records) &#123; System.out.printf(&quot;partition &#x3D; %s, key &#x3D; %d, value &#x3D; %s\\n&quot;, record.partition(), record.key(), record.value()); &#125; consumer.commitSync();&#125;复制代码附录 : Kafka消费者可选属性1. fetch.min.byte消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。2. fetch.max.wait.msbroker 返回给消费者数据的等待时间，默认是 500ms。3. max.partition.fetch.bytes该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。4. session.timeout.ms消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。5. auto.offset.reset该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。6. enable.auto.commit是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。7. client.id客户端 id，服务器用来识别消息的来源。8. max.poll.records单次调用 poll() 方法能够返回的记录数量。9. receive.buffer.bytes &amp; send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。转载自Kafka 系列（四）—— Kafka 消费者详解","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"Spark OOM解决方案","slug":"Spark-OOM解决方案","date":"2019-02-27T11:16:38.000Z","updated":"2020-06-28T11:18:37.972Z","comments":true,"path":"2019/02/27/Spark-OOM解决方案/","link":"","permalink":"cpeixin.cn/2019/02/27/Spark-OOM%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"","text":"OOM场景Spark中的OOM问题不外乎以下两种情况Driver 内存不够读取数据太大数据回传Executor 内存不够map执行中内存溢出shuffle后内存溢出map执行中内存溢出代表了所有map类型的操作，包括：flatMap，filter，mapPatitions等。shuffle后内存溢出的shuffle操作包括join，reduceByKey，repartition等操作。对于Drive端的内存溢出，我们要谨慎使用collect等操作，将所有executor的数据聚合到driver导致。尽量不要使用collect操作即可。Spark 内存模型Spark在一个Executor中的内存分为三块，一块是execution内存，一块是storage内存，一块是other内存。execution内存是执行内存，主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据，满了再写入磁盘，能够减少IO。其实map过程也是在这个内存中执行的。storage内存主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；other内存是程序执行时预留给自己的内存。execution和storage是Spark Executor中内存的大户，占到了堆内内存的60%，other占用内存相对少很多，这里就不说了。在spark-1.6.0以前的版本，execution和storage的内存分配是固定的，使用的参数配置分别是spark.shuffle.memoryFraction（execution内存占Executor总内存大小，default 0.2）和spark.storage.memoryFraction（storage内存占Executor内存大小，default 0.6），因为是1.6.0以前这两块内存是互相隔离的，这就导致了Executor的内存利用率不高，而且需要根据Application的具体情况，使用者自己来调节这两个参数才能优化Spark的内存使用。在spark-1.6.0以上的版本，execution内存和storage内存可以相互借用，提高了内存的Spark中内存的使用率，同时也减少了OOM的情况。在Spark-1.6.0后加入了堆外内存，进一步优化了Spark的内存使用，堆外内存使用JVM堆以外的内存，不会被gc回收，可以减少频繁的full gc，所以在Spark程序中，会长时间逗留再Spark程序中的大内存对象可以使用堆外内存存储。使用堆外内存有两种方式：一种是在rdd调用persist的时候传入参数StorageLevel.OFF_HEAP，这种使用方式需要配合Tachyon一起使用。另外一种是使用Spark自带的spark.memory.offHeap.enabled 配置为true进行使用OOM的问题通常出现在execution这块内存中，因为storage这块内存在存放数据满了之后，会直接丢弃内存中旧的数据，对性能有影响但是不会有OOM的问题。内存溢出解决方法map过程产生大量对象导致内存溢出这种溢出的原因是在单个map中产生了大量的对象导致的，例如：1rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：1rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区，不会有shuffle的过程。数据不平衡导致内存溢出数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。coalesce调用导致内存溢出这是我最近才遇到的一个问题，因为hdfs中不适合存小问题，所以Spark计算后如果产生的文件太小，我们会调用coalesce合并文件再存入hdfs中。但是这会导致一个问题，例如在coalesce之前有100个文件，这也意味着能够有100个Task，现在调用coalesce(10)，最后只产生10个文件，因为coalesce并不是shuffle操作，这意味着coalesce并不是按照我原本想的那样先执行100个Task，再将Task的执行结果合并成10个，而是从头到位只有10个Task在执行，原本100个文件是分开执行的，现在每个Task同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。解决这个问题的方法是令程序按照我们想的先执行100个Task再将结果合并成10个文件，这个问题同样可以通过repartition解决，调用repartition(10)，因为这就有一个shuffle的过程，shuffle前后是两个Stage，一个100个分区，一个是10个分区，就能按照我们的想法执行。shuffle后内存溢出shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ，** spark.default.parallelism参数只对HashPartitioner有效**，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。standalone模式下资源分配不均匀导致内存溢出在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。在RDD中，共用对象能够减少OOM的情况这个比较特殊，这里说记录一下，遇到过一种情况，类似这样rdd.flatMap(x=&gt;for(i &lt;- 1 to 1000) yield (“key”,”value”))导致OOM，但是在同样的情况下，使用rdd.flatMap(x=&gt;for(i &lt;- 1 to 1000) yield “key”+”value”)就不会有OOM的问题，这是因为每次(“key”,”value”)都产生一个Tuple对象，而”key”+”value”，不管多少个，都只有一个对象，指向常量池。具体测试如下：这个例子说明(“key”,”value”)和(“key”,”value”)在内存中是存在不同位置的,也就是存了两份,但是”key”+”value”虽然出现了两次,但是只存了一份,在同一个地址,这用到了JVM常量池的知识.于是乎,如果RDD中有大量的重复数据,或者Array中需要存大量重复数据的时候我们都可以将重复数据转化为String,能够有效的减少内存使用.代码层面优化使用mapPartitions代替大部分map操作，或者连续使用的map操作这里需要稍微讲一下RDD和DataFrame的区别。RDD强调的是不可变对象，每个RDD都是不可变的，当调用RDD的map类型操作的时候，都是产生一个新的对象，这就导致了一个问题，如果对一个RDD调用大量的map类型操作的话，每个map操作会产生一个到多个RDD对象，这虽然不一定会导致内存溢出，但是会产生大量的中间数据，增加了gc操作。另外RDD在调用action操作的时候，会出发Stage的划分，但是在每个Stage内部可优化的部分是不会进行优化的，例如rdd.map(+1).map(+1)，这个操作在数值型RDD中是等价于rdd.map(_+2)的，但是RDD内部不会对这个过程进行优化。DataFrame则不同，DataFrame由于有类型信息所以是可变的，并且在可以使用sql的程序中，都有除了解释器外，都会有一个sql优化器，DataFrame也不例外，有一个优化器Catalyst上面说到的这些RDD的弊端，有一部分就可以使用mapPartitions进行优化，mapPartitions可以同时替代rdd.map,rdd.filter,rdd.flatMap的作用，所以在长操作中，可以在mapPartitons中将RDD大量的操作写在一起，避免产生大量的中间rdd对象，另外是mapPartitions在一个partition中可以复用可变类型，这也能够避免频繁的创建新对象。使用mapPartitions的弊端就是牺牲了代码的易读性。broadcast join和普通join在大数据分布式系统中，大量数据的移动对性能的影响也是巨大的。基于这个思想，在两个RDD进行join操作的时候，如果其中一个RDD相对小很多，可以将小的RDD进行collect操作然后设置为broadcast变量，这样做之后，另一个RDD就可以使用map操作进行join，这样能够有效的减少相对大很多的那个RDD的数据移动。先filter在join这个就是谓词下推，这个很显然，filter之后再join，shuffle的数据量会减少，这里提一点是spark-sql的优化器已经对这部分有优化了，不需要用户显示的操作，个人实现rdd的计算的时候需要注意这个。partitonBy优化combineByKey的使用这个操作在Map-Reduce中也有，这里举个例子：rdd.groupByKey().mapValue(_.sum)比rdd.reduceByKey的效率低，原因如下两幅图所示:上下两幅图的区别就是上面那幅有combineByKey的过程减少了shuffle的数据量，下面的没有。combineByKey是key-value型rdd自带的API，可以直接使用。在内存不足的使用，使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache():rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。在spark使用hbase的时候，spark和hbase搭建在同一个集群在spark结合hbase的使用中，spark和hbase最好搭建在同一个集群上上，或者spark的集群节点能够覆盖hbase的所有节点。hbase中的数据存储在HFile中，通常单个HFile都会比较大，另外Spark在读取Hbase的数据的时候，不是按照一个HFile对应一个RDD的分区，而是一个region对应一个RDD分区。所以在Spark读取Hbase的数据时，通常单个RDD都会比较大，如果不是搭建在同一个集群，数据移动会耗费很多的时间。参数层面优化*–driver-memory MEM *造成 Driver 内存溢出,解决思路是增加 Driver 内存，具体做法为设置参数spark.driver.memory (default 1g)这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。*spark.rdd.compress (default false) *这个参数在内存吃紧的时候，又需要persist数据有良好的性能，就可以设置这个参数为true，这样在使用persist(StorageLevel.MEMORY_ONLY_SER)的时候，就能够压缩内存中的rdd数据。减少内存消耗，就是在使用的时候会占用CPU的解压时间。spark.serializer (default org.apache.spark.serializer.JavaSerializer )建议设置为 org.apache.spark.serializer.KryoSerializer，Kryo序列化机制比Java序列化机制性能提高10倍左右，Spark之所以没有默认使用Kryo作为序列化类库，是因为它不支持所有对象的序列化，同时Kryo需要用户在使用前注册需要序列化的类型，不够方便12345678910import com.esotericsoftware.kryo.Kryo;import org.apache.spark.serializer.KryoRegistrator;public class DemoRegistrator implements KryoRegistrator &#123; @Override public void registerClasses (Kryo kryo) &#123; //以下为示例类，需注册自定义类 kryo.register(AggrateKey.class); kryo.register(AggrateValue.class); &#125;&#125;123val conf = new SparkConf()conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .set(\"spark.kryo.registrator\", \"com.etl.common.DemoRegistrator\")spark.memory.storageFraction (default 0.5)这个参数设置内存表示 Executor内存中 storage/(storage+execution)，虽然spark-1.6.0+的版本内存storage和execution的内存已经是可以互相借用的了，但是借用和赎回也是需要消耗性能的，所以如果明知道程序中storage是多是少就可以调节一下这个参数。spark.locality.wait (default 3s)spark中有4种本地化执行level，PROCESS_LOCAL-&gt;NODE_LOCAL-&gt;RACK_LOCAL-&gt;ANY,一个task执行完，等待spark.locality.wait时间如果，第一次等待PROCESS的Task到达，如果没有，等待任务的等级下调到NODE再等待spark.locality.wait时间，依次类推，直到ANY。分布式系统是否能够很好的执行本地文件对性能的影响也是很大的。如果RDD的每个分区数据比较多，每个分区处理时间过长，就应该把 spark.locality.wait 适当调大一点，让Task能够有更多的时间等待本地数据。特别是在使用persist或者cache后，这两个操作过后，在本地机器调用内存中保存的数据效率会很高，但是如果需要跨机器传输内存中的数据，效率就会很低。spark.speculation (default false)一个大的集群中，每个节点的性能会有差异，spark.speculation这个参数表示空闲的资源节点会不会尝试执行还在运行，并且运行时间过长的Task，避免单个节点运行速度过慢导致整个任务卡在一个节点上。这个参数最好设置为true。与之相配合可以一起设置的参数有spark.speculation.×开头的参数。参考中有文章详细说明这个参数。最后，关于OOM的问题，还是要具体问题具体分析，排查日志，找到问题所在，基本上就是围绕这加内存，调整并行度，代码调优，参数调优四个方面来进行。参考链接：https://blog.csdn.net/yhb315279058/article/details/51035631","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"创建hive表时格式如何选择","slug":"创建hive表时格式如何选择","date":"2019-02-10T09:24:25.000Z","updated":"2020-06-14T09:26:39.186Z","comments":true,"path":"2019/02/10/创建hive表时格式如何选择/","link":"","permalink":"cpeixin.cn/2019/02/10/%E5%88%9B%E5%BB%BAhive%E8%A1%A8%E6%97%B6%E6%A0%BC%E5%BC%8F%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9/","excerpt":"","text":"本文转载自https://www.cnblogs.com/barneywill/p/10109508.html偶然看到这篇文章，这位老哥做的对比测试很好，当初我们在新数仓设计时，都没有对比测试的这么详细。常用格式textfile需要定义分隔符，占用空间大，读写效率最低，非常容易发生冲突（分隔符）的一种格式，基本上只有需要导入数据的时候才会使用，比如导入csv文件；ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘\\u0001’LINES TERMINATED BY ‘\\n’STORED AS TEXTFILEjsonhive3.0后官方支持json格式，之前需要使用第三方，导入jar，http://www.congiu.net/hive-json-serde/，add jar hdfs://nn/jarpath/json-udf-1.3.8-jar-with-dependencies.jar;add jar hdfs://nn/jarpath/json-serde-1.3.8-jar-with-dependencies.jar;占用空间最大，读写效率低，基本上只有需要导入数据的时候才会使用，比如导入json文件；ROW FORMAT SERDE ‘org.openx.data.jsonserde.JsonSerDe’STORED AS TEXTFILExmlhttp://central.maven.org/maven2/com/ibm/spss/hive/serde2/xml/hivexmlserde/1.0.0.0/hivexmlserde-1.0.0.0.jar12345678910111213CREATE TABLE xml_bank(customer_id STRING, income BIGINT, demographics map&lt;string,string&gt;, financial map&lt;string,string&gt;) ROW FORMAT SERDE 'com.ibm.spss.hive.serde2.xml.XmlSerDe' WITH SERDEPROPERTIES ( \"column.xpath.customer_id\"=\"/record/@customer_id\", \"column.xpath.income\"=\"/record/income/text()\", \"column.xpath.demographics\"=\"/record/demographics/*\", \"column.xpath.financial\"=\"/record/financial/*\" ) TBLPROPERTIES ( \"xmlinput.start\"=\"&lt;record customer\", \"xmlinput.end\"=\"&lt;/record&gt;\" );lzo相比textfile多了lzo压缩，占用空间更小；ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘\\t’STORED AS INPUTFORMAT‘com.hadoop.mapred.DeprecatedLzoTextInputFormat’OUTPUTFORMAT‘org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat’orc列式存储，占用空间最小，非常适合用来做数仓；STORED AS ORC压缩STORED AS ORC TBLPROPERTIES (“orc.compression”=”ZLIB”)STORED AS ORC TBLPROPERTIES (“orc.compression”=”SNAPPY”)注意设置orc压缩格式前一定要先设置：set hive.exec.orc.compression.strategy=COMPRESSION;否则压缩不生效；parquet列式存储，占用空间居中，如果后期使用spark来处理，parquet是最佳格式；STORED AS PARQUETparquet+snappySTORED AS PARQUET TBLPROPERTIES (“parquet.compression”=”SNAPPY”)对比测试测试表：test_table测试行数：10亿测试sql类型：aggregation测试sql：select col_1, count(1) from test_table group by col_1;测试结果fshdfskuduformattextfilelzoparquetparquet snappyorcorc snappy** **capacity464.0 G169.4 G177.2 G111.3 G71.5 G65.7G184 G100%36%37%23%15%14%39%Hive2.3.4816 s711 s250 s158 s130 s127 sHive2.3.4Tuning251 s163 s109 s96 sHive2.3.4Onspark2.4.054 s47 s149 s138 sSpark2.1.1371 s293 s17 s16 s51 sSpark2.4.0496 s297 s16 s16 s21 s21 sDrill1.15.059 s57 s75 s45 sImpala2.1215 s16 sPresto0.21525 s21 s13 s12 s** **** **从数据大小和查询效率上看，表现最好的是presto+orc+snappy；hive下最佳格式为orc snappy，数据大小最小，并且查询最快；hive切换engine为spark后，对parquet格式的查询有一些提升，但是占用相同资源的情况下，远不如直接使用spark sql快；spark2.3以后对orc格式相比之前有很大优化，已经很接近parquet格式；impala+parquet+hdfs的性能和impala+kudu差不多，kudu的好处是支持实时更新；drill看起来没有必要；spark2.4.0中的parquet为2.4，parquet从2.5开始支持column index，预计以后的spark版本对parquet的查询会更快；impala对orc的支持从3.1开始作为实验功能的一部分；详细数据yarn 200g 50core1 hive-2.3.412345678910111213141516171819202122232425262728293031323334set mapreduce.map.memory.mb=4096;set mapreduce.map.java.opts=-Xmx3072m;hive-textfile:Time taken: 816.202 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 1831 Reduce: 1009 Cumulative CPU: 27614.77 sec HDFS Read: 498267775168 HDFS Write: 88861 SUCCESSTotal MapReduce CPU Time Spent: 0 days 7 hours 40 minutes 14 seconds 770 msechive-lzo:Time taken: 711.266 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 183 Reduce: 711 Cumulative CPU: 13949.24 sec HDFS Read: 181881436157 HDFS Write: 62935 SUCCESSTotal MapReduce CPU Time Spent: 0 days 3 hours 52 minutes 29 seconds 240 msechive-orc:Time taken: 130.194 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 275 Reduce: 300 Cumulative CPU: 4368.67 sec HDFS Read: 626004573 HDFS Write: 27178 SUCCESSTotal MapReduce CPU Time Spent: 0 days 1 hours 12 minutes 48 seconds 670 msechive-orc snappy:Time taken: 127.803 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 191 Reduce: 276 Cumulative CPU: 4374.74 sec HDFS Read: 580889407 HDFS Write: 25090 SUCCESSTotal MapReduce CPU Time Spent: 0 days 1 hours 12 minutes 54 seconds 740 msechive-orc-tuning:Time taken: 109.539 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 275 Reduce: 300 Cumulative CPU: 3051.67 sec HDFS Read: 627064673 HDFS Write: 40321 SUCCESSTotal MapReduce CPU Time Spent: 50 minutes 51 seconds 670 msechive-orc snappy-tuning:Time taken: 94.135 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 191 Reduce: 276 Cumulative CPU: 2393.92 sec HDFS Read: 581727151 HDFS Write: 37201 SUCCESSTotal MapReduce CPU Time Spent: 39 minutes 53 seconds 920 msechive-parquet:Time taken: 250.786 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 642 Reduce: 744 Cumulative CPU: 10919.85 sec HDFS Read: 873784253 HDFS Write: 65806 SUCCESSTotal MapReduce CPU Time Spent: 0 days 3 hours 1 minutes 59 seconds 850 msechive-parquet snappy:Time taken: 158.009 seconds, Fetched: 32 row(s)Stage-Stage-1: Map: 367 Reduce: 467 Cumulative CPU: 6246.0 sec HDFS Read: 721915438 HDFS Write: 41707 SUCCESSTotal MapReduce CPU Time Spent: 0 days 1 hours 44 minutes 6 seconds 0 msec ### 2 hive-2.3.4 on spark-2.4.01234567891011set spark.driver.memory=4g;set spark.executor.memory=4g;set spark.executor.instances=10;hive on spark-parquet:Time taken: 54.446 seconds, Fetched: 32 row(s)hive on spark-parquet snappy:Time taken: 47.364 seconds, Fetched: 32 row(s)hive on spark-orc:Time taken: 149.901 seconds, Fetched: 32 row(s)hive on spark-orc snappy:Time taken: 138.844 seconds, Fetched: 32 row(s) ### 3 impala-2.121234567891011121314MEM_LIMIT=20g * 3impala-parquet snappy:Fetched 32 row(s) in 15.10s+--------------+--------+----------+----------+-------+------------+-----------+---------------+---------------------------------------------------+| Operator | #Hosts | Avg Time | Max Time | #Rows | Est. #Rows | Peak Mem | Est. Peak Mem | Detail |+--------------+--------+----------+----------+-------+------------+-----------+---------------+---------------------------------------------------+| 04:EXCHANGE | 1 | 211.45us | 211.45us | 32 | 50 | 208.00 KB | 0 B | UNPARTITIONED || 03:AGGREGATE | 3 | 2.58ms | 2.91ms | 32 | 50 | 34.03 MB | 128.00 MB | FINALIZE || 02:EXCHANGE | 3 | 29.23us | 30.92us | 96 | 1.04B | 32.00 KB | 0 B | HASH(cpp_addr_province) || 01:AGGREGATE | 3 | 13.29s | 13.97s | 96 | 1.04B | 34.05 MB | 128.00 MB | STREAMING || 00:SCAN HDFS | 3 | 723.09ms | 760.01ms | 1.04B | 1.04B | 36.55 MB | 88.00 MB | temp.app_ba_userprofile_prop_nonpolar_view_ext_ps |+--------------+--------+----------+----------+-------+------------+-----------+---------------+---------------------------------------------------+impala-kudu:Fetched 32 row(s) in 15.61s ### 4 drill-1.1512345678910g+10g+1g+1g * 3drill-parquet:32 rows selected (59.501 seconds)drill-parquet snappy:32 rows selected (57.653 seconds)drill-orc:32 rows selected (75.749 seconds)drill-orc snappy:32 rows selected (45.323 seconds)5 spark-sql –master yarn –num-executors 10 –executor-memory 4g –driver-memory 4g5.1 spark-2.1.112345678910spark sql-textfile:Time taken: 371.77 seconds, Fetched 32 row(s)spark sql-lzo:Time taken: 293.391 seconds, Fetched 32 row(s)spark sql-parquet:Time taken: 17.338 seconds, Fetched 32 row(s)spark sql-parquet snappy:Time taken: 16.609 seconds, Fetched 32 row(s)spark sql-orc:Time taken: 51.959 seconds, Fetched 32 row(s)5.2 spark-2.4.0123456789101112spark sql-textfile:Time taken: 496.395 seconds, Fetched 32 row(s)spark sql-lzo:Time taken: 297.142 seconds, Fetched 32 row(s)spark sql-parquet:Time taken: 16.728 seconds, Fetched 32 row(s)spark sql-parquet snappy:Time taken: 16.879 seconds, Fetched 32 row(s)spark sql-orc:Time taken: 21.432 seconds, Fetched 32 row(s)spark sql-orc snappy:Time taken: 21.935 seconds, Fetched 32 row(s)6 presto123456789101112presto-parquet:Splits: 3,182 total, 3,182 done (100.00%)0:25 [1.04B rows, 612MB] [42.2M rows/s, 24.9MB/s]presto-parquet snappy:Splits: 2,088 total, 2,088 done (100.00%)0:21 [1.04B rows, 584MB] [49.3M rows/s, 27.8MB/spresto-orc：Splits: 1,532 total, 1,532 done (100.00%)0:13 [1.04B rows, 850MB] [81.7M rows/s, 66.8MB/s]presto-orc snappy：Splits: 1,353 total, 1,353 done (100.00%)0:12 [1.04B rows, 1.13GB] [87.5M rows/s, 97.4MB/s]","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"Structured Streaming 重温","slug":"Structured-Streaming-重温","date":"2019-02-10T08:03:44.000Z","updated":"2020-09-13T14:00:30.333Z","comments":true,"path":"2019/02/10/Structured-Streaming-重温/","link":"","permalink":"cpeixin.cn/2019/02/10/Structured-Streaming-%E9%87%8D%E6%B8%A9/","excerpt":"","text":"总览_Structured Streaming 则是在 Spark 2.0 加入的经过重新设计的全新流式引擎。它的模型十分简洁，易于理解。一个流的数据源从逻辑上来说就是一个不断增长的动态表格，随着时间的推移，新数据被持续不断地添加到表格的末尾。用户可以使用 Dataset/DataFrame 或者 SQL 来对这个动态数据源进行实时查询。每次查询在逻辑上就是对当前的表格内容执行一次 SQL 查询。如何执行查询则是由用户通过触发器（Trigger）来设定。用户既可以设定定期执行，也可以让查询尽可能快地执行，从而达到实时的效果。最后，系统通过 checkpointing 和 Write-Ahead Logs来确保端到端的一次容错保证。一个流的输出有多种模式，既可以是基于整个输入执行查询后的完整结果，也可以选择只输出与上次查询相比的差异，或者就是简单地追加最新的结果。这个模型对于熟悉 SQL 的用户来说很容易掌握，对流的查询跟查询一个表格几乎完全一样。在内部，默认情况下，结构化流查询是使用_微批量处理_引擎_处理的_，该引擎将数据流作为一系列小批量作业进行处理，从而实现了低至100毫秒的端到端延迟以及 exactly-once的容错保证。但是，自Spark 2.3起，我们引入了一种称为“ 连续处理”的新低延迟处理模式，该模式可以实现一次最少保证的低至1毫秒的端到端延迟。在不更改查询中的Dataset / DataFrame操作的情况下，您将能够根据应用程序需求选择模式Structured Streaming 是对 Spark Streaming 的改进么？Structured Streaming 并不是对 Spark Streaming 的简单改进，而是我们吸取了过去几年在开发 Spark SQL 和 Spark Streaming 过程中的经验教训，以及 Spark 社区和 Databricks 众多客户的反馈，重新开发的全新流式引擎，致力于为批处理和流处理提供统一的高性能 API。同时，在这个新的引擎中，我们也很容易实现之前在 Spark Streaming 中很难实现的一些功能，比如 Event Time 的支持，Stream-Stream Join，毫秒级延迟类似于 Dataset/DataFrame 代替 Spark Core 的 RDD 成为为 Spark 用户编写批处理程序的首选，Dataset/DataFrame 也将替代 Spark Streaming 的 DStream，成为编写流处理程序的首选。Structured Streaming 的 Spark 有什么优劣势吗？简洁的模型。Structured Streaming 的模型很简洁，易于理解。用户可以直接把一个流想象成是无限增长的表格。一致的 API。由于和 Spark SQL 共用大部分 API，对 Spaprk SQL 熟悉的用户很容易上手，代码也十分简洁。同时批处理和流处理程序还可以共用代码，不需要开发两套不同的代码，显著提高了开发效率。卓越的性能。Structured Streaming 在与 Spark SQL 共用 API 的同时，也直接使用了 Spark SQL 的 Catalyst 优化器和 Tungsten，数据处理性能十分出色。此外，Structured Streaming 还可以直接从未来 Spark SQL 的各种性能优化中受益。多语言支持。Structured Streaming 直接支持目前 Spark SQL 支持的语言，包括 Scala，Java，Python，R 和 SQL。用户可以选择自己喜欢的语言进行开发。呃～～ 关于Structured Streaming的介绍就说到这里，如果想看更详细更准确的介绍呢，还是乖乖的去官网吧。在2017年10月份的时候，新立项的一个app用户行为实时项目，我有意使用Structured Streaming，所以就调研了一下，自己写了一个demo，我记忆里那时写Structured很别扭，就像一个模版一样，输入源和输出源都被规定好了函数和参数，并且在那时候测试后的时候，不怎么稳定，而且官方并没有给出成熟的版本，当时所测试的功能还都是 alpha 版本，所以当时就还是使用了Spark Streaming不过现在来看，Structured Streaming 越来越成熟，Spark Streaming感觉似乎停止了更新。Structured streaming应该是Spark流处理的未来，但是很难替代Flink。Flink在流处理上的天然优势很难被Spark超越。在读完了Structured Streaming官网后，还是亲手的写一些实例感受一下，以后做架构的时候，如果适合的话，还可以加进来。实例 ### complete, append, update123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.apache.spark.sql.streaming.StreamingQueryimport org.apache.spark.sql.&#123;DataFrame, Dataset, SparkSession&#125;import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.log4j.&#123;Level, Logger&#125;object structured_kafka &#123; val logger:Logger = Logger.getRootLogger Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; case class kafka_format(date_time: String, keyword_list: String) val spark: SparkSession = SparkSession .builder() .appName(\"Structrued-Streaming\") .master(\"local[2]\") .getOrCreate() import spark.implicits._ val kafka_df: DataFrame = spark .readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"localhost:9092\") .option(\"subscribe\", \"weibo_keyword\") .option(\"startingOffsets\", \"earliest\") .option(\"includeTimestamp\", value = true)// .option(\"endingOffsets\", \"latest\")// .option(\"startingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":23,\"1\":-2&#125;,\"topic2\":&#123;\"0\":-2&#125;&#125;\"\"\")// .option(\"endingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":50,\"1\":-1&#125;,\"topic2\":&#123;\"0\":-1&#125;&#125;\"\"\") .load() val keyvalue_df: DataFrame = kafka_df .selectExpr(\"CAST(value AS STRING)\") .as[String] .map((x: String) =&gt; &#123; val date_time: String = JSON.parseObject(x).getString(\"datetime\") val keyword_list: String = JSON.parseObject(x).getString(\"keywordList\") (date_time, keyword_list) &#125;) .flatMap((x: (String, String)) =&gt;&#123; x._2.split(\",\").map((word: String) =&gt;(x._1,word)) &#125;) .toDF(\"date_time\", \"keyword\") .groupBy(\"keyword\").count() .orderBy($\"count\".desc) val query: StreamingQuery = keyvalue_df.writeStream .outputMode(\"complete\") //append .format(\"console\") .start() query.awaitTermination() &#125;&#125;其中需要注意：__Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;__Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;未进行aggregate的stream不能sort__ ### window窗口 _1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import com.alibaba.fastjson.JSONimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.sql.streaming.&#123;StreamingQuery, Trigger&#125;import org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.functions._object structured_kafka_window &#123; val logger:Logger = Logger.getRootLogger Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; case class kafka_format(date_time: String, keyword_list: String) val spark: SparkSession = SparkSession .builder() .appName(\"Structrued-Streaming\") .master(\"local[2]\") .getOrCreate() import spark.implicits._ val kafka_df: DataFrame = spark .readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"localhost:9092\") .option(\"subscribe\", \"weibo_keyword\") .option(\"startingOffsets\", \"latest\") .option(\"includeTimestamp\", value = true)// .option(\"endingOffsets\", \"latest\")// .option(\"startingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":23,\"1\":-2&#125;,\"topic2\":&#123;\"0\":-2&#125;&#125;\"\"\")// .option(\"endingOffsets\", \"\"\"&#123;\"topic1\":&#123;\"0\":50,\"1\":-1&#125;,\"topic2\":&#123;\"0\":-1&#125;&#125;\"\"\") .load() val keyvalue_df: DataFrame = kafka_df .selectExpr(\"CAST(value AS STRING)\") .as[String] .map((x: String) =&gt; &#123; val date_time: String = JSON.parseObject(x).getString(\"datetime\") val keyword_list: String = JSON.parseObject(x).getString(\"keywordList\") (date_time, keyword_list) &#125;) .flatMap((x: (String, String)) =&gt;&#123; x._2.split(\",\").map((word: String) =&gt;(x._1,word)) &#125;) .toDF(\"date_time\", \"keyword\") .groupBy(window($\"date_time\", \"5 minutes\", \"1 minutes\"),$\"keyword\") .count() .orderBy(\"window\") val query: StreamingQuery = keyvalue_df.writeStream .outputMode(\"complete\") //append .format(\"console\") .option(\"truncate\", \"false\") .trigger(Trigger.ProcessingTime(\"5 seconds\")) .start() query.awaitTermination() &#125;&#125;这里关于window窗口的划分，我建议大家好好的研读一下源码：**位置：package **org.apache.spark.sql.catalyst.analysis![11.29.50.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1588952005543-007c1291-12fd-4148-a625-587b1a6149f3.png#align=left&display=inline&height=1694&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-05-08%20%E4%B8%8B%E5%8D%8811.29.50.png&originHeight=1694&originWidth=1936&size=488433&status=done&style=none&width=1936) ### Watermark12345678910111213// 基于event-time的window，words包含timestamp和word两列word .withWatermark(\"timestamp\", \"30 minutes\")//某窗口结果为x，但是部分数据在这个窗口的最后一个timestamp过后还没到达，Spark在这会等30min，过后就不再更新x了。 .dropDuplicates(\"User\", \"timestamp\") .groupBy(window(col(\"timestamp\"), \"10 minutes\"),col(\"User\"))// 10min后再加一个参数变为Sliding windows，表示每隔多久计算一次。 .count() .writeStream .queryName(\"events_per_window\") .format(\"memory\") .outputMode(\"complete\") .start()spark.sql(\"SELECT * FROM events_per_window\")watermark = max event time seen by the engine - late threshold，相当于Flink的BoundedOutOfOrdernessTximestampExtractor。在window计算被触发时，Spark会删除结束时间低于当前wm的window的中间结果，属于该window的迟到数据“可能”会被忽略，越迟越可能被忽略，删除完后才更新wm，所以即便下一批没有数据加入，Spark所依据的wm也是新的，下下一批wm不变。上面是update mode，如果是append模式，那么结果要等到trigger后发现window的结束时间低于更新后的水位线时才会出来。另外，max event time seen by the engine - late threshold机制意味着如果下一批计算没有更晚的数据加入，那么wm就不会前进，那么数据的append就会被延后。Conditions for watermarking to clean aggregation state(as of Spark 2.1.1, subject to change in the future)不支持complete模式。groupBy必须包含timestamp列或者window(col(timestamp))，withWatermark中的列要和前面的timestamp列相同顺序必须是先withWatermark再到groupBy参考：结构化流编程指南Spark之Structured Streaming","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"来聊一聊存储格式","slug":"来聊一聊存储格式","date":"2019-02-04T03:51:23.000Z","updated":"2020-09-04T03:52:30.520Z","comments":true,"path":"2019/02/04/来聊一聊存储格式/","link":"","permalink":"cpeixin.cn/2019/02/04/%E6%9D%A5%E8%81%8A%E4%B8%80%E8%81%8A%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"在Hadoop生态圈的快速发展过程中，涌现了一批开源的数据分析引擎，例如Hive、Spark SQL、Impala、Presto等，同时也产生了多个高性能的列式存储格式，例如RCFile、ORC、Parquet等Google DremelGoogle 在 2004-2006 年期间发表了著名的“三驾马车”论文，开启了大数据时代。在 2010 年，Google 又发表了 3 篇论文，被称为 Google 的“新三驾马车”，可见其分量之重，其中一篇《Dremel: Interactive Analysis of Web-Scale Datasets》，提出了列式存储与多级执行树，文中介绍了 Google 运用 Dremel 分析来自互联网的千亿条级别数据的实践。与论文中提出的列式存储相比，行式存储可以看成是一个行的集合，其中每一行都要求对齐，哪怕某个字段为空（下图中的左半部分），而列式存储则可以看成一个列的集合（下图中的右半部分）。列式存储的优点很明显，主要有以下 4 点：查询时可以只读取涉及的列（选择操作），并且列可以直接作为索引，非常高效，而行式存储则必须读入整行。列式存储的投影操作非常高效。在数据稀疏的情况下，压缩率比行式存储高很多，甚至可以考虑将相关的表进行预先连接，来完全避免投影操作。因为可以直接作用于某一列上，聚合分析非常迅速。行式存储一般擅长的是插入与更新操作，而列式存储一般适用于数据为只读的场景。对于结构化数据，列式存储并不陌生。因此，列式存储技术经常用于传统数据仓库中。下图分别展示了行式存储和列式存储的区别。在文章中，Dremel 在一开始就指出其面对的是只读的嵌套数据，而嵌套数据属于半结构化数据，例如 JSON、XML，所以 Dremel 的创新之处在于提出了一种支持嵌套数据的列式存储，而如今互联网上的数据又正好多是嵌套结构。下图左边是一个嵌套的 Schema，而右边的 r1、r2 为两条样例记录：这个 Schema 其实可以转换为一个树形结构，如下图所示。该树结构有 6 个叶子节点，可以看到叶子节点其实就是 Schema 中的基本数据类型，如果将这种嵌套结构的数据展平，那么展平后的表应该有 6 列。如果要应用列式存储来存储这种嵌套结构，还需要解决一个问题，我们看到 r1、r2 的数据结构还是差别非常大，所以需要标识出哪些列的值组成一条完整的记录，但我们不可能为每条记录都维护一个树结构。Google 提出的 record shredding and assembly algorithm 算法很好地解决了这个问题，该算法规定，在保存字段值时，还需要额外存储两个数字，分别表示 Repetition level（r）和 Definition level（d）其中，Repetition level 值记录了当前值属于哪一条记录以及它处于该记录的什么位置；另外对于 repeated 和 optional 类型的列，可能一条记录中某一列是没有值的，如果不进行标识就会导致本该属于下一条记录的值被当作当前记录的一部分，对于这种情况就需要用 Definition level 来标识这种情况，通过 Striping &amp; Assembly 算法我们可以将一整条记录还原出来，如下图所示。这样就能用尽可能少的存储空间来表达复杂的嵌套数据格式了。Dremel 的另外一个组成部分是查询执行树，利用这种架构，Dremel 可以用很低的延迟分析大量数据，这使得 Dremel 非常适合进行交互式分析。Dremel 有很多种开源实现，与 Dremel 一样，它主要分为两部分，一个实现了 Dremel 的嵌套列式存储，如 Apache Parquet、Apache ORC，还有一些实现了 Dremel 的查询执行架构，也就是多级执行树，如 Apache Impala、Aapche Drill 与 Presto。这里要特别说明的是，多级执行树这种技术与 Spark 这种 MapReduce 类型的计算框架完全不同，它类似于一种大规模并行处理，希望以较低的延迟完成查询，所以并行程度要远远大于 Spark，但是每个执行者的性能要远远弱于 Spark。如果把 Spark 看成是对 CPU 核心的抽象，那么多级执行树可以看成是对线程的抽象。基于此，多级执行树 + 列式存储的组合往往用于 OLAP 的场景。Parquet 和 ORC 这两种数据格式和 Json 一样都是自描述数据格式，Spark 很早就支持由 Parquet、ORC 格式的数据直接生成 DataFrame。我们可以非常方便地通过 read 读取器和 write 写入器读取和生成 Parquet 和 ORC 文件。列式存储在选择、投影操作的性能优化提升非常明显查询操作 – 投影问题：假如要查看 小黑的 数学及语文的成绩，如何操作？【行式存储】（1）寻址：在磁盘中找到小黑的一整条记录（包含不需要的信息）；（2）将信息载入内存，在内存中剔除不需要的字段（如，英语成绩）；（3）展示给用户最终结果。【列式存储】（1）寻址：在磁盘中数学列处找到小黑的数学成绩，在语文列处找到小黑的语文成绩，相当于进行了两次磁盘寻址；（2）将信息载入内存，载入内存时没有冗余（不同于行式存储）；（3）展示给用户最终结果。如果只是投影一个属性（少数属性），那么行式存储与列式存储，个人感觉差别是不太大的。但是一旦涉及到多个属性，那么列式存储就处于相对的劣势了。此外，Dremel 的高压缩比率也对 Spark 这种 I/O 密集型作业非常友好。在目前 Hadoop、Spark 体系的数据仓库中，已经很少采用 CSV、TEXT 这种格式了。列式存储的实现Apache ParquetApache Parquet 是 Dremel 的开源实现，它最先是由 Twitter 与 Cloudera 合作开发并开源，和 Impala 配合使用。Parquet 支持几乎 Hadoop 生态圈的所有项目，与数据处理框架、数据结构以及编程语言无关。Parquet文件是以二进制方式存储的，是不可以直接读取和修改的，Parquet文件是自解析的，文件中包括该文件的数据和元数据。在HDFS文件系统和Parquet文件中存在如下几个概念：HDFS块(Block)：它是HDFS上的最小的副本单位，HDFS会把一个Block存储在本地的一个文件并且维护分散在不同的机器上的多个副本，通常情况下一个Block的大小为256M、512M等。HDFS文件(File)：一个HDFS的文件，包括数据和元数据，数据分散存储在多个Block中。行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的。列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。不同的列块可能使用不同的算法进行压缩。页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。通常情况下，在存储Parquet数据的时候会按照HDFS的Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。上图展示了一个Parquet文件的结构，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length存储了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和当前文件的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页，但是在后面的版本中增加。Apache ORCApache ORC（OptimizedRC file）来源于 RC（RecordColumnar file）格式，但目前已基本取代 RC 格式。ORC 提供 ACID 支持、也提供不同级别的索引，如布隆过滤器、列统计信息（数量、最值等），和 Parquet 一样，它也是自描述的数据格式，但与 Parquet 不同的是，ORC 支持多种复杂数据结构，如集合、映射等。ORC 与 Presto 配合使用，效果非常好。Apache CarbonDataCarbonData 是华为开源的一种列式存储格式，是专门为海量数据分析和处理而生的。CarbonData 于 2016 年开源，目前发展非常迅猛，与 Apache Kylin 并列为由国人主导的两个Apache 顶级项目。它的设计初衷源于，在很多时候，对于同样一份数据，处理方式是不同的，比如以下几种处理方式：全表扫描，或者选取几列进行过滤；随机访问，如行键值查询，要求低延迟；ad-hoc 交互式分析，如多维聚合分析、上卷、下钻、切片等。不同的处理方式对于数据格式的需求侧重点是不同的，但 CarbonData 旨在为大数据多样化的分析需求提供一种统一的数据格式。CarbonData 的设计目标为：支持低延迟访问多种数据访问类型；允许在压缩编码过的数据上进行快速查询；确保存储空间的高效性；很好地支持 Hadoop 生态系统；读最优化的列式存储；利用多级索引实现低延迟；支持利用列组来获得基于行的优点；能够对聚合的延迟解码进行字典编码。如下图所示，这是一个 CarbonData 数据文件，也是 HDFS 上的一个数据块，每个文件由 File Header、File Footer 与若干个 Blocklet 组成，其中 File Header 保存了文件版本号、Schema 以及更新时间戳；File Footer 包含了一些统计信息（每个 Blocklet 的最值）、多维索引等。一个 Blocklet 的默认大小为 64MB，包含多个 Column Page Group，Blocklet 可以看成一个表的水平切片，这个表有多少列，就有多少个 Column Page Group，在一个 Column Page Group 中，一列被分为若干个连续文件，每一个文件被称为 Page，一个 Page 默认为 32000 行，如下图所示。这里要特别说明的是，CarbonData 在设计理念上没有采取 Dremel 提出的嵌套的列式存储，而是引入了索引和元数据的设计，但仍然属于列式存储格式。对比测试使用列式存储对 Spark 性能提升的影响是非常巨大的，下面是一份测试结果，包含了对于同样一份数据（368.4G），各种数据格式压缩率的对比，以及一些计算作业耗时的对比：TEXTParquetORCCarbonData压缩后大小368.4G298G148.4G145.8G压缩率100%19.11%59.72%60.42%TEXTParquetORCCarbonDataCount67s116s119s6sGroup By138s75s71s92sJoin231s172s140s95s可以看到 ORC 的压缩率最高，而 CarbonData 在 Spark 批处理这种场景下，性能表现得非常好，是一种非常有前景的技术。列式存储的压缩率如此之高，从第一张图也可以看出原因，列式存储作为列的集合，空间几乎没有多余的浪费。如此高的压缩效率也带来了一个优化思路：可以将若干相关的表预先进行连接，连接而成的表可以看成是一张稀疏的宽表，这张宽表对分析来说就非常友好了，但由于采用了列式存储，所以宽表所占的空间并不是指数上涨而是线性增加。在这种场景下，列式存储使得空间换时间成为可能。映射下推(Project PushDown)说到列式存储的优势，映射下推是最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，所以分区取出每一列的所有值就可以实现TableScan算子，而避免扫描整个表文件内容。在Parquet中原生就支持映射下推，执行查询的时候可以通过Configuration传递需要读取的列的信息，这些列必须是Schema的子集，映射每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。谓词下推(Predicate PushDown)在数据库之类的查询系统中最常用的优化手段就是谓词下推了，通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能，例如”select count(1) from A Join B on A.id = B.id where A.a &gt; 10 and B.b &lt; 100”SQL查询中，在处理Join操作之前需要首先对A和B执行TableScan操作，然后再进行Join，再执行过滤，最后计算聚合函数返回，但是如果把过滤条件A.a &gt; 10和B.b &lt; 100分别移到A表的TableScan和B表的TableScan的时候执行，可以大大降低Join操作的输入数据。无论是行式存储还是列式存储，都可以在将过滤条件在读取一条记录之后执行以判断该记录是否需要返回给调用者，在Parquet做了更进一步的优化，优化的方法时对每一个Row Group的每一个Column Chunk在存储的时候都计算对应的统计信息，包括该Column Chunk的最大值、最小值和空值个数。通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推。在使用Parquet的时候可以通过如下两种策略提升查询性能：1、类似于关系数据库的主键，对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以最大化的利用最大值、最小值实现谓词下推。2、减小行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的I/O负载。性能相比传统的行式存储，Hadoop生态圈近年来也涌现出诸如RC、ORC、Parquet的列式存储格式，它们的性能优势主要体现在两个方面：更高的压缩比，由于相同类型的数据更容易针对不同类型的列使用高效的编码和压缩方式。更小的I/O操作，由于映射下推和谓词下推的使用，可以减少一大部分不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能。上图是展示了使用不同格式存储TPC-H和TPC-DS数据集中两个表数据的文件大小对比，可以看出Parquet较之于其他的二进制文件存储格式能够更有效的利用存储空间，而新版本的Parquet(2.0版本)使用了更加高效的页存储方式，进一步的提升存储空间。上图展示了Twitter在Impala中使用不同格式文件执行TPC-DS基准测试的结果，测试结果可以看出Parquet较之于其他的行式存储格式有较明显的性能提升。上图展示了criteo公司在Hive中使用ORC和Parquet两种列式存储格式执行TPC-DS基准测试的结果，测试结果可以看出在数据存储方面，两种存储格式在都是用snappy压缩的情况下量中存储格式占用的空间相差并不大，查询的结果显示Parquet格式稍好于ORC格式，两者在功能上也都有优缺点，Parquet原生支持嵌套式数据结构，而ORC对此支持的较差，这种复杂的Schema查询也相对较差；而Parquet不支持数据的修改和ACID，但是ORC对此提供支持，但是在OLAP环境下很少会对单条数据修改，更多的则是批量导入。Parquet 列式存储带来的性能上的提高在业内已经得到了充分的认可，特别是当你们的表非常宽（column 非常多）的时候，Parquet 无论在资源利用率还是性能上都优势明显。Spark 已经将 Parquet 设为默认的文件存储格式，Cloudera 投入了很多工程师到 Impala+Parquet 相关开发中，Hive/Pig 都原生支持 Parquet。Parquet 现在为 Twitter 至少节省了 1/3 的存储空间，同时节省了大量的表扫描和反序列化的时间。这两方面直接反应就是节约成本和提高性能。如果说 HDFS 是大数据时代文件系统的事实标准的话，Parquet 就是大数据时代存储格式的事实标准。如何选择不同的数据格式考虑因素：读写速度按行读多还是按列读多是否支持文件分割压缩率是否支持schema evolution不同数据格式最佳实践读取少数列可以选择面向列存储的ORC或者Parquet如果需要读取的列比较多，选择AVRO更优如果schema 变更频繁最佳选择avro实际上随着版本不断更新，现在parquet和orc都在一定程度上支持schema evolution，比如在最后面加列ORC的查询性能优于Parquet","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Parquet","slug":"Parquet","permalink":"cpeixin.cn/tags/Parquet/"}]},{"title":"Spark Streaming Exactly once 语义","slug":"Spark-Streaming-Exactly-once-语义","date":"2019-01-28T15:11:36.000Z","updated":"2020-06-28T15:15:07.563Z","comments":true,"path":"2019/01/28/Spark-Streaming-Exactly-once-语义/","link":"","permalink":"cpeixin.cn/2019/01/28/Spark-Streaming-Exactly-once-%E8%AF%AD%E4%B9%89/","excerpt":"","text":"Exactly-OnceExactly-Once不是指对输入的数据只处理一次，指的是, 在流计算引擎中, 算子给下游的结果是Exactly-Once的(即:给下游的结果有且仅有一个，且不重复、不少算)。如在Spark Streaming处理过程中，从一个算子(Operator)到另一个算子(Operator)，可能会因为各种不可抗力如机器挂掉等原因，导致某些Task处理失败，Spark内部会基于Lineage或Checkpoint启动重试Task去重新处理同样的数据。因不可抗力的存在，流处理引擎内部不可能做到一条数据仅被处理一次。所以，当流处理引擎声称提供Exactly-Once语义时，指的是从一个Operator到另一个Operator，同样的数据，无论重复处理多少次，最终的结果状态是Exactly-Once。Spark Streaming保证Exactly-Once语义一个Spark Streaming流处理程序，从广义上讲，包含三个步骤。接收数据:从Source中接收数据。转换数据:用DStream和RDD算子转换。储存数据:将结果保存至外部系统。如果流处理程序需要实现Exactly-Once语义，那么每一个步骤都要保证Exactly-Once。接收数据Exactly-OnceSpark Streaming上游对接kafka时如何保证Exactly Once？简要说一下，Spark Streaming使用Direct模式对接上游kafka。无论kafka有多少个partition， 使用Direct模式总能保证Spark Streaming中有相同数量的partition与之相对， 也就是说Spark Streaming中的KafkaRDD的并发数量在Direct模式下是由上游kafka决定的。在这个模式下，kafka的offset是作为KafkaRDD的一部分存在，会存储在checkpoints中， 由于checkpoints只存储offset内容，而不存储数据，这就使得checkpoints是相对轻的操作。 这就使得Spark Streaming在遇到故障时，可以从checkpoint中恢复上游kafka的offset，从而保证exactly once。这里有个需要注意的地方这里有个需要注意的地方，那就是checkpoint方法在spark中主要有两块应用：一块是在spark core中对RDD做checkpoint，可以切断做checkpoint RDD的依赖关系，将RDD数据保存到可靠存储（如HDFS）以便数据恢复；另外一块是应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理 （如之前waiting batch的job会在重启后继续处理）。1.1 checkpoint 机制能准确保证不重复消费吗？要解决这个问题，需要明确checkpoint这个功能是如何实现的。cache 和checkpoint的区别和联系- Checkpoint 的局限1.2 解决checkpoint自身缺陷的方案？针对这种情况，在我们结合 Spark Streaming + kafka 的应用中， 我们自行维护了消费的 offsets，这样一来即使重新编译 application， 还是可以从需要的 offsets 来消费数据。将 Spark Streaming + Kafka direct 的 offset 存入Zookeeper并重用Spark Streaming Crash 如何保证Exactly Once SemanticsSpark Streaming 与 Kafka 集成分析Exactly-Once Streaming from Kafka1.3 checkpoint错误实现主要出现的问题是针对stream操作的时候，出现在了functionToCreateContext函数的外面；这样的话，会导致下次启动的时候，假如想通过checkpoint启动的话会没法初始化的错误；1val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)https://stackoverflow.com/questions/35090180/why-does-spark-throw-sparkexception-dstream-has-not-been-initialized-when-res下面有个中文的详细的介绍： https://blog.csdn.net/Dax1n/article/details/534256681.4 正确的实现123456789101112131415161718192021222324252627282930313233343536373839404142import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.&#123;SparkConf, TaskContext&#125;import org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject feed_ops &#123; def main(args: Array[String]): Unit = &#123; val checkpointDirectory = \"./check\" def functionToCreateContext(): StreamingContext = &#123; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"feed:alg:test\") val ssc = new StreamingContext(conf, Seconds(1)) ssc.checkpoint(checkpointDirectory) // set checkpoint directory val kafkaParams = Map[String, Object] ( \"bootstrap.servers\" -&gt; \"xxx:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"feed:alg:test\", \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"test\") val stream = KafkaUtils.createDirectStream(ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) stream.map(record =&gt; (record.key, record.value)). filter(x=&gt;x._2.contains(\"kankan_v2\")).filter(x=&gt;x._2.contains(\"NewsList\") || x._2.contains(\"NewsPage\") ). map(x =&gt;(x._1, x._2.split(\"\\u0001\",0).filter(x=&gt;x!=\"\")) ).map(x=&gt;(x._1, x._2.mkString(\"^\"))). print() stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition &#123; iter =&gt; val o:OffsetRange = offsetRanges(TaskContext.get.partitionId) println(\"data: \", iter.map(x=&gt;(x.key, x.value)).take(2).foreach(println)) println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125; &#125; ssc &#125; val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext) ssc.start() ssc.awaitTermination() &#125;&#125;1.5 streaming中checkpoint写流程同样，针对streaming中checkpoint的写流程，主要有以下三个问题，并对此做相关解释。Q1：streaming中checkpoint是在何时做的？A1：在spark streaming中，jobGenerator会定期生成任务（jobGenerator.generateJobs)。在任务生成后将会调用doCheckpoint方法对系统做checkpoint。此外，在当前批次任务结束，清理metadata（jobGenerator.clearMetadata）时，也会调用doCheckpoint方法。Q2：在streaming checkpoint过程中，具体都写入了哪些数据到checkpoint目录？A2: 做checkpoint的主要逻辑基本都在JobGenerator.doCheckpoint方法中。在该方法中，首先更新当前时间段需要做checkpoint RDD的相关信息，如在DirectKafkaInputDStream中，将已经生成的RDD信息的时间，topic，partition，offset等相关信息进行更新。其次，通过checkpointWriter将Checkpoint对象写入到checkpoint目录中（CheckPoint.write → CheckpointWriteHandle）。至此，我们清楚了，写入到checkpoint目录的数据其实就是Checkpoint对象。Checkpoint主要包含的信息如下：12345678val master = ssc.sc.masterval framework = ssc.sc.appNameval jars = ssc.sc.jarsval graph = ssc.graphval checkpointDir = ssc.checkpointDirval checkpointDuration = ssc.checkpointDurationval pendingTimes = ssc.scheduler.getPendingTimes().toArrayval sparkConfPairs = ssc.conf.getAll具体包括相关配置信息，checkpoint目录，DStreamGraph等。对于DStreamGraph，主要包含InputDstream以及outputStream等相关信息，从而我们可以看出定义应用相关的计算函数也被序列化保存到checkpoint目录中了。Q3: streaming checkpoint都有哪些坑？A3：从A2中可以看到，应用定义的计算函数也被序列化到checkpoint目录，当应用代码发生改变时，此时就没法从checkpoint恢复。个人感觉这是checkpoint在生产环境使用中碰到的最大障碍。另外，当从checkpoint目录恢复streamingContext时，配置信息也都是从checkpoint读取的（只有很少的一部分配置是reload的，具体见读流程），当重启任务时，新改变的配置就可能不生效，导致很奇怪的问题。此外，broadcast变量在checkpoint中使用也受到限制（SPARK-5206）。输出数据Exactly-Once首先输出操作是具有At-least Once语义的，也就是说Spark Streaming可以保证需要输出的数据一定会输出出去，只不过由于失败等原因可能会输出多次。 那么如何保证Exactly once？第一种幂等输出，就是期望下游（数据）具有幂等特性。将kafka参数enable.auto.commit设置为false。生产中可用Kafka、Zookeeper、HBase等保存offset。第二种使用事务更新1234567dstream.foreachRDD &#123; (rdd, time) =&gt; rdd.foreachPartition &#123; partitionIterator =&gt; val partitionId = TaskContext.get.partitionId() val uniqueId = generateUniqueId(time.milliseconds, partitionId) // use this uniqueId to transactionally commit the data in partitionIterator &#125;&#125;这样保证同一个partition要么一起更新成功，要么一起失败，通过uniqueId来标识这一次的更新，这就要求下游支持事务机制。转换数据Exactly-OnceSpark Streaming内部的实现机制是基于RDD模型的，RDD为保证计算过程中数据不丢失使用了checkpoint机制， 也就是说其计算逻辑是RDD的变换过程，也就是DAG，可以在计算过程中的任何一个阶段（也就是这个阶段的RDD） 上使用checkpoint方法，就可以保证当后续计算失败，可以从这个checkpoint重新算起，使得计算延续下去。当Spark Streaming场景下，其天然会进行batch操作，也就是说kafka过来的数据， 每秒（一个固定batch的时间周期）会对当前kafka中的数据产生一个RDD， 那么后续计算就是在这个RDD上进行的。只需要在kafkaRDD这个位置合理使用了checkpoint （这一点在前面已经讲过，可以保证）就能保证Spark Streaming内部的Exactly once。注意一点：Spark Streaming中没有Tuple级别的ACK，其操作必然是在RDD的某个partition上的， 要么全做，要么不做，要么失败，要么成功，都是基于RDD的partition的。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Kafka Exactly once 语义","slug":"Kafka-Exactly-once-语义","date":"2019-01-04T10:23:47.000Z","updated":"2020-07-11T10:26:18.303Z","comments":true,"path":"2019/01/04/Kafka-Exactly-once-语义/","link":"","permalink":"cpeixin.cn/2019/01/04/Kafka-Exactly-once-%E8%AF%AD%E4%B9%89/","excerpt":"","text":"在很多的流处理框架的介绍中，都会说kafka是一个可靠的数据源，并且推荐使用Kafka当作数据源来进行使用。这是因为与其他消息引擎系统相比，kafka提供了可靠的数据保存及备份机制。并且通过消费者位移这一概念，可以让消费者在因某些原因宕机而重启后，可以轻易得回到宕机前的位置。但其实kafka的可靠性也只能说是相对的，在整条数据链条中，总有可以让数据出现丢失的情况，今天就来讨论如何避免kafka数据丢失，以及实现精确一致处理的语义。kafka无消息丢失处理在讨论如何实现kafka无消息丢失的时候，首先要先清楚大部分情况下消息丢失是在什么情况下发生的。为什么是大部分，因为总有一些非常特殊的情况会被人忽略，而我们只需要关注普遍的情况就足够了。接下来我们来讨论如何较为普遍的数据丢失情况。1.1 生产者丢失在Kafka分区和副本中，producer客户端有一个acks的配置，这个配置为0的时候，producer是发送之后不管的，这个时候就很有可能因为网络等原因造成数据丢失，所以应该尽量避免。但是将ack设置为1就没问题了吗，那也不一定，因为有可能在leader副本接收到数据，但还没同步给其他副本的时候就挂掉了，这时候数据也是丢失了。并且这种时候是客户端以为消息发送成功，但kafka丢失了数据。要达到最严格的无消息丢失配置，应该是要将acks的参数设置为-1（也就是all），并且将min.insync.replicas配置项调高到大于1，同时还需要使用带有回调的producer api，来发送数据。注意这里讨论的都是异步发送消息，同步发送不在讨论范围。1234567891011121314151617181920212223242526272829303132333435363738394041public class send&#123; ...... public static void main()&#123; ... /* * 第一个参数是 ProducerRecord 类型的对象，封装了目标 Topic，消息的 kv * 第二个参数是一个 CallBack 对象，当生产者接收到 Kafka 发来的 ACK 确认消息的时候， * 会调用此 CallBack 对象的 onCompletion() 方法，实现回调功能 */ producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr), new DemoCallBack(startTime, messageNo, messageStr)); ... &#125; ......&#125;class DemoCallBack implements Callback &#123; /* 开始发送消息的时间戳 */ private final long startTime; private final int key; private final String message; public DemoCallBack(long startTime, int key, String message) &#123; this.startTime = startTime; this.key = key; this.message = message; &#125; /** * 生产者成功发送消息，收到 Kafka 服务端发来的 ACK 确认消息后，会调用此回调函数 * @param metadata 生产者发送的消息的元数据，如果发送过程中出现异常，此参数为 null * @param exception 发送过程中出现的异常，如果发送成功为 null */ @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; long elapsedTime = System.currentTimeMillis() - startTime; if (metadata != null) &#123; System.out.printf(\"message: (%d, %s) send to partition %d, offset: %d, in %d\\n\", key, message, metadata.partition(), metadata.offset(), elapsedTime); &#125; else &#123; exception.printStackTrace(); &#125; &#125;&#125;我们之前提到过，producer发送到kafka broker的时候，是有多种可能会失败的，而回调函数能准确告诉你是否确认发送成功，当然这依托于acks和min.insync.replicas的配置。而当数据发送丢失的时候，就可以进行手动重发或其他操作，从而确保生产者发送成功。1.2 kafka内部丢失有些时候，kafka内部因为一些不大好的配置，可能会出现一些极为隐蔽的数据丢失情况，那么我们分别讨论下大致都有哪几种情况。首先是replication.factor配置参数，这个配置决定了副本的数量，默认是1。注意这个参数不能超过broker的数量。说这个参数其实是因为如果使用默认的1，或者不在创建topic的时候指定副本数量（也就是副本数为1），那么当一台机器出现磁盘损坏等情况，那么数据也就从kafka里面丢失了。所以replication.factor这个参数最好是配置大于1，比如说3。接下来要说的还是和副本相关的，也是上一篇副本中提到的unclean.leader.election.enable 参数，这个参数是在主副本挂掉，然后在ISR集合中没有副本可以成为leader的时候，要不要让进度比较慢的副本成为leader的。不用多说，让进度比较慢的副本成为leader，肯定是要丢数据的。虽然可能会提高一些可用性，但如果你的业务场景丢失数据更加不能忍受，那还是将unclean.leader.election.enable设置为false吧。1.3 消费者丢失消费者丢失的情况，其实跟消费者位移处理不当有关。消费者位移提交有一个参数，enable.auto.commit，默认是true，决定是否要让消费者自动提交位移。如果开启，那么consumer每次都是先提交位移，再进行消费，比如先跟broker说这5个数据我消费好了，然后才开始慢慢消费这5个数据。这样处理的话，好处是简单，坏处就是漏消费数据，比如你说要消费5个数据，消费了2个自己就挂了。那下次该consumer重启后，在broker的记录中这个consumer是已经消费了5个的。所以最好的做法就是将enable.auto.commit设置为false，改为手动提交位移，在每次消费完之后再手动提交位移信息。当然这样又有可能会重复消费数据，毕竟exactly once处理一直是一个问题呀（/摊手）。遗憾的是kafka目前没有保证consumer幂等消费的措施，如果确实需要保证consumer的幂等，可以对每条消息维持一个全局的id，每次消费进行去重，当然耗费这么多的资源来实现exactly once的消费到底值不值，那就得看具体业务了。1.4 无消息丢失小结那么到这里先来总结下无消息丢失的主要配置吧：producer的acks设置位-1，同时min.insync.replicas设置大于1。并且使用带有回调的producer api发生消息。默认副本数replication.factor设置为大于1，或者创建topic的时候指定大于1的副本数。unclean.leader.election.enable 设置为false，防止定期副本leader重选举消费者端，自动提交位移enable.auto.commit设置为false。在消费完后手动提交位移。那么接下来就来说说kafka实现精确一次（exactly once）处理的方法吧。实现精确一次（exactly once）处理在分布式环境下，要实现消息一致与精确一次（exactly once）语义处理是很难的。精确一次处理意味着一个消息只处理一次，造成一次的效果，不能多也不能少。那么kafka如何能够实现这样的效果呢？在介绍之前，我们先来介绍其他两个语义，至多一次（at most once）和至少一次（at least once）。最多一次和至少一次最多一次就是保证一条消息只发送一次，这个其实最简单，异步发送一次然后不管就可以，缺点是容易丢数据，所以一般不采用。至少一次语义是kafka默认提供的语义，它保证每条消息都能至少接收并处理一次，缺点是可能有重复数据。前面有介绍过acks机制，当设置producer客户端的acks是1的时候，broker接收到消息就会跟producer确认。但producer发送一条消息后，可能因为网络原因消息超时未达，这时候producer客户端会选择重发，broker回应接收到消息，但很可能最开始发送的消息延迟到达，就会造成消息重复接收。那么针对这些情况，要如何实现精确一次处理的语义呢？业务控制对生产者：每个分区只有一个生产者写入消息，当出现异常或超时，生产者查询此分区最后一个消息，用于决定后续操作时重传还是继续发送。为每个消息增加唯一主键，生产者不做处理，由消费者根据主键去重。对消费者：关闭自动提交 offset 的功能，不使用 Offsets Topic 这个内部 Topic 记录其 offset，而是由消费者自动保存 offset。将 offset 和消息处理放在一个事务里面，事务执行成功认为消息被消费，否则事务回滚需要重新处理。当出现消费者重启或者 Rebalance 操作，可以从数据库找到对应的 offset，然后调用 KafkaConsumer.seek() 设置消费者位置，从此 offset 开始消费。kafka控制幂等的producer要介绍幂等的producer之前，得先了解一下幂等这个词是什么意思。幂等这个词最早起源于函数式编程，意思是一个函数无论执行多少次都会返回一样的结果。比如说让一个数加1就不是幂等的，而让一个数取整就是幂等的。因为这个特性所以幂等的函数适用于并发的场景下。但幂等在分布式系统中含义又做了进一步的延申，比如在kafka中，幂等性意味着一个消息无论重复多少次，都会被当作一个消息来持久化处理。kafka的producer默认是支持最少一次语义，也就是说不是幂等的，这样在一些比如支付等要求精确数据的场景会出现问题，在0.11.0后，kafka提供了让producer支持幂等的配置操作。即：props.put(“enable.idempotence”, ture)在创建producer客户端的时候，添加这一行配置，producer就变成幂等的了。注意开启幂等性的时候，acks就自动是“all”了，如果这时候手动将ackss设置为0，那么会报错。而底层实现其实也很简单，就是对每条消息生成一个id值，broker会根据这个id值进行去重，从而实现幂等，这样一来就能够实现精确一次的语义了。但是！幂等的producer也并非万能。有两个主要是缺陷：幂等性的producer仅做到单分区上的幂等性，即单分区消息不重复，多分区无法保证幂等性。只能保持单会话的幂等性，无法实现跨会话的幂等性，也就是说如果producer挂掉再重启，无法保证两个会话间的幂等（新会话可能会重发）。因为broker端无法获取之前的状态信息，所以无法实现跨会话的幂等。事务的producer上述幂等设计只能保证单个 Producer 对于同一个 &lt;Topic, Partition&gt; 的 Exactly Once 语义。Kafka 现在通过新的事务 API 支持跨分区原子写入。这将允许一个生产者发送一批到不同分区的消息，这些消息要么全部对任何一个消费者可见，要么对任何一个消费者都不可见。这个特性也允许在一个事务中处理消费数据和提交消费偏移量，从而实现端到端的精确一次语义。为了实现这种效果，应用程序必须提供一个稳定的（重启后不变）唯一的 ID，也即Transaction ID 。 Transactin ID 与 PID 可能一一对应。区别在于 Transaction ID 由用户提供，将生产者的 transactional.id 配置项设置为某个唯一ID。而 PID 是内部的实现对用户透明。另外，为了保证新的 Producer 启动后，旧的具有相同 Transaction ID 的 Producer 失效，每次 Producer 通过 Transaction ID 拿到 PID 的同时，还会获取一个单调递增的 epoch。由于旧的 Producer 的 epoch 比新 Producer 的 epoch 小，Kafka 可以很容易识别出该 Producer 是老的 Producer 并拒绝其请求。事务可以支持多分区的数据完整性，原子性。并且支持跨会话的exactly once处理语义，也就是说如果producer宕机重启，依旧能保证数据只处理一次。开启事务也很简单，首先需要开启幂等性，即设置enable.idempotence为true。然后对producer发送代码做一些小小的修改。12345678910111213//初始化事务producer.initTransactions();try &#123; //开启一个事务 producer.beginTransaction(); producer.send(record1); producer.send(record2); //提交 producer.commitTransaction();&#125; catch (KafkaException e) &#123; //出现异常的时候，终止事务 producer.abortTransaction();&#125;但无论开启幂等还是事务的特性，都会对性能有一定影响，这是必然的。所以kafka默认也并没有开启这两个特性，而是交由开发者根据自身业务特点进行处理。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"}]},{"title":"深度学习 Let's go","slug":"深度学习-Let-s-go","date":"2019-01-01T15:03:48.000Z","updated":"2020-05-06T16:54:06.970Z","comments":true,"path":"2019/01/01/深度学习-Let-s-go/","link":"","permalink":"cpeixin.cn/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Let-s-go/","excerpt":"","text":"前段时间讲了数据挖掘十大经典算法，在实战中也了解了随机森林、逻辑回归的概念及工具使用。这些算法都属于传统的机器学习算法。你肯定听说过这两年很火的深度学习，那么机器学习算法和深度学习有什么关联呢？在这篇文章中，我们会通过以下几个方面了解深度学习：数据挖掘、机器学习和深度学习的区别是什么？这些概念都代表什么？我们通过深度学习让机器具备人的能力，甚至某些技能的水平超过人类，比如图像识别、下棋对弈等。那么深度学习的大脑是如何工作的？深度学习是基于神经网络构建的，都有哪些常用的网络模型？深度学习有三个重要的应用领域，这三个应用领域分别是什么？数据挖掘，机器学习，深度学习的区别是什么？实际上数据挖掘和机器学习在很大程度上是重叠的。一些常用算法，比如 K-Means、KNN、SVM、决策树和朴素贝叶斯等，既可以说是数据挖掘算法，又可以说是机器学习算法。那么数据挖掘和机器学习之间有什么区别呢？数据挖掘通常是从现有的数据中提取规律模式（pattern）以及使用算法模型（model）。核心目的是找到这些数据变量之间的关系，因此我们也会通过数据可视化对变量之间的关系进行呈现，用算法模型挖掘变量之间的关联关系。通常情况下，我们只能判断出来变量 A 和变量 B 是有关系的，但并不一定清楚这两者之间有什么具体关系。在我们谈论数据挖掘的时候，更强调的是从数据中挖掘价值。机器学习是人工智能的一部分，它指的是通过训练数据和算法模型让机器具有一定的智能。一般是通过已有的数据来学习知识，并通过各种算法模型形成一定的处理能力，比如分类、聚类、预测、推荐能力等。这样当有新的数据进来时，就可以通过训练好的模型对这些数据进行预测，也就是通过机器的智能帮我们完成某些特定的任务。深度学习属于机器学习的一种，它的目标同样是让机器具有智能，只是与传统的机器学习算法不同，它是通过神经网络来实现的。神经网络就好比是机器的大脑，刚开始就像一个婴儿一样，是一张白纸。但通过多次训练之后，“大脑”就可以逐渐具备某种能力。这个训练过程中，我们只需要告诉这个大脑输入数据是什么，以及对应的输出结果是什么即可。通过多次训练，“大脑”中的多层神经网络的参数就会自动优化，从而得到一个适应于训练数据的模型。所以你能看到在传统的机器学习模型中，我们都会讲解模型的算法原理，比如 K-Means 的算法原理，KNN 的原理等。而到了神经网络，我们更关注的是网络结构，以及网络结构中每层神经元的传输机制。我们不需要告诉机器具体的特征规律是什么，只需把我们想要训练的数据和对应的结果告诉机器大脑即可。深度学习会自己找到数据的特征规律！而传统机器学习往往需要专家（我们）来告诉机器采用什么样的模型算法，这就是深度学习与传统机器学习最大的区别。另外深度学习的神经网络结构通常比较深，一般都是 5 层以上，甚至也有 101 层或更多的层数。这些深度的神经网络可以让机器更好地自动捕获数据的特征。神经网络是如何工作的神经网络可以说是机器的大脑，经典的神经网络结构可以用下面的图来表示。这里有一些概念你需要了解。节点：神经网络是由神经元组成的，也称之为节点，它们分布在神经网络的各个层中，这些层包括输入层，输出层和隐藏层。输入层：负责接收信号，并分发到隐藏层。一般我们将数据传给输入层。输出层：负责输出计算结果，一般来说输出层节点数等于我们要分类的个数。隐藏层：除了输入层和输出层外的神经网络都属于隐藏层，隐藏层可以是一层也可以是多层，每个隐藏层都会把前一层节点传输出来的数据进行计算（你可以理解是某种抽象表示），这相当于把数据抽象到另一个维度的空间中，可以更好地提取和计算数据的特征。工作原理：神经网络就好比一个黑盒子，我们只需要告诉这个黑盒子输入数据和输出数据，神经网络就可以自我训练。一旦训练好之后，就可以像黑盒子一样使用，当你传入一个新的数据时，它就会告诉你对应的输出结果。在训练过程中，神经网络主要是通过前向传播和反向传播机制运作的。什么是前向传播和反向传播呢？前向传播：数据从输入层传递到输出层的过程叫做前向传播。这个过程的计算结果通常是通过上一层的神经元的输出经过矩阵运算和激活函数得到的。这样就完成了每层之间的神经元数据的传输。反向传播：当前向传播作用到输出层得到分类结果之后，我们需要与实际值进行比对，从而得到误差。反向传播也叫作误差反向传播，核心原理是通过代价函数对网络中的参数进行修正，这样更容易让网络参数得到收敛。所以，整个神经网络训练的过程就是不断地通过前向 - 反向传播迭代完成的，当达到指定的迭代次数或者达到收敛标准的时候即可以停止训练。然后我们就可以拿训练好的网络模型对新的数据进行预测。当然，深度神经网络是基于神经网络发展起来的，它的原理与神经网络的原理一样，只不过强调了模型结构的深度，通常有 5 层以上，这样模型的学习能力会更强大。常用的神经网络都有哪些按照中间层功能的不同，神经网络可以分为三种网络结构，分别为 FNN、CNN 和 RNN。FNN（Fully-connected Neural Network）指的是全连接神经网络，全连接的意思是每一层的神经元与上一层的所有神经元都是连接的。不过在实际使用中，全连接的参数会过多，导致计算量过大。因此在实际使用中全连接神经网络的层数一般比较少。CNN 叫作卷积神经网络，在图像处理中有广泛的应用，了解图像识别的同学对这个词一定不陌生。CNN 网络中，包括了卷积层、池化层和全连接层。这三个层都有什么作用呢？卷积层相当于一个滤镜的作用，它可以把图像进行分块，对每一块的图像进行变换操作。池化层相当于对神经元的数据进行降维处理，这样输出的维数就会减少很多，从而降低整体的计算量。全连接层通常是输出层的上一层，它将上一层神经元输出的数据转变成一维的向量。RNN 称为循环神经网络，它的特点是神经元的输出可以在下一个时刻作用到自身，这样 RNN 就可以看做是在时间上传递的神经网络。它可以应用在语音识别、自然语言处理等与上下文相关的场景。深度学习网络往往包括了这三种网络的变种形成，常用的深度神经网络包括 AlexNet、VGG19、GoogleNet、ResNet 等，我总结了这些网络的特点，你可以看下：你能看出随着时间的推进，提出的深度学习网络层数越来越深，Top-5 错误率越来越低。你可能会问什么是 Top-5 错误率，实际上这些网络结构的提出和一个比赛相关，这个比赛叫做 ILSVRC，英文全称叫做 Large Scale Visual Recognition Challenge。它是一个关于大规模图像可视化识别的比赛，所基于的数据集就是著名的 ImageNet 数据集，一共包括了 1400 万张图片，涵盖 2 万多个类别。表格中的 AlexNet 就是 2012 年的 ILSVRC 冠军，当时的 Top-5 正确率是 84.7%，VGG 和 GoogleNet 是 2014 年 ILSVRC 比赛的模型，其中 GoogleNet 是当时比赛的冠军，而 VGG 是当时比赛的亚军，它的效率低于 GoogleNet。VGG 有两个版本，VGG16 和 VGG19，分别是 16 层和 19 层的 VGG 网络，这两者没有本质的区别，只是网络深度不同。到了 2015 年，比赛冠军是 ResNet，Top-5 正确率达到了 96.43%。ResNet 也有不同的版本，比如 ResNet50、ResNet101 和 ResNet152 等，名称后面的数字代表的是不同的网络深度。之后 ResNet 在其他图像比赛中也多次拿到冠军。深度学习的应用领域从 ImageNet 跑出来的这些优秀模型都是基于 CNN 卷积神经网络的。实际上深度学习有三大应用领域，图像识别就是其中之一，其他领域分别是语音识别和自然语言处理。这三个应用领域有一个共同的特性，就是都来自于信号处理。我们人类平时会处理图像信息，语音信息以及语言文字信息。机器可以帮助我们完成这三个应用里的某些工作。比如图像识别领域中图像分类和物体检测就是两个核心的任务。我们可以让机器判断图像中都有哪些物体，类别是什么，以及这些物体所处的位置。图像识别被广泛应用在安防检测中。此外人脸识别也是图像识别重要的应用场景。Siri 大家一定不陌生，此外还有我们使用的智能电视等，都采用了语音识别技术。语音识别技术可以识别人类的语音指令并进行交互。在语音导航中，还采用了语音合成技术，这样就可以让机器模拟人的声音为我们服务，Siri 语音助手也采用了语音识别和合成的技术。自然语言处理的英文缩写是 NLP，它被广泛应用到自动问答、智能客服、过滤垃圾邮件和短信等领域中。在电商领域，我们可以通过 NLP 自动给商品评论打标签，在用户决策的时候提供数据支持。在自动问答中，我们可以输入自己想问的问题，让机器来回答，比如在百度中输入“姚明的老婆”，就会自动显示出”叶莉“。此外这些技术还可以相互组合为我们提供服务，比如在无人驾驶中就采用了图像识别、语音识别等技术。在超市购物中也采用了集成图像识别、意图识别等技术等。总结今天我们大概了解了一下深度学习。深度学习也是机器学习的一种。我们之前讲解了数据挖掘十大经典算法，还有逻辑回归、随机森林算法等，这些都是传统的机器学习算法。在日常工作中，可以满足大部分的机器学习任务。但是对于数据量更大，更开放性的问题，我们就可以采用深度学习的算法，让机器自己来找规律，而不是通过我们指定的算法来找分类规律。所以深度学习的普适性会更强一些，但也并不代表深度学习就优于机器学习。一方面深度学习需要大量的数据，另一方面深度学习的学习时间，和需要的计算资源都要大于传统的机器学习。你能看到各种深度学习的训练集一般都还是比较大的，比如 ImageNet 就包括了 1400 万张图片。如果我们没有提供大量的训练数据，训练出来的深度模型识别结果未必好于传统的机器学习。实际上神经网络最早是在 1986 年提出来的，之后不温不火，直到 ImageNet 于 2009 年提出，在 2010 年开始举办每年的 ImageNet 大规模视觉识别挑战赛（ILSVRC），深度学习才得到迅猛发展。2016 年 Google 研发的 AlphaGo 击败了人类冠军李世石，更是让人们看到了深度学习的力量。一个好问题的提出，可以激发无穷的能量，这是科技进步的源泉，也是为什么在科学上，我们会有各种公开的数据集。一个好的数据集就代表了一个好的问题和使用场景。正是这些需求的出现，才能让我们的算法有更好的用武之地，同时也有了各种算法相互比拼的平台。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Spark Streaming性能调优","slug":"Spark-Streaming性能调优","date":"2018-12-03T09:32:26.000Z","updated":"2020-09-03T09:33:58.213Z","comments":true,"path":"2018/12/03/Spark-Streaming性能调优/","link":"","permalink":"cpeixin.cn/2018/12/03/Spark-Streaming%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","excerpt":"","text":"Spark Streaming框架的使用和配置从三个维度介绍性能调优硬件优化使用层面的优化配置优化硬件优化Spark Streaming 与 Spark 离线计算相比，I/O 并没有那么密集，整体负载也低于 Spark 离线计算，数据基本存放于 Executor 的内存中。但是，它对于 CPU 的要求相对较高，例如更低的延迟（较小的批次间隔）、大量微批作业的同时提交与处理。但是对基于时间窗口的操作以及对状态进行操作的算子来说，需要在内存中将这部分数据缓存，如果时间窗口跨度较长的话，需要的内存也会比较高，像 updateStateByKey 这种算子，更需要全程追踪状态，这也需要耗费不少内存，因此 Spark Streaming 集群的硬件配置也可参照离线计算型的配置。使用层面的优化对于使用层面的下列优化，有些是使用技巧，有些是在某些场景下得到的经验，你可以根据自己的需求选择。1. 批次间隔虽然说 Spark Streaming 号称可以达到毫秒级（理论上 50ms）的延迟，但是在设置批次间隔时，一般不会低于 0.5s，否则大量的作业同时提交会引起负载过高。这个值可以通过反复实验来得到，我们可以先将该值设置为比较大的值，比如 10s，如果作业很快就完成了，我们可以减小批次间隔，直到 Spark Streaming 在这个时间段内刚好处理完上一批的数据，此时的批次间隔就是比较合适的了。2. 窗口大小与滑动步长这两个配置的值同样对性能有巨大影响，当性能低下时，可以考虑减小窗口大小和增加滑动步长。3. updateStateByKey 与 mapWithState在绝大多数情况下，使用 mapWithState 而不用 updateStateByKey，实践证明，前者的延迟表现和能够同时维护的 key 数量都远远优于后者。4. mapPartition 与 map在与外部数据库交互，如写操作时，使用 mapPartition 而不要使用 map 算子，mapPartition 会在处理每个分区时连接一次数据库，而不像 map 每条数据连接一次数据库，性能优势明显。5. reduceByKey/aggregateByKey 与 groupByKey前者的聚合性能要明显优于后者，因此尽量使用 reduceByKey/aggregateByKey。7. 序列化采用 Kyro 进行序列化，可以改善 GC。8. 数据处理的并行程度可以通过增大计算的并行度来提升性能，如 reduceByKey、join 等，如果不指定，并行度为配置项 spark.default.parallelism 的值，如果遇到数据倾斜还可以使用 repartition。9. filter 与 coalesce与离线计算相似，在 filter 算子作用后，会产生大量零碎的分区，不利于计算，可以在后面接 coalesce 或者 repartition 算子将其进行合并或者重分。10. 将 Checkpoint 存储到 Alluxio使用 Alluxio 作为 Spark Streaming 的 Checkpoint 存储介质，这有助于提高读写 Checkpoint 的性能。11. 资源调度如果使用统一资源管理平台，那么批处理作业与流处理作业有可能会运行在同一个节点的不同容器中。如果批处理作业负载较高，就会对流处理作业造成较大影响，建议分离部署。如果从提高资源利用率的角度出发，确实需要部署在一个集群，那么建议采用 Hadoop 2.6 以后引入的新特性：基于标签的调度（Label based scheduling），使流处理计算作业得到稳定且独立的计算资源。12. 缓存数据与清除数据与 Spark 离线计算一样，需要重复计算的数据需要用 cache 算子进行缓存。但是，这些缓存会不断占用内存，可以设置 spark.streaming.unpersist 为 true，让 Spark 来决定哪些数据需要缓存，否则需要手动控制，这样通常性能开销还会大一点。配置优化配置方面的优化具体如下1. JVM GC在 Executor 的堆足够大（大概 30GB 以上）时，使用 G1 GC 代替 CMS GC，否则采用 Parallel GC，如下所示：–conf “spark.executor.extraJavaOptions=-XX:+UseG1GC”–conf “spark.executor.extraJavaOptions=-XX:+UseParallelGC”2. spark.streaming.blockInterval该参数设置了 Receiver 的接收块间隔时间，默认为 200ms。对于大多数 Receiver，接收的数据在存储到 Spark 的 Executor 之前，会先聚合成块的形式，每个块就是一个分区，也就是说，每个批次间隔的数据中，块的数量决定了后面类似 map 算子所处理的任务数，这也影响了数据处理的并行程度。一个批次的数据块的数量（分区数）的计算公式为：batch interval /spark.streaming.blockInterval，分子为我们设置的批次间隔，假设为 2s，那么每个批次会有 2000/200=10 个数据块。如果这个数字低于节点的 CPU 核数，说明没有充分发挥 CPU 的能力，那么可以考虑降低 spark.streaming.blockInterval 的值，但是一般也不推荐低于 50 ms。3. 反压反压在流处理场景里面比较常见，是每个流处理框架必须考虑的问题。反压的实质是，当每批数据处理时间大于批次间隔时间时，长久以往，数据会在 Executor 中的内存中迅速累积，内存会很快溢出，如果设定持久化存储基本为硬盘，则会出现大量磁盘 I/O，增加延迟。防止反压的关键是做好流量控制，如果一味地限制 Receiver 接收数据的速度，会降低整个集群的资源利用率。Spark Streaming 在 1.5 之后引入了反压机制，可以通过 spark.streaming. backpressure.enabled 来开启，开启后系统会根据每一批次作业调度与完成的情况让系统按照处理数据的速率来接收数据。实际上，就是限制 Receiver 接收数据的速度，上限由 spark.streaming. receiver.maxRate 设置，如果以 Kafka Direct 方式接收的话，上限由 spark.streaming.kafka.maxRatePerPartition 来配置。开启反压机制后，资源利用率肯定会有所下降，因此 spark.streaming.backpressure.enabled 默认关闭。Spark Streaming 是利用 PID（proportional-integral-derivative）算法来确定新的数据接收速率的，开启反压机制后的速率公式为（单位：条/秒）：其中，VnewRate 为下一批次的接收速率；VlatestRate 为在上一批次中所确定当前批次的接收速率；Verror 为 VlatestRate 减去当前批次的实际处理速率；VhistoricalError 为当前批次等待调度的时间乘以当前批次的处理速率再除以批次间隔；dError 为Verror 减去上一批次的 Verror 的差，除以当前批次完成的时间，减去上一批次完成的时间的结果。Kproportional、Kintegral、Kderivative 为 PID 算法的 3 个重要的调适参数。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"ETL里的34个子系统","slug":"ETL里的34个子系统","date":"2018-11-02T18:16:48.000Z","updated":"2020-06-01T02:38:13.981Z","comments":true,"path":"2018/11/03/ETL里的34个子系统/","link":"","permalink":"cpeixin.cn/2018/11/03/ETL%E9%87%8C%E7%9A%8434%E4%B8%AA%E5%AD%90%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"ETL里的34个子系统Ralph Kimball和Joe Caserta于2004年编写的《The Data Warehouse ETL Toolkit》一书系统的阐述了ETL这一概念及建设ETL系统的要点，将ETL从BI的一部分抽离了出来。随后，这本书里的一些思想形成了一篇文章《ETL里的38个子系统》，系统总结了ETL项目要面临的不同任务。我们还可以在网上找到原始的这篇文章https://www.informationweek.com/software/information-management/the-38-subsystems-of-etl/d/d-id/1028653。在2008年，Wiley出版了最流行的一本BI图书的第二版：也是由Kimball和他同事编写的《The Data Warehouse Lifecycle Toolkit》。在这本书里ETL子系统被重构成了34种子系统。接下来本文将简要介绍这34种ETL里的子系统。这34种子系统提供了一套框架，帮助我们理解ETL解决方案的实现和管理，并对其进行分类。在ETL解决方案的设计与实现之前，我们需要清楚的了解需要、已存在的系统、可用的技巧和技术来确认我们的预期是什么，以及达到预期的推动和限制因素。这34个子系统可以分为４个组成部分，其中很多都是管理类的子系统，因为当项目发布时，系统的生命周期才刚刚开始，管理也是ETL的重中之重。抽取：ETL的第一个单词Extract就是抽取的意思，数据抽取是ETL系统的最大挑战之一，子系统1~3属于这个主题。清洗和更正：无论使用什么数据仓库架构，在某个时间点上，数据都要经过清洗以满足业务的要求。经典数据仓库模型，数据进入到数据仓库之前就被清洗了（“真实的数据只有一个版本”）。而DataVault模型，数据是按照原样进入数据仓库的（“事实的数据只有一个版本”），而清洗和更正过程发生在后面的阶段。子系统4~8属于这个主题。发布：34个子系统中有13个都是关于如何把数据发布到目标数据库中的，发布并不仅仅意味着把数据写入到目标数据库中，也包括把数据写入到维度表或事实表中的那些转换。管理：任何的信息基础架构都要可以被管理和监控，ETL系统也不例外，子系统22~34属于这一个主题。下面的主要内容都是摘录网上和书里资料，好记性不如烂笔头。抽取ETL方案的第一部分就是要从不同的数据源抽取数据。访问数据源会有很多困难，政策性问题是最难以逾越的障碍。另外，基于安全性和性能方面的考虑，数据系统的管理人员不会让未经授权的用户访问系统。一些ERP系统的厂商（SAP或Oracle）也不允许其他系统访问ERP底层数据库。子系统1：数据剖析系统目标是要分析不同数据源的结构和内容。数据剖析提供了类似行统计、NULL值个数统计等简单的统计项，当然也有一些更复杂的分析，如单词模式分析等。子系统2：增量数据捕获系统目标是捕获源系统里数据的变化，CDC（Changed Data Capture，变化数据获取）常用的方式有：审计列：源系统包含审计列，比如插入或修改的时间。定时获取全差异化比较数据库日志抓取消息队列监控子系统3：抽取系统抽取子系统从不同的数据源抽取数据，并输入到ETL流程里。Kimball明确区分了基于文件的和基于流的两种抽取。注意，这里基于流的抽取并不意味着实时数据流。这种区分方法不太恰当，因为无论从数据库，文件、实时数据源、Web Services还是其他任何数据源，只要可以访问到数据，数据都是以流的方式通过整个转换的。事实上唯一有区别的地方是在ETL作业运行的过程中，数据源的数据是否在发生变化。所以抽取的主要的区别不是文件或流，而是静态或动态的问题。如果转换失败，这种区分方式就显得更为重要。如果你的数据源是静态的（文件的情况基本都是如此），重新启动一个作业就可以了。而如果你的数据源是动态的。例如事务型的数据库，在你运行作业的时候，数据库里面的数据已经发生了变化。例如，一个加载销售数据的作业，所有的维度都正常加载，在加载销售订单的事实数据时停电了，而在加载维度数据的同时，源系统中有了一个新的客户，并产生了一个新的订单，也就是说在源系统中有了新的维度数据和事实数据。除非把所有的维度表重新跑一遍，否则在重新加载事实表时，就会发现有的客户维度没有找到。另外，CDC的实现方式不同，从这类错误中进行数据恢复也是非常困难的事情。清洗和更正数据世界上没有任何一个组织的数据是没有质量问题的，这也就是为什么我们在把数据加载到数据仓库之前要增加一些步骤来清洗和更正这些数据，以满足业务的需求。另外，只使用一个单一的系统来存储数据的组织也很少；通常，为了支撑业务运行，都会存在多个系统，可能每个部门都会有自己的系统。如财务、人力、采购或客服管理等。每个系统存储数据的方式可能都不相同。例如在系统A里，客户性别保存为F（female）、M（male）、U（unknown）；在系统B里，分别使用0、1、NULL来代表这三类数据。所有的这些系统都应该遵照数据仓库的统一标准来存储。子系统4：数据清洗和质量处理系统数据清洗是指修改或整理进入到ETL流程中的脏数据。虽然通常来说，数据清洗应该在原始系统中产生数据的地方进行。但往往提高原始数据质量所需要的时间不能满足开发数据仓库的时间要求。但是无论如何，我们都要给用户提供一份干净的数据。所以一般就需要使用ETL项目来提高数据质量，ETL项目的优势在于：首先，在ETL的数据剖析阶段，可以找出有哪些错误数据；其次，在源系统中需要的数据清洗规则，同样可以使用于ETL环境中。最后，最终使用数据的业务人员可以加入到ETL开发中，只有业务人员才能告诉我们哪些数据是正确的数据。理想情况下，业务人员/数据所有者、源系统开发人员/管理者和ETL开发人员需要共同完成提高数据质量的工作。在很多情况下，不正确数据主要来源于那些把数据输入到系统里的业务人员。例如：一套包括ETL流程的数据质量解决方案，这个方案读取并转换业务系统中的数据，最后把数据加载到一个检查系统，在这个检查系统里用户可以可视化查看数据，并给不正确数据打标记。另外，ETL流程还可以自动给某些常见的错误打标记，如字段为空、不正确的格式或错误的电话号码等。每周数据质量的检查结果就会报告给数据管理人员。尽管业务上要求100%没有错误数据，但实际上，在没有做这个数据质量项目之前，正确数据的比例低于50%，在做了这个可视化的数据质量项目后，第一年，正确数据的比例已经几乎达到了90%。这个例子显示了一个简单的ETL流程再加上一些报告，如何使用户重视并提高数据质量问题。子系统5：错误事件处理错误事件处理的目的是记录下ETL过程中的每一个错误。这样便于管理员定期监控和分析错误是数据质量错误还是系统错误或其他错误。Kimball提到要使用一个独立的错误事件模式来保留这些错误。子系统6：审计维度尽管错误事件模式和数据仓库的业务数据是独立的，但审计维度表却是数据仓库内部的。审计维度表是一类特殊的维度表，数据仓库里的所有事实表都和审计维度表关联，审计维度表包含了对事实表变更的元数据，如加载数据的日期和时间、数据的质量指标等。实际上，给数据仓库增加审计维度，可以带来很多好处。就像在多维数据仓库上使用Data Vault架构所带来的好处一样。子系统7：排除重复记录系统排除重复记录可能是ETL中最棘手的问题，大部分ETL工具也没有能自动处理重复数据的能力。在大多数情况下，排重是指删除重复的数据，或者把不同系统里互相冲突的数据统一。子系统8：数据一致性数据经过数据排重子系统和前面提到的其他数据质量步骤处理后，就交给数据一致性子系统来处理。这个步骤的目的就是使来源于多个业务系统的事实数据遵照相同的维度。例如，一个公司有一个客服管理系统，这个系统有自己的客户数据库，为了把客服管理系统和销售系统放到同一个数据仓库里，需要把客服管理系统的客户数据和销售系统的客户数据统一成一个客户维度表，当分别加载来自这两个系统的事实数据时，需要把来自两个系统的事实数据指向同一个客户维度表。解决这个问题最常用的方法就是维度表中保留从不同系统带来的自然键，在加载事实数据时，可以查找维度表中的这些源系统的自然键。数据发布发布新的数据并不只是往目标数据库里插入新的数据这么简单，发布新的数据其实有很多工作。首先，我们从不同缓慢变更维度技术可以看到，更新维度表就有很多种方式。另外，你需要生成代理键、查询正确的维度键、确保维度数据在事实数据加载前就已经加载完、准备要加载的事实数据。加载事实数据本身也是一项有挑战性的工作：事实数据的数据量可能比较大，也有可能还要更新事实数据，或者同时出现这两种情况。所以需要特别关心表和存储的功能，如OLAP数据库。这也是为什么34种子系统里有很多都属于数据发布范畴。子系统9：缓慢变更维度处理缓慢变更维度（SCDs）是多维数据仓库或者总线架构的基础。我们知道维度表里保存了用来对事实进行分析或分组的信息。例如，客户维度里有客户所在城市字段，这样我们就可以统计或列出某个城市里客户的销售情况。如果客户换了另一个城市，业务系统里肯定要相应修改这个客户所在的城市，缓慢变更维度的过程也会根据不同的规则来变更数据仓库中的客户维度。总的来说，有以下几种缓慢变更维度的方法。覆盖：直接用新值代替旧值。增加新行：把当前行标记为“old”并设置一个“结束”时间戳，同时创建一个新行，标记为“current”，并设置一个“开始”时间戳。增加新列：给表增加一个新列，来存储新值，同时保留原来的值不变。增加一个小维度表：把经常变更的表属性从主维度表里分离出来，保存在自己的表里。分离历史表：把每次变化保存到一个历史表里，同时保存变化的类型和变化的时间。这样的历史表可以回答类似“去年有多少客户从成都移动到了重庆”这样的问题。混合型：把类型1、2、3结合起来（1+2+3=6）。子系统10：代理键生成系统ETL流程应该可以生成代理键。一般有三种方式：使用表里现在代理键的最大值+1。使用数据库序列。使用一个自增的字段。后面一种方法也可用于表输出步骤。子系统11：层次维度构建在数据仓库里还要考虑如何构建和维护数据仓库里的层次。实际上，这个子系统的完整的名称是“构建固定的，可变深度的，可有级别缺失的层次维度系统”。层次可以让用户分析查看维度不同级别上的数据。最简单的层次概念就是时间维度的层次。在现实中，时间维度都需要至少一个以上的层次。例如有“年—季—月—日”这样的层次，也有“年—周—日”这样的层次。时间维度也是“平衡层次”的一个例子。在时间维度里，所有级别的深度都是固定一样的。组织结构的维度更复杂一些，这种维度通常都是“不平衡的”或称为“可变深度的”（子树的深度不同）或“级别缺失的”（在层次上缺失了一些级别）。关于后面“缺失的”，可以想一下地理维度，地理维度通常的层次是“国家—地区—州（省）—城市”。一些国家可能没有“地区”或“州（省）”或都没有，这就是缺失了级别。在源系统里，通常使用“递归”的关系来实现“不平衡的”或“级别缺失的”的情况。子系统12：特殊维度生成系统除了缓慢变化维度，基于多维模型的数据仓库，至少都包含一个特殊维度，时间维度。下面一些类型的维度也都是特殊维度。杂项维度（也称为垃圾维度）：一些零散的属性，分析需要但又不适合放在其他维度表里。例如状态标志、yes/no和其他低阶（lowcardinality）字段都可以放在杂项维度表里。小维度：从大维度表里分离出经常发生变化的一些属性，单独放在一个小维度表里。我们也把这种小维度表称为SCD4。收缩的或上卷的维度：普通维度表的子集，为了避免冲突，这种维度是根据普通维度表创建和更新的。这种维度适用于聚集的数据，例如底层保存的是每天的数据，聚集的数据是按月保存的。静态维度：通常是小的字典表或参照表，这类表在源系统中没有对应的数据，如状态编码描述或性别。用户自定义维度：源系统里没有的而报表需要的自定义的描述、分组和层次。可以是任意的维度。唯一区分它们的就是这些维度是通过用户来维护的，而不是通过数据仓库团队或一个ETL过程。子系统13：事实表加载（事务粒度、周期快照粒度、累积快照粒度事实表的加载系统）在往数据仓库加载事实表之前，需要把数据准备好。加载事实表过程并不是重点，之所以把加载事实表单独作为一个子系统分出来，主要是为了强调如下三种不同类型的事实表。事务粒度事实表：以每一个事务或事件为单位，例如一个销售记录、一个电话呼叫记录，作为事实表里的一行数据。周期快照事实表：事实表里并不保存全部数据，只保存固定时间间隔的数据，例如每天或每月的库存水平，或每月的账户余额。累积快照事实表：当有新的数据时，更新事实表里的记录。数据仓库里总是保存最新的数据。例如订单过程，订单过程里有很多独立的日期，如订单日期、期望发货日期、实际发货日期、期望收货日期、实际收货日期和付款日期。当这个过程进行时，随着上面各种时间的出现，事实表里的记录也在不断更新。加载事实表，通常要加载几百万行数据。为了快速加载，大多数数据库系统都提供了批量加载方式，批量加载方式通常规避了数据库的事务引擎，直接把数据写入到目标表。有时为了提高处理数据的速度，要删除事实表上的所有索引，在加载完后再重建索引。子系统14：代理键管道这个子系统负责抽取正确的代理键，用于加载事实表。这里用“管道”一词是因为事实表的加载看起来像一个工序，工序里的每个环节都使用数据的自然键去查找维度表里的代理键。为了让这个查询过程更高效，最好把要查询的维度数据预先装载到内存里。子系统15：多值维度桥接表生成系统处理不同深度的层次时需要桥接表。例如一个客户，是一个公司，它有子公司和子子公司。每一级的公司都可能去购买商品，如果想从母公司的角度去看一共购买了多少商品，就需要使用桥接表来实现。当有多个维度项和事实表或其他维度表关联时，也要使用桥接表。例如：电影票和电影演员，如果想汇总一个演员有多少电影票收入，就需要在电影和电影演员维度之间建立一个桥接表，这个桥接表把电影和电影演员关联起来，桥接表里还可以设置电影演员的权重因子。子系统16：迟到数据处理到目前为止，我们的讨论都是在要处理数据同时到达的假设前提下。但在一些场合下，并非如此：事实表数据和维度表数据都可能晚到。对事实表来说这不是什么大问题，唯一不同的就是要根据维度的有效时间查找业务发生时的维度代理键。只要在查询条件里增加 valid_from 和 valid_to 两个字段就可以。“维度查询和更新”步骤默认就有这两个字段。如果维度表数据晚到，情况就要麻烦一些。如果事实表已经加载完了，但维度表的数据不是最新的。当要更新的维度数据过来后，按照SCD2，会在维度表里增加一条记录，此时要使用新创建的维度的代理键来更新事实表里有上一个代理键的数据。另外还有一个方法，当事实数据过来，但根据事实表里的维度自然键，从维度表里找不到对应的代理键。此时先创建一个新的维度记录，所有的字段都设置成默认值和空值，使用这条记录的代理键。然后当正确的维度数据从源系统中过来时，再更新这些默认值和空值。子系统17：维度管理系统（中心控制系统）“中心控制系统，用来准备和向数据仓库发布正确的维度”。中心控制系统不只是组织，还负责管理所有和维度相关的任务。子系统18：事实表管理系统这个子系统负责任何创建、组织、管理和事实表相关的任务。子系统17和18在一起结伴工作：事实表管理系统获取到由维度管理系统管理的维度，并把这些维度放到事实表中。子系统19：聚集构建如果数据库是用于分析的，一定会有性能方面的要求。这种对速度的要求产生了几种解决方案，在这几种方案里，聚集表对性能的提升最大。如果能把平均30分钟的响应时间降低到几毫秒，客户会非常高兴。聚集表就可以达到这样的效果。但仅有聚集表是不行的，还需要维护聚集表，数据库还需要知道聚集表的存在以利用聚集表。这也就是MySQL、PostgreSQL、Ingres这些开源产品和Oracle，SQLServer及DB2这些商业产品的差距所在（这些商业产品都有自动聚集导航功能）。有聚集表功能的唯一的一个开源产品是Mondrian，但这些聚集表还是需要由Mondrian聚集表设计器来创建和维护。另外，也可以使用特殊的分析型数据库，如LucidDB、InfoBright、MonetDB、InfiniDB、Ingres/Vectorwise，或把分析型数据库如LucidDB和Pentaho聚集表设计器结合起来。生成和加载聚集表数据只是一次性的工作，但当数据仓库的数据发生变化后，LucidDB和Pentaho聚集表设计器都不会去维护聚集表。子系统20：OLAPCube构建系统OLAP数据库有特殊的存储结构，当加载的时候，可以预先聚集数据。一些OLAP数据库只能写不能更新，所以，在做更新之前要把源数据清除。其他OLAP数据库（如微软的分析服务器）可以更新事实表，但有它自己的更新机制。子系统21：数据整合管理系统这个子系统用来从数据仓库获取数据，并把数据发送到其他环境中，通常用于离线数据分析或者其他特殊目的，如给特定客户发送报表。管理ETL环境后将介绍14个ETL子系统，这些子系统用来完成管理功能。子系统22：作业调度任务的ETL作业任务都需要跑起来，这时就需要作业高度系统来管理这些任务。子系统23：备份系统备份ETL过程中产生的中间数据也应该是ETL方案的一部分。Ralph Kimball推荐在ETL流程中的三个地方缓存（备份）这些数据：从源系统中抽取之后，做任何改动之前。清洗、排重、更正之后，此时可能还在文本文件中或使用正规化的格式。已经做完最后处理，可以写入到数据仓库之前。备份数据仓库本身通常不是ETL团队的工作，但ETL团队可以和DBA紧密合作来实现错误恢复的方案。子系统24：恢复和重新启动系统ETL设计的一个重要部分就是在ETL失败时，可以重新启动。我们要尽量避免丢失数据和重复数据的情况，所以这个子系统非常重要。遵照前面子系统描述的策略可以更容易重新启动一个失败的作业。子系统25：版本控制系统；子系统26：从开发环境到测试、生产环境的版本移植系统有很多种方法可以实现版本控制，可以使用Git这样的版本控制系统来管理。版本控制系统也不应该成为一个事后才想到的问题。在这里引用Ralph Kimball对版本控制系统的观点。你需要给ETL系统里的每个部分都确定一个主版本号，另外ETL系统作为一个整体也要有一个主版本号。这样如果今天发布的版本发生严重的错误，可以快速恢复到昨天的ETL版本。详细见：《The Data Warehouse Lifecycle Toolkit，2nd Edition》。子系统27：工作流监控是否尝试过不使用计时器和温度指示器来烤蛋糕？非常难吧？同样运行一个ETL作业，而不使用任何方法去监控运行过程，不显示执行作业的执行细节，也会使运行ETL作业非常困难。已经处理了多少行，处理的速度有多快？消耗了多少内存？哪条记录出错了，为什么出错？监控过程应该可以回答所有的这些问题，一般日志框架就是监控过程。子系统28：排序系统对于一些操作（如分组、排序合并操作），数据事先要进行排序。一般这个步骤在内存里操作，但如果数据太大了会在硬盘上分页。对于非常大的文件，可以需要独立的排序工具。我们不讨论这些专用的排序工具而只是使用我们的“排序”步骤来完成我们的排序工作。子系统29：血统和依赖分析ETL系统应该同时提供血统分析和影响分析功能。血统分析从处理后的数据开始向后追溯查看这个数据起源于哪里，然后在中间的环节对这个数据进行了哪些处理。依赖或影响分析的方向和血统分析相反，即从数据的起源开始，查看哪些步骤或转换使用了这个数据，这样可以显示出如果一个数据或表发生了变化会影响到系统的哪些部分。子系统30：问题报告系统万一运行中出了错误（相信我们，肯定会出错的），你需要尽快知道运行中发生了错误。子系统31：并行/管道系统为了能在短时间内处理大量数据，任务应该可以并行运行，甚至在多台机器上同时运行。现在的Reactive Stream（反应式流）规范是数据处理很好的一个方式。在云计算环境下（如阿里云的ECS、Amazon的EC2）运行ETL作业，可以避免大规模的硬件投资，以很低的运营成本带来大规模的按需可扩展的计算能力。子系统32：安全系统在现在的IT领域，安全和合规是很热门的主题。数据仓库里保存了企业所有的数据，也是最容易产生风险的地方。另外在很多情况下，ETL过程可以直接访问到很多源系统，所以ETL解决方案本身也是易被攻击的地方。子系统33：合规报告系统确保一个ETL流程遵照规章制度，所需要的大多数方法已经在其他子系统中涉及到了。合规意味着要对数据进行详细的审计，审计包括数据从哪里来，在数据上面执行了什么操作（血统），数据在写入到数据仓库之前是什么样子（基于时间戳的备份），在每个时间点的值是什么（审计表，SCD2），谁访问了数据（日志）。Data Vault是一种提供了很好审计功能的数据模型。子系统34：元数据资源库管理系统这个子系统的目标就是捕获到和ETL相关的所有业务、过程和技术元数据。这个子系统中重要的一部分就是把系统文档化，在第11章介绍，当然Kettle的全部架构也是元数据驱动的，我们曾经在第2章讨论过。总结介绍和解释了Ralph定义的34种ETL子系统。这些子系统的列表也可看成是ETL架构的通用的定义：它描述了每个子系统应该去做那些工作，而不是如何去做或者拿什么工具去做。这34种子系统涉及的四个主要方面如下。抽取：从不同的数据源里获取数据。清洗和更正数据：转换和集成数据，为数据进数据仓库之前做准备。发布数据：加载和更新数据仓库里的数据。管理环境：控制和监控ETL解决方案所有组件的处理过程。**补充：**Data Vault2.0(DV2)是一个商业智能系统，包括建模、方法论、架构和实施这四个方面的最佳实践，包括以下四个组件：a. DV2建模（对模型性能和可扩展性的更改）； b. DV2方法论（遵循Scrum和敏捷最佳实践）； c. DV2架构（包含NoSQL系统和大数据系统）； d. DV2实施（基于模式、自动化生成能力成熟度模型集成（CMMI）第五层级）Data Vault模型是一种中心辐射式模型，其设计重点围绕着业务键的集成模式。这些业务键是存储在多个系统中的、针对各种信息的键（最好是主密钥），用于定位和唯一标识记录或数据。模型中有三种基本的实体（结构）：a. 中心表：唯一业务键的列表，表示了以横向方式贯穿企业的实际业务键或者主密钥集合； b. 链接表：键与键之间唯一关系的列表，表示了企业中存在于业务键之间的关系和联系；c. 卫星表：历史的描述性数据，真正的数据仓库组件，存储了岁时间推移的非易失数据。Data Vault建模的基本规则：a. 业务键是按照粒度和语义内涵进行分割的； b. 关系、事件和跨两个或者多个业务键的交叉关系都要存放在链接结构中； c. 链接结构没有开始或者结束日期，他们只是对数据到达数据仓库那一时刻的关系的一种表达； d. 卫星表是按照数据类型以及变更的类别和速度进行分割的。数据类型一般都是单一的源系统。多链接结构是为了允许DataVault模型中的关系随时间不断扩展，否则数据模型和ELT/ETL装载程序都需要重新构建。散列键代替顺序号是为了在装载数据时消除依赖性，采用顺序方式会迫使负载堆叠到一起，不仅减缓了装载过程，还扼杀了并行处理的可能性，甚至切断了参照完整性。3. Data Vault架构介绍DV基于三层数据仓库架构：集结区（登陆区）、数据仓库和信息交付层（或数据集市）。多层结构使实现人员可以对企业数据仓库去耦合化，将数据来源和获取功能与信息交付和数据供应功能分解开来。转载自ETL里的34个子系统","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ETL","slug":"ETL","permalink":"cpeixin.cn/tags/ETL/"}]},{"title":"比特币走势预测","slug":"比特币走势预测","date":"2018-10-04T16:43:58.000Z","updated":"2020-05-01T14:21:22.500Z","comments":true,"path":"2018/10/05/比特币走势预测/","link":"","permalink":"cpeixin.cn/2018/10/05/%E6%AF%94%E7%89%B9%E5%B8%81%E8%B5%B0%E5%8A%BF%E9%A2%84%E6%B5%8B/","excerpt":"","text":"我们之前介绍了数据挖掘算法中的分类、聚类、回归和关联分析算法，那么对于比特币走势的预测，采用哪种方法比较好呢？可能有些人会认为采用回归分析会好一些，因为预测的结果是连续的数值类型。实际上，数据挖掘算法还有一种叫时间序列分析的算法，时间序列分析模型建立了观察结果与时间变化的关系，能帮我们预测未来一段时间内的结果变化情况。那么时间序列分析和回归分析有哪些区别呢？首先，在选择模型前，我们需要确定结果与变量之间的关系。回归分析训练得到的是目标变量 y 与自变量 x（一个或多个）的相关性，然后通过新的自变量 x 来预测目标变量 y。而时间序列分析得到的是目标变量 y 与时间的相关性。另外，回归分析擅长的是多变量与目标结果之间的分析，即便是单一变量，也往往与时间无关。而时间序列分析建立在时间变化的基础上，它会分析目标变量的趋势、周期、时期和不稳定因素等。这些趋势和周期都是在时间维度的基础上，我们要观察的重要特征。那么针对今天要进行的预测比特币走势的项目，我们都需要掌握哪些目标呢？了解时间序列预测的概念，以及常用的模型算法，包括 AR、MA、ARMA、ARIMA 模型等；掌握并使用 ARMA 模型工具，对一个时间序列数据进行建模和预测；对比特币的历史数据进行时间序列建模，并预测未来 6 个月的走势。时间序列预测关于时间序列，你可以把它理解为按照时间顺序组成的数字序列。实际上在中国古代的农业社会中，人们就将一年中不同时间节点和天气的规律总结了下来，形成了二十四节气，也就是从时间序列中观察天气和太阳的规律（只是当时没有时间序列模型和相应工具），从而使得农业得到迅速发展。在现代社会，时间序列在金融、经济、商业领域拥有广泛的应用。在时间序列预测模型中，有一些经典的模型，包括 AR、MA、ARMA、ARIMA。我来给你简单介绍一下。AR 的英文全称叫做 Auto Regressive，中文叫自回归模型。这个算法的思想比较简单，它认为过去若干时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点。在我们日常生活环境中就存在白噪声，在数据挖掘的过程中，你可以把它理解为一个期望为 0，方差为常数的纯随机过程。AR 模型还存在一个阶数，称为 AR（p）模型，也叫作 p 阶自回归模型。它指的是通过这个时刻点的前 p 个点，通过线性组合再加上白噪声来预测当前时刻点的值。MA 的英文全称叫做 Moving Average，中文叫做滑动平均模型。它与 AR 模型大同小异，AR 模型是历史时序值的线性组合，MA 是通过历史白噪声进行线性组合来影响当前时刻点。AR 模型中的历史白噪声是通过影响历史时序值，从而间接影响到当前时刻点的预测值。同样 MA 模型也存在一个阶数，称为 MA(q) 模型，也叫作 q 阶移动平均模型。我们能看到 AR 和 MA 模型都存在阶数，在 AR 模型中，我们用 p 表示，在 MA 模型中我们用 q 表示，这两个模型大同小异，与 AR 模型不同的是 MA 模型是历史白噪声的线性组合。ARMA 的英文全称是 Auto Regressive Moving Average，中文叫做自回归滑动平均模型，也就是 AR 模型和 MA 模型的混合。相比 AR 模型和 MA 模型，它有更准确的估计。同样 ARMA 模型存在 p 和 q 两个阶数，称为 ARMA(p,q) 模型。ARIMA 的英文全称是 Auto Regressive Integrated Moving Average 模型，中文叫差分自回归滑动平均模型，也叫求合自回归滑动平均模型。相比于 ARMA，ARIMA 多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARIMA 的原理和ARMA 模型一样。相比于 ARMA(p,q) 的两个阶数，ARIMA 是一个三元组的阶数 (p,d,q)，称为 ARIMA(p,d,q) 模型。其中 d 是差分阶数。ARMA 模型工具上面介绍的 AR，MA，ARMA，ARIMA 四种模型，你只需要了解基础概念即可，中间涉及到的一些数学公式这里不进行展开。在实际工作中，我们更多的是使用工具，我在这里主要讲解下如何使用 ARMA 模型工具。在使用 ARMA 工具前，你需要先引用相关工具包：1from statsmodels.tsa.arima_model import ARMA然后通过 ARMA(endog,order,exog=None) 创建 ARMA 类，这里有一些主要的参数简单说明下：endog：英文是 endogenous variable，代表内生变量，又叫非政策性变量，它是由模型决定的，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目中需要用到的变量。order：代表是 p 和 q 的值，也就是 ARMA 中的阶数。exog：英文是 exogenous variables，代表外生变量。外生变量和内生变量一样是经济模型中的两个重要变量。相对于内生变量而言，外生变量又称作为政策性变量，在经济机制内受外部因素的影响，不是我们模型要研究的变量。举个例子，如果我们想要创建 ARMA(7,0) 模型，可以写成：ARMA(data,(7,0))，其中 data 是我们想要观察的变量，(7,0) 代表 (p,q) 的阶数。创建好之后，我们可以通过 fit 函数进行拟合，通过 predict(start, end) 函数进行预测，其中 start 为预测的起始时间，end 为预测的终止时间。下面我们使用 ARMA 模型对一组时间序列做建模，代码如下：1234567891011121314151617181920212223242526# coding:utf-8# 用ARMA进行时间序列预测import pandas as pdimport matplotlib.pyplot as pltimport statsmodels.api as smfrom statsmodels.tsa.arima_model import ARMAfrom statsmodels.graphics.api import qqplot# 创建数据data = [5922, 5308, 5546, 5975, 2704, 1767, 4111, 5542, 4726, 5866, 6183, 3199, 1471, 1325, 6618, 6644, 5337, 7064, 2912, 1456, 4705, 4579, 4990, 4331, 4481, 1813, 1258, 4383, 5451, 5169, 5362, 6259, 3743, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 7644, 6472, 8680, 6366, 5252, 8223, 8181, 10548, 11823, 14640, 9873, 6613, 14415, 13204, 14982, 9690, 10693, 8276, 4519, 7865, 8137, 10022, 7646, 8749, 5246, 4736, 9705, 7501, 9587, 10078, 9732, 6986, 4385, 8451, 9815, 10894, 10287, 9666, 6072, 5418]data=pd.Series(data)data_index = sm.tsa.datetools.dates_from_range('1901','1990')# 绘制数据图data.index = pd.Index(data_index)data.plot(figsize=(12,8))plt.show()# 创建ARMA模型# 创建ARMA模型arma = ARMA(data,(7,0)).fit()print('AIC: %0.4lf' %arma.aic)# 模型预测predict_y = arma.predict('1990', '2000')# 预测结果绘制fig, ax = plt.subplots(figsize=(12, 8))ax = data.ix['1901':].plot(ax=ax)predict_y.plot(ax=ax)plt.show())我创建了 1901 年 -1990 年之间的时间序列数据 data，然后创建 ARMA(7,0) 模型，并传入时间序列数据 data，使用 fit 函数拟合，然后对 1990 年 -2000 年之间的数据进行预测，最后绘制预测结果。你能看到 ARMA 工具的使用还是很方便的，只是我们需要 p 和 q 的取值。实际项目中，我们可以给 p 和 q 指定一个范围，让 ARMA 都运行一下，然后选择最适合的模型。你可能会问，怎么判断一个模型是否适合？我们需要引入 AIC 准则，也叫作赤池消息准则，它是衡量统计模型拟合好坏的一个标准，数值越小代表模型拟合得越好。在这个例子中，你能看到 ARMA(7,0) 这个模型拟合出来的 AIC 是 1619.6323（并不一定是最优）。对比特币走势进行预测我们都知道比特币的走势除了和历史数据以外，还和很多外界因素相关，比如用户的关注度，各国的政策，币圈之间是否打架等等。当然这些外界的因素不是我们这节课需要考虑的对象。假设我们只考虑比特币以往的历史数据，用 ARMA 这个时间序列模型预测比特币的走势。比特币历史数据（从 2012-01-01 到 2018-10-31）可以从 GitHub 上下载：https://github.com/cystanford/bitcoin。你能看到数据一共包括了 8 个字段，代表的含义如下：我们的目标是构造 ARMA 时间序列模型，预测比特币（平均）价格走势。p 和 q 参数具体选择多少呢？我们可以设置一个区间范围，然后选择 AIC 最低的 ARMA 模型。首先我们需要加载数据。在准备阶段，我们需要先探索数据，采用数据可视化方式查看比特币的历史走势。按照不同的时间尺度（天，月，季度，年）可以将数据压缩，得到不同尺度的数据，然后做可视化呈现。这 4 个时间尺度上，我们选择月作为预测模型的时间尺度，相应的，我们选择 Weighted_Price 这个字段的数值作为观察结果，在原始数据中，Weighted_Price 对应的是比特币每天的平均价格，当我们以“月”为单位进行压缩的时候，对应的 Weighted_Price 得到的就是当月的比特币平均价格。压缩代码如下：1df_month = df.resample('M').mean()最后在预测阶段创建 ARMA 时间序列模型。我们并不知道 p 和 q 取什么值时，模型最优，因此我们可以给它们设置一个区间范围，比如都是 range(0,3)，然后计算不同模型的 AIC 数值，选择最小的 AIC 数值对应的那个 ARMA 模型。最后用这个最优的 ARMA 模型预测未来 8 个月的比特币平均价格走势，并将结果做可视化呈现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# -*- coding: utf-8 -*-# 比特币走势预测，使用时间序列ARMAimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom statsmodels.tsa.arima_model import ARMAimport warningsfrom itertools import productfrom datetime import datetimewarnings.filterwarnings('ignore')# 数据加载df = pd.read_csv('./bitcoin_2012-01-01_to_2018-10-31.csv')# 将时间作为df的索引df.Timestamp = pd.to_datetime(df.Timestamp)df.index = df.Timestamp# 数据探索print(df.head())# 按照月，季度，年来统计df_month = df.resample('M').mean()df_Q = df.resample('Q-DEC').mean()df_year = df.resample('A-DEC').mean()# 按照天，月，季度，年来显示比特币的走势fig = plt.figure(figsize=[15, 7])plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.suptitle('比特币金额（美金）', fontsize=20)plt.subplot(221)plt.plot(df.Weighted_Price, '-', label='按天')plt.legend()plt.subplot(222)plt.plot(df_month.Weighted_Price, '-', label='按月')plt.legend()plt.subplot(223)plt.plot(df_Q.Weighted_Price, '-', label='按季度')plt.legend()plt.subplot(224)plt.plot(df_year.Weighted_Price, '-', label='按年')plt.legend()plt.show()# 设置参数范围ps = range(0, 3)qs = range(0, 3)parameters = product(ps, qs)parameters_list = list(parameters)# 寻找最优ARMA模型参数，即best_aic最小results = []best_aic = float(\"inf\") # 正无穷for param in parameters_list: try: model = ARMA(df_month.Weighted_Price,order=(param[0], param[1])).fit() except ValueError: print('参数错误:', param) continue aic = model.aic if aic &lt; best_aic: best_model = model best_aic = aic best_param = param results.append([param, model.aic])# 输出最优模型result_table = pd.DataFrame(results)result_table.columns = ['parameters', 'aic']print('最优模型: ', best_model.summary())# 比特币预测df_month2 = df_month[['Weighted_Price']]date_list = [datetime(2018, 11, 30), datetime(2018, 12, 31), datetime(2019, 1, 31), datetime(2019, 2, 28), datetime(2019, 3, 31), datetime(2019, 4, 30), datetime(2019, 5, 31), datetime(2019, 6, 30)]future = pd.DataFrame(index=date_list, columns= df_month.columns)df_month2 = pd.concat([df_month2, future])df_month2['forecast'] = best_model.predict(start=0, end=91)# 比特币预测结果显示plt.figure(figsize=(20,7))df_month2.Weighted_Price.plot(label='实际金额')df_month2.forecast.plot(color='r', ls='--', label='预测金额')plt.legend()plt.title('比特币金额（月）')plt.xlabel('时间')plt.ylabel('美金')plt.show()结果：))我们通过 product 函数创建了 (p,q) 在 range(0,3) 范围内的所有可能组合，并对每个 ARMA(p,q) 模型进行了 AIC 数值计算，保存了 AIC 数值最小的模型参数。然后用这个模型对比特币的未来 8 个月进行了预测。从结果中你能看到，在 2018 年 10 月之后 8 个月的时间里，比特币会触底到 4000 美金左右，实际上比特币在这个阶段确实降低到了 4000 元美金甚至更低。在时间尺度的选择上，我们选择了月，这样就对数据进行了降维，也节约了 ARMA 的模型训练时间。你能看到比特币金额（美金）这张图中，按月划分的比特币走势和按天划分的比特币走势差别不大，在减少了局部的波动的同时也能体现出比特币的趋势，这样就节约了 ARMA 的模型训练时间。总结今天我给你讲了一个比特币趋势预测的实战项目。通过这个项目你应该能体会到，当我们对一个数值进行预测的时候，如果考虑的是多个变量和结果之间的关系，可以采用回归分析，如果考虑单个时间维度与结果的关系，可以使用时间序列分析。根据比特币的历史数据，我们使用 ARMA 模型对比特币未来 8 个月的走势进行了预测，并对结果进行了可视化显示。你能看到 ARMA 工具还是很好用的，虽然比特币的走势受很多外在因素影响，比如政策环境。不过当我们掌握了这些历史数据，也不妨用时间序列模型来分析预测一下。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"时间序列","slug":"时间序列","permalink":"cpeixin.cn/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"}]},{"title":"Spark SQL用户自定义函数","slug":"Spark-SQL用户自定义函数","date":"2018-10-03T17:45:03.000Z","updated":"2020-09-03T17:46:05.111Z","comments":true,"path":"2018/10/04/Spark-SQL用户自定义函数/","link":"","permalink":"cpeixin.cn/2018/10/04/Spark-SQL%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/","excerpt":"","text":"在实际使用中，函数和自定义函数的使用频率非常高，可以说，对于复杂的需求，如果用好了函数，那么事情会简单许多，反之，则会事倍功半。窗口函数首先，我们来看下窗口函数，窗口函数可以使用户针对某个范围的数据进行聚合操作，如：累积和差值加权移动平均可以想象一个窗口在全量数据集上进行滑动，用户可以自定义在窗口中的操作，如下图所示。使用窗口函数，首先需要定义窗口，DataFrame 提供了 API 定义窗口，以及窗口中的计算逻辑，还是以学生成绩为例，现在需要得出每个学生单科最佳成绩以及成绩所在的年份，这个需求就要用到窗口中的 row_number 函数，row_number 函数可以根据窗口中的数据生成行号，定义窗口窗口函数，代码如下：原始数据：1234567891011121314151617181920212223242526272829303132import org.apache.spark.sql.expressions.&#123;Window, WindowSpec&#125;import org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions._object window_func &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"\") .master(\"local[2]\") .getOrCreate() val student_grade_df: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/student_grade.json\") // 计算需求：每个学生单科最佳成绩以及成绩所在的年份 // 定义窗口函数 import spark.implicits._ val window: WindowSpec = Window .partitionBy(\"name\", \"subject\") .orderBy(student_grade_df(\"grade\").desc) val res_df: DataFrame = student_grade_df .select(student_grade_df(\"name\"), student_grade_df(\"subject\"), student_grade_df(\"year\"), student_grade_df(\"grade\"), row_number().over(window).alias(\"rank_num\") ).where(\"rank_num = 1\") res_df.show() &#125;&#125;结果数据：上面的代码定义了窗口的范围：按照每个人的姓名与科目的组合进行开窗，并控制了数据在窗口中的顺序：按照 grade 降序进行排序，row_number 函数就可以作用在这个窗口上，对每个人每个科目成绩赋予行号此外，DataFrame 还提供了 rowsBetween 和 rangeBetween 来进一步定义窗口范围，其中 rowsBetween 是通过物理行号进行控制，rangeBetween 是通过逻辑条件来对窗口进行控制，来看一个简单的例子，一份两个字段的样例数据：123456789101112&#123;\"key\":\"1\", \"num\":2&#125;&#123;\"key\":\"1\", \"num\":2&#125;&#123;\"key\":\"1\", \"num\":3&#125;&#123;\"key\":\"1\", \"num\":4&#125;&#123;\"key\":\"1\", \"num\":5&#125;&#123;\"key\":\"1\", \"num\":6&#125;&#123;\"key\":\"2\", \"num\":2&#125;&#123;\"key\":\"2\", \"num\":2&#125;&#123;\"key\":\"2\", \"num\":3&#125;&#123;\"key\":\"2\", \"num\":4&#125;&#123;\"key\":\"2\", \"num\":5&#125;&#123;\"key\":\"2\", \"num\":6&#125;现在通过窗口函数对相同 key 的 num 字段做累加计算。代码如下：12345678910111213141516171819202122232425def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"\") .master(\"local[2]\") .getOrCreate() val row_df: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/row_range.json\") row_df.createOrReplaceTempView(\"table\") // 写法：1 spark.sql(\"select key, num, sum(num) over(partition by key order by num range between 2 following and 20 following) as sum from table\") .show() // 写法：2 val windowSlide: WindowSpec = Window .partitionBy(\"key\") .orderBy(\"num\") .rangeBetween(Window.currentRow + 2, Window.currentRow + 20) row_df .select(col(\"key\"),sum(\"num\").over(windowSlide)) .sort(\"key\") .show()在 rangeBetween 中，定义的窗口是当前行的 num 值 +2 到当前行的 num 值 +20 这个区间中的数据，如下所示：12345678910111213141516171819&#123;\"key\":\"1\", \"num\":2&#125; 窗口为[4,22] 累加和为4 + 5 + 6 = 15&#123;\"key\":\"1\", \"num\":2&#125; 窗口为[4,22] 累加和为4 + 5 + 6 = 15&#123;\"key\":\"1\", \"num\":3&#125; 窗口为[5,23] 累加和为5 + 6 = 11&#123;\"key\":\"1\", \"num\":4&#125; 窗口为[6,24] 累加和为6&#123;\"key\":\"1\", \"num\":5&#125; 窗口为[8,25] 累加和为null&#123;\"key\":\"1\", \"num\":6&#125; 窗口为[8,26] 累加和为null&#123;\"key\":\"2\", \"num\":1&#125; 窗口为[3,21] 累加和为12&#123;\"key\":\"2\", \"num\":2&#125; 窗口为[4,22] 累加和为12&#123;\"key\":\"2\", \"num\":5&#125; 窗口为[7,25] 累加和为7&#123;\"key\":\"2\", \"num\":7&#125; 窗口为[9,27] 累加和为nullrangeBetween 通过字段的值定义了参与计算的逻辑窗口大小，也可以使用 rowsBetween 通过行号来指定参与计算的物理窗口，如下所示：123456789val windowSlide &#x3D; Window.partitionBy(&quot;key&quot;).orderBy(&quot;num&quot;).rowsBetween(Window.currentRow - 1,Window.currentRow + 1) dfWin.select(col(&quot;key&quot;),sum(&quot;num&quot;).over(windowSlide)).sort(&quot;key&quot;).show()代码中定义的窗口由当前行、当前行的前一行、当前行的后一行组成，也就是说窗口大小为 3，计算结果如下：12345678910&#123;\"key\":\"1\", \"num\":2&#125; 累加和为2 + 2 = 4&#123;\"key\":\"1\", \"num\":2&#125; 累加和为2 + 2 + 3 = 7&#123;\"key\":\"1\", \"num\":3&#125; 累加和为2 + 3 + 4 = 9&#123;\"key\":\"1\", \"num\":4&#125; 累加和为3 + 4 + 5 = 12&#123;\"key\":\"1\", \"num\":5&#125; 累加和为4 + 5 + 6 = 15&#123;\"key\":\"1\", \"num\":6&#125; 累加和为5 + 6 = 11&#123;\"key\":\"2\", \"num\":1&#125; 累加和为1 + 2 = 3&#123;\"key\":\"2\", \"num\":2&#125; 累加和为1 + 2 + 5 = 8&#123;\"key\":\"2\", \"num\":5&#125; 累加和为2 + 5 + 7 = 14&#123;\"key\":\"2\", \"num\":7&#125; 累加和为5 + 7 = 12函数在需要对数据进行分析的时候，我们经常会使用到函数，Spark SQL 提供了丰富的函数供用户选择，基本涵盖了大部分的日常使用。下面介绍一些常用函数：1. 转换函数cast(value AS type) → type它显式转换一个值的类型。可以将字符串类型的值转为数字类型，反过来转换也可以，在转换失败的时候，会返回 null。这个函数非常常用。2. 数学函数log(double base, Column a)求与以 base 为底的 a 的对数。factorial(Column e)返回 e 的阶乘。3. 字符串函数split(Column str,String pattern)根据正则表达式 pattern 匹配结果作为依据来切分字符串 str。substring(Column str,int pos,int len)返回字符串 str 中，起始位置为 pos，长度为 len 的字符串。concat(Column… exprs)连接多个字符串列，形成一个单独的字符串。translate(Column src,String matchingString,String replaceString)在字符串 src 中，用 replaceString 替换 mathchingString。字符串函数也是非常常用的函数类型。4. 二进制函数bin(Column e)返回输入内容 e 的二进制值。base64(Column e)计算二进制列e的 base64 编码，并以字符串返回。5. 日期时间函数current_date()获取当前日期current_timestamp()获取当前时间戳date_format(Column dateExpr,String format)将日期/时间戳/字符串形式的时间列，按 format 指定的格式表示，并以字符串返回。6. 正则表达式函数regexp_extract(Column e,String exp,int groupIdx)首先在 e 中匹配正则表达式 exp，按照 groupIdx 的值返回结果，groupIdx 默认值为 1，返回第 1 个匹配成功的内容，0 表示返回全部匹配成功的内容。regexp_replace(Column e,String pattern,String replacement)用 replacement 替换在 e 中根据 pattern 匹配成功的字符串。7. JSON 函数get_json_object(Column e,String path)解析 JSON 字符串 e，返回 path 指定的值。8. URL 函数parse_url(string urlString, string partToExtract [, stringkeyToExtract])该函数专门用来解析 URL，提取其中的信息，partToExtract 的选项包含 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、USEINFO，函数会根据选项提取出相应的信息。9. 聚合函数countDistinct(Column expr,Column… exprs)返回一列数据或一组数据中不重复项的个数。expr 为返回 column 的表达式。avg(Column e)返回 e 列的平均数。count(Column e)返回 e 列的行数。max(Column e)返回 e 中的最大值sum(Column e)返回 e 中所有数据之和skewness(Column e)返回 e 列的偏度。stddev_samp(Column e)stddev(Column e)返回 e 的样本标准差。var_samp(Column e)variance(Column e)返回 e 的样本方差。var_pop(Column e)返回 e 的总体方差。这类函数顾名思义，作用于很多行，所以往往与统计分析相关。10. 窗口函数row_number()对窗口中的数据依次赋予行号。rank()与 row_number 函数类似，也是对窗口中的数据依次赋予行号，但是 rank 函数考虑到了 over 子句中排序字段值相同的情况，如下表所示。dense_rank()与 row_number 函数类似，也是对窗口中的数据依次赋予行号，但是 dense_rank 函数考虑到over 子句中排序字段值相同的情况，并保证了序号连续。ntile(n)将每一个窗口中的数据放入 n 个桶中，用 1-n 的数字加以区分。在实际开发过程中，大量的需求都可以直接通过函数以及函数的组合完成，一般来说，函数的丰富程度往往超乎你的想象，所以在面临新需求时，建议首先查阅文档，看看有没有函数可以利用，如果实在不行，我们才会使用用户自定义函数（User Defined Function）。Spark SQL 的函数文档目前我没有发现特别全面的，所以我通常就会直接阅读源码，源码列出了所有的函数，如下：https://github.com/apache/spark/blob/6646b3e13e46b220a33b5798ef266d8a14f3c85b/sql/core/src/main/scala/org/apache/spark/sql/functions.scala用户自定义函数DataFrame API 支持用户自定义函数，自定义函数有两种：UDF 和 UDAF，前者是类似于 map操作的行处理，一行输入一行输出，后者是聚合处理，多行输入，一行输出，先来看看 UDF，下面的代码会开发一个根据得分显示分数等级的函数 level：12345678910111213141516171819202122232425262728293031323334353637import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.functions._import scala.reflect.api.materializeTypeTag object MyUDF &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master(\"local[2]\") .appName(\"Test\") .getOrCreate() import spark.implicits._ val dfSG = spark.read.json(\"examples/target/scala-2.11/classes/student_grade.json\") def level(grade: Int): String = &#123; if(grade &gt;= 85) \"A\" else if(grade &lt; 85 &amp; grade &gt;= 75) \"B\" else if(grade &lt; 75 &amp; grade &gt;= 60) \"C\" else if(grade &lt; 60) \"D\" else \"ERROR\" &#125; val myUDF = udf(level _) dfSG.select(col(\"name\"),myUDF(col(\"grade\"))).show() &#125; &#125;接下来看看 UDAF，UDAF 是用户自定义聚合函数，分为两种：un-type UDAF 和 safe-type UDAF，前者是与 DataFrame 配合使用，后者只能用于 Dataset，UDAF 需要实现 UserDefinedAggregateFunction 抽象类，本例实现了一个求某列最大值的 UDAF，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import org.apache.spark.sql.expressions._import org.apache.spark.sql.types._import org.apache.spark.sql.Rowimport org.apache.spark.sql.functions._import org.apache.spark.sql.SparkSession object MyMaxUDAF extends UserDefinedAggregateFunction &#123; //指定输入的类型 override def inputSchema: StructType = StructType(Array(StructField(\"input\", IntegerType, true))) //指定中间输出的类型，可指定多个 override def bufferSchema: StructType = StructType(Array(StructField(\"max\", IntegerType, true))) //指定最后输出的类型 override def dataType: DataType = IntegerType override def deterministic: Boolean = true //初始化中间结果 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123;buffer(0) = 0&#125; //实现作用在每个分区的结果 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; val temp = input.getAs[Int](0) val current = buffer.getAs[Int](0) if(temp &gt; current) buffer(0) = temp &#125; //合并多个分区的结果 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; if(buffer1.getAs[Int](0) &lt; buffer2.getAs[Int](0)) buffer1(0) = buffer2.getAs[Int](0) &#125; //返回最后的结果 override def evaluate(buffer: Row): Any = buffer.getAs[Int](0)&#125; object MyMaxUDAFDriver extends App&#123; val spark = SparkSession .builder .master(\"local[2]\") .appName(\"Test\") .getOrCreate() import spark.implicits._ val dfSG = spark.read.json(\"examples/target/scala-2.11/classes/student_grade.json\") dfSG.select(MyMaxUDAF(col(\"grade\"))).show()&#125;可以从代码看到 UDAF 的逻辑，还是类似于 MapReduce 的思想，先通过 update 函数处理每个分区，最后再通过 merge 函数汇总结果。Dataset 的 UDAF 对应的是 safe-type UDAF，这种类型的 UDAF 只有 Dataset 能够使用，因为 Dataset 是类型安全的。使用方式和 un-type UDAF 类似，也是先要结合自己聚合的逻辑实现 Aggregator 抽象类，最后再通过 Dataset API 调用，此处实现一个求学生成绩平均值的 UDAF，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import org.apache.spark.sql.Encodersimport org.apache.spark.sql.Encoderimport org.apache.spark.sql.expressions._import org.apache.spark.sql.types._import org.apache.spark.sql.functions._import scala.reflect.api.materializeTypeTagimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.Dataset case class StudentGrade(name: String, subject: String, grade: Long) case class Average(var sum: Long, var count: Long) //这里定义的三个类型分别是输入类型、中间结果类型、输出类型object MyAvgUDAF extends Aggregator[StudentGrade,Average,Double]&#123; //初始中间状态 def zero: Average = Average(0L,0L) //更新中间状态 def reduce(buffer: Average, sg: StudentGrade): Average = &#123; buffer.sum += sg.grade buffer.count += 1 buffer &#125; //合并状态 def merge(b1: Average, b2: Average): Average = &#123; b1.sum += b2.sum b1.count += b2.count b1 &#125; //得到最后结果 def finish(reduction: Average): Double = reduction.sum / reduction.count //为中间结果指定编译器 def bufferEncoder: Encoder[Average] = Encoders.product //为输出结果指定编译器 def outputEncoder: Encoder[Double] = Encoders.scalaDouble &#125; 通过Dataset API调用：object MyAvgUDAFDriver extends App&#123; val spark = SparkSession .builder .master(\"local[2]\") //.config(\"spark.reducer.maxSizeInFlight\", \"128M\") .appName(\"Test\") .getOrCreate() import spark.implicits._ //读取数据 val dfSG = spark.read.json(\"examples/target/scala-2.11/classes/student_grade.json\") //生成Dataset val dsSG: Dataset[StudentGrade] = dfSG.map(a =&gt; StudentGrade(a.getAs[String](0),a.getAs[String](1),a.getAs[Long](2))) //注册UDAF val MyAvg = MyAvgUDAF.toColumn.name(\"MyAvg\") //查询 dsSG.select(MyAvg).show() &#125;自定义函数注册以后，同样可以在 Spark SQL 中使用。总结RDD API、DataFrame API 和 Dataset API，对于数据处理来说，它们都能胜任，那么在实际使用中应该如何选择呢。一般来说，在任何情况下，都不推荐使用 RDD 算子，原因如下：在某种抽象层面上来说，使用 RDD 算子编程相当于直接使用汇编语言或者机器代码进行编程；RDD算子与 SQL、DataFrame API 和 Dataset API 相比，更偏向于如何做，而非做什么，这样优化的空间很少；RDD 语言不如 SQL 语法友好。此外，在其他情况，应优先考虑 Dataset，因为静态类型的特点会使计算更加迅速，但用户必须使用静态语言才行，如 Java 与 Scala，像 Python 这种动态语言是没有 Dataset API 的。下图是用户用不同语言基于 RDD API 和 DataFrame API 开发的应用性能对比，可以看到 Python + RDD API 的组合是远远落后其他组合的，此外，RDD API 开发应用的性能整体要明显落后于 DataFrame API 开发的应用性能。从开发速度和性能上来说，DataFrame + SQL 无疑是最好选择。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"信用卡诈骗分析","slug":"信用卡诈骗分析","date":"2018-10-02T16:43:58.000Z","updated":"2020-05-01T14:20:36.160Z","comments":true,"path":"2018/10/03/信用卡诈骗分析/","link":"","permalink":"cpeixin.cn/2018/10/03/%E4%BF%A1%E7%94%A8%E5%8D%A1%E8%AF%88%E9%AA%97%E5%88%86%E6%9E%90/","excerpt":"","text":"上一篇文章中，我们用随机森林以及之前讲过的 SVM、决策树和 KNN 分类器对信用卡违约数据进行了分析，这节课我们来研究下信用卡欺诈。相比于信用卡违约的比例，信用卡欺诈的比例更小，但是危害极大。如何通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷风险是我们这次项目的目标。通过今天的学习，你需要掌握以下几个方面：了解逻辑回归分类，以及如何在 sklearn 中使用它；信用卡欺诈属于二分类问题，欺诈交易在所有交易中的比例很小，对于这种数据不平衡的情况，到底采用什么样的模型评估标准会更准确；完成信用卡欺诈分析的实战项目，并通过数据可视化对数据探索和模型结果评估进一步加强了解。构建逻辑回归分类器逻辑回归虽然不在我们讲解的十大经典数据挖掘算法里面，但也是常用的数据挖掘算法。逻辑回归，也叫作 logistic 回归。虽然名字中带有“回归”，但它实际上是分类方法，主要解决的是二分类问题，当然它也可以解决多分类问题，只是二分类更常见一些。在逻辑回归中使用了 Logistic 函数，也称为 Sigmoid 函数。Sigmoid 函数是在深度学习中经常用到的函数之一，函数公式为：函数的图形如下所示，类似 S 状：你能看出 g(z) 的结果在 0-1 之间，当 z 越大的时候，g(z) 越大，当 z 趋近于无穷大的时候，g(z) 趋近于 1。同样当 z 趋近于无穷小的时候，g(z) 趋近于 0。同时，函数值以 0.5 为中心。为什么逻辑回归算法是基于 Sigmoid 函数实现的呢？你可以这样理解：我们要实现一个二分类任务，0 即为不发生，1 即为发生。我们给定一些历史数据 X 和 y。其中 X 代表样本的 n 个特征，y 代表正例和负例，也就是 0 或 1 的取值。通过历史样本的学习，我们可以得到一个模型，当给定新的 X 的时候，可以预测出 y。这里我们得到的 y 是一个预测的概率，通常不是 0% 和 100%，而是中间的取值，那么我们就可以认为概率大于 50% 的时候，即为发生（正例），概率小于 50% 的时候，即为不发生（负例）。这样就完成了二分类的预测。逻辑回归模型的求解这里不做介绍，我们来看下如何使用 sklearn 中的逻辑回归工具。在 sklearn 中，我们使用 LogisticRegression() 函数构建逻辑回归分类器，函数里有一些常用的构造参数：penalty：惩罚项，取值为 l1 或 l2，默认为 l2。当模型参数满足高斯分布的时候，使用 l2，当模型参数满足拉普拉斯分布的时候，使用 l1；solver：代表的是逻辑回归损失函数的优化方法。有 5 个参数可选，分别为 liblinear、lbfgs、newton-cg、sag 和 saga。默认为 liblinear，适用于数据量小的数据集，当数据量大的时候可以选用 sag 或 saga 方法。max_iter：算法收敛的最大迭代次数，默认为 10。n_jobs：拟合和预测的时候 CPU 的核数，默认是 1，也可以是整数，如果是 -1 则代表 CPU 的核数。当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。模型评估指标我们之前对模型做评估时，通常采用的是准确率 (accuracy)，它指的是分类器正确分类的样本数与总体样本数之间的比例。这个指标对大部分的分类情况是有效的，不过当分类结果严重不平衡的时候，准确率很难反应模型的好坏。举个例子，对于机场安检中恐怖分子的判断，就不能采用准确率对模型进行评估。我们知道恐怖分子的比例是极低的，因此当我们用准确率做判断时，如果准确率高达 99.999%，就说明这个模型一定好么？其实正因为现实生活中恐怖分子的比例极低，就算我们不能识别出一个恐怖分子，也会得到非常高的准确率。因为准确率的评判标准是正确分类的样本个数与总样本数之间的比例。因此非恐怖分子的比例会很高，就算我们识别不出来恐怖分子，正确分类的个数占总样本的比例也会很高，也就是准确率高。实际上我们应该更关注恐怖分子的识别，这里先介绍下数据预测的四种情况：TP、FP、TN、FN。我们用第二个字母 P 或 N 代表预测为正例还是负例，P 为正，N 为负。第一个字母 T 或 F 代表的是预测结果是否正确，T 为正确，F 为错误。所以四种情况分别为：TP：预测为正，判断正确；FP：预测为正，判断错误；TN：预测为负，判断正确；FN：预测为负，判断错误。我们知道样本总数 =TP+FP+TN+FN，预测正确的样本数为 TP+TN，因此准确率 Accuracy = (TP+TN)/(TP+TN+FN+FP)。实际上，对于分类不平衡的情况，有两个指标非常重要，它们分别是精确度和召回率。精确率 P = TP/ (TP+FP)，对应上面恐怖分子这个例子，在所有判断为恐怖分子的人数中，真正是恐怖分子的比例。召回率 R = TP/ (TP+FN)，也称为查全率。代表的是恐怖分子被正确识别出来的个数与恐怖分子总数的比例。为什么要统计召回率和精确率这两个指标呢？假设我们只统计召回率，当召回率等于 100% 的时候，模型是否真的好呢？举个例子，假设我们把机场所有的人都认为是恐怖分子，恐怖分子都会被正确识别，这个数字与恐怖分子的总数比例等于 100%，但是这个结果是没有意义的。如果我们认为机场里所有人都是恐怖分子的话，那么非恐怖分子（极高比例）都会认为是恐怖分子，误判率太高了，所以我们还需要统计精确率作为召回率的补充。实际上有一个指标综合了精确率和召回率，可以更好地评估模型的好坏。这个指标叫做 F1，用公式表示为：F1 作为精确率 P 和召回率 R 的调和平均，数值越大代表模型的结果越好。信用卡欺诈分析我们来看一个信用卡欺诈分析的项目，这个数据集你可以从百度网盘（因为数据集大于 100M，所以采用了网盘）中下载：链接：https://pan.baidu.com/s/14F8WuX0ZJntdB_r1EC08HA 提取码：58gp数据集包括了 2013 年 9 月份两天时间内的信用卡交易数据，284807 笔交易中，一共有 492 笔是欺诈行为。输入数据一共包括了 28 个特征 V1，V2，……V28 对应的取值，以及交易时间 Time 和交易金额 Amount。为了保护数据隐私，我们不知道 V1 到 V28 这些特征代表的具体含义，只知道这 28 个特征值是通过 PCA 变换得到的结果。另外字段 Class 代表该笔交易的分类，Class=0 为正常（非欺诈），Class=1 代表欺诈。我们的目标是针对这个数据集构建一个信用卡欺诈分析的分类器，采用的是逻辑回归。从数据中你能看到欺诈行为只占到了 492/284807=0.172%，数据分类结果的分布是非常不平衡的，因此我们不能使用准确率评估模型的好坏，而是需要统计 F1 值（综合精确率和召回率）。我们先梳理下整个项目的流程：加载数据；准备阶段：我们需要探索数据，用数据可视化的方式查看分类结果的情况，以及随着时间的变化，欺诈交易和正常交易的分布情况。上面已经提到过，V1-V28 的特征值都经过 PCA 的变换，但是其余的两个字段，Time 和 Amount 还需要进行规范化。Time 字段和交易本身是否为欺诈交易无关，因此我们不作为特征选择，只需要对 Amount 做数据规范化就行了。同时数据集没有专门的测试集，使用 train_test_split 对数据集进行划分；分类阶段：我们需要创建逻辑回归分类器，然后传入训练集数据进行训练，并传入测试集预测结果，将预测结果与测试集的结果进行比对。这里的模型评估指标用到了精确率、召回率和 F1 值。同时我们将精确率 - 召回率进行了可视化呈现。基于上面的流程，具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# -*- coding:utf-8 -*-# 使用逻辑回归对信用卡欺诈进行分类import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport itertoolsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matrix, precision_recall_curvefrom sklearn.preprocessing import StandardScalerimport warningswarnings.filterwarnings('ignore') # 混淆矩阵可视化def plot_confusion_matrix(cm, classes, normalize = False, title = 'Confusion matrix\"', cmap = plt.cm.Blues) : plt.figure() plt.imshow(cm, interpolation = 'nearest', cmap = cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation = 0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) : plt.text(j, i, cm[i, j], horizontalalignment = 'center', color = 'white' if cm[i, j] &gt; thresh else 'black') plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') plt.show() # 显示模型评估结果def show_metrics(): tp = cm[1,1] fn = cm[1,0] fp = cm[0,1] tn = cm[0,0] print('精确率: &#123;:.3f&#125;'.format(tp/(tp+fp))) print('召回率: &#123;:.3f&#125;'.format(tp/(tp+fn))) print('F1值: &#123;:.3f&#125;'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))))# 绘制精确率-召回率曲线def plot_precision_recall(): plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post') plt.fill_between(recall, precision, step ='post', alpha = 0.2, color = 'b') plt.plot(recall, precision, linewidth=2) plt.xlim([0.0,1]) plt.ylim([0.0,1.05]) plt.xlabel('召回率') plt.ylabel('精确率') plt.title('精确率-召回率 曲线') plt.show(); # 数据加载data = pd.read_csv('./creditcard.csv')# 数据探索print(data.describe())# 设置plt正确显示中文plt.rcParams['font.sans-serif'] = ['SimHei']# 绘制类别分布plt.figure()ax = sns.countplot(x = 'Class', data = data)plt.title('类别分布')plt.show()# 显示交易笔数，欺诈交易笔数num = len(data)num_fraud = len(data[data['Class']==1]) print('总交易笔数: ', num)print('诈骗交易笔数：', num_fraud)print('诈骗交易比例：&#123;:.6f&#125;'.format(num_fraud/num))# 欺诈和正常交易可视化f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))bins = 50ax1.hist(data.Time[data.Class == 1], bins = bins, color = 'deeppink')ax1.set_title('诈骗交易')ax2.hist(data.Time[data.Class == 0], bins = bins, color = 'deepskyblue')ax2.set_title('正常交易')plt.xlabel('时间')plt.ylabel('交易次数')plt.show()# 对Amount进行数据规范化data['Amount_Norm'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))# 特征选择y = np.array(data.Class.tolist())data = data.drop(['Time','Amount','Class'],axis=1)X = np.array(data.as_matrix())# 准备训练集和测试集train_x, test_x, train_y, test_y = train_test_split (X, y, test_size = 0.1, random_state = 33) # 逻辑回归分类clf = LogisticRegression()clf.fit(train_x, train_y)predict_y = clf.predict(test_x)# 预测样本的置信分数score_y = clf.decision_function(test_x) # 计算混淆矩阵，并显示cm = confusion_matrix(test_y, predict_y)class_names = [0,1]# 显示混淆矩阵plot_confusion_matrix(cm, classes = class_names, title = '逻辑回归 混淆矩阵')# 显示模型评估分数show_metrics()# 计算精确率，召回率，阈值用于可视化precision, recall, thresholds = precision_recall_curve(test_y, score_y)plot_precision_recall()结果：123456789101112 Time V1 ... Amount Classcount 284807.000000 2.848070e+05 ... 284807.000000 284807.000000mean 94813.859575 1.165980e-15 ... 88.349619 0.001727std 47488.145955 1.958696e+00 ... 250.120109 0.041527min 0.000000 -5.640751e+01 ... 0.000000 0.00000025% 54201.500000 -9.203734e-01 ... 5.600000 0.00000050% 84692.000000 1.810880e-02 ... 22.000000 0.00000075% 139320.500000 1.315642e+00 ... 77.165000 0.000000max 172792.000000 2.454930e+00 ... 25691.160000 1.000000[8 rows x 31 columns]1234总交易笔数: 284807诈骗交易笔数： 492诈骗交易比例：0.001727)1234精确率: 0.841召回率: 0.617F1值: 0.712你能看出来欺诈交易的笔数为 492 笔，占所有交易的比例是很低的，即 0.001727，我们可以通过数据可视化的方式对欺诈交易和正常交易的分布进行呈现。另外通过可视化，我们也能看出精确率和召回率之间的关系，当精确率高的时候，召回率往往很低，召回率高的时候，精确率会比较低。代码有一些模块需要说明下。我定义了 plot_confusion_matrix 函数对混淆矩阵进行可视化。什么是混淆矩阵呢？混淆矩阵也叫误差矩阵，实际上它就是 TP、FP、TN、FN 这四个数值的矩阵表示，帮助我们判断预测值和实际值相比，对了多少。从这个例子中，你能看出 TP=37，FP=7，FN=23。所以精确率 P=TP/(TP+FP)=37/(37+7)=0.841，召回率 R=TP/(TP+FN)=37/(37+23)=0.617。然后使用了 sklearn 中的 precision_recall_curve 函数，通过预测值和真实值来计算精确率 - 召回率曲线。precision_recall_curve 函数会计算在不同概率阈值情况下的精确率和召回率。最后定义 plot_precision_recall 函数，绘制曲线。总结今天我给你讲了逻辑回归的概念和相关工具的使用，另外学习了在数据样本不平衡的情况下，如何评估模型。这里你需要了解精确率，召回率和 F1 的概念和计算方式。最后在信用卡欺诈分析的项目中，我们使用了逻辑回归工具，并对混淆矩阵进行了计算，同时在模型结果评估中，使用了精确率、召回率和 F1 值，最后得到精确率 - 召回率曲线的可视化结果。从这个项目中你能看出来，不是所有的分类都是样本平衡的情况，针对正例比例极低的情况，比如信用卡欺诈、某些疾病的识别，或者是恐怖分子的判断等，都需要采用精确率 - 召回率来进行统计。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"逻辑回归","slug":"逻辑回归","permalink":"cpeixin.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}]},{"title":"信用卡风险评估","slug":"信用卡风险评估","date":"2018-09-30T17:21:02.000Z","updated":"2020-05-01T14:20:47.150Z","comments":true,"path":"2018/10/01/信用卡风险评估/","link":"","permalink":"cpeixin.cn/2018/10/01/%E4%BF%A1%E7%94%A8%E5%8D%A1%E9%A3%8E%E9%99%A9%E8%AF%84%E4%BC%B0/","excerpt":"","text":"今天我来带你做一个数据挖掘的项目。在数据挖掘的过程中，我们经常会遇到一些问题，比如：如何选择各种分类器，到底选择哪个分类算法，是 SVM，决策树，还是 KNN？如何优化分类器的参数，以便得到更好的分类准确率？这两个问题，是数据挖掘核心的问题。当然对于一个新的项目，我们还有其他的问题需要了解，比如掌握数据探索和数据可视化的方式，还需要对数据的完整性和质量做评估。这些内容我在之前的课程中都有讲到过。今天的学习主要围绕下面的三个目标，并通过它们完成信用卡违约率项目的实战这三个目标分别是：创建各种分类器，包括已经掌握的 SVM、决策树、KNN 分类器，以及随机森林分类器；掌握 GridSearchCV 工具，优化算法模型的参数；使用 Pipeline 管道机制进行流水线作业。因为在做分类之前，我们还需要一些准备过程，比如数据规范化，或者数据降维等。构建随机森林分类器在算法篇中，我主要讲了数据挖掘十大经典算法。实际工作中，你也可能会用到随机森林。随机森林的英文是 Random Forest，英文简写是 RF。它实际上是一个包含多个决策树的分类器，每一个子分类器都是一棵 CART 分类回归树。所以随机森林既可以做分类，又可以做回归。当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。当它做回归的时候，输出结果是每棵 CART 树的回归结果的平均值。在 sklearn 中，我们使用 RandomForestClassifier() 构造随机森林模型，函数里有一些常用的构造参数：当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。使用 GridSearchCV 工具对模型参数进行调优在做分类算法的时候，我们需要经常调节网络参数（对应上面的构造参数），目的是得到更好的分类结果。实际上一个分类算法有很多参数，取值范围也比较广，那么该如何调优呢？Python 给我们提供了一个很好用的工具 GridSearchCV，它是 Python 的参数自动搜索模块。我们只要告诉它想要调优的参数有哪些以及参数的取值范围，它就会把所有的情况都跑一遍，然后告诉我们哪个参数是最优的，结果如何。使用 GridSearchCV 模块需要先引用工具包，方法如下：1from sklearn.model_selection import GridSearchCV然后我们使用 GridSearchCV(estimator, param_grid, cv=None, scoring=None) 构造参数的自动搜索模块，这里有一些主要的参数需要说明下：构造完 GridSearchCV 之后，我们就可以使用 fit 函数拟合训练，使用 predict 函数预测，这时预测采用的是最优参数情况下的分类器。这里举一个简单的例子，我们用 sklearn 自带的 IRIS 数据集，采用随机森林对 IRIS 数据分类。假设我们想知道 n_estimators 在 1-10 的范围内取哪个值的分类结果最好，可以编写代码：12345678910111213141516# -*- coding: utf-8 -*-# 使用RandomForest对IRIS数据集进行分类# 利用GridSearchCV寻找最优参数from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisrf = RandomForestClassifier()parameters = &#123;\"n_estimators\": range(1,11)&#125;iris = load_iris()# 使用GridSearchCV进行参数调优clf = GridSearchCV(estimator=rf, param_grid=parameters)# 对iris数据集进行分类clf.fit(iris.data, iris.target)print(\"最优分数： %.4lf\" %clf.best_score_)print(\"最优参数：\", clf.best_params_)123运行结果如下：最优分数： 0.9667最优参数： &#123;'n_estimators': 6&#125;你能看到当我们采用随机森林作为分类器的时候，最优准确率是 0.9667，当 n_estimators=6 的时候，是最优参数，也就是随机森林一共有 6 个子决策树。使用 Pipeline 管道机制进行流水线作业做分类的时候往往都是有步骤的，比如先对数据进行规范化处理，你也可以用 PCA 方法（一种常用的降维方法）对数据降维，最后使用分类器分类。Python 有一种 Pipeline 管道机制。管道机制就是让我们把每一步都按顺序列下来，从而创建 Pipeline 流水线作业。每一步都采用 (‘名称’, 步骤) 的方式来表示。我们需要先采用 StandardScaler 方法对数据规范化，即采用数据规范化为均值为 0，方差为 1 的正态分布，然后采用 PCA 方法对数据进行降维，最后采用随机森林进行分类。具体代码如下：1234567from sklearn.model_selection import GridSearchCVpipeline = Pipeline([ ('scaler', StandardScaler()), ('pca', PCA()), ('randomforestclassifier', RandomForestClassifier())])那么我们现在采用 Pipeline 管道机制，用随机森林对 IRIS 数据集做一下分类。先用 StandardScaler 方法对数据规范化，然后再用随机森林分类，编写代码如下：123456789101112131415161718192021# -*- coding: utf-8 -*-# 使用RandomForest对IRIS数据集进行分类# 利用GridSearchCV寻找最优参数,使用Pipeline进行流水作业from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinerf = RandomForestClassifier()parameters = &#123;\"randomforestclassifier__n_estimators\": range(1,11)&#125;iris = load_iris()pipeline = Pipeline([ ('scaler', StandardScaler()), ('randomforestclassifier', rf)])# 使用GridSearchCV进行参数调优clf = GridSearchCV(estimator=pipeline, param_grid=parameters)# 对iris数据集进行分类clf.fit(iris.data, iris.target)print(\"最优分数： %.4lf\" %clf.best_score_)print(\"最优参数：\", clf.best_params_)123运行结果：最优分数： 0.9667最优参数： &#123;'randomforestclassifier__n_estimators': 9&#125;你能看到是否采用数据规范化对结果还是有一些影响的，有了 GridSearchCV 和 Pipeline 这两个工具之后，我们在使用分类器的时候就会方便很多。对信用卡违约率进行分析我们现在来做一个信用卡违约率的项目，这个数据集你可以从 GitHub 上下载：https://github.com/cystanford/credit_default。这个数据集是台湾某银行 2005 年 4 月到 9 月的信用卡数据，数据集一共包括 25 个字段，具体含义如下：现在我们的目标是要针对这个数据集构建一个分析信用卡违约率的分类器。具体选择哪个分类器，以及分类器的参数如何优化，我们可以用 GridSearchCV 这个工具跑一遍。先梳理下整个项目的流程：加载数据；准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更直观的了解，比如我们想要了解信用卡违约率和不违约率的人数。因为数据集没有专门的测试集，我们还需要使用 train_test_split 划分数据集。分类阶段：之所以把数据规范化放到这个阶段，是因为我们可以使用 Pipeline 管道机制，将数据规范化设置为第一步，分类为第二步。因为我们不知道采用哪个分类器效果好，所以我们需要多用几个分类器，比如 SVM、决策树、随机森林和 KNN。然后通过 GridSearchCV 工具，找到每个分类器的最优参数和最优分数，最终找到最适合这个项目的分类器和该分类器的参数。基于上面的流程，具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import matplotlibimport pandas as pdfrom matplotlib import pyplot as pltimport seaborn as snsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import train_test_split, GridSearchCV# 构造各种分类器from sklearn.neighbors import KNeighborsClassifierfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierclassifiers = [ SVC(random_state = 1, kernel = 'rbf'), DecisionTreeClassifier(random_state = 1, criterion = 'gini'), RandomForestClassifier(random_state = 1, criterion = 'gini'), KNeighborsClassifier(metric = 'minkowski'),]# 分类器名称classifier_names = [ 'svc', 'decisiontreeclassifier', 'randomforestclassifier', 'kneighborsclassifier',]# 分类器参数classifier_param_grid = [ &#123;'svc__C':[1], 'svc__gamma':[0.01]&#125;, &#123;'decisiontreeclassifier__max_depth':[6,9,11]&#125;, &#123;'randomforestclassifier__n_estimators':[3,5,6]&#125; , &#123;'kneighborsclassifier__n_neighbors':[4,6,8]&#125;,]# 对具体的分类器进行GridSearchCV参数调优def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = 'accuracy'): response = &#123;&#125; gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score) # 寻找最优的参数 和最优的准确率分数 search = gridsearch.fit(train_x, train_y) print(\"GridSearch最优参数：\", search.best_params_) print(\"GridSearch最优分数： %0.4lf\" %search.best_score_) predict_y = gridsearch.predict(test_x) print(\"准确率 %0.4lf\" %accuracy_score(test_y, predict_y)) response['predict_y'] = predict_y response['accuracy_score'] = accuracy_score(test_y,predict_y) return responsedef get_data(): data = pd.read_csv('UCI_Credit_Card.csv') # 数据条数和字段数量 print(data.shape) # 数据探索 print(data.describe) return datadef show_data(data): # 查看下一个月的违约率 next_month = data['default.payment.next.month'].value_counts() print(next_month) # 统计违约率结果 df = pd.DataFrame(&#123;'default.payment.next.month': next_month.index, 'values': next_month.values&#125;) print(df) # 正常显示中文标签 # plt.rcParams['font.sans-serif'] = ['FangSong'] # 用来正常显示中文标签 plt.rcParams[\"font.family\"] = 'Arial Unicode MS' plt.figure(figsize=(6, 6)) plt.title('信用卡违约率客户 (违约：1，守约：0)') sns.set_color_codes('pastel') sns.barplot(x='default.payment.next.month', y=\"values\", data=df) locs, labels = plt.xticks() plt.show()def get_feature(data): # 特征选择，去掉ID字段、最后一个结果字段即可 data.drop(['ID'], inplace=True, axis=1) # ID这个字段没有用 target = data['default.payment.next.month'].values columns = data.columns.tolist() columns.remove('default.payment.next.month') features = data[columns].values return features, targetdef main(): data = get_data() show_data(data) features, target = get_feature(data) # 30%作为测试集，其余作为训练集 train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1) for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid): pipeline = Pipeline([ ('scaler', StandardScaler()), (model_name, model) ]) result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid, score='accuracy')if __name__ == '__main__': main()结果：1234567891011121314151617181920212223242526272829303132(30000, 25)&lt;bound method NDFrame.describe of ID LIMIT_BAL SEX ... PAY_AMT5 PAY_AMT6 default.payment.next.month0 1 20000.0 2 ... 0.0 0.0 11 2 120000.0 2 ... 0.0 2000.0 12 3 90000.0 2 ... 1000.0 5000.0 03 4 50000.0 2 ... 1069.0 1000.0 04 5 50000.0 1 ... 689.0 679.0 0 ... ... ... ... ... ... ...29995 29996 220000.0 1 ... 5000.0 1000.0 029996 29997 150000.0 1 ... 0.0 0.0 029997 29998 30000.0 1 ... 2000.0 3100.0 129998 29999 80000.0 1 ... 52964.0 1804.0 129999 30000 50000.0 1 ... 1000.0 1000.0 1[30000 rows x 25 columns]&gt;0 233641 6636Name: default.payment.next.month, dtype: int64 default.payment.next.month values0 0 233641 1 6636GridSearch最优参数： &#123;'svc__C': 1, 'svc__gamma': 0.01&#125;GridSearch最优分数： 0.8186准确率 0.8172GridSearch最优参数： &#123;'decisiontreeclassifier__max_depth': 6&#125;GridSearch最优分数： 0.8208准确率 0.8113GridSearch最优参数： &#123;'randomforestclassifier__n_estimators': 6&#125;GridSearch最优分数： 0.8004准确率 0.7994GridSearch最优参数： &#123;'kneighborsclassifier__n_neighbors': 8&#125;GridSearch最优分数： 0.8040准确率 0.8036从结果中，我们能看到 SVM 分类器的准确率最高，测试准确率为 0.8172。在决策树分类中，我设置了 3 种最大深度，当最大深度 =6 时结果最优，测试准确率为 0.8113；在随机森林分类中，我设置了 3 个决策树个数的取值，取值为 6 时结果最优，测试准确率为 0.7994；在 KNN 分类中，我设置了 3 个 n 的取值，取值为 8 时结果最优，测试准确率为 0.8036。总结今天我给你讲了随机森林的概念及工具的使用，另外针对数据挖掘算法中经常采用的参数调优，也介绍了 GridSearchCV 工具这个利器。并将这两者结合起来，在信用卡违约分析这个项目中进行了使用。很多时候，我们不知道该采用哪种分类算法更适合。即便是对于一种分类算法，也有很多参数可以调优，每个参数都有一定的取值范围。我们可以把想要采用的分类器，以及这些参数的取值范围都设置到数组里，然后使用 GridSearchCV 工具进行调优。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Random Forest","slug":"Random-Forest","permalink":"cpeixin.cn/tags/Random-Forest/"}]},{"title":"数据分析 - PageRank 实战","slug":"数据分析 - PageRank-实战","date":"2018-09-12T16:17:42.000Z","updated":"2020-05-01T14:20:06.568Z","comments":true,"path":"2018/09/13/数据分析 - PageRank-实战/","link":"","permalink":"cpeixin.cn/2018/09/13/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%AE%9E%E6%88%98/","excerpt":"","text":"上文讲了 PageRank 算法经常被用到网络关系的分析中，比如在社交网络中计算个人的影响力，计算论文的影响力或者网站的影响力等。今天我们就来做一个关于 PageRank 算法的实战，在这之前，你需要思考三个问题：如何使用工具完成 PageRank 算法，包括使用工具创建网络图，设置节点、边、权重等，并通过创建好的网络图计算节点的 PR 值；对于一个实际的项目，比如希拉里的 9306 封邮件（工具包中邮件的数量），如何使用 PageRank 算法挖掘出有影响力的节点，并且绘制网络图；如何对创建好的网络图进行可视化，如果网络中的节点数较多，如何筛选重要的节点进行可视化，从而得到精简的网络关系图。如何使用工具实现 PageRank 算法PageRank 算法工具在 sklearn 中并不存在，我们需要找到新的工具包。实际上有一个关于图论和网络建模的工具叫 NetworkX，它是用 Python 语言开发的工具，内置了常用的图与网络分析算法，可以方便我们进行网络数据分析。上节课，我举了一个网页权重的例子，假设一共有 4 个网页 A、B、C、D，它们之间的链接信息如图所示：针对这个例子，我们看下用 NetworkX 如何计算 A、B、C、D 四个网页的 PR 值，具体代码如下：123456789import networkx as nx# 创建有向图G = nx.DiGraph() # 有向图之间边的关系edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"), (\"B\", \"A\"), (\"B\", \"D\"), (\"C\", \"A\"), (\"D\", \"B\"), (\"D\", \"C\")]for edge in edges: G.add_edge(edge[0], edge[1])pagerank_list = nx.pagerank(G, alpha=1)print(\"pagerank值是：\", pagerank_list)结果：1pagerank值是： &#123;&#39;A&#39;: 0.33333396911621094, &#39;B&#39;: 0.22222201029459634, &#39;C&#39;: 0.22222201029459634, &#39;D&#39;: 0.22222201029459634&#125;我们通过 NetworkX 创建了一个有向图之后，设置了节点之间的边，然后使用 PageRank 函数就可以求得节点的 PR 值，结果和上节课中我们人工模拟的结果一致。好了，运行完这个例子之后，我们来看下 NetworkX 工具都有哪些常用的操作。1. 关于图的创建图可以分为无向图和有向图，在 NetworkX 中分别采用不同的函数进行创建。无向图指的是不用节点之间的边的方向，使用 nx.Graph() 进行创建；有向图指的是节点之间的边是有方向的，使用 nx.DiGraph() 来创建。在上面这个例子中，存在 A→D 的边，但不存在 D→A 的边。2. 关于节点的增加、删除和查询如果想在网络中增加节点，可以使用 G.add_node(‘A’) 添加一个节点，也可以使用 G.add_nodes_from([‘B’,‘C’,‘D’,‘E’]) 添加节点集合。如果想要删除节点，可以使用 G.remove_node(node) 删除一个指定的节点，也可以使用 G.remove_nodes_from([‘B’,‘C’,‘D’,‘E’]) 删除集合中的节点。那么该如何查询节点呢？如果你想要得到图中所有的节点，就可以使用 G.nodes()，也可以用 G.number_of_nodes() 得到图中节点的个数。3. 关于边的增加、删除、查询增加边与添加节点的方式相同，使用 G.add_edge(“A”, “B”) 添加指定的“从 A 到 B”的边，也可以使用 add_edges_from 函数从边集合中添加。我们也可以做一个加权图，也就是说边是带有权重的，使用 add_weighted_edges_from 函数从带有权重的边的集合中添加。在这个函数的参数中接收的是 1 个或多个三元组[u,v,w]作为参数，u、v、w 分别代表起点、终点和权重。另外，我们可以使用 remove_edge 函数和 remove_edges_from 函数删除指定边和从边集合中删除。另外可以使用 edges() 函数访问图中所有的边，使用 number_of_edges() 函数得到图中边的个数。以上是关于图的基本操作，如果我们创建了一个图，并且对节点和边进行了设置，就可以找到其中有影响力的节点，原理就是通过 PageRank 算法，使用 nx.pagerank(G) 这个函数，函数中的参数 G 代表创建好的图。如何用 PageRank 揭秘希拉里邮件中的人物关系了解了 NetworkX 工具的基础使用之后，我们来看一个实际的案例：希拉里邮件人物关系分析。希拉里邮件事件相信你也有耳闻，对这个数据的背景我们就不做介绍了。你可以从 GitHub 上下载这个数据集：https://github.com/cystanford/PageRank。整个数据集由三个文件组成：Aliases.csv，Emails.csv 和 Persons.csv，其中 Emails 文件记录了所有公开邮件的内容，发送者和接收者的信息。Persons 这个文件统计了邮件中所有人物的姓名及对应的 ID。因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用 Aliases 文件来查询别名和人物的对应关系。整个数据集包括了 9306 封邮件和 513 个人名，数据集还是比较大的。不过这一次我们不需要对邮件的内容进行分析，只需要通过邮件中的发送者和接收者（对应 Emails.csv 文件中的 MetadataFrom 和 MetadataTo 字段）来绘制整个关系网络。因为涉及到的人物很多，因此我们需要通过 PageRank 算法计算每个人物在邮件关系网络中的权重，最后筛选出来最有价值的人物来进行关系网络图的绘制。了解了数据集和项目背景之后，我们来设计到执行的流程步骤：首先我们需要加载数据源；在准备阶段：我们需要对数据进行探索，在数据清洗过程中，因为邮件中存在别名的情况，因此我们需要统一人物名称。另外邮件的正文并不在我们考虑的范围内，只统计邮件中的发送者和接收者，因此我们筛选 MetadataFrom 和 MetadataTo 这两个字段作为特征。同时，发送者和接收者可能存在多次邮件往来，需要设置权重来统计两人邮件往来的次数。次数越多代表这个边（从发送者到接收者的边）的权重越高；在挖掘阶段：我们主要是对已经设置好的网络图进行 PR 值的计算，但邮件中的人物有 500 多人，有些人的权重可能不高，我们需要筛选 PR 值高的人物，绘制出他们之间的往来关系。在可视化的过程中，我们可以通过节点的 PR 值来绘制节点的大小，PR 值越大，节点的绘制尺寸越大。设置好流程之后，实现的代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# -*- coding: utf-8 -*-# 用 PageRank 挖掘希拉里邮件中的重要任务关系import pandas as pdimport networkx as nximport numpy as npfrom collections import defaultdictimport matplotlib.pyplot as plt# 数据加载emails = pd.read_csv(\"./input/Emails.csv\")# 读取别名文件file = pd.read_csv(\"./input/Aliases.csv\")aliases = &#123;&#125;for index, row in file.iterrows(): aliases[row['Alias']] = row['PersonId']# 读取人名文件file = pd.read_csv(\"./input/Persons.csv\")persons = &#123;&#125;for index, row in file.iterrows(): persons[row['Id']] = row['Name']# 针对别名进行转换 def unify_name(name): # 姓名统一小写 name = str(name).lower() # 去掉, 和 @后面的内容 name = name.replace(\",\",\"\").split(\"@\")[0] # 别名转换 if name in aliases.keys(): return persons[aliases[name]] return name# 画网络图def show_graph(graph, layout='spring_layout'): # 使用 Spring Layout 布局，类似中心放射状 if layout == 'circular_layout': positions=nx.circular_layout(graph) else: positions=nx.spring_layout(graph) # 设置网络图中的节点大小，大小与 pagerank 值相关，因为 pagerank 值很小所以需要 *20000 nodesize = [x['pagerank']*20000 for v,x in graph.nodes(data=True)] # 设置网络图中的边长度 edgesize = [np.sqrt(e[2]['weight']) for e in graph.edges(data=True)] # 绘制节点 nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=0.4) # 绘制边 nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=0.2) # 绘制节点的 label nx.draw_networkx_labels(graph, positions, font_size=10) # 输出希拉里邮件中的所有人物关系图 plt.show()# 将寄件人和收件人的姓名进行规范化emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)emails.MetadataTo = emails.MetadataTo.apply(unify_name)# 设置遍的权重等于发邮件的次数edges_weights_temp = defaultdict(list)for row in zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText): temp = (row[0], row[1]) if temp not in edges_weights_temp: edges_weights_temp[temp] = 1 else: edges_weights_temp[temp] = edges_weights_temp[temp] + 1# 转化格式 (from, to), weight =&gt; from, to, weightedges_weights = [(key[0], key[1], val) for key, val in edges_weights_temp.items()]# 创建一个有向图graph = nx.DiGraph()# 设置有向图中的路径及权重 (from, to, weight)graph.add_weighted_edges_from(edges_weights)# 计算每个节点（人）的 PR 值，并作为节点的 pagerank 属性pagerank = nx.pagerank(graph)# 将 pagerank 数值作为节点的属性nx.set_node_attributes(graph, name = 'pagerank', values=pagerank)# 画网络图show_graph(graph)# 将完整的图谱进行精简# 设置 PR 值的阈值，筛选大于阈值的重要核心节点pagerank_threshold = 0.005# 复制一份计算好的网络图small_graph = graph.copy()# 剪掉 PR 值小于 pagerank_threshold 的节点for n, p_rank in graph.nodes(data=True): if p_rank['pagerank'] &lt; pagerank_threshold: small_graph.remove_node(n)# 画网络图,采用circular_layout布局让筛选出来的点组成一个圆show_graph(small_graph, 'circular_layout')结果如下：针对代码中的几个模块我做个简单的说明：1. 函数定义人物的名称需要统一，因此我设置了 unify_name 函数，同时设置了 show_graph 函数将网络图可视化。NetworkX 提供了多种可视化布局，这里我使用 spring_layout 布局，也就是呈中心放射状。除了 spring_layout 外，NetworkX 还有另外三种可视化布局，circular_layout（在一个圆环上均匀分布节点），random_layout（随机分布节点 ），shell_layout（节点都在同心圆上）。2. 计算边权重邮件的发送者和接收者的邮件往来可能不止一次，我们需要用两者之间邮件往来的次数计算这两者之间边的权重，所以我用 edges_weights_temp 数组存储权重。而上面介绍过在 NetworkX 中添加权重边（即使用 add_weighted_edges_from 函数）的时候，接受的是 u、v、w 的三元数组，因此我们还需要对格式进行转换，具体转换方式见代码。3.PR 值计算及筛选我使用 nx.pagerank(graph) 计算了节点的 PR 值。由于节点数量很多，我们设置了 PR 值阈值，即 pagerank_threshold=0.005，然后遍历节点，删除小于 PR 值阈值的节点，形成新的图 small_graph，最后对 small_graph 进行可视化（对应运行结果的第二张图）。总结我们通过矩阵乘法求得网页的权重，这我们使用 NetworkX 可以得到相同的结果。另外我带你用 PageRank 算法做了一次实战，我们将一个复杂的网络图，通过 PR 值的计算、筛选，最终得到了一张精简的网络图。在这个过程中我们学习了 NetworkX 工具的使用，包括创建图、节点、边及 PR 值的计算。实际上掌握了 PageRank 的理论之后，在实战中往往就是一行代码的事。但项目与理论不同，项目中涉及到的数据量比较大，你会花 80% 的时间（或 80% 的代码量）在预处理过程中，比如今天的项目中，我们对别名进行了统一，对边的权重进行计算，同时还需要把计算好的结果以可视化的方式呈现。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"}]},{"title":"PageRank 原理","slug":"数据分析 - PageRank-原理","date":"2018-09-10T14:37:25.000Z","updated":"2020-05-01T14:19:56.808Z","comments":true,"path":"2018/09/10/数据分析 - PageRank-原理/","link":"","permalink":"cpeixin.cn/2018/09/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%20-%20PageRank-%E5%8E%9F%E7%90%86/","excerpt":"","text":"互联网发展到现在，搜索引擎已经非常好用，基本上输入关键词，都能找到匹配的内容，质量还不错。但在 1998 年之前，搜索引擎的体验并不好。早期的搜索引擎，会遇到下面的两类问题：返回结果质量不高：搜索结果不考虑网页的质量，而是通过时间顺序进行检索；容易被人钻空子：搜索引擎是基于检索词进行检索的，页面中检索词出现的频次越高，匹配度越高，这样就会出现网页作弊的情况。有些网页为了增加搜索引擎的排名，故意增加某个检索词的频率。基于这些缺陷，当时 Google 的创始人拉里·佩奇提出了 PageRank 算法，目的就是要找到优质的网页，这样 Google 的排序结果不仅能找到用户想要的内容，而且还会从众多网页中筛选出权重高的呈现给用户。Google 的两位创始人都是斯坦福大学的博士生，他们提出的 PageRank 算法受到了论文影响力因子的评价启发。当一篇论文被引用的次数越多，证明这篇论文的影响力越大。正是这个想法解决了当时网页检索质量不高的问题。PageRank 的简化模型我们先来看下 PageRank 是如何计算的。我假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：这里有两个概念你需要了解一下。出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：u 为待评估的页面，Bu 为页面 u 的入链集合。针对入链集合中的任意页面 v，它能给 u 带来的影响力是其自身的影响力 PR(v) 除以 v 页面的出链数量，即页面 v 把影响力 PR(v) 平均分配给了它的出链，这样统计所有能给 u 带来链接的页面 v，得到的总和就是网页 u 的影响力，即为 PR(u)。所以你能看到，出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。在这个例子中，你能看到 A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：转移矩阵解释：第一列是A的出链的概率A-&gt;A: 0 A-&gt;B: 1/3 A-&gt;C: 1/3 A-&gt;D: 1/3第二列是B的的出链的概率B-&gt;A: 1/2 B-&gt;B: 0 B-&gt;C:0 B-&gt;D: 1/2第三列是C的出链概率C-&gt;A:1 C-&gt;B:0 C-&gt;C:0 C-&gt;D: 0第四列是D的出链概率D-&gt;A: 0 D-&gt;B:1/2 D-&gt;C:1/2 D-&gt;D: 0我们假设 A、B、C、D 四个页面的初始影响力都是相同的，即：当进行第一次转移之后，各页面的影响力 w1 变为：然后我们再用转移矩阵乘以 w1 得到 w2 结果，直到第 n 次迭代后 wn 影响力不再发生变化，可以收敛到 (0.3333，0.2222，0.2222，0.2222），也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。你能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。至此，我们模拟了一个简化的 PageRank 的计算过程，实际情况会比这个复杂，可能会面临两个问题：1. 等级泄露（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。2. 等级沉没（Rank Sink）：如果一个网页只有出链，没有入链（如下图所示），计算的过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。针对等级泄露和等级沉没的情况，我们需要灵活处理。比如针对等级泄露的情况，我们可以把没有出链的节点，先从图中去掉，等计算完所有节点的 PR 值之后，再加上该节点进行计算。不过这种方法会导致新的等级泄露的节点的产生，所以工作量还是很大的。有没有一种方法，可以同时解决等级泄露和等级沉没这两个问题呢？PageRank 的随机浏览模型为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。其中 N 为网页总数，这样我们又可以重新迭代网页的权重计算了，因为加入了阻尼因子 d，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理（这里不进行讲解）也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。PageRank 在社交影响力评估中的应用网页之间会形成一个网络，是我们的互联网，论文之间也存在着相互引用的关系，可以说我们所处的环境就是各种网络的集合。只要是有网络的地方，就存在出链和入链，就会有 PR 权重的计算，也就可以运用我们今天讲的 PageRank 算法。我们可以把 PageRank 算法延展到社交网络领域中。比如在微博上，如果我们想要计算某个人的影响力，该怎么做呢？一个人的微博粉丝数并不一定等于他的实际影响力。如果按照 PageRank 算法，还需要看这些粉丝的质量如何。如果有很多明星或者大 V 关注，那么这个人的影响力一定很高。如果粉丝是通过购买僵尸粉得来的，那么即使粉丝数再多，影响力也不高。同样，在工作场景中，比如说脉脉这个社交软件，它计算的就是个人在职场的影响力。如果你的工作关系是李开复、江南春这样的名人，那么你的职场影响力一定会很高。反之，如果你是个学生，在职场上被链入的关系比较少的话，职场影响力就会比较低。同样，如果你想要看一个公司的经营能力，也可以看这家公司都和哪些公司有合作。如果它合作的都是世界 500 强企业，那么这个公司在行业内一定是领导者，如果这个公司的客户都是小客户，即使数量比较多，业内影响力也不一定大。除非像淘宝一样，有海量的中小客户，最后大客户也会找上门来寻求合作。所以权重高的节点，往往会有一些权重同样很高的节点在进行合作。PageRank 给我们带来的启发PageRank 可以说是 Google 搜索引擎重要的技术之一，在 1998 年帮助 Google 获得了搜索引擎的领先优势，现在 PageRank 已经比原来复杂很多，但它的思想依然能带给我们很多启发。比如，如果你想要自己的媒体影响力有所提高，就尽量要混在大 V 圈中；如果想找到高职位的工作，就尽量结识公司高层，或者认识更多的猎头，因为猎头和很多高职位的人员都有链接关系。同样，PageRank 也可以帮我们识别链接农场。链接农场指的是网页为了链接而链接，填充了一些没有用的内容。这些页面相互链接或者指向了某一个网页，从而想要得到更高的权重。总结今天我给你讲了 PageRank 的算法原理，对简化的 PageRank 模型进行了模拟。针对简化模型中存在的等级泄露和等级沉没这两个问题，PageRank 的随机浏览模型引入了阻尼因子 d 来解决。同样，PageRank 有很广的应用领域，在许多网络结构中都有应用，比如计算一个人的微博影响力等。它也告诉我们，在社交网络中，链接的质量非常重要。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"}]},{"title":"Spark SQL 处理结构化数据：DataFrame和Dataset","slug":"Spark-SQL-处理结构化数据：DataFrame和Dataset","date":"2018-09-03T14:56:04.000Z","updated":"2020-09-03T14:57:01.277Z","comments":true,"path":"2018/09/03/Spark-SQL-处理结构化数据：DataFrame和Dataset/","link":"","permalink":"cpeixin.cn/2018/09/03/Spark-SQL-%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%9ADataFrame%E5%92%8CDataset/","excerpt":"","text":"RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集合完成数据处理的逻辑更像是程序员们的思维方式。对于数据处理逻辑，分析师们更习惯用 SQL 而非算子来表达。所以，Spark 借鉴了 Python 数据分析库 pandas 中 DataFrame 的概念，推出了 DataFrame、Dataset 与 Spark SQL。在数据科学领域中，DataFrame 抽象了矩阵，如 R、pandas 中的 DataFrame；在数据工程领域，如 Spark SQL 中，DataFrame 更多地代表了关系型数据库中的表，这样就可以利用简单易学的 SQL 来进行数据分析；在 Spark 中，我们既可以用 Spark SQL + DataFrame 的组合实现海量数据分析，也可用 DataFrame + MLlib（Spark 机器学习库）的组合实现海量数据挖掘。在计算机领域中，高级往往意味着简单、封装程度高，而与之对应的通常是复杂、底层。对于 Spark 编程来说，RDD + 算子的组合无疑是比较底层的，而 DataFrame + Spark SQL 的组合无论从学习成本，还是从性能开销上来说，都显著优于前者组合，所以无论是分析师还是程序员，这种方式才是使用 Spark 的首选。 此外，对于分析师来说，DataFrame 对他们来说并不陌生，熟悉的概念也能让他们快速上手。DataFrame、Dataset 的起源与演变DataFrame 在 Spark 1.3 被引入，它的出现取代了 SchemaRDD，Dataset 最开始在 Spark 1.6 被引入，当时还属于实验性质，在 2.0 版本时正式成为 Spark 的一部分，并且在 Spark 2.0 中，DataFrame API 与 Dataset API 在形式上得到了统一。Dataset API 提供了类型安全的面向对象编程接口。Dataset 可以通过将表达式和数据字段暴露给查询计划程序和 Tungsten 的快速内存编码，从而利用 Catalyst 优化器。但是，现在 DataFrame 和 Dataset 都作为 Apache Spark 2.0 的一部分，其实 DataFrame 现在是 Dataset Untyped API 的特殊情况。更具体地说：复制1DataFrame &#x3D; Dataset[Row]下面这张图比较清楚地表示了 DataFrame 与 Dataset 的变迁与关系。由于 Python 不是类型安全的语言，所以 Spark Python API 没有 Dataset API，而只提供了 DataFrame API。当然，Java 和 Scala 就没有这种问题。DataFrame APIDataFrame 与 Dataset API 提供了简单的、统一的并且更富表达力的 API ，简言之，与 RDD 与算子的组合相比，DataFrame 与 Dataset API 更加高级，所以这也是为什么我将这个模块命名为 Spark 高级编程。DataFrame 不仅可以使用 SQL 进行查询，其自身也具有灵活的 API 可以对数据进行查询，与 RDD API 相比，DataFrame API 包含了更多的应用语义，所谓应用语义，就是能让计算框架知道你的目标的信息，这样计算框架就能更有针对性地对作业进行优化，本课时主要介绍如何创建DataFrame 以及如何利用 DataFrame 进行查询。1、创建DataFrameDataFrame 目前支持多种数据源、文件格式，如 Json、CSV 等，也支持由外部数据库直接读取数据生成，此外还支持由 RDD 通过类型反射生成，甚至还可以通过流式数据源生成，这在下个模块会详细介绍。DataFrame API 非常标准，创建 DataFrame 都通过 read 读取器进行读取。下面列举了如何读取几种常见格式的文件。读取 Json 文件。Json 文件如下：12345&#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;......val df &#x3D; spark.read.json(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.json&quot;)我们可以利用初始化好的 SparkSession（spark）读取 Json 格式文件。读取 CSV 文件：1val df &#x3D; spark.read.csv(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)从 Parquet 格式文件中生成：1val df &#x3D; spark.read.parquet(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)从 ORC 格式文件中生成：12val df &#x3D; spark.read.orc(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)关于 ORC 与 Parquet 文件格式会在后面详细介绍。从文本中生成：1val df &#x3D; spark.read.text(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.csv&quot;)通过 JDBC 连接外部数据库读取数据生成1234567val df &#x3D; spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;).option(&quot;dbtable&quot;, &quot;schema.tablename&quot;).option(&quot;user&quot;, &quot;username&quot;).option(&quot;password&quot;, &quot;password&quot;).load()上面的代码表示通过 JDBC 相关配置，读取数据。通过 RDD 反射生成。此种方法是字符串反射为 DataFrame 的 Schema，再和已经存在的 RDD 一起生成 DataFrame，代码如下所示：123456789101112131415import spark.implicits._val schemaString &#x3D; &quot;id f1 f2 f3 f4&quot;&#x2F;&#x2F; 通过字符串转换和类型反射生成schemaval fields &#x3D; schemaString.split(&quot; &quot;).map(fieldName &#x3D;&gt; StructField(fieldName, StringType, nullable &#x3D; true))val schema &#x3D; StructType(fields)&#x2F;&#x2F; 需要将RDD转化为RDD[Row]类型val rowRDD &#x3D; spark.sparkContext.textFile(textFilePath).map(_.split(&quot;,&quot;)).map(attributes &#x3D;&gt; Row(attributes(0), attributes(1),attributes(2),attributes(3),attributes(4).trim))&#x2F;&#x2F; 生成DataFrameval df &#x3D; spark.createDataFrame(rowRDD, schema)注意这种方式需要隐式转换，需在转换前写上第一行：1import spark.implicits._DataFrame 初始化完成后，可以通过 show 方法来查看数据，Json、Parquet、ORC 等数据源是自带 Schema 的，而那些无 Schema 的数据源，DataFrame 会自己生成 Schema。Json 文件生成的 DataFrame 如下：CSV 文件生成的 DataFrame 如下：2、查询完成初始化的工作之后就可以使用 DataFrame API 进行查询了，DataFrame API 主要分为两种风格，一种依然是 RDD 算子风格，如 reduce、groupByKey、map、flatMap 等，另外一种则是 SQL 风格，如 select、where 等。2.1 算子风格我们简单选取几个有代表性的 RDD 算子风格的 API，具体如下：1def groupByKey[K: Encoder](func: T &#x3D;&gt; K): KeyValueGroupedDataset[K, T]与 RDD 算子版作用相同，返回类型为 Dataset。1def map[U : Encoder](func: T &#x3D;&gt; U): Dataset[U]与 RDD 算子版作用相同，返回类型为 Dataset。1def flatMap[U : Encoder](func: T &#x3D;&gt; TraversableOnce[U]): Dataset[U]与 RDD 算子版作用相同，返回类型为 Dataset。这些算子用法大同小异，但都需要传入 Encoder 参数，这可以通过隐式转换解决，在调用算子前，需加上：1import spark.implicits._2.2 SQL风格这类 API 的共同之处就是支持将部分 SQL 语法的字符串作为参数直接传入。select 和 where12def select(cols: Column*): DataFramedef where(conditionExpr: String): Dataset[T]条件查询，例如：1df.select(&quot;age&quot;).where(&quot;name is not null and age &gt; 10&quot;).foreach(println(_))groupBy1def groupBy(col1: String, cols: String*): RelationalGroupedDataset分组统计。例如：1df.select(&quot;name&quot;,&quot;age&quot;).groupBy(&quot;age&quot;).count().foreach(println(_))此外，某些 RDD 算子风格的 API 也可以传入部分 SQL 语法的字符串，如 filter。例如：1df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).foreach(println(_))join1def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrameDataFrame API 还支持最普遍的连接操作，代码如下：123val leftDF &#x3D; ...val rightDF &#x3D; ...leftDF.join(rightDF, leftDF(&quot;pid&quot;) &#x3D;&#x3D;&#x3D; rightDF(&quot;fid&quot;), &quot;left_outer&quot;).foreach(println(_))其中 joinType 参数支持常用的连接类型，选项有 inner、cross、outer、full、full_outer、left、left_outer、right、right_outer、left_semi 和 left_anti，其中 cross 表示笛卡儿积，这在实际使用中比较少见；left_semi 是左半连接，是 Spark 对标准 SQL 中的 in 关键字的变通实现；left_anti 是 Spark 对标准 SQL 中的 not in 关键字的变通实现。除了 groupBy 这种分组方式，DataFrame 还支持一些特别的分组方式如 pivot、rollup、cube 等，以及常用的分析函数，先来看一个数据集：复制123456789101112131415161718192021222324&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:92, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:75, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:62, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:96, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:98, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:78, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2017&quot;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:90, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:76, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:74, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:68, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:95, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:81, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2016&quot;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:95, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:91, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:85, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:77, &quot;subject&quot;:&quot;Chinese&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:63, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:99, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:79, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:85, &quot;subject&quot;:&quot;math&quot;, &quot;year&quot;:&quot;2015&quot;&#125;以上是某班学生 3 年的成绩单，一共有 3 个维度，即 name、subject 和 year，度量为 grade，也就是成绩，因此，这个 DataFrame 可以看成三维数据立方体，如下图所示。现在需要统计每个学生各科目 3 年的平均成绩，该操作可以通过下面的方式实现：1dfSG.groupBy(&quot;name&quot;,&quot;subject&quot;).avg(&quot;subject&quot;)但是，这种形式使结果数据集只有两列——name 和 subject，不利于进一步分析，而利用 DataFrame 的数据透视 pivot 功能无疑更加方便。 pivot 功能在 pandas、Excel 等分析工具已得到了广泛应用，用户想使用透视功能，需要指定分组规则、需要透视的列以及聚合的维度列。所谓“透视”比较形象，即在分组结果上，对每一组进行“透视”，透视的结果会导致每一组基于透视列展开，最后再根据聚合操作进行聚合，统计每个学生每科 3 年平均成绩实现如下：复制1dfSG.groupBy(&quot;name&quot;).pivot(&quot;subject&quot;).avg(&quot;grade&quot;).show()结果如下：除了 groupBy 之外，DataFrame 还提供 rollup 和 cube 的方式进行分组聚合，如下：1def rollup(col1: String, cols: String*): RelationalGroupedDatasetrollup 也是用来进行分组统计，只不过分组逻辑有所不同，假设 rollup(A,B,C)，其中 A、B、C 分别为 3 列，那么会先对 A、B、C 进行分组，然后依次对 A、B 进行分组、对 A 进行分组、对全表进行分组，执行：1dfSG.rollup(&quot;name&quot;, &quot;subject&quot;).avg(&quot;grade&quot;).show()结果如下：可以看到，除了按照 name + subject 的组合键进行分组，还分别对每个人进行了分组，如 Michael,null，此外还将全表分为了一组，如 null,null。1def cube(col1: String, cols: String*): RelationalGroupedDatasetcube 与 rollup 类似，分组依据有所不同，仍以 cube(A,B,C) 为例，分组依据分别是 (A,B,C)、(A,B)、(A,C)、(B,C)、(A)、(B)、(C)、全表，执行：复制1dfSG.cube(&quot;name&quot;, &quot;subject&quot;).avg(&quot;grade&quot;).show()结果如下：可以看到与 rollup 不同，这里还分别对每个科目进行分组，如 null、math。在实际使用中，你应该尽量选用并习惯于用 SQL 风格的算子完成开发任务，SQL 风格的查询 API 不光表现力强，另外也非常易读。3、写出与创建 DataFrame 的 read 读取器相对应，写出为 write 输出器 API。下面列举了如何输出几种常见格式的文件。写出为 Json 文件：1df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.json(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)写出为 Parquet 文件：1df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.parquet(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)写出为 ORC 文件：1df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.orc(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)写出为文本文件：1df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.text(&quot;&#x2F;your&#x2F;output&#x2F;path&quot;)写出为 CSV 文件：1234567val saveOptions &#x3D; Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;path&quot; -&gt; &quot;csvout&quot;)df.select(&quot;age&quot;, &quot;name&quot;).filter(&quot;age &gt; 10&quot;).write.format(&quot;com.databricks.spark.csv&quot;).mode(SaveMode.Overwrite).options(saveOptions).save()我们还可以在保存时对格式已经输出的方式进行设定，例如本例中是保留表头，并且输出方式是 Overwrite，输出方式有 Append、ErrorIfExist、Ignore、Overwrite，分别代表追加到已有输出路径中、如果输出路径存在则报错、存在则停止、存在则覆盖。写出到关系型数据库：1234val prop &#x3D; new java.util.Propertiesprop.setProperty(&quot;user&quot;,&quot;spark&quot;)prop.setProperty(&quot;password&quot;,&quot;123&quot;)df.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;test&quot;,&quot;tablename&quot;,prop)写出到关系型数据库同样基于 JDBC ，用此种方式写入关系型数据库，表名可以不存在。Dataset API从本质上来说，DataFrame 只是 Dataset 的一种特殊情况，在 Spark 2.x 中已经得到了统一：1DataFrame &#x3D; Dataset[Row]因此，在使用 DataFrame API 的过程中，很容易就会自动转换为 Dataset[String]、Dataset[Int] 等类型。除此之外，用户还可以自定义类型。下面来看看 DataFrame 转成 Dataset 的例子，下面是一个 Json 文件，记录了学生的单科成绩：12345678&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:92, &quot;subject&quot;:&quot;Chinese&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;Chinese&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:75, &quot;subject&quot;:&quot;Chinese&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:62, &quot;subject&quot;:&quot;Chinese&quot;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;grade&quot;:96, &quot;subject&quot;:&quot;math&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;grade&quot;:98, &quot;subject&quot;:&quot;math&quot;&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;grade&quot;:78, &quot;subject&quot;:&quot;math&quot;&#125;&#123;&quot;name&quot;:&quot;Berta&quot;, &quot;grade&quot;:87, &quot;subject&quot;:&quot;math&quot;&#125;代码如下：1234567891011121314151617181920212223&#x2F;&#x2F; 首先定义StudentGrade类case class StudentGrade(name: String, subject: String, grade: Long)&#x2F;&#x2F; 生成DataFrameval dfSG &#x3D; spark.read.json(&quot;data&#x2F;examples&#x2F;target&#x2F;scala-2.11&#x2F;classes&#x2F;student_grade.json&quot;)&#x2F;&#x2F; 方法1:通过map函数手动转换为Dataset[StudentGrade]类型val dsSG: Dataset[StudentGrade] &#x3D; dfSG.map(a &#x3D;&gt; StudentGrade(a.getAs[String](0),a.getAs[String](1),a.getAs[Long](2)))&#x2F;&#x2F; 方法2:使用DataFrame的as函数进行转换val dsSG2: Dataset[StudentGrade] &#x3D; dfSG.as[StudentGrade]&#x2F;&#x2F; 方法3：通过RDD转换而成（基于同样内容的CSV文件）val dsSG3 &#x3D; spark.sparkContext.textFile(&quot;data&#x2F;examples&#x2F;target&#x2F;scala-2.11&#x2F;classes&#x2F;student_grade.csv&quot;).map[StudentGrade](row &#x3D;&gt; &#123; val fields &#x3D; row.split(&quot;,&quot;) StudentGrade( fields(0).toString(), fields(1).toString(), fields(2).toLong )&#125;).toDS &#x2F;&#x2F; 求每科的平均分dsSG3.groupBy(&quot;subject&quot;).mean(&quot;grade&quot;).foreach(println(_))以上 3 种方法都可以将 DataFrame 转换为 Dataset 。转换完成后，就可以使用其 API 对数据进行分析，使用方式与 DataFrame 并无不同。Spark SQL在实际工作中，使用频率最高的当属 Spark SQL，通常一个大数据处理项目中，70% 的数据处理任务都是由 Spark SQL 完成，它贯穿于数据预处理、数据转换和最后的数据分析。由于 SQL 的学习成本低、用户基数大、函数丰富，Spark SQL 也通常是使用 Spark 最方便的方式。此外，由于 SQL 包含了丰富的应用语义，所以 Catalyst 优化器带来的性能巨大提升也使 Spark SQL 成为编写 Spark 作业的最佳方式。接下来我将为你介绍 Spark SQL 的使用。从使用层面上来讲，要想用好 Spark SQL，只需要编写 SQL 就行了，本课时的最后简单介绍了下 SQL 的常用语法，方便没有接触过 SQL 的同学快速入门。1、创建临时视图想使用 Spark SQL，可以先创建临时视图，相当于数据库中的表，这可以通过已经存在的 DataFrame、Dataset 直接生成；也可以直接从 Hive 元数据库中获取元数据信息直接进行查询。先来看看创建临时视图：复制123456789101112131415161718case class StudentGrade(name: String, subject: String, grade: Long)&#x2F;&#x2F; 生成DataFrameval dfSG &#x3D; spark.read.json(&quot;data&#x2F;examples&#x2F;target&#x2F;scala-2.11&#x2F;classes&#x2F;student_grade.json&quot;)&#x2F;&#x2F; 生成Datasetval dsSG &#x3D; dfSG.map( a &#x3D;&gt; StudentGrade( a.getAs[String](&quot;name&quot;), a.getAs[String](&quot;subject&quot;), a.getAs[Long](&quot;grade&quot;) )) &#x2F;&#x2F; 创建临时视图dfSG.createOrReplaceTempView(&quot;student_grade_df&quot;)dsSG.createOrReplaceTempView(&quot;student_grade_ds&quot;)&#x2F;&#x2F; 计算每科的平均分spark.sql(&quot;SELECT subject, AVG(grade) FROM student_grade_df GROUP BY subject&quot;).show()spark.sql(&quot;SELECT subject, AVG(grade) FROM student_grade_ds GROUP BY subject&quot;).show()对于 Dataset 来说，对象类型的数据结构会作为临时视图的元数据，在 SQL 中可以直接使用。2、使用Hive元数据随着 Spark 越来越流行，有很多情况，需要将 Hive 作业改写成 Spark SQL 作业，Spark SQL 可以通过 hive-site.xml 文件的配置，直接读取 Hive 元数据。这样，改写的工作量就小了很多，代码如下：复制12345678val spark &#x3D; SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;Hive on Spark&quot;).enableHiveSupport().getOrCreate()&#x2F;&#x2F; 直接查询spark.sql(…………)代码中通过 enableHiveSupport 方法开启对 Hive 的支持，但需要将 Hive 配置文件 hive-site.xml 复制到 Spark 的配置文件夹下。3、查询语句Spark 的 SQL 语法源于 Presto （一种支持 SQL 的大规模并行处理技术，适合 OLAP），在源码中我们可以看见，Spark 的 SQL 解析引擎直接采用了 Presto 的 SQL 语法文件。查询是 Spark SQL 的核心功能，Spark SQL 的查询语句模式如下：复制123456789[ WITH with_query [, ...] ]SELECT [ ALL | DISTINCT ] select_expr [, ...][ FROM from_item [, ...] ][ WHERE condition ][ GROUP BY expression [, ...] ][ HAVING condition][ UNION [ ALL | DISTINCT ] select ][ ORDER BY expression [ ASC | DESC ] [, ...] ][ LIMIT count ]其中 from_item 为以下之一：12table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ]from_item join_type from_item [ ON join_condition | USING ( join_column [, ...] ) ]该模式基本涵盖了 Spark SQL 中查询语句的各种写法。3.1 SELECT 与 FROM 子句**SELECT 与 FROM 是构成查询语句的最小单元，SELECT 后面跟列名表示要查询的列，或者用 * 表示所有列，FROM 后面跟表名，示例如下：1SELECT name, grade FROM student_grade t;在使用过程中，对列名和表名都可以赋予别名，这里对 student_grade 赋予别名 t，此外我们还可以对某一列用关键字 DISTINCT 进行去重，默认为 ALL，表示不去重：1SELECT COUNT( DISTINCT name) FROM student_grade;上面这条 SQL 代表统计有多少学生参加了考试。3.2 WHERE 子句**WHERE 子句经常和 SELECT 配合使用，用来过滤参与查询的数据集，WHERE 后面一般会由运算符组合成谓词表达式（返回值为 True 或者 False ），例如：123SELECT * FROM student_grade WHERE grade &gt; 90;SELECT * FROM student_grade WHERE name IS NOT NULL;SELECT * FROM student_grade WHERE name LIKE &quot;*ndy&quot;;常见的运算符还有 !=、&lt;&gt; 等，此外还可以用逻辑运算符：AND、OR 组合谓词表达式进行查询，例如：1SELECT * FROM student_grade WHERE grade &gt; 90 AND name IS NOT NULL3.3 GROUP BY 子句**GROUP BY 子句用于对 SELECT 语句的输出进行分组，分组中是匹配值的数据行。GROUP BY 子句支持指定列名或列序号（从 1 开始）表达式。以下查询是等价的，都会对 subject 列进行分组，第一个查询使用列序号，第二个查询使用列名：12SELECT avg(grade), subject FROM student_grade GROUP BY 2;SELECT avg(grade), subject FROM student_grade GROUP BY subject;使用 GROUP BY 子句时需注意，出现在 SELECT 后面的列，要么同时出现在 GROUP BY 后面，要么就在聚合函数中。3.4 HAVING 子句**HAVING 子句与聚合函数以及 GROUP BY 子句配合使用，用来过滤分组统计的结果。HAVING 子句去掉不满足条件的分组。在分组和聚合计算完成后，HAVING 对分组进行过滤。例如以下查询会过滤掉平均分大于 90 分的科目：1234SELECT subject,AVG(grade) FROM student_grade GROUP BY subject HAVING AVG(grade) &lt; 90;3.5 UNION 子句**UNION 子句用于将多个查询语句的结果合并为一个结果集：1query UNION [ALL | DISTINCT] query参数 ALL 或 DISTINCT 可以控制最终结果集包含哪些行。如果指定参数 ALL，则包含全部行，即使行完全相同；如果指定参数 DISTINCT ，则合并结果集，结果集只有唯一不重复的行；如果不指定参数，执行时默认使用 DISTINCT。下面这句 SQL 是将两个班级的成绩进行合并：123SELECT * FROM student_grade_class1UNION ALL SELECT * FROM student_grade_class2;多个 UNION 子句会从左向右执行，除非用括号明确指定顺序。3.6 ORDER BY 子句**ORDER BY 子句按照一个或多个输出表达式对结果集排序：1ORDER BY expression [ ASC | DESC ] [ NULLS &#123; FIRST | LAST &#125; ] [, ...]每个表达式由列名或列序号（从 1 开始）组成。ORDER BY 子句作为查询的最后一步，在 GROUP BY 和 HAVING 子句之后。ASC 为默认升序，DESC 为降序。下面这句 SQL 会对结果进行过滤，并按照平均分进行排序，注意这里使用了列别名：12345SELECT subject,AVG(grade) avg FROM student_grade GROUP BY subject HAVING AVG(grade) &lt; 90 ORDER BY avg DESC;3.7 LIMIT 子句**LIMIT 子句限制结果集的行数，这在查询大表时很有用。以下示例为对单科成绩进行排序并只返回前 3 名的记录：1234SELECT * FROM student_grade WHERE subject &#x3D; &#39;math&#39; ORDER BY grade DESC LIMIT 3;3.8 JOIN 子句**JOIN 操作可以将多个有关联的表进行关联查询，下面这句 SQL 是查询数学成绩在 90 分以上的学生的院系，其中院系信息可以从学生基础信息表内，通过姓名连接得到：12345SELECT g.*, a.department FROM student_grade g JOIN student_basic b ON g.name &#x3D; b.name WHERE g.subject &#x3D; &#39;math&#39; and grade &gt; 90在这句 SQL 中，表 g 被称为驱动表或是左表，表 b 被称为右表。如前所述，Spark 支持多种连接类型。小结由于 Spark 对于 SQL 支持得非常好，而 pandas 在这方面没那么强大，所以，在某些场景，你可以选择 Spark SQL 来代替 pandas，这有时对于分析师来说非常好用。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"数据分析-关联规则原理","slug":"数据分析-关联规则原理","date":"2018-09-02T14:19:09.000Z","updated":"2020-05-01T14:19:31.444Z","comments":true,"path":"2018/09/02/数据分析-关联规则原理/","link":"","permalink":"cpeixin.cn/2018/09/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%8E%9F%E7%90%86/","excerpt":"","text":"关联规则这个概念，最早是由 Agrawal 等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于关联规则的 Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。关联规则挖掘可以让我们从数据集中发现项与项（item 与 item）之间的关系，它在我们的生活中有很多应用场景，“购物篮分析”就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。在今天的内容中，希望你能带着问题，和我一起来搞懂以下几个知识点：搞懂关联规则中的几个重要概念：支持度、置信度、提升度；Apriori 算法的工作原理；在实际工作中，我们该如何进行关联规则挖掘。关联规则概念搞懂关联规则中的几个概念我举一个超市购物的例子，下面是几名客户购买的商品列表：什么是支持度呢？支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。在这个例子中，我们能看到“牛奶”出现了 4 次，那么这 5 笔订单中“牛奶”的支持度就是 4/5=0.8。同样“牛奶 + 面包”出现了 3 次，那么这 5 笔订单中“牛奶 + 面包”的支持度就是 3/5=0.6。什么是置信度呢？它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度 (牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2 次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。什么是提升度呢？我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升的”程度。还是看上面的例子，如果我们单纯看置信度 (可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品 A 对商品 B 的提升度：提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。所以提升度有三种可能：提升度 (A→B)&gt;1：代表有提升；提升度 (A→B)=1：代表有没有提升，也没有下降；提升度 (A→B)&lt;1：代表有下降。提升度 (牛奶→啤酒)Apriori 的工作原理明白了关联规则中支持度、置信度和提升度这几个重要概念，我们来看下 Apriori 算法是如何工作的。首先我们把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：Apriori 算法其实就是查找频繁项集 (frequent itemset) **的过程，所以首先我们需要定义什么是频繁项集。频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集**，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。我们再来看下这个例子，假设我随机指定最小支持度是 50%，也就是 0.5。我们来看下 Apriori 算法是如何运算的。首先，我们先计算单个商品的支持度，也就是得到 K=1 项的支持度：因为最小支持度是 0.5，所以你能看到商品 4、6 是不符合最小支持度的，不属于频繁项集，于是经过筛选商品的频繁项集就变成：在这个基础上，我们将商品两两组合， 根据订单编号图，得到 k=2 项的支持度：我们再筛掉小于最小值支持度的商品组合，可以得到：我们再将商品进行 K=3 项的商品组合，可以得到：商品项集支持度1，2，33/51，2，51/51，3，52/52，3，52/5再筛掉小于最小值支持度的商品组合，可以得到：通过上面这个过程，我们可以得到 K=3 项的频繁项集{1,2,3}，也就是{牛奶、面包、尿布}的组合。到这里，你已经和我模拟了一遍整个 Apriori 算法的流程，下面我来给你总结下 Apriori 算法的递归流程：K=1，计算 K 项集的支持度；筛选掉小于最小支持度的项集；如果项集为空，则对应 K-1 项集的结果为最终结果。否则 K=K+1，重复 1-3 步。Apriori 的改进算法：FP-Growth 算法我们刚完成了 Apriori 算法的模拟，你能看到 Apriori 在计算的过程中有以下几个缺点：可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；每次计算都需要重新扫描数据集，来计算每个项集的支持度。所以 Apriori 算法会浪费很多计算空间和计算时间，为此人们提出了 FP-Growth 算法，它的特点是：创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；整个生成过程只遍历数据集 2 次，大大减少了计算量。所以在实际工作中，我们常用 FP-Growth 来做频繁项集的挖掘，下面我给你简述下 FP-Growth 的原理。1. 创建项头表（item header table）创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。2. 构造 FP 树FP 树的根节点记为 NULL 节点。整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。3. 通过 FP 树挖掘频繁项集到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。具体的操作会用到一个概念，叫“条件模式基”，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。我以“啤酒”的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：你能看出来，相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以“啤酒”为节点的 FP 子树，也就是说，在频繁项集中一定要含有“啤酒”这个项。你可以再看下原始的数据，其中订单 1{牛奶、面包、尿布}和订单 5{牛奶、面包、尿布、可乐}并不存在“啤酒”这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以“啤酒”为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。条件模式基不包括“啤酒”节点，而且祖先节点如果小于最小支持度就会被剪枝，所以“啤酒”的条件模式基为空。同理，我们可以求得“面包”的条件模式基为：所以可以求得面包的频繁项集为{尿布，面包}，{尿布，牛奶，面包}。同样，我们还可以求得牛奶，尿布的频繁项集，这里就不再计算展示。总结今天我给你讲了 Apriori 算法，它是在“购物篮分析”中常用的关联规则挖掘算法，在 Apriori 算法中你最主要是需要明白支持度、置信度、提升度这几个概念，以及 Apriori 迭代计算频繁项集的工作流程。Apriori 算法在实际工作中需要对数据集扫描多次，会消耗大量的计算时间，所以在 2000 年 FP-Growth 算法被提出来，它只需要扫描两次数据集即可以完成关联规则的挖掘。FP-Growth 算法最主要的贡献就是提出了 FP 树和项头表，通过 FP 树减少了频繁项集的存储以及计算时间。当然 Apriori 的改进算法除了 FP-Growth 算法以外，还有 CBA 算法、GSP 算法，这里就不进行介绍。你能发现一种新理论的提出，往往是先从最原始的概念出发，提出一种新的方法。原始概念最接近人们模拟的过程，但往往会存在空间和时间复杂度过高的情况。所以后面其他人会对这个方法做改进型的创新，重点是在空间和时间复杂度上进行降维，比如采用新型的数据结构。你能看出树在存储和检索中是一个非常好用的数据结构。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"}]},{"title":"数据分析-关联规则 实战","slug":"数据分析-关联规则-实战","date":"2018-09-01T14:19:27.000Z","updated":"2020-05-01T14:19:45.546Z","comments":true,"path":"2018/09/01/数据分析-关联规则-实战/","link":"","permalink":"cpeixin.cn/2018/09/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-%E5%AE%9E%E6%88%98/","excerpt":"","text":"昨天讲解了关联规则挖掘的原理。关联规则挖掘在生活中有很多使用场景，不仅是商品的捆绑销售，甚至在挑选演员决策上，你也能通过关联规则挖掘看出来某个导演选择演员的倾向。今天我来带你用 Apriori 算法做一个项目实战。你需要掌握的是以下几点：熟悉上节课讲到的几个重要概念：支持度、置信度和提升度；熟悉与掌握 Apriori 工具包的使用；在实际问题中，灵活运用。包括数据集的准备等。如何使用 AprioriApriori 虽然是十大算法之一，不过在 sklearn 工具包中并没有它，也没有 FP-Growth 算法。这里教你个方法，来选择 Python 中可以使用的工具包，你可以通过https://pypi.org/ 搜索工具包。这个网站提供的工具包都是 Python 语言的，你能找到 8 个 Python 语言的 Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即 efficient-apriori。后面我会讲到为什么推荐这个工具包。首先你需要通过 pip install efficient-apriori 安装这个工具包。然后看下如何使用它，核心的代码就是这一行：1itemsets, rules = apriori(data, min_support, min_confidence)其中 data 是我们要提供的数据集，它是一个 list 数组类型。min_support 参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。min_confidence 是最小置信度，数值也代表百分比，比如 1 代表 100%。一般来说最小支持度常见的取值有0.5，0.1, 0.05。最小置信度常见的取值有1.0, 0.9, 0.8。可以通过尝试一些取值，然后观察关联结果的方式来调整最小值尺度和最小置信度的取值。关于支持度、置信度和提升度，我们再来简单回忆下。支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的概率越大。置信度是一个条件概念，就是在 A 发生的情况下，B 发生的概率是多少。提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升了多少”。接下来我们用这个工具包，跑一下上节课中讲到的超市购物的例子。下面是客户购买的商品列表：具体实现的代码如下：123456789101112from efficient_apriori import apriori# 设置数据集data = [('牛奶','面包','尿布'), ('可乐','面包', '尿布', '啤酒'), ('牛奶','尿布', '啤酒', '鸡蛋'), ('面包', '牛奶', '尿布', '啤酒'), ('面包', '牛奶', '尿布', '可乐')]# 挖掘频繁项集和频繁规则itemsets, rules = apriori(data, min_support=0.5, min_confidence=1)print(itemsets)print(rules)结果：1234&#123;1: &#123;('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 4&#125;, 2: &#123;('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3&#125;, 3: &#123;('尿布', '牛奶', '面包'): 3&#125;&#125;[&#123;啤酒&#125; -&gt; &#123;尿布&#125;, &#123;牛奶&#125; -&gt; &#123;尿布&#125;, &#123;面包&#125; -&gt; &#123;尿布&#125;, &#123;牛奶, 面包&#125; -&gt; &#123;尿布&#125;]你能从代码中看出来，data 是个 List 数组类型，其中每个值都可以是一个集合。实际上你也可以把 data 数组中的每个值设置为 List 数组类型，比如：123456data = [['牛奶','面包','尿布'], ['可乐','面包', '尿布', '啤酒'], ['牛奶','尿布', '啤酒', '鸡蛋'], ['面包', '牛奶', '尿布', '啤酒'], ['面包', '牛奶', '尿布', '可乐']]两者的运行结果是一样的，efficient-apriori 工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。而其他的 Apriori 算法可能会因为考虑了先后顺序，出现计算频繁项集结果不对的情况。所以这里采用的是 efficient-apriori 这个工具包。**挖掘-导演是如何选择演员在实际工作中，数据集是需要自己来准备的，比如今天我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的 Python 爬虫进行数据采集。不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。先来梳理下采集的工作流程。首先我们先在https://movie.douban.com搜索框中输入导演姓名，比如“宁浩”。页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：页面默认是 15 条数据反馈，第一页会返回 16 条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用“/”分割。有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在 csv 文件中。完整的抓取代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# -*- coding: utf-8 -*-# 下载某个导演的电影数据集from efficient_apriori import apriorifrom lxml import etreeimport timefrom selenium import webdriverimport csvdriver = webdriver.Chrome()# 设置想要下载的导演 数据集director = u'宁浩'# 写CSV文件file_name = './' + director + '.csv'base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&amp;cat=1002&amp;start='out = open(file_name,'w', newline='', encoding='utf-8-sig')csv_write = csv.writer(out, dialect='excel')flags=[]# 下载指定页面的数据def download(request_url): driver.get(request_url) time.sleep(1) html = driver.find_element_by_xpath(\"//*\").get_attribute(\"outerHTML\") html = etree.HTML(html) # 设置电影名称，导演演员 的XPATH movie_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']\") name_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']\") # 获取返回的数据个数 num = len(movie_lists) if num &gt; 15: #第一页会有16条数据 # 默认第一个不是，所以需要去掉 movie_lists = movie_lists[1:] name_lists = name_lists[1:] for (movie, name_list) in zip(movie_lists, name_lists): # 会存在数据为空的情况 if name_list.text is None: continue # 显示下演员名称 print(name_list.text) names = name_list.text.split('/') # 判断导演是否为指定的director if names[0].strip() == director and movie.text not in flags: # 将第一个字段设置为电影名称 names[0] = movie.text flags.append(movie.text) csv_write.writerow(names) print('OK') # 代表这页数据下载成功 print(num) if num &gt;= 14: #有可能一页会有14个电影 # 继续下一页 return True else: # 没有下一页 return False# 开始的ID为0，每页增加15start = 0while start&lt;10000: #最多抽取1万部电影 request_url = base_url + str(start) # 下载数据，并返回是否有下一页 flag = download(request_url) if flag: start = start + 15 else: breakout.close()print('finished')爬取的代码在这里就不赘述了，其中有一点就是这里用到了selenium模拟打开窗口爬取。下面是爬取下来的数据：我们用获取到的少量宁浩数据，来做一次关联规则分析：12345678910111213141516171819# -*- coding: utf-8 -*-from efficient_apriori import aprioriimport csvdirector = u'宁浩'file_name = './'+director+'.csv'lists = csv.reader(open(file_name, 'r', encoding='utf-8-sig'))# 数据加载data = []for names in lists: name_new = [] for name in names: # 去掉演员数据中的空格 name_new.append(name.strip()) data.append(name_new[1:])# 挖掘频繁项集和关联规则itemsets, rules = apriori(data, min_support=0.5, min_confidence=1)print(itemsets)print(rules)代码中使用的 apriori 方法和开头中用 Apriori 获取购物篮规律的方法类似，比如代码中都设定了最小支持度和最小置信系数，这样我们可以找到支持度大于 50%，置信系数为 1 的频繁项集和关联规则。这是最后的运行结果：123&#123;1: &#123;('徐峥',): 5, ('黄渤',): 6&#125;, 2: &#123;('徐峥', '黄渤'): 5&#125;&#125;[&#123;徐峥&#125; -&gt; &#123;黄渤&#125;]你能看出来，宁浩导演喜欢用徐峥和黄渤，并且有徐峥的情况下，一般都会用黄渤。你也可以用上面的代码来挖掘下其他导演选择演员的规律。总结Apriori 算法的核心就是理解频繁项集和关联规则。在算法运算的过程中，还要重点掌握对支持度、置信度和提升度的理解。在工具使用上，你可以使用 efficient-apriori 这个工具包，它会把每一条数据中的项（item）放到一个集合（篮子）里来处理，不考虑项（item）之间的先后顺序。在实际运用中你还需要灵活处理，比如导演如何选择演员这个案例，虽然工具的使用会很方便，但重要的还是数据挖掘前的准备过程，也就是获取某个导演的电影数据集。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"}]},{"title":"数据仓库 - 元数据管理","slug":"数据仓库-元数据管理","date":"2018-08-30T13:20:15.000Z","updated":"2020-08-02T13:25:23.733Z","comments":true,"path":"2018/08/30/数据仓库-元数据管理/","link":"","permalink":"cpeixin.cn/2018/08/30/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/","excerpt":"","text":"好文分享https://www.jianshu.com/p/9fe3ff2bbe99","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 数据分层","slug":"数据仓库数据分层","date":"2018-08-25T15:26:15.000Z","updated":"2020-08-02T13:25:44.345Z","comments":true,"path":"2018/08/25/数据仓库数据分层/","link":"","permalink":"cpeixin.cn/2018/08/25/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E5%88%86%E5%B1%82/","excerpt":"","text":"简述在上篇博客‘数据仓库建模方法’中我们探讨过数据模型的相关问题，实际场景中，我们团队在确定了数据模型使用经典的星型模型和3NF之后，接下来要进行讨论的问题则是，每天大批量的数据，应该怎样的放进库中，怎么存放合理。在数据对接业务之前，要经过哪些提炼的过程，在整个提炼的过程中，要分成几个步骤，还是一步到位的直接从原始数据ETL给业务人员。其中提到的这个提炼过程，也就是数据分层的过程，分层设计的好，数据分布合理，节约计算资源，避免多余的数据开发。所以，数据分层是数据仓库建设中，重要的一环。各种重复计算，严重浪费了计算资源，需要优化性能。为什么要数据分层我们对数据进行分层的一个主要原因就是希望在管理数据的时候，能对数据有一个更加清晰的掌控，详细来讲，主要有下面几个原因：清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。数据血缘追踪：简单来讲可以这样理解，我们最终给业务诚信的是一能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。把复杂问题简单化。讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。屏蔽原始数据的异常。屏蔽业务的影响，不必改一次业务就需要重新接入数据。怎么分层在这里，就拿我们团队对我们公司设计的数据仓库来举例。首先呢，我们公司的数据量级和数据主题复杂度，相当于一个二线互联网公司，数据主题目前这一版数据仓库可以分为用户行为，订单，游戏活动等7个主题数据，业务并不是很复杂。所以在数据分层上，总共分成三层。源数据层 （dw = staging）宽表数据层 （dw = dw）指标数据层 （dw = subjectName）源数据层源数据层拉取业务表有20张左右，使用spark读取底层RDBMS来获取数据，写入到Hive中，在这一层中，除了选择合适的分区字段，我们的分析业务中，时间粒度多为天为单位，业务数据来源于多个产品线，所以这一层，会以snapshot_date（‘YYYY-MM-DD’）和product_id为分区字段，对数据表进行分区外，不做任何数据处理，不做脏数据处理，不做合并，保持数据的原始性，随时可以做到追本溯源。我们对于近源数据层的定位是可以”快速”的构建基础数据平台. 不做业务相关的处理可以让这部分的工作专注在大数据架构正确性和稳定性的问题，近源数据层出现以后, 实际上我们已经可以开始主要的数据分析工作了。宽表数据层宽表数据层的数据是从源数据层经过ETL字段清洗，计算并且相关主题表，维度表进行join而来的。例如订单表：Orders表order_*为订单主题（order_id,order_number,order_amount,create_date…）t_users_dim 用户维度表(user_id,city,last_login_time,device_type…)t_activity_dim 活动维度表(activity_id,activity_name,create_date…)t_promotion_dim 优惠维度表(promotion_id,promotion_name,promotion_type…)t_deposit 充值表(deposit_id,deposit_type,channal,deposit_time…)以上五张表经过相关键join就组成了订单宽表 如下图所示：指标数据层指标数据层是从宽表数据层计算而来，这一层我们采用的策略是由总到分，关联时间维度表，缩小时间粒度到小时级别。可以对运营人员提供小时级别的数据指标。对于一些金额相关数据表，也会经过计算添加一些常用的数据模型，例如某用户近20次充值的max值和min值，中位数，四分位数等字段。这一层也是最不稳定的一层，经常要根据业务的变化增加字段或者新加指标数据表。由于的依赖Hive建设的数据仓库，那么对于增加字段的这件事，要小心对数据的影响，注意选择合适的Hive表格式。这层就不画图了 有点困了 困了～～～～这篇数据仓库数据分层设计，是根据我们公司的业务数据和主题来设计的，也是我们大数据工程团队的这几个渣渣工程师和架构师一起商讨的结果方案，对于其他行业和大规模海量数据肯定是不能全部适用的。在写这篇文章前，我有看过美团技术团队美团点评酒旅数据仓库建设实践这篇文章，发现业务复杂度和数据规模对数据建模和数据分层的影响很大，像我们公司的数据量量级要完全按照美团数仓去做，是完全没有必要的，而且会弄的更加复杂，所以在这里想表明的就是，对于数仓的建设，要选择适合自己的，对自己量身定制，照搬某个公司的，是没有意义的，反而可能会增加使用的难度。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 建模方法","slug":"数据仓库建模方法","date":"2018-08-24T12:26:15.000Z","updated":"2020-08-02T13:24:33.547Z","comments":true,"path":"2018/08/24/数据仓库建模方法/","link":"","permalink":"cpeixin.cn/2018/08/24/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/","excerpt":"","text":"背景接着上个文章数据仓库架构设计，想写一篇数据仓库常用模型的文章，但是自己对数据仓库模型的理解程度和建设架构并没有下面这个技术专家理解的深刻，并且自己去组织语言，可能会有不准确的地方，怕影响大家对数据仓库建模的理解，数据仓库属于一个工程学科，在设计上要体验出工程严谨性，所以这次向大家推荐这篇文章，毕竟IBM在数据仓库和数据集市方面已经做得很成熟了，已经有成型的商业数据仓库组件，这篇文章写的很好，可以让大家很好的理解数据仓库。版权作者 周三保(zhousb@cn.ibm.com) IBM 软件部信息技术专家.原文地址本文主要的主线就是回答下面三个问题：什么是数据模型为什么需要数据模型如何建设数据模型什么是数据模型数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图 1 所示。通过上面的图形，我们能够很容易的看出在整个数据仓库得建模过程中，我们需要经历一般四个过程：业务建模，生成业务模型，主要解决业务层面的分解和程序化。领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。为什么需要数据模型在数据仓库的建设中，我们一再强调需要数据模型，那么数据模型究竟为什么这么重要呢？首先我们需要了解整个数据仓库的建设的发展史。数据仓库的发展大致经历了这样的三个过程：简单报表阶段：这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。数据集市阶段：这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。数据仓库阶段：这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题：进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。如何建设数据模型建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。数据仓库数据模型架构数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。图 3. 数据仓库数据模型架构从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分：系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。数据仓库建模阶段划分我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容：图 4. 数据仓库建模阶段划分从上图我们可以清楚地看出，数据仓库的数据建模大致分为四个阶段：业务建模，这部分建模工作，主要包含以下几个部分：划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。深入了解各个业务部门的内具体业务流程并将其程序化。提出修改和改进业务部门工作流程的方法并程序化。数据建模的范围界定，整个数据仓库项目的目标和阶段划分。领域概念建模，这部分得建模工作，主要包含以下几个部分：抽取关键业务概念，并将之抽象化。将业务概念分组，按照业务主线聚合类似的分组概念。细化分组概念，理清分组概念内的业务流程并抽象化。理清分组概念之间的关联，形成完整的领域概念模型。逻辑建模，这部分的建模工作，主要包含以下几个部分：业务概念实体化，并考虑其具体的属性事件实体化，并考虑其属性内容说明实体化，并考虑其属性内容物理建模，这部分得建模工作，主要包含以下几个部分：针对特定物理化平台，做出相应的技术调整针对模型的性能考虑，对特定平台作出相应的调整针对管理的需要，结合特定的平台，做出相应的调整生成最后的执行脚本，并完善之。从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。数据仓库建模方法大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库得建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法，维度建模法，实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。范式建模法（Third Normal Form，3NF）范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :每个属性值唯一，不具有多义性 ;每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。根据 Inmon 的观点，数据仓库模型得建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例话。从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于：数据仓库的域模型应该包含企业数据模型得域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。以笔者的观点来看，Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。维度建模法维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模型 和 雪花模型。这里做一下 星型模型 和 雪花模型的扩充：星型模型星型模是一种多维的数据关系，它由一个事实表和一组维表组成。每个维表都有一个维作为主键，所有这些维的主键组合成事实表的主键。强调的是对维度进行预处理，将多个维度集合到一个事实表，形成一个宽表。这也是我们在使用hive时，经常会看到一些大宽表的原因，大宽表一般都是事实表，包含了维度关联的主键和一些度量信息，而维度表则是事实表里面维度的具体信息，使用时候一般通过join来组合数据，相对来说对OLAP的分析比较方便。雪花模型当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。雪花模型更加符合数据库范式，减少数据冗余，但是在分析数据的时候，操作比较复杂，需要join的表比较多所以其性能并不一定比星型模型高。星型模型和雪花模型的优劣对比应用场景星型模型的设计方式主要带来的好处是能够提升查询效率，因为生成的事实表已经经过预处理，主要的数据都在事实表里面，所以只要扫描实时表就能够进行大量的查询，而不必进行大量的join，其次维表数据一般比较少，在join可直接放入内存进行join以提升效率，除此之外，星型模型的事实表可读性比较好，不用关联多个表就能获取大部分核心信息，设计维护相对比较简答。雪花模型的设计方式是比较符合数据库范式的理念，设计方式比较正规，数据冗余少，但在查询的时候可能需要join多张表从而导致查询效率下降，此外规范化操作在后期维护比较复杂。总结通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。图 6. 维度建模法上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。实体建模法实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示：图 7. 实体建模法上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分：实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。说明，主要是针对实体和事件的特殊说明。由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。数据仓库数据模型与业务系统数据模型设计的区别数据仓库建模样例上面介绍得是一些抽象得建模方法和理论，可能理解起来相对有些难度，因此，笔者在这里举一个例子，读者可以跟着我们的这个样例，来初步了解整个数据仓库建模的大概过程。背景介绍熟悉社保行业的读者可以知道，目前我们国家的社保主要分为养老，失业，工伤，生育，医疗保险和劳动力市场这 6 大块主要业务领域。在这 6 大业务领域中，目前的状况养老和事业的系统已经基本完善，已经有一部分数据开始联网检测。而，对于工伤，生育，医疗和劳动力市场这一块业务，有些地方发展的比较成熟，而有些地方还不够成熟。1.业务建模阶段基于以上的背景介绍，我们在业务建模阶段，就很容易来划分相应的业务。因此，在业务建模阶段，我们基本上确定我们本次数据仓库建设的目标，建设的方法，以及长远规划等。如下图：图 8. 业务建模阶段在这里，我们将整个业务很清楚地划分成了几个大的业务主线，例如：养老，失业，工伤，生育，医疗，劳动力等着几个大的部分，然后我们可以根据这些大的模块，在每个业务主线内，考虑具体的业务主线内需要分析的业务主题。因此，业务建模阶段其实是一次和业务人员梳理业务的过程，在这个过程中，不仅能帮助我们技术人员更好的理解业务，另一方面，也能够发现业务流程中的一些不合理的环节，加以改善和改进。同时，业务建模阶段的另一个重要工作就是确定我们数据建模的范围，例如：在某些数据准备不够充分的业务模块内，我们可以考虑先不建设相应的数据模型。等到条件充分成熟的情况下，我们可以再来考虑数据建模的问题。领域概念建模阶段领域概念建模阶段是数据仓库数据建模的一个重要阶段，由于我们在业务建模阶段已经完全理清相应的业务范围和流程，因此，我们在这个领域概念建模阶段的最主要的工作就是进行概念的抽象，整个领域概念建模的工作层次如下图所示：图 9. 领域概念建模阶段从上图我们可以清楚地看到，领域概念建模就是运用了实体建模法，从纷繁的业务表象背后通过实体建模法，抽象出实体，事件，说明等抽象的实体，从而找出业务表象后抽象实体间的相互的关联性，保证了我们数据仓库数据按照数据模型所能达到的一致性和关联性。从图上看，我们可以把整个抽象过程分为四个层次，分别为：抽象方法层，整个数据模型的核心方法，领域概念建模的实体的划分通过这种抽象方法来实现。领域概念层，这是我们整个数据模型的核心部分，因为不同程度的抽象方法，决定了我们领域概念的不同。例如：在这里，我们可以使用“参与方”这个概念，同时，你也可以把他分成三个概念：“个人”，“公司”，和“经办机构”这三个概念。而我们在构建自己的模型的时候，可以参考业务的状况以及我们自己模型的需要，选择抽象程度高的概念或者是抽象程度低的概念。相对来说，抽象程度高的概念，理解起来较为复杂，需要专业的建模专家才能理解，而抽象程度低的概念，较适合于一般业务人员的理解，使用起来比较方便。笔者在这里建议读者可以选用抽象概念较低的实体，以方便业务人员和技术人员之间的交流和沟通。具体业务层，主要是解决具体的业务问题，从这张图我们可以看出，具体的业务层，其实只是领域概念模型中实体之间的一些不同组合而已。因此，完整的数据仓库的数据模型应该能够相应灵活多变的前端业务的需求，而其本身的模型架构具有很强的灵活性。这也是数据仓库模型所具备的功能之一。业务主线层，这个层次主要划分大的业务领域，一般在业务建模阶段即已经完成这方面的划分。我们一般通过这种大的业务主线来划分整个业务模型大的框架。通过领域概念建模，数据仓库的模型已经被抽象成一个个的实体，模型的框架已经搭建完毕，下面的工作就是给这些框架注入有效的肌体。逻辑建模阶段通过领域概念建模之后，虽然模型的框架已经完成，但是还有很多细致的工作需要完成。一般在这个阶段，我们还需要做非常多的工作，主要包括：实例话每一个抽象的实体，例如：在上面的概念模型之后，我们需要对“人”和“公司”等这些抽象实体进行实例化。主要是，我们需要考虑“人”的属性包括那些，在业务模块中，用到的所有跟“人”相关的属性是哪些，我们都需要将这些属性附着在我们数据模型的“人”这个实体上，例如“人”得年龄，性别，受教育程度等等。同理，我们对其他属性同样需要做这个工作。找出抽象实体间的联系，并将其实例话。这里，我们主要考虑是“事件”这个抽象概念的实例话，例如：对于养老金征缴这个“事件”的属性得考虑，对于失业劳动者培训这个“事件”的属性得考虑等等。找出抽象事件的关系，并对其进行说明。在这里我们主要是要针对“事件”进行完善的“说明”。例如：对于“事件”中的地域，事件等因素的考量等等。总而言之，在逻辑建模阶段，我们主要考虑得是抽象实体的一些细致的属性。通过逻辑建模阶段，我们才能够将整个概念模型完整串联成一个有机的实体，才能够完整的表达出业务之间的关联性。在这个阶段，笔者建议大家可以参考 3NF 的建模方法，表达出实体的属性，以及实体与实体之间的联系。例如：在这个阶段，我们可以通过采用 ERWIN 等建模工具等作出符合 3NF 的关系型数据模型来。物理建模阶段物理建模阶段是整个数据建模的最后一个过程，这个过程其实是将前面的逻辑数据模型落地的一个过程。考虑到数据仓库平台的不同，因此，数据模型得物理建模过程可能会稍微有一些不同，在这个阶段我们主要的工作是：生成创建表的脚本。不同的数据仓库平台可能生成不同的脚本。针对不同的数据仓库平台，进行一些相应的优化工作，例如对于 DB2 数据仓库来说，创建一些 MQT 表，来加速报表的生成等等。针对数据集市的需要，按照维度建模的方法，生成一些事实表，维表等工作。针对数据仓库的 ETL 车和元数据管理的需要，生成一些数据仓库维护的表，例如：日志表等。经过物理建模阶段，整个数据仓库的模型已经全部完成，我们可以按照自己的设计来针对当前的行业创建满足自己需要的数据模型来。这里，笔者通过一个数据建模的样例，希望能够给读者一个关于数据仓库建模的感性的认识。希望读者在利用这些数据仓库得建模方法创建自己的数据模型的时候，可以根据业务实际的需要和自己对抽象能力的把握来创建适合自己的数据模型。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 架构设计","slug":"数据仓库架构设计","date":"2018-08-22T15:26:15.000Z","updated":"2020-08-02T13:25:08.051Z","comments":true,"path":"2018/08/22/数据仓库架构设计/","link":"","permalink":"cpeixin.cn/2018/08/22/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"业务数据背景我所在的公司处理的数据主要是游戏过程数据，用户行为数据，游戏币消费数据还有其他业务部门维护的例如用户优惠数据，游戏活动等业务数据。其中对于业务系统DBMS中的数据，基本按照“天”作为时间粒度来采集到数据仓库中，对于实时性要求较强的游戏数据，则是需要采用埋点实时采集和处理的。需求的多样化和复杂性，运营人员希望做到精细化运营和数据指标的实时响应, 这要求数据仓库具有提供高效明细数据能力,也能随时提供最新的数据调取，为了满足不同层次的数据提出和分析, 就要求着数据仓库在设计之初，就要考虑到要同时支持离线和实时的情况。数据仓库架构理念设计数据仓库架构之初，我们大数据工程组团队开会讨论的第一件事就是：Who is the User of Data in Data Warehouse？这件事情还是很重要的，弄清楚了这件事，对于接下来架构设计和数仓数据主题的确定都是有影响的，其中对于架构设计来说，这决定了我们架构上层需要提供哪些接口，对接不同的业务系统，我们应该设计怎样的查询和索引，不同的使用者，能够接受的数据粒度或者是数据响应时间，整个数据仓库的数据流大致应该怎样流向，这都与使用用户有关。在架构设计方面，基本上都是由下到上的去设计，但是呢，也不妨从上层需求开始，向下层实现设计开始✌️架构图话不多说，po图👽架构层次&amp;技术选型数据源由于公司各业务部门的数据都由运维DBA统一管理和做权限控制，RDBMS中的数据的采集，需要由运维部门给出备库URL，避免直接来取主库数据，对线上业务产生影响。这部份的数据采集，主要涉及Sqoop，NIFI，Spark 这三个工具。Sqoop在上一版的数据仓库中，Sqoop是作为RDBMS -&gt; HDFS中主要的搬运工。主要原因是开发成本小，数据量小，使用简单。但是随着数仓原始层数据量激增了原来的三倍，Sqoop计算能力就捉襟见肘了，每天的第一个任务就是拉取原始层数据，如果继续使用Sqoop，会严重影响接下来其他例行任务的按时进行。所以，我们决定抛弃这个使用MapReduce作为计算框架的数据同步工具。Spark提升数仓原始数据层拉取速度，就要选择合适的拉去工具。面对拉去原始数据，基本上不需要对任何数据字段进行清洗，基本上直接下 select columns就可以满足要求， 所以选用Spark SQL， read jdbc -&gt; dataframe -&gt; writeInto -&gt; Hive。这样运行的速度相对Sqoop来说，提升了四倍✌️NIFINIFI这个工具，并没有作为主要的数据工具来使用，起初只是作为团队的技术调研对象，在几次的分享会上，大家对NIFI都比较感兴趣，可能是大家都想解放双手，用拖拽的形式来拒绝写代码 哈哈哈哈。就这样，NIFI作为一个技术储备，承担了test库中测试数据的同步，等以后团队人手充足了，有维护多框架的能力时可以考虑发挥NIFI的更多用处。另外对于埋点数据，产品方会将数据打到运维的kafka中，随后我们会使用fluentd将运维kafka中的数据实时转发到我们大数据平台中的kafka中，做接下来的处理Fluentd运维的kafka中，会收集这我们想要的日志数据和埋点数据，埋点数据一般为json格式，将数据解析和打到大数据平台kafka的这一段中，我们在Logstash,Fluentd两个工具中进行选择，Logstash是非常出名的ELK中的 ‘L’ , 成熟度和稳定性不用说。Fluentd则是我们第一次接触，我们并没有亲自对Fluentd和Logstash进行效率和资源消耗上做测试，但是根据网上几篇对比测评文章，Fluentd的支持度虽没有Logstash那么完整，但是Fluentd中的插件完全能满足我们的开发需求，最重要的，Fluentd在CPU和内存上的消耗，都要优于Logstash。所以最后我们选择了FluentdFlink &amp; Spark StreamingFlink，Spark Streaming这两个实时计算框架就不多介绍了，家喻户晓。在这里说一下，在实时计算的这一层面，团队为什么选择了两个计算框架。主要是因为对待不同的需求，使用更适合的工具，团队成员对Spark Streaming更加熟悉，在对实时性要求更高的项目中，例如游戏监控系统，对于数据状态管理，watermark的需求都是有要求的，在这方面，Flink的实现就是要比Spark Streaming好。数据存储层数据存储层分为离线和实时HiveHive是Hadoop的一个数据仓库工具，底层存储为HDFS，可以将我们按照主题设计好的结构化数据映射成表，提供SQL接口来分析数据。从存储层到共享层中，大部分的计算任务，都是在Hive表中，使用Spark作为计算框架，ETL将数据清洗到HBase,MySQL等结果库中。HBase&amp;Elasticsearch&amp;GrafanaHBaseHBase的用途基本上是存储流式ETL后的结果数据，或者是直接存入原始数据作为备份ElasticsearchES的用途基本上是搭配Fluentd和Kibana，组成EFK，做数据展示，多数情况下会存储日志数据，做详情的检索。还有一个常用的功能就是结合HBase做二级索引，会在ES索引里面存储一个hbase的关键字rowkey，根据查询条件定位到某个rowkey，直接去hbase里面get数据，秒级返回GrafanaGrafana是团队后期引入的，主要是被Grafana的UI所吸引，Grafana和Kibana对比起来，UI展示要强大的很多，对于Grafana，我们正在调研，使用Grafana来做数据质量的监控，等使用成熟了，也会分享出来。数据共享层共享层的数据，都是经过ETL清洗的，都是即拿即用的。业务系统 ，对接数据仓库的，都是通过GoApi结构进行调取的。报表数据 ，工程师会通过Spark写成CSV文件，提供给运营人员即席查询 ，我们还开放了Presto接口（presto的查询速度比hive快5-10倍），同时对数据仓库做了仅查权限控制，希望业务分析人员可以自主的通过presto进行数据调取，但是理想是丰满的，现实是骨感的。根本没人用 哈哈哈哈哈哈哈数据应用层略数据用户层略","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 实际作用与需求","slug":"数据仓库实际作用与需求","date":"2018-08-20T15:26:15.000Z","updated":"2020-08-02T13:24:15.066Z","comments":true,"path":"2018/08/20/数据仓库实际作用与需求/","link":"","permalink":"cpeixin.cn/2018/08/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AE%9E%E9%99%85%E4%BD%9C%E7%94%A8%E4%B8%8E%E9%9C%80%E6%B1%82/","excerpt":"","text":"数据仓库的产生背景数据仓库的起源可以追溯到计算机的发展初期，并且数据仓库是信息技术长期发展的产物，在以后也会一直发展。从工程角度来讲，在计算机领域出现了大型在线事务处理系统后，对于数据之间的交互和使用就逐渐的多了起来。随后出现的抽取程序，可以通过设置参数，在文件中搜索满足条件的数据，然后把这些数据传送到其他文件或者数据库中。慢慢的人们发现在抽取结果中，加上一些条件限制可以更方便的得到想要的数据，于是就出现了基于抽取之上的抽取。这样就造成了如下问题：无休止的抽取带来诸多问题，抽取程序过多，功能单一，不可复用，无公共使用数据源，数据之间会产生差异等，所以开始思考是否可以建立成体系的机构化环境，以减少数据的差异。这也就是数据仓库出现的原因。数据仓库从操作型数据库中抽取数据，通过规范的加工过程，得到粒度化数据，并且这些数据时面向主题、集成、不易失、随时间变化的从商业角度来讲，在一次大会上，马云爸爸谈到当今时代，是IT时代到DT时代的变革。数据资源是比石油还要重要的资源，数据就是生产资料。随着企业多年的经营，业务数据量的不断增大，如何能够更好地利用数据，将数据转化成商业价值，已经成为人们越来越关心的问题。举例来说，数据库系统可以很好地解决事务处理，实现对数据的“增删改查”等功能，但是却不能提供很好的决策分析支持。因为事务处理首先考虑响应的及时性，多数情况都是在处理当前数据，而决策分析需要考虑的是数据的集成性和历史性，可能对分析处理的时效性要求不高。所以为了提高决策分析的有效性和完整性，人们逐渐将一部分或者大部分数据从联机事物处理系统中剥离出来，形成今天的数据仓库。数据团队眼中的数据仓库从实习工作，到目前为止，我参与了两次数仓的建设，实习公司当时大数据团队刚刚成立，那时并没有建立数仓，而是依据当时的各个项目需求，直接建立数据集市，将每个审计局的业务需求，分别建立不同的业务表，数据来源有来自Excel文件的，也有来自Oracle数据库的。再一个就是当前公司，这次的数仓建设，是我完整的一次参与整体流程，建立数仓的目的就是来应对各个产品所会提出的各种数据需求和统一各个产品的数据标准，现在的产品方，确实具有一定的数据头脑，不再像以前只追求盈利，不去思考数据背后的价值和数据指标对经营带来的影响。我所在的大数据团队负责公司层面和下面8个产品线的数据驱动服务，公司层面经常会调取盈利相关指标，产品方的需求则是各种各样，时间长度也是从年到天都有。对于公司和产品方，他们并不关心数据的来源和数仓的重要性，因为他们是使用数据结果的人，但是对于大数据团队，建立数据仓库是一个必要的事情，数据仓库对企业的价值数据仓库对企业带来的价值是很难估算的，但是数据仓库的成本倒是可以估算出大概的。就我们公司而言，数据仓库数据主题分成两个大类来说，就是用户数据与业务数据，那么产品方最想看到的就是用户数据与业务数据联合起来，这就是数据仓库对公司产品运营方带来的价值。数据仓库具有历史性，其中存储的数据大多是结构化数据，这些数据并非企业全量数据，而是根据需求针对性抽取的，因此数据仓库对于业务的价值是各种各样的报表，但这些报表又无法实时产生。数据仓库报表虽然能够提供部分业务价值，但不能直接影响业务，需要数据开发人员对数据仓库中的数据进行ETL开发，给出指导性的数据分析结果。数据仓库中数据与数据库中数据的不同这里呢，简单的说，应该就是OLTP和OLAP的区别数据仓库需求分析阶段数据仓库的需求分析，在很多公司中，都是由数据团队或者数据仓库工程师几个技术人员协定下来的。但是一个合格的数据仓库，应该是由业务方，产品方，运营方，数据团队技术方一同来商定的。一个完整的流程应该是去了解数据的背景，细节，去了解用户需要用到哪些主题的数据和数据粒度的大小，去了解主题数据中数据的维度，最后才是数据仓库设计人员开始拿出数据仓库的架构方案和ETL方案。数据仓库作用整合公司所有业务数据，建立统一的数据中心产生业务报表，用于作出决策为网站运营提供运营上的数据支持可以作为各个业务的数据源，形成业务数据互相反馈的良性循环分析用户行为数据，通过数据挖掘来降低投入成本，提高投入效果开发数据产品，直接或间接地为公司盈利","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据仓库 - 简述","slug":"数据仓库简述","date":"2018-08-19T15:26:15.000Z","updated":"2020-08-02T13:24:52.139Z","comments":true,"path":"2018/08/19/数据仓库简述/","link":"","permalink":"cpeixin.cn/2018/08/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0/","excerpt":"","text":"前言说到数据仓库，在一年前，我的答案是非常模糊的，在两年前，我根本不知道数据仓库的存在。相信大多数的人都是经常接触数据库，上大学的时候学过Oracle，MySQL这两个关系型数据库，操作着各种增删改查，现在人们对这种类型的数据库都称作传统数据库，数据量偏小，每一张表都是为了某一系统功能而设计的。在之后从事大数据行业了，数据仓库这几个字也就总出现在各种博客中，各种技术方案中。现在正在和同事主导公司数据仓库的设计和搭建工作，所以准备把我这段工程经验，拿出来分享一下。数据库与数据仓库的区别数据库这里我们所指的数据库，也就是一系列经典的RDBMS，如Oracle，MySQL，SQL Server等关系型数据库。其中的一些设计过程如 ER图的设计，逻辑模型的设计，物理模型设计，还有规范化设计 如 至少要符合第一范式，尽可量的去符合第二范式，第三范式等设计细节，这里就不赘述了，但是以上所提到的都是关系型数据库的精华。关系型数据库的用途，现在也会被分成两大类（1）操作型数据库，主要用于业务支撑。一个公司往往会使用并维护若干个数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等；（2）分析型数据库，主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析；数据仓库接下来先给大家看百度百科给出的数据仓库概念数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。从上面的定义来看，数据仓库的主要功能是用于做企业各个业务层面的分析，企业层面的数据总集，曾经看过一个博主对数据仓库的定义是“面向分析的存储系统”，这个定义是很接地气的，如果按照这个思路往下去思考，数据仓库存储的数据是很庞大的，不是一个单一的业务数据集，不用精准到每一条数据，例如，淘宝在存储每一条下单记录的时候，在关系型数据库中，这条记录是不会存储到其他业务表里面，只会存储在下单记录表里面，并且下单记录表里面不会存储两条相同的订单数据，这是因为关系型数据库要严格满足完整性/参照性约束以及范式设计要求，但是在数据仓库中，这条订单数据可能会存在用户行为表中，也可能存在下单数据表中。也就是说，数据仓库不应让传统关系数据库来实现，因为关系数据库最少也要求满足第1范式，而数据仓库里的关系表可以不满足第1范式。也就是说，同样的记录在一个关系表里可以出现N次。但由于大多数数据仓库内的表的统计分析还是用SQL，因此很多人把它和关系数据库搞混了。数据仓库的特点面向主题面向主题特性是数据仓库和操作型数据库的根本区别。操作型数据库是为了支撑各种业务而建立，而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；集成性集成性是指数据仓库会将不同源数据库中的数据汇总到一起；企业范围数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来；历史性较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年；时变性时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告；数据仓库组件数据仓库的核心组件有四个：各源数据库，ETL，数据仓库，前端应用。如下图所示：业务系统业务系统包含各种源数据库或者网站前端埋点实时数据，数据源可以是离线的，也可以是实时的（时间粒度为小时）。比如我们数据仓库中的离线数据来源是公司业务系统的Oracle,这些源数据库既为业务系统提供数据支撑，同时也作为数据仓库的数据源，部分实时数据是通过kafka，flume，NIFI收集来的埋点数据。ETLETL分别代表：提取extraction、转换transformation、加载load。其中提取过程表示操作型数据库搜集指定数据，转换过程表示将数据转化为指定格式并进行数据清洗保证数据质量，加载过程表示将转换过后满足指定格式的数据加载进数据仓库。数据仓库会周期不断地从源数据库提取清洗好了的数据。（ETL应该是数据工程师必会的技能😂）前端应用数据仓库的数据，肯定是为了前端应用而准备的，前端应用则是为了使用数据的用户而准备的。目前我搭建的数仓一般不会直接对接前端应用，而是将数仓中的数据，按照用户方需求进行计算聚合到HBase或者ES中。中间加一层API,API可以是Go或者Python开发的，这样前端应用直接调用API,也有一些特殊场景，会直接调用presto来访问Hive数据集市数据集市是数据仓库下面衍生出来的概念，在这里，我举一个例子来帮助大家理解，我们可以把数据仓库理解成万达购物中心，其中每层都卖着不同种类的商品，其中，一层卖的服装，二层买的家电…这样就可以分成一层是一个服装主题的数据集市，二层是家电主题的数据集市。同时可以理解为是一种”小型数据仓库”，它只包含单个主题，且关注范围也非全局。集市可以分为两种，一种是独立数据集市(independent data mart)，这类数据集市有自己的源数据库和ETL架构；另一种是非独立数据集市(dependent data mart)，这种数据集市没有自己的源系统，它的数据来自数据仓库。当用户或者应用程序不需要/不必要/不允许用到整个数据仓库的数据时，非独立数据集市就可以简单为用户提供一个数据仓库的”子集”。数据仓库搭建流程大家先看这个流程图，其中的各个步骤，我都会去介绍，其中着重介绍的会是数据仓库建模和ETL工程，因为，建模是整个数据仓库的核心，ETL工程师整个数据仓库中最耗时耗力的。今天呢，先简单介绍一下，铺垫一下","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}]},{"title":"数据分析 - EM聚类 实战","slug":"数据分析-EM聚类-实战","date":"2018-08-18T14:18:53.000Z","updated":"2020-05-01T14:16:07.111Z","comments":true,"path":"2018/08/18/数据分析-EM聚类-实战/","link":"","permalink":"cpeixin.cn/2018/08/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-EM%E8%81%9A%E7%B1%BB-%E5%AE%9E%E6%88%98/","excerpt":"","text":"今天进行 EM 的实战。上篇讲了 EM 算法的原理，EM 算法相当于一个聚类框架，里面有不同的聚类模型，比如 GMM 高斯混合模型，或者 HMM 隐马尔科夫模型。其中你需要理解的是 EM 的两个步骤，E 步和 M 步：E 步相当于通过初始化的参数来估计隐含变量，M 步是通过隐含变量来反推优化参数。最后通过 EM 步骤的迭代得到最终的模型参数。今天我们进行 EM 算法的实战，你需要思考的是：如何使用 EM 算法工具完成聚类？什么情况下使用聚类算法？我们用聚类算法的任务目标是什么？面对王者荣耀的英雄数据，EM 算法能帮助我们分析出什么？如何使用 EM 工具包在 Python 中有第三方的 EM 算法工具包。由于 EM 算法是一个聚类框架，所以你需要明确你要用的具体算法，比如是采用 GMM 高斯混合模型，还是 HMM 隐马尔科夫模型。这节课我们主要讲解 GMM 的使用，在使用前你需要引入工具包：1from sklearn.mixture import GaussianMixture我们看下如何在 sklearn 中创建 GMM 聚类。首先我们使用 gmm = GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100) 来创建 GMM 聚类，其中有几个比较主要的参数（GMM 类的构造参数比较多，我筛选了一些主要的进行讲解），我分别来讲解下：1.n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果你不指定 n_components，最终的聚类结果都会为同一个值。2.covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。协方差类型有 4 种取值：covariance_type=full，代表完全协方差，也就是元素都不为 0；covariance_type=tied，代表相同的完全协方差；covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。3.max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。创建完 GMM 聚类器之后，我们就可以传入数据让它进行迭代拟合。我们使用 fit 函数，传入样本特征矩阵，模型会自动生成聚类器，然后使用 prediction=gmm.predict(data) 来对数据进行聚类，传入你想进行聚类的数据，可以得到聚类结果 prediction。你能看出来拟合训练和预测可以传入相同的特征矩阵，这是因为聚类是无监督学习，你不需要事先指定聚类的结果，也无法基于先验的结果经验来进行学习。只要在训练过程中传入特征值矩阵，机器就会按照特征值矩阵生成聚类器，然后就可以使用这个聚类器进行聚类了。如何用 EM 算法对王者荣耀数据进行聚类了解了 GMM 聚类工具之后，我们看下如何对王者荣耀的英雄数据进行聚类。首先我们知道聚类的原理是“人以群分，物以类聚”。通过聚类算法把特征值相近的数据归为一类，不同类之间的差异较大，这样就可以对原始数据进行降维。通过分成几个组（簇），来研究每个组之间的特性。或者我们也可以把组（簇）的数量适当提升，这样就可以找到可以互相替换的英雄，比如你的对手选择了你擅长的英雄之后，你可以选择另一个英雄作为备选。我们先看下数据长什么样子：这里我们收集了 69 名英雄的 20 个特征属性，这些属性分别是最大生命、生命成长、初始生命、最大法力、法力成长、初始法力、最高物攻、物攻成长、初始物攻、最大物防、物防成长、初始物防、最大每 5 秒回血、每 5 秒回血成长、初始每 5 秒回血、最大每 5 秒回蓝、每 5 秒回蓝成长、初始每 5 秒回蓝、最大攻速和攻击范围等。现在我们需要对王者荣耀的英雄数据进行聚类，我们先设定项目的执行流程：首先我们需要加载数据源；在准备阶段，我们需要对数据进行探索，包括采用数据可视化技术，让我们对英雄属性以及这些属性之间的关系理解更加深刻，然后对数据质量进行评估，是否进行数据清洗，最后进行特征选择方便后续的聚类算法；聚类阶段：选择适合的聚类模型，这里我们采用 GMM 高斯混合模型进行聚类，并输出聚类结果，对结果进行分析。按照上面的步骤，我们来编写下代码。完整的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-import pandas as pdimport csvimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.mixture import GaussianMixturefrom sklearn.preprocessing import StandardScaler # 数据加载，避免中文乱码问题data_ori = pd.read_csv('./heros7.csv', encoding = 'gb18030')features = [u'最大生命',u'生命成长',u'初始生命',u'最大法力', u'法力成长',u'初始法力',u'最高物攻',u'物攻成长',u'初始物攻',u'最大物防',u'物防成长',u'初始物防', u'最大每5秒回血', u'每5秒回血成长', u'初始每5秒回血', u'最大每5秒回蓝', u'每5秒回蓝成长', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']data = data_ori[features] # 对英雄属性之间的关系进行可视化分析# 设置plt正确显示中文plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号# 用热力图呈现features_mean字段之间的相关性corr = data[features].corr()plt.figure(figsize=(14,14))# annot=True显示每个方格的数据sns.heatmap(corr, annot=True)plt.show() # 相关性大的属性保留一个，因此可以对属性进行降维features_remain = [u'最大生命', u'初始生命', u'最大法力', u'最高物攻', u'初始物攻', u'最大物防', u'初始物防', u'最大每5秒回血', u'最大每5秒回蓝', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']data = data_ori[features_remain]data[u'最大攻速'] = data[u'最大攻速'].apply(lambda x: float(x.strip('%'))/100)data[u'攻击范围']=data[u'攻击范围'].map(&#123;'远程':1,'近战':0&#125;)# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()data = ss.fit_transform(data)# 构造GMM聚类gmm = GaussianMixture(n_components=30, covariance_type='full')gmm.fit(data)# 训练数据prediction = gmm.predict(data)print(prediction)# 将分组结果输出到CSV文件中data_ori.insert(0, '分组', prediction)data_ori.to_csv('./hero_out.csv', index=False, sep=',')1234[28 14 8 9 5 5 15 8 3 14 18 14 9 7 16 18 13 3 5 4 19 12 4 12 12 12 4 17 24 2 7 2 2 24 2 2 24 6 20 22 22 24 24 2 2 22 14 20 14 24 26 29 27 25 25 28 11 1 23 5 11 0 10 28 21 29 29 29 17]同时你也能看到输出的聚类结果文件 hero_out.csv（它保存在你本地运行的文件夹里，程序会自动输出这个文件，你可以自己看下）。我来简单讲解下程序的几个模块。关于引用包首先我们会用 DataFrame 数据结构来保存读取的数据，最后的聚类结果会写入到 CSV 文件中，因此会用到 pandas 和 CSV 工具包。另外我们需要对数据进行可视化，采用热力图展现属性之间的相关性，这里会用到 matplotlib.pyplot 和 seaborn 工具包。在数据规范化中我们使用到了 Z-Score 规范化，用到了 StandardScaler 类，最后我们还会用到 sklearn 中的 GaussianMixture 类进行聚类。数据可视化的探索你能看到我们将 20 个英雄属性之间的关系用热力图呈现了出来，中间的数字代表两个属性之间的关系系数，最大值为 1，代表完全正相关，关系系数越大代表相关性越大。从图中你能看出来“最大生命”“生命成长”和“初始生命”这三个属性的相关性大，我们只需要保留一个属性即可。同理我们也可以对其他相关性大的属性进行筛选，保留一个。你在代码中可以看到，我用 features_remain 数组保留了特征选择的属性，这样就将原本的 20 个属性降维到了 13 个属性。关于数据规范化我们能看到“最大攻速”这个属性值是百分数，不适合做矩阵运算，因此我们需要将百分数转化为小数。我们也看到“攻击范围”这个字段的取值为远程或者近战，也不适合矩阵运算，我们将取值做个映射，用 1 代表远程，0 代表近战。然后采用 Z-Score 规范化，对特征矩阵进行规范化。在聚类阶段我们采用了 GMM 高斯混合模型，并将结果输出到 CSV 文件中。这里我将输出的结果截取了一段（设置聚类个数为 30）：第一列代表的是分组（簇），我们能看到张飞、程咬金分到了一组，牛魔、白起是一组，老夫子自己是一组，达摩、典韦是一组。聚类的特点是相同类别之间的属性值相近，不同类别的属性值差异大。因此如果你擅长用典韦这个英雄，不防试试达摩这个英雄。同样你也可以在张飞和程咬金中进行切换。这样就算你的英雄被别人选中了，你依然可以有备选的英雄可以使用。总结今天我带你一起做了 EM 聚类的实战，具体使用的是 GMM 高斯混合模型。从整个流程中可以看出，我们需要经过数据加载、数据探索、数据可视化、特征选择、GMM 聚类和结果分析等环节。聚类和分类不一样，聚类是无监督的学习方式，也就是我们没有实际的结果可以进行比对，所以聚类的结果评估不像分类准确率一样直观，那么有没有聚类结果的评估方式呢？这里我们可以采用 Calinski-Harabaz 指标，代码如下：123from sklearn.metrics import calinski_harabaz_scoreprint(calinski_harabaz_score(data, prediction))指标分数越高，代表聚类效果越好，也就是相同类中的差异性小，不同类之间的差异性大。当然具体聚类的结果含义，我们需要人工来分析，也就是当这些数据被分成不同的类别之后，具体每个类表代表的含义。另外聚类算法也可以作为其他数据挖掘算法的预处理阶段，这样我们就可以将数据进行降维了。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"}]},{"title":"数据分析-K-Means_1","slug":"数据分析-K-Means-1","date":"2018-08-01T15:26:13.000Z","updated":"2020-05-01T14:15:41.551Z","comments":true,"path":"2018/08/01/数据分析-K-Means-1/","link":"","permalink":"cpeixin.cn/2018/08/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-K-Means-1/","excerpt":"","text":"数据分析 - K-Means 原理K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，你可以理解这个算法的本质是确定 K 类的中心点，当你找到了这些中心点，也就完成了聚类。那么请你和我思考以下三个问题：如何确定 K 类的中心点？如何将其他点划分到 K 类中？如何区分 K-Means 与 KNN？如果理解了上面这 3 个问题，那么对 K-Means 的原理掌握得也就差不多了。先请你和我思考一个场景，假设我有 20 支亚洲足球队，想要将它们按照成绩划分成 3 个等级，可以怎样划分？K-Means 的工作原理对亚洲足球队的水平，你可能也有自己的判断。比如一流的亚洲球队有谁？你可能会说伊朗或韩国。二流的亚洲球队呢？你可能说是中国。三流的亚洲球队呢？你可能会说越南。其实这些都是靠我们的经验来划分的，那么伊朗、中国、越南可以说是三个等级的典型代表，也就是我们每个类的中心点。所以回过头来，如何确定 K 类的中心点？一开始我们是可以随机指派的，当你确认了中心点后，就可以按照距离将其他足球队划分到不同的类别中。这也就是 K-Means 的中心思想，就是这么简单直接。你可能会问：如果一开始，选择一流球队是中国，二流球队是伊朗，三流球队是韩国，中心点选择错了怎么办？其实不用担心，K-Means 有自我纠正机制，在不断的迭代过程中，会纠正中心点。中心点在整个迭代过程中，并不是唯一的，只是你需要一个初始值，一般算法会随机设置初始的中心点。好了，那我来把 K-Means 的工作原理给你总结下：选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点；重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。如何给亚洲球队做聚类对于机器来说需要数据才能判断类中心点，所以我整理了 2015-2019 年亚洲球队的排名，如下表所示。我来说明一下数据概况。其中 2019 年国际足联的世界排名，2015 年亚洲杯排名均为实际排名。2018 年世界杯中，很多球队没有进入到决赛圈，所以只有进入到决赛圈的球队才有实际的排名。如果是亚洲区预选赛 12 强的球队，排名会设置为 40。如果没有进入亚洲区预选赛 12 强，球队排名会设置为 50。针对上面的排名，我们首先需要做的是数据规范化。你可以把这些值划分到[0,1]或者按照均值为 0，方差为 1 的正态分布进行规范化。我先把数值都规范化到[0,1]的空间中，得到了以下的数值表：如果我们随机选取中国、日本、韩国为三个类的中心点，我们就需要看下这些球队到中心点的距离。距离有多种计算的方式，有关距离的计算我在 KNN 算法中也讲到过：欧氏距离曼哈顿距离切比雪夫距离余弦距离欧氏距离是最常用的距离计算方式，这里我选择欧氏距离作为距离的标准，计算每个队伍分别到中国、日本、韩国的距离，然后根据距离远近来划分。我们看到大部分的队，会和中国队聚类到一起。这里我整理了距离的计算过程，比如中国和中国的欧氏距离为 0，中国和日本的欧式距离为 0.732003。如果按照中国、日本、韩国为 3 个分类的中心点，欧氏距离的计算结果如下表所示：然后我们再重新计算这三个类的中心点，如何计算呢？最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类，再根据球队的分类更新中心点的位置。计算过程这里不展开，最后一直迭代（重复上述的计算过程：计算中心点和划分分类）到分类不再发生变化，可以得到以下的分类结果：所以我们能看出来第一梯队有日本、韩国、伊朗、沙特、澳洲；第二梯队有中国、伊拉克、阿联酋、乌兹别克斯坦；第三梯队有卡塔尔、泰国、越南、阿曼、巴林、朝鲜、印尼、叙利亚、约旦、科威特和巴勒斯坦。如何使用 sklearn 中的 K-Means 算法sklearn 是 Python 的机器学习工具库，如果从功能上来划分，sklearn 可以实现分类、聚类、回归、降维、模型选择和预处理等功能。这里我们使用的是 sklearn 的聚类函数库，因此需要引用工具包，具体代码如下：1from sklearn.cluster import KMeans当然 K-Means 只是 sklearn.cluster 中的一个聚类库，实际上包括 K-Means 在内，sklearn.cluster 一共提供了 9 种聚类方法，比如 Mean-shift，DBSCAN，Spectral clustering（谱聚类）等。这些聚类方法的原理和 K-Means 不同，这里不做介绍。我们看下 K-Means 如何创建：1KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')我们能看到在 K-Means 类创建的过程中，有一些主要的参数：n_clusters: 即 K 值，一般需要多试一些 K 值来保证更好的聚类效果。你可以随机设置一些 K 值，然后选择聚类效果最好的作为最终的 K 值；max_iter： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长；n_init：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。如果 K 值比较大的时候，你可以适当增大 n_init 这个值；init： 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式。自己设置中心点一般是对于个性化的数据进行设置，很少采用。random 的方式则是完全随机的方式，一般推荐采用优化过的 k-means++ 方式；algorithm：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的”auto”。简单说下这三个取值的区别，如果你选择”full”采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。在创建好 K-Means 类之后，就可以使用它的方法，最常用的是 fit 和 predict 这个两个函数。你可以单独使用 fit 函数和 predict 函数，也可以合并使用 fit_predict 函数。其中 fit(data) 可以对 data 数据进行 k-Means 聚类。 predict(data) 可以针对 data 中的每个样本，计算最近的类。现在我们要完整地跑一遍 20 支亚洲球队的聚类问题。1234567891011121314151617181920212223# coding: utf-8from sklearn.cluster import KMeansfrom sklearn import preprocessingimport pandas as pdimport numpy as np# 输入数据data = pd.read_csv('data.csv', encoding='gbk')train_x = data[[\"2019年国际排名\",\"2018世界杯\",\"2015亚洲杯\"]]df = pd.DataFrame(train_x)# kmeans = KMeans(n_clusters=3)kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')# 规范化到[0,1]空间min_max_scaler=preprocessing.MinMaxScaler()train_x=min_max_scaler.fit_transform(train_x)# kmeans算法kmeans.fit(train_x)predict_y = kmeans.predict(train_x)# 合并聚类结果，插入到原数据中result = pd.concat((data,pd.DataFrame(predict_y)),axis=1)result.rename(&#123;0:u'聚类'&#125;,axis=1,inplace=True)print(result)结果：123456789101112131415161718192021 国家 2019年国际排名 2018世界杯 2015亚洲杯 聚类0 中国 73 40 7 21 日本 60 15 5 02 韩国 61 19 2 03 伊朗 34 18 6 04 沙特 67 26 10 05 伊拉克 91 40 4 26 卡塔尔 101 40 13 17 阿联酋 81 40 6 28 乌兹别克斯坦 88 40 8 29 泰国 122 40 17 110 越南 102 50 17 111 阿曼 87 50 12 112 巴林 116 50 11 113 朝鲜 110 50 14 114 印尼 164 50 17 115 澳洲 40 30 1 016 叙利亚 76 40 17 117 约旦 118 50 9 118 科威特 160 50 15 119 巴勒斯坦 96 50 16 1总结如何确定 K 类的中心点？其中包括了初始的设置，以及中间迭代过程中中心点的计算。在初始设置中，会进行 n_init 次的选择，然后选择初始中心点效果最好的为初始值。在每次分类更新后，你都需要重新确认每一类的中心点，一般采用均值的方式进行确认。如何将其他点划分到 K 类中？这里实际上是关于距离的定义，我们知道距离有多种定义的方式，在 K-Means 和 KNN 中，我们都可以采用欧氏距离、曼哈顿距离、切比雪夫距离、余弦距离等。对于点的划分，就看它离哪个类的中心点的距离最近，就属于哪一类。如何区分 K-Means 和 KNN 这两种算法呢？刚学过 K-Means 和 KNN 算法的同学应该能知道两者的区别，但往往过了一段时间，就容易混淆。所以我们可以从三个维度来区分 K-Means 和 KNN 这两个算法：首先，这两个算法解决数据挖掘的两类问题。K-Means 是聚类算法，KNN 是分类算法。这两个算法分别是两种不同的学习方式。K-Means 是非监督学习，也就是不需要事先给出分类标签，而 KNN 是有监督学习，需要我们给出训练数据的分类标识。最后，K 值的含义不同。K-Means 中的 K 值代表 K 类。KNN 中的 K 值代表 K 个最接近的邻居。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"K-Means","slug":"K-Means","permalink":"cpeixin.cn/tags/K-Means/"}]},{"title":"数据分析-KNN_2","slug":"数据分析-KNN-2","date":"2018-07-26T16:14:49.000Z","updated":"2020-05-01T14:14:37.744Z","comments":true,"path":"2018/07/27/数据分析-KNN-2/","link":"","permalink":"cpeixin.cn/2018/07/27/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-2/","excerpt":"","text":"通过 sklearn 中自带的手写数字数据集来进行实战。如何在 sklearn 中使用 KNN在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。如果是做分类，你需要引用：1from sklearn.neighbors import KNeighborsClassifier如果是做回归，你需要引用：1from sklearn.neighbors import KNeighborsRegressor从名字上你也能看出来 Classifier 对应的是分类，Regressor 对应的是回归。一般来说如果一个算法有 Classifier 类，都能找到相应的 Regressor 类。比如在决策树分类中，你可以使用 DecisionTreeClassifier，也可以使用决策树来做回归 DecisionTreeRegressor。好了，我们看下如何在 sklearn 中创建 KNN 分类器。这里，我们使用构造函数1KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)这里有几个比较主要的参数，我分别来讲解下：1.n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。2.weights：是用来确定邻居的权重，有三种方式：weights=uniform，代表所有邻居的权重相同；weights=distance，代表权重是距离的倒数，即与距离成反比；自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。3.algorithm：用来规定计算邻居的方法，它有四种方式：algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。4.leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。如何用 KNN 对手写数字进行识别分类手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。今天我们用 sklearn 自带的手写数字数据集做 KNN 分类，你可以把这个数据集理解成一个简版的 MNIST 数据集，它只包括了 1797 幅数字图像，每幅图像大小是 8*8 像素。好了，我们先来规划下整个 KNN 分类的流程：整个训练过程基本上都会包括三个阶段：数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8*8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。好了，按照上面的步骤，我们一起来实现下这个项目。首先是加载数据和对数据的探索：1234567891011121314# 加载数据digits = load_digits()data = digits.data# 数据探索print(data.shape)# 查看第5幅图像print(digits.images[4])# 第5幅图像代表的数字含义print(digits.target[4])# 将第5幅图像显示出来plt.gray()plt.imshow(digits.images[4])plt.show()结果：12345678910(1797, 64)[[ 0. 0. 0. 1. 11. 0. 0. 0.] [ 0. 0. 0. 7. 8. 0. 0. 0.] [ 0. 0. 1. 13. 6. 2. 2. 0.] [ 0. 0. 7. 15. 0. 9. 8. 0.] [ 0. 5. 16. 10. 0. 16. 6. 0.] [ 0. 4. 15. 16. 13. 16. 1. 0.] [ 0. 0. 0. 3. 15. 10. 0. 0.] [ 0. 0. 0. 2. 16. 4. 0. 0.]]4对应的手写图像数据：我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个 88 的像素矩阵，上面这幅图像是一个“4”，从训练集的分类标注中我们也可以看到分类标注为“4”。sklearn 自带的手写数字数据集一共包括了 1797 个样本，每幅图像都是 88 像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化，代码如下：1234567# 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# 采用Z-Score规范化ss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)注：上面代码中，在train的时候用到了：train_ss_x = ss.fit_transform(train_x)实际上：fit_transform是fit和transform两个函数都执行一次。所以ss是进行了fit拟合的。只有在fit拟合之后，才能进行transform，在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。另外，如果我们没有fit，直接进行transform会报错，因为需要先fit拟合，才可以进行transform。然后我们构造一个 KNN 分类器 knn，把训练集的数据传入构造好的 knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到 KNN 分类器准确率，代码如下：123456# 创建KNN分类器knn = KNeighborsClassifier() knn.fit(train_ss_x, train_y) predict_y = knn.predict(test_ss_x) print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))运行结果：12KNN准确率: 0.9756好了，这样我们就构造好了一个 KNN 分类器。之前我们还讲过 SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：1234567891011121314151617181920# 创建SVM分类器svm = SVC()svm.fit(train_ss_x, train_y)predict_y=svm.predict(test_ss_x)print('SVM准确率: %0.4lf' % accuracy_score(test_y, predict_y))# 采用Min-Max规范化mm = preprocessing.MinMaxScaler()train_mm_x = mm.fit_transform(train_x)test_mm_x = mm.transform(test_x)# 创建Naive Bayes分类器mnb = MultinomialNB()mnb.fit(train_mm_x, train_y) predict_y = mnb.predict(test_mm_x) print(\"多项式朴素贝叶斯准确率: %.4lf\" % accuracy_score(test_y, predict_y))# 创建CART决策树分类器dtc = DecisionTreeClassifier()dtc.fit(train_mm_x, train_y) predict_y = dtc.predict(test_mm_x) print(\"CART决策树准确率: %.4lf\" % accuracy_score(test_y, predict_y))结果：1234SVM准确率: 0.9867多项式朴素贝叶斯准确率: 0.8844CART决策树准确率: 0.8556这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为 Z-Score 会将数值规范化为一个标准的正态分布，即均值为 0，方差为 1，数值会包含负数。因此我们需要采用 Min-Max 规范化，将数据规范化到[0,1]范围内。你能看出来 KNN 的准确率还是不错的，和 SVM 不相上下。完整代码：123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*-# 手写数字分类from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as plt# 加载数据digits = load_digits()data = digits.data# 数据探索print(data.shape)# 查看第一幅图像print(digits.images[0])# 第一幅图像代表的数字含义print(digits.target[0])# 将第一幅图像显示出来plt.gray()plt.imshow(digits.images[0])plt.show()# 分割数据，将25%的数据作为测试集，其余作为训练集train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# 采用Z-Score规范化ss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# 创建KNN分类器knn = KNeighborsClassifier()knn.fit(train_ss_x, train_y) predict_y = knn.predict(test_ss_x) print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))总结手写数字分类识别的实战，分别用 KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在这个过程中你应该对数据探索、数据可视化、数据规范化、模型训练和结果评估的使用过程有了一定的体会。在数据量不大的情况下，使用 sklearn 还是方便的。如果数据量很大，比如 MNIST 数据集中的 6 万个训练数据和 1 万个测试数据，那么采用深度学习 +GPU 运算的方式会更适合。因为深度学习的特点就是需要大量并行的重复计算，GPU 最擅长的就是做大量的并行计算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"}]},{"title":"数据分析-KNN_1","slug":"数据分析-KNN-1","date":"2018-07-25T16:14:49.000Z","updated":"2020-05-01T14:14:47.958Z","comments":true,"path":"2018/07/26/数据分析-KNN-1/","link":"","permalink":"cpeixin.cn/2018/07/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-KNN-1/","excerpt":"","text":"数据分析 - KNN 原理KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。我们先用一个例子体会下。假设，我们想对电影的类型进行分类，统计了电影中打斗次数、接吻次数，当然还有其他的指标也可以被统计到，如下表所示。我们很容易理解《战狼》《红海行动》《碟中谍 6》是动作片，《前任 3》《春娇救志明》《泰坦尼克号》是爱情片，但是有没有一种方法让机器也可以掌握这个分类的规则，当有一部新电影的时候，也可以对它的类型自动分类呢？我们可以把打斗次数看成 X 轴，接吻次数看成 Y 轴，然后在二维的坐标轴上，对这几部电影进行标记，如下图所示。对于未知的电影 A，坐标为 (x,y)，我们需要看下离电影 A 最近的都有哪些电影，这些电影中的大多数属于哪个分类，那么电影 A 就属于哪个分类。实际操作中，我们还需要确定一个 K 值，也就是我们要观察离电影 A 最近的电影有多少个。KNN 的工作原理“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。整个计算过程分为三步：计算待分类物体与其他物体之间的距离；统计距离最近的 K 个邻居；对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。K 值如何选择你能看出整个 KNN 的分类过程，K 值的选择还是很重要的。那么问题来了，K 值选择多少是适合的呢？如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。距离如何计算在 KNN算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。关于距离的计算方式有下面五种方式：欧氏距离；曼哈顿距离；闵可夫斯基距离；切比雪夫距离；余弦距离。其中前三种距离是 KNN 中最常用的距离，我给你分别讲解下。欧氏距离是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：同理，我们也可以求得两点在 n 维空间中的距离：曼哈顿距离在几何空间中用的比较多。以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：闵可夫斯基距离不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。那么切比雪夫距离怎么计算呢？二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。其公式为p为极限无穷的情况：余弦距离实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。KD 树其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。用 KNN 做回归KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。那么 KNN 如何做回归呢？对于一个新电影 X，我们要预测它的某个属性值，比如打斗次数，具体特征属性和数值如下所示。此时，我们会先计算待测点（新电影 X）到已知点的距离，选择距离最近的 K 个点。假设 K=3，此时最近的 3 个点（电影）分别是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数就是这 3 个点的该属性值的平均值，即 (100+95+105)/3=100 次。总结今天我给你讲了 KNN 的原理，以及 KNN 中的几个关键因素。比如针对 K 值的选择，我们一般采用交叉验证的方式得出。针对样本点之间的距离的定义，常用的有 5 种表达方式，你也可以自己来定义两个样本之间的距离公式。不同的定义，适用的场景不同。比如在搜索关键词推荐中，余弦距离是更为常用的。另外你也可以用 KNN 进行回归，通过 K 个邻居对新的点的属性进行值的预测。KNN 的理论简单直接，针对 KNN 中的搜索也有相应的 KD 树这个数据结构。KNN 的理论成熟，可以应用到线性和非线性的分类问题中，也可以用于回归分析。不过 KNN 需要计算测试点与样本点之间的距离，当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间。另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。当然在实际工作中，我们需要考虑到各种可能存在的情况，比如针对某类样本少的情况，可以增加该类别的权重。同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"}]},{"title":"数据分析-SVM_2","slug":"数据分析-SVM-2","date":"2018-07-25T12:27:13.000Z","updated":"2020-05-01T14:14:15.085Z","comments":true,"path":"2018/07/25/数据分析-SVM-2/","link":"","permalink":"cpeixin.cn/2018/07/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-2/","excerpt":"","text":"SVM 是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。sklearn 中使用 SVM在 Python 的 sklearn 工具包中有 SVM 算法，首先需要引用工具包：1from sklearn import svmSVM 既可以做回归，也可以做分类器。当用 SVM 做回归的时候，我们可以使用 SVR 或 LinearSVR。SVR 的英文是 Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。当做分类器的时候，我们使用的是 SVC 或者 LinearSVC。SVC 的英文是 Support Vector Classification。我简单说一下这两者之前的差别。从名字上你能看出 LinearSVC 是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到 SVM 是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。如果是针对非线性的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。创建一个 SVM 分类器我们首先使用 SVC 的构造函数：1model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)这里有三个重要的参数 kernel、C 和 gamma。kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。linear：线性核函数poly：多项式核函数rbf：高斯核函数（默认）sigmoid：sigmoid 核函数这四种函数代表不同的映射方式，你可能会问，在实际工作中，如何选择这 4 种核函数呢？我来给你解释一下：线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。了解深度学习的同学应该知道 sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。上面介绍的 4 种核函数，除了第一种线性核函数外，其余 3 种都可以处理线性不可分的数据。参数 C 代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。参数 gamma 代表核函数的系数，默认为样本特征数的倒数，即 gamma = 1 / n_features。在创建 SVM 分类器之后，就可以输入训练集对它进行训练。我们使用 model.fit(train_X,train_y)，传入训练集中的特征值矩阵 train_X 和分类标识 train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用 prediction=model.predict(test_X) 来对结果进行预测，传入测试集中的样本特征矩阵 test_X，可以得到测试集的预测分类结果 prediction。同样我们也可以创建线性 SVM 分类器，使用 model=svm.LinearSVC()。在 LinearSVC 中没有 kernel 这个参数，限制我们只能使用线性核函数。由于 LinearSVC 对线性分类做了优化，对于数据量大的线性可分问题，使用 LinearSVC 的效率要高于 SVC。如果你不知道数据集是否为线性，可以直接使用 SVC 类创建 SVM 分类器。在训练和预测中，LinearSVC 和 SVC 一样，都是使用 model.fit(train_X,train_y) 和 model.predict(test_X)。SVM 进行乳腺癌检测在了解了如何创建和使用 SVM 分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集，点击这里进行下载 https://github.com/cystanford/breast_cancer_data/blob/master/data.csv。医疗人员采集了患者乳腺肿块经过细针穿刺 (FNA) 后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：数据表一共包括了 32 个字段，代表的含义如下：上面的表格中，mean 代表平均值，se 代表标准差，worst 代表最大值（3 个最大值的平均值）。每张图像都计算了相应的特征，得出了这 30 个特征值（不包括 ID 字段和分类标识结果字段 diagnosis），实际上是 10 个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry 和 fractal_dimension_mean）的 3 个维度，平均、标准差和最大值。这些特征值都保留了 4 位数字。字段中没有缺失的值。在 569 个患者中，一共有 357 个是良性，212 个是恶性。好了，我们的目标是生成一个乳腺癌诊断的 SVM 分类器，并计算这个分类器的准确率。首先设定项目的执行流程：首先我们需要加载数据源；在准备阶段，需要对加载的数据源进行探索，查看样本特征和特征值，这个过程你也可以使用数据可视化，它可以方便我们对数据及数据之间的关系进一步加深了解。然后按照“完全合一”的准则来评估数据的质量，如果数据质量不高就需要做数据清洗。数据清洗之后，你可以做特征选择，方便后续的模型训练；在分类阶段，选择核函数进行训练，如果不知道数据是否为线性，可以考虑使用 SVC(kernel=‘rbf’) ，也就是高斯核函数的 SVM 分类器。然后对训练好的模型用测试集进行评估。按照上面的流程，我们来编写下代码，加载数据并对数据做部分的探索：123456789# 加载数据集，你需要把数据放到目录中data = pd.read_csv(\"./data.csv\")# 数据探索# 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来pd.set_option('display.max_columns', None)print(data.columns)print(data.head(5))print(data.describe())运行结果中，你能看到 32 个字段里，id 是没有实际含义的，可以去掉。diagnosis 字段的取值为 B 或者 M，我们可以用 0 和 1 来替代。另外其余的 30 个字段，其实可以分成三组字段，下划线后面的 mean、se 和 worst 代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。12345678910# 将特征字段分成3组features_mean= list(data.columns[2:12])features_se= list(data.columns[12:22])features_worst=list(data.columns[22:32])# 数据清洗# ID列没有用，删除该列data.drop(\"id\",axis=1,inplace=True)# 将B良性替换为0，M恶性替换为1data['diagnosis']=data['diagnosis'].map(&#123;'M':1,'B':0&#125;)然后我们要做特征字段的筛选，首先需要观察下 features_mean 各变量之间的关系，这里我们可以用 DataFrame 的 corr() 函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。12345678910# 将肿瘤诊断结果可视化sns.countplot(data['diagnosis'],label=\"Count\")plt.show()# 用热力图呈现features_mean字段之间的相关性corr = data[features_mean].corr()plt.figure(figsize=(14,14))# annot=True显示每个方格的数据sns.heatmap(corr, annot=True)plt.show()图表展示：热力图中对角线上的为单变量自身的相关系数是 1。颜色越浅代表相关性越大。所以你能看出来 radius_mean、perimeter_mean 和 area_mean 相关性非常大，compactness_mean、concavity_mean、concave_points_mean 这三个字段也是相关的，因此我们可以取其中的一个作为代表。那么如何进行特征选择呢？特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。我们能看到 mean、se 和 worst 这三组特征是对同一组内容的不同度量方式，我们可以保留 mean 这组特征，在特征选择中忽略掉 se 和 worst。同时我们能看到 mean 这组特征中，radius_mean、perimeter_mean、area_mean 这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean 这三个属性相关性大。我们分别从这 2 类中选择 1 个属性作为代表，比如 radius_mean 和 compactness_mean。这样我们就可以把原来的 10 个属性缩减为 6 个属性，代码如下：123# 特征选择features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']对特征进行选择之后，我们就可以准备训练集和测试集：12345678# 抽取30%的数据作为测试集，其余作为训练集train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test# 抽取特征选择的数值作为训练和测试数据train_X = train[features_remain]train_y=train['diagnosis']test_X= test[features_remain]test_y =test['diagnosis']在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：12345# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()train_X = ss.fit_transform(train_X)test_X = ss.transform(test_X)最后我们可以让 SVM 做训练和预测了：12345678# 创建SVM分类器model = svm.SVC()# 用训练集做训练model.fit(train_X,train_y)# 用测试集做预测prediction=model.predict(test_X)print('准确率: ', metrics.accuracy_score(prediction,test_y))运行结果：12准确率: 0.9181286549707602乳腺癌诊断分类的 SVM 实战，从这个过程中整个执行的流程，包括数据加载、数据探索、数据清洗、特征选择、SVM 训练和结果评估等环节。sklearn 已经为我们提供了很好的工具，对上节课中讲到的 SVM 的创建和训练都进行了封装，让我们无需关心中间的运算细节。但正因为这样，我们更需要对每个流程熟练掌握，通过实战项目训练数据化思维和对数据的敏感度。全部代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pandasimport matplotlib.pyplot as pltfrom sklearn import svm, metricsimport seaborn as snsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerdata = pandas.read_csv(\"data.csv\")pandas.set_option('display.max_columns', None)# 将特征字段分成3组features_mean= list(data.columns[2:12])features_se= list(data.columns[12:22])features_worst=list(data.columns[22:32])# 数据清洗# ID列没有用，删除该列data.drop(\"id\",axis=1,inplace=True)# 将B良性替换为0，M恶性替换为1data['diagnosis']=data['diagnosis'].map(&#123;'M':1,'B':0&#125;)# # 将肿瘤诊断结果可视化# sns.countplot(data['diagnosis'],label=\"Count\")# plt.show()# # 用热力图呈现features_mean字段之间的相关性# corr = data[features_mean].corr()# plt.figure(figsize=(14,14))# # annot=True显示每个方格的数据# sns.heatmap(corr, annot=True)# plt.show()# 特征选择features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean','symmetry_mean', 'fractal_dimension_mean']# 抽取30%的数据作为测试集，其余作为训练集train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test# 抽取特征选择的数值作为训练和测试数据train_X = train[features_remain]train_y=train['diagnosis']test_X= test[features_remain]test_y =test['diagnosis']# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1ss = StandardScaler()train_X = ss.fit_transform(train_X)test_X = ss.transform(test_X)# 创建SVM分类器model = svm.SVC()# 用训练集做训练model.fit(train_X,train_y)# 用测试集做预测prediction=model.predict(test_X)print('准确率: ', metrics.accuracy_score(prediction,test_y))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"}]},{"title":"数据分析-SVM_1","slug":"数据分析-SVM-1","date":"2018-07-24T14:27:13.000Z","updated":"2020-05-01T14:14:27.095Z","comments":true,"path":"2018/07/24/数据分析-SVM-1/","link":"","permalink":"cpeixin.cn/2018/07/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SVM-1/","excerpt":"","text":"SVM 支持向量机SVM 的英文叫 Support Vector Machine，中文名为支持向量机。它是常见的一种分类方法，在机器学习中，SVM 是有监督的学习模型。什么是有监督的学习模型呢？它指的是我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。桌子上我放了红色和蓝色两种球，请你用一根棍子将这两种颜色的球分开。你可以很快想到解决方案，在红色和蓝色球之间画条直线就好了，如下图所示：这次难度升级，桌子上依然放着红色、蓝色两种球，但是它们的摆放不规律，如下图所示。如何用一根棍子把这两种颜色分开呢？你可能想了想，认为一根棍子是分不开的。除非把棍子弯曲，像下面这样：所以这里直线变成了曲线。如果在同一个平面上来看，红蓝两种颜色的球是很难分开的。那么有没有一种方式，可以让它们自然地分开呢？这里你可能会灵机一动，猛拍一下桌子，这些小球瞬间腾空而起，如下图所示。在腾起的那一刹那，出现了一个水平切面，恰好把红、蓝两种颜色的球分开。在这里，二维平面变成了三维空间。原来的曲线变成了一个平面。这个平面，我们就叫做超平面。SVM 的工作原理用SVM 计算的过程就是帮我们找到那个超平面的过程，这个超平面就是我们的 SVM 分类器。我们再过头来看最简单的练习 1，其实我们可以有多种直线的划分，比如下图所示的直线 A、直线 B 和直线 C，究竟哪种才是更好的划分呢？很明显图中的直线 B 更靠近蓝色球，但是在真实环境下，球再多一些的话，蓝色球可能就被划分到了直线 B 的右侧，被认为是红色球。同样直线 A 更靠近红色球，在真实环境下，如果红色球再多一些，也可能会被误认为是蓝色球。所以相比于直线 A 和直线 B，直线 C 的划分更优，因为它的鲁棒性更强。那怎样才能寻找到直线 C 这个更优的答案呢？这里，我们引入一个 SVM 特有的概念：分类间隔。实际上，我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：如图中的决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。点到超平面的距离公式**在上面这个例子中，如果我们把红蓝两种颜色的球放到一个三维空间里，你发现决策面就变成了一个平面。这里我们可以用线性函数来表示，如果在一维空间里就表示一个点，在二维空间里表示一条直线，在三维空间中代表一个平面，当然空间维数还可以更多，这样我们给这个线性函数起个名称叫做“超平面”。超平面的数学表达可以写成：在这个公式里，w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。在这个过程中，支持向量就是离分类超平面最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。所以说， SVM 就是求解最大分类间隔的过程，我们还需要对分类间隔的大小进行定义。首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用 di 代表点 xi 到超平面 wxi+b=0 的欧氏距离。因此我们要求 di 的最小值，用它来代表这个样本到超平面的最短距离。di 可以用公式计算得出：其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导最大间隔的优化模型我们的目标就是找出所有分类间隔中最大的那个值对应的超平面。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开）。通过凸优化问题，最后可以求出最优的 w 和 b，也就是我们想要找的最优超平面。中间求解的过程会用到拉格朗日乘子，和 KKT（Karush-Kuhn-Tucker）条件。数学公式比较多，这里不进行展开。硬间隔、软间隔和非线性 SVM假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。我们知道，实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点。所以线性可分是个理想情况。这时，我们需要使用到软间隔 SVM（近似线性可分），比如下面这种情况：另外还存在一种情况，就是非线性支持向量机。比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状。那么这种情况下，不论是多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念：核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。用 SVM 如何解决多分类问题SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。一对多法假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：（1）样本 A 作为正集，B，C，D 作为负集；（2）样本 B 作为正集，A，C，D 作为负集；（3）样本 C 作为正集，A，B，D 作为负集；（4）样本 D 作为正集，A，B，C 作为负集。这种方法，针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。2. 一对一法一对一法的初衷是想在训练的时候更加灵活。我们可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：（1）分类器 1：A、B；（2）分类器 2：A、C；（3）分类器 3：B、C。当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。总结SVM 分类器，它在文本分类尤其是针对二分类任务性能卓越。同样，针对多分类的情况，我们可以采用一对多，或者一对一的方法，多个二值分类器组合成一个多分类器。另外关于 SVM 分类器的概念，我希望你能掌握以下的三个程度：完全线性可分情况下的线性分类器，也就是线性可分的情况，是最原始的 SVM，它最核心的思想就是找到最大的分类间隔；大部分线性可分情况下的线性分类器，引入了软间隔的概念。软间隔，就是允许一定量的样本分类错误；线性不可分情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到了一个高维的空间中，从而变得线性可分。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"}]},{"title":"数据分析-朴素贝叶斯（下）","slug":"数据分析-朴素贝叶斯（下）","date":"2018-07-21T02:34:38.000Z","updated":"2020-05-01T14:18:06.830Z","comments":true,"path":"2018/07/21/数据分析-朴素贝叶斯（下）/","link":"","permalink":"cpeixin.cn/2018/07/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。今天我带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是 sklearn 这个机器学习神器。sklearn机器学习包sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。TF-IDF 值我在多项式朴素贝叶斯中提到了“词的 TF-IDF 值”，如何理解这个概念呢？TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。词频 TF 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。TF-IDF 如何计算首先我们看下词频 TF 和逆向文档概率 IDF 的公式。为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。TF-IDF=TF*IDF你可以看到，TF-IDF 值就是 TF 与 IDF 的乘积, 这样可以更准确地对文档进行分类。比如“我”这样的高频单词，虽然 TF 词频高，但是 IDF 值很低，整体的 TF-IDF 也不高。我在这里举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。针对“this”，计算 TF-IDF 值：所以 TF-IDF=0.02(-0.0414)=-8.28e-4。针对“bayes”，计算 TF-IDF 值：TF-IDF=0.0050.5229=2.61e-3。很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。如何求 TF-IDF在 sklearn 中我们直接使用 TfidfVectorizer 类，它可以帮我们计算单词 TF-IDF 向量的值。在这个类中，取 sklearn 计算的对数 log 时，底数是 e，不是 10。下面我来讲下如何创建 TfidfVectorizer 类。创建 TfidfVectorizer 的方法是：1TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)我们在创建的时候，有两个构造参数，可以自定义停用词 stop_words 和规律规则 token_pattern。需要注意的是传递的数据结构，停用词 stop_words 是一个列表 List 类型，而过滤规则 token_pattern 是正则表达式。什么是停用词？停用词就是在分类中没有用的词，这些词一般词频 TF 高，但是 IDF 很低，起不到分类的作用。为了节省空间和计算时间，我们把这些词作为停用词 stop words，告诉机器这些词不需要帮我计算。当我们创建好 TF-IDF 向量类型时，可以用 fit_transform 帮我们计算，返回给我们文本矩阵，该矩阵表示了每个单词在每个文档中的 TF-IDF 值。在我们进行 fit_transform 拟合模型后，我们可以得到更多的 TF-IDF 向量属性，比如，我们可以得到词汇的对应关系（字典类型）和向量的 IDF 值，当然也可以获取设置的停用词 stop_words。如何对文档进行分类如果我们要对文档进行分类，有两个重要的阶段：基于分词的数据准备，包括分词、单词权重计算、去掉停用词；应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。整个流程将分成下面几个模块：模块 1：对文档进行分词在准备阶段里，最重要的就是分词。那么如果给文档进行分词呢？英文文档和中文文档所使用的分词工具不同。在英文文档中，最常用的是 NTLK 包。NTLK 包中包含了英文的停用词 stop words、分词和标注方法。1234import nltkword_list = nltk.word_tokenize(text) #分词nltk.pos_tag(word_list) #标注单词的词性在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。123import jiebaword_list = jieba.cut (text) #中文分词模块 2：加载停用词表我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。1stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]模块 3：计算单词的权重这里我们用到 sklearn 里的 TfidfVectorizer 类，上面我们介绍过它使用的方法。直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。123tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)features = tf.fit_transform(train_contents)这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。一般很少设置 min_df，因为 min_df 通常都会很小。模块 4：生成朴素贝叶斯分类器我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。当 0&lt; alpha &lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。1234# 多项式贝叶斯分类器from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)模块 5：使用生成的分类器做预测首先我们需要得到测试集的特征矩阵。方法是用训练集的分词创建一个 TfidfVectorizer 类，使用同样的 stop_words 和 max_df，然后用这个 TfidfVectorizer 类对测试集的内容进行 fit_transform 拟合，得到测试集的特征矩阵 test_features。123test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)test_features=test_tf.fit_transform(test_contents)然后我们用训练好的分类器对新数据做预测。方法是使用 predict 函数，传入测试集的特征矩阵 test_features，得到分类结果 predicted_labels。predict 函数做的工作就是求解所有后验概率并找出最大的那个。12predicted_labels=clf.predict(test_features)模块 6：计算准确率计算准确率实际上是对分类模型的评估。我们可以调用 sklearn 中的 metrics 包，在 metrics 中提供了 accuracy_score 函数，方便我们对实际结果和预测的结果做对比，给出模型的准确率。使用方法如下：123from sklearn import metricsprint metrics.accuracy_score(test_labels, predicted_labels)数据挖掘神器 sklearn从数据挖掘的流程来看，一般包括了获取数据、数据清洗、模型训练、模型评估和模型部署这几个过程。sklearn 中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。在文档分类的项目中，我们针对文档的特点，给出了基于分词的准备流程。一般来说 NTLK 包适用于英文文档，而 jieba 适用于中文文档。我们可以根据文档选择不同的包，对文档提取分词。这些分词就是贝叶斯分类中最重要的特征属性。基于这些分词，我们得到分词的权重，即特征矩阵。通过特征矩阵与分类结果，我们就可以创建出朴素贝叶斯分类器，然后用分类器进行预测，最后预测结果与实际结果做对比即可以得到分类器在测试集上的准确率。实战文档共有 4 种类型：女性、体育、文学、校园根据下面给定的训练数据和测试数据进行 朴素贝叶斯文本分类实战。数据地址：https://github.com/cystanford/text_classification/tree/master/text classification12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding: utf-8# 中文文本分类import osimport jiebaimport warningsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn import metricswarnings.filterwarnings('ignore')def cut_words(file_path): \"\"\" 对文本进行切词 :param file_path: txt文本路径 :return: 用空格分词的字符串 \"\"\" text_with_spaces = '' with open(file_path, 'r', encoding='gb18030', errors='ignore') as f: text = f.read() textcut = jieba.cut(text) for word in textcut: text_with_spaces += word + ' ' return text_with_spacestext_category = ['女性', '体育', '文学', '校园']def loadfile(data_path): \"\"\" 将路径下的所有文件加载 :param file_dir: 保存txt文件目录 :param label: 文档标签 :return: 分词后的文档列表和标签 \"\"\" words_list = [] labels = [] for category in text_category: file_path = data_path + category + '/' file_list = os.listdir(file_path) for file in file_list: words_list.append(cut_words(file_path+file)) labels.append(category) return words_list, labelsdef get_stop_words(stop_words_path): stop_words = open(stop_words_path, 'r', encoding='utf-8').read() stop_words = stop_words.encode('utf-8').decode('utf-8-sig') # 列表头部\\ufeff处理 stop_words = stop_words.split('\\n') # 根据分隔符分隔 return stop_wordsif __name__ == '__main__': train_data_path = 'train' + '/' test_data_path = 'test' + '/' stop_words_path = 'stop/stopword.txt' train_words_list, train_labels = loadfile(train_data_path) test_words_list, test_labels = loadfile(test_data_path) stop_words = get_stop_words(stop_words_path) tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5) train_features = tf.fit_transform(train_words_list) # 上面fit过了，这里transform test_features = tf.transform(test_words_list) # 多项式贝叶斯分类器 from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels) predicted_labels = clf.predict(test_features) # 计算准确率 print('准确率为：', metrics.accuracy_score(test_labels, predicted_labels))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"}]},{"title":"数据分析-朴素贝叶斯（上）","slug":"数据分析-朴素贝叶斯（上）","date":"2018-07-20T15:19:16.000Z","updated":"2020-05-01T14:18:19.882Z","comments":true,"path":"2018/07/20/数据分析-朴素贝叶斯（上）/","link":"","permalink":"cpeixin.cn/2018/07/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"贝叶斯原理贝叶斯原理是怎么来的呢？贝叶斯为了解决一个叫“逆向概率”问题写了一篇文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。什么是“逆向概率”呢？所谓“逆向概率”是相对“正向概率”而言。正向概率的问题很容易理解，比如我们已经知道袋子里面有 N 个球，不是黑球就是白球，其中 M 个是黑球，那么把手伸进去摸一个球，就能知道摸出黑球的概率是多少。但这种情况往往是上帝视角，即了解了事情的全貌再做判断。在现实生活中，我们很难知道事情的全貌。贝叶斯则从实际场景出发，提了一个问题：如果我们事先不知道袋子里面黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里面黑白球的比例么？正是这样的一个问题，影响了接下来近 200 年的统计学理论。这是因为，贝叶斯原理与其他统计学推断方法截然不同，它是建立在主观判断的基础上：在我们不了解所有客观事实的情况下，同样可以先估计一个值，然后根据实际结果不断进行修正。我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那是不是实际上患“贝叶死”的概率也是 99.9% 呢？实际上不是的。你自己想想，在 10000 个人中，还存在 0.1% 的误查的情况，也就是 10 个人没有患病但是被诊断成阳性。当然 10000 个人中，也确实存在一个患有贝叶死的人，他有 99.9% 的概率被检查出来。所以你可以粗算下，患病的这个人实际上是这 11 个人里面的一员，即实际患病比例是 1/11≈9%。上面这个例子中，实际上涉及到了贝叶斯原理中的几个概念：先验概率：通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。后验概率：后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。条件概率：事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。似然函数（likelihood function）：你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。介绍完贝叶斯原理中的这几个概念，我们再来看下贝叶斯原理，实际上贝叶斯原理就是求解后验概率，我们假设：A 表示事件 “测出为阳性”, 用 B1 表示“患有贝叶死”, B2 表示“没有患贝叶死”。根据上面那道题，我们可以得到下面的信息。患有贝叶死的情况下，测出为阳性的概率为 P(A|B1)=99.9%，没有患贝叶死，但测出为阳性的概率为 P(A|B2)=0.1%。另外患有贝叶死的概率为 P(B1)=0.01%，没有患贝叶死的概率 P(B2)=99.99%。那么我们检测出来为阳性，而且是贝叶死的概率 P(B1，A）=P(B1)_P(A|B1)=0.01%_99.9%=0.00999%。这里 P(B1,A) 代表的是联合概率，同样我们可以求得 P(B2,A)=P(B2)_P(A|B2)=99.99%_0.1%=0.09999%。然后我们想求得是检查为阳性的情况下，患有贝叶死的概率，也即是 P(B1|A)。所以检查出阳性，且患有贝叶死的概率为：检查出是阳性，但没有患有贝叶死的概率为：这里我们能看出来 0.01%+0.1% 均出现在了 P(B1|A) 和 P(B2|A) 的计算中作为分母。我们把它称之为论据因子，也相当于一个权值因子。其中 P(B1）、P(B2) 就是先验概率，我们现在知道了观测值，就是被检测出来是阳性，来求患贝叶死的概率，也就是求后验概率。求后验概率就是贝叶斯原理要求的，基于刚才求得的 P(B1|A)，P(B2|A)，我们可以总结出贝叶斯公式为：由此，我们可以得出通用的贝叶斯公式：朴素贝叶斯讲完贝叶斯原理之后，我们再来看下今天重点要讲的算法，朴素贝叶斯。它是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。朴素贝叶斯模型由两种类型的概率组成：每个类别的概率P(Cj)；每个属性的条件概率P(Ai|Cj)。我来举个例子说明下什么是类别概率和条件概率。假设我有 7 个棋子，其中 3 个是白色的，4 个是黑色的。那么棋子是白色的概率就是 3/7，黑色的概率就是 4/7，这个就是类别概率。假设我把这 7 个棋子放到了两个盒子里，其中盒子 A 里面有 2 个白棋，2 个黑棋；盒子 B 里面有 1 个白棋，2 个黑棋。那么在盒子 A 中抓到白棋的概率就是 1/2，抓到黑棋的概率也是 1/2，这个就是条件概率，也就是在某个条件（比如在盒子 A 中）下的概率。在朴素贝叶斯中，我们要统计的是属性的条件概率，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。另外我想告诉你的是，贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。朴素贝叶斯分类工作原理朴素贝叶斯分类是常用的贝叶斯分类方法。我们日常生活中看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。离散数据案例我们遇到的数据可以分为两种，一种是离散数据，另一种是连续数据。那什么是离散数据呢？离散就是不连续的意思，有明确的边界，比如整数 1，2，3 就是离散数据，而 1 到 3 之间的任何数，就是连续数据，它可以取在这个区间里的任何数值。我以下面的数据为例，这些是根据你之前的经验所获得的数据。然后给你一个新的数据：身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？针对这个问题，我们先确定一共有 3 个属性，假设我们用 A 代表属性，用 A1, A2, A3 分别为身高 = 高、体重 = 中、鞋码 = 中。一共有两个类别，假设用 C 代表类别，那么 C1,C2 分别是：男、女，在未知的情况下我们用 Cj 表示。那么我们想求在 A1、A2、A3 属性下，Cj 的概率，用条件概率表示就是 P(Cj|A1A2A3)。根据上面讲的贝叶斯的公式，我们可以得出：因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。在这个公式里，因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。我们假定 Ai 之间是相互独立的，那么：P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出 P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。我分别求下这些条件下的概率：P(A1|C1)=1/2, P(A2|C1)=1/2, P(A3|C1)=1/4，P(A1|C2)=0, P(A2|C2)=1/2, P(A3|C2)=1/2，所以 P(A1A2A3|C1)=1/16, P(A1A2A3|C2)=0。因为 P(A1A2A3|C1)P(C1)&gt;P(A1A2A3|C2)P(C2)，所以应该是 C1 类别，即男性。连续数据案例我们做了一个离散的数据案例，实际生活中我们得到的是连续的数值，比如下面这组数据：那么如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？公式还是上面的公式，这里的困难在于，由于身高、体重、鞋码都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办呢？这时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。比如，男性的身高是均值 179.5、标准差为 3.697 的正态分布。所以男性的身高为 180 的概率为 0.1069。怎么计算得出的呢? 你可以使用 EXCEL 的 NORMDIST(x,mean,standard_dev,cumulative) 函数，一共有 4 个参数：x：正态分布中，需要计算的数值；Mean：正态分布的平均值；Standard_dev：正态分布的标准差；Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE 时，函数结果为累积分布；为 False 时，函数结果为概率密度。这里我们使用的是 NORMDIST(180,179.5,3.697,0)=0.1069。同理我们可以计算得出男性体重为 120 的概率为 0.000382324，男性鞋码为 41 号的概率为 0.120304111。所以我们可以计算得出：P(A1A2A3|C1)=P(A1|C1)P(A2|C1)P(A3|C1)=0.10690.0003823240.120304111=4.9169e-6同理我们也可以计算出来该人为女的可能性：P(A1A2A3|C2)=P(A1|C2)P(A2|C2)P(A3|C2)=0.000001474890.0153541440.120306074=2.7244e-9很明显这组数据分类为男的概率大于分类为女的概率。当然在 Python 中，有第三方库可以直接帮我们进行上面的操作，这个我们会在下节课中介绍。这里主要是给你讲解下具体的运算原理。朴素贝叶斯分类器工作流程朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。流程可以用下图表示：从图片你也可以看出来，朴素贝叶斯分类器需要三个流程，我来给你一一讲解下这几个流程。第一阶段：准备阶段在这个阶段我们需要确定特征属性，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。第二阶段：训练阶段这个阶段就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。输入是特征属性和训练样本，输出是分类器。第三阶段：应用阶段这个阶段是使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"}]},{"title":"数据分析 - EM聚类","slug":"数据分析-EM聚类","date":"2018-07-15T14:18:39.000Z","updated":"2020-05-01T14:15:10.136Z","comments":true,"path":"2018/07/15/数据分析-EM聚类/","link":"","permalink":"cpeixin.cn/2018/07/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-EM%E8%81%9A%E7%B1%BB/","excerpt":"","text":"今天要来学习 EM 聚类。EM 的英文是 Expectation Maximization，所以 EM 算法也叫最大期望算法。我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子 A 中，然后再把剩余的分到碟子 B 中，再来观察碟子 A 和 B 里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子 A 和 B 里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是 EM 算法的过程。EM 算法的工作原理说到 EM 算法，我们先来看一个概念“最大似然”，英文是 Maximum Likelihood，Likelihood 代表可能性，所以最大似然也就是最大可能性的意思。什么是最大似然呢？举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是最大似然的概念。最大似然估计是什么呢？它指的就是一件事情已经发生了，然后反推更有可能是什么因素造成的。还是用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性。最大似然估计是一种通过已知结果，估计参数的方法。那么 EM 算法是什么？它和最大似然估计又有什么关系呢？EM 算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。再回过来看下开头我给你举的分菜的这个例子，实际上最终我们想要的是碟子 A 和碟子 B 中菜的份量，你可以把它们理解为想要求得的模型参数。然后我们通过 EM 算法中的 E 步来进行观察，然后通过 M 步来进行调整 A 和 B 的参数，最后让碟子 A 和碟子 B 的参数不再发生变化为止。实际我们遇到的问题，比分菜复杂。我再给你举个一个投掷硬币的例子，假设我们有 A 和 B 两枚硬币，我们做了 5 组实验，每组实验投掷 10 次，然后统计出现正面的次数，实验结果如下：投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是 A 还是 B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：我们现在想要求得硬币 A 和 B 出现正面次数的概率，可以直接求得：而实际情况是我不知道每次投掷的硬币是 A 还是 B，那么如何求得硬币 A 和硬币 B 出现正面的概率呢？这里就需要采用 EM 算法的思想。1. 初始化参数。我们假设硬币 A 和 B 的正面概率（随机指定）是θA=0.5 和θB=0.9。2. 计算期望值。假设实验 1 投掷的是硬币 A，那么正面次数为 5 的概率为：所以实验 1 更有可能投掷的是硬币 A。然后我们对实验 2~5 重复上面的计算过程，可以推理出来硬币顺序应该是{A，A，B，B，A}。这个过程实际上是通过假设的参数来估计未知参数，即“每次投掷是哪枚硬币”。3. 通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。然后一直重复第二步和第三步，直到参数不再发生变化。简单总结下上面的步骤，你能看出 EM 算法中的 E 步骤就是通过旧的参数来计算隐藏变量。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。EM 聚类的工作原理上面你能看到 EM 算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是 EM 聚类的原理。相比于 K-Means 算法，EM 聚类更加灵活，比如下面这两种情况，K-Means 会得到下面的聚类结果。因为 K-Means 是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。你可以把 EM 算法理解成为是一个框架，在这个框架中可以采用不同的模型来用 EM 进行求解。常用的 EM 聚类有 GMM 高斯混合模型和 HMM 隐马尔科夫模型。GMM（高斯混合模型）聚类就是 EM 聚类的一种。比如上面这两个图，可以采用 GMM 来进行聚类。和 K-Means 一样，我们事先知道聚类的个数，但是不知道每个样本分别属于哪一类。通常，我们可以假设样本是符合高斯分布的（也就是正态分布）。每个高斯分布都属于这个模型的组成部分（component），要分成 K 类就相当于是 K 个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是 E 步骤。再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即 M 步骤。反复 EM 步骤，直到每个组成部分的高斯分布参数不变为止。这样也就相当于将样本按照 GMM 模型进行了 EM 聚类。总结EM 算法相当于一个框架，你可以采用不同的模型来进行聚类，比如 GMM（高斯混合模型），或者 HMM（隐马尔科夫模型）来进行聚类。GMM 是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而 HMM 用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM 在自然语言处理和语音识别领域中有广泛的应用。在 EM 这个框架中，E 步骤相当于是通过初始化的参数来估计隐含变量。M 步骤就是通过隐含变量反推来优化参数。最后通过 EM 步骤的迭代得到模型参数。在这个过程里用到的一些数学公式这节课不进行展开。你需要重点理解 EM 算法的原理。通过上面举的炒菜的例子，你可以知道 EM 算法是一个不断观察和调整的过程。通过求硬币正面概率的例子，你可以理解如何通过初始化参数来求隐含数据的过程，以及再通过求得的隐含数据来优化参数。通过上面 GMM 图像聚类的例子，你可以知道很多 K-Means 解决不了的问题，EM 聚类是可以解决的。在 EM 框架中，我们将潜在类别当做隐藏变量，样本看做观察值，把聚类问题转化为参数估计问题，最终把样本进行聚类。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"}]},{"title":"数据分析-决策树实战-泰坦尼克号乘客生存预测","slug":"数据分析-决策树实战-泰坦尼克号乘客生存预测","date":"2018-07-15T14:12:49.000Z","updated":"2020-05-01T14:18:39.272Z","comments":true,"path":"2018/07/15/数据分析-决策树实战-泰坦尼克号乘客生存预测/","link":"","permalink":"cpeixin.cn/2018/07/15/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/","excerpt":"","text":"在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。今天我来带你用决策树进行项目的实战。决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。在了解决策树的原理后，今天我们用 sklearn 工具解决一个实际的问题：泰坦尼克号乘客的生存预测。sklearn 中的决策树模型首先，我们需要掌握 sklearn 中自带的决策树分类器 DecisionTreeClassifier，方法如下：1clf = DecisionTreeClassifier(criterion='entropy')到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 ginientropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。我们通过设置 criterion=’entropy’可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在 sklearn 中是个什么东西？123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。我整理了这些参数代表的含义：![1585397401268-c9ea5ebf-53aa-44e0-8588-13582a240b5d.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586919851176-2a57ac36-49da-475d-adc2-dc6f3f2434b3.png#align=left&display=inline&height=930&margin=%5Bobject%20Object%5D&name=1585397401268-c9ea5ebf-53aa-44e0-8588-13582a240b5d.png&originHeight=930&originWidth=620&size=366558&status=done&style=none&width=620)在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。下面这个表格是 fit 方法、predict 方法和 score 方法的作用:![1585397402135-ed1fff6c-1bee-49f9-bf07-9fb87d83e0f3.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586919851152-b2b5be44-9293-4e4d-998f-d1acd73a33da.png#align=left&display=inline&height=158&margin=%5Bobject%20Object%5D&name=1585397402135-ed1fff6c-1bee-49f9-bf07-9fb87d83e0f3.png&originHeight=158&originWidth=468&size=27331&status=done&style=none&width=468) ### Titanic 乘客生存预测 #### 问题描述 泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据，具体数据你可以从 GitHub 上下载：[点我](https://github.com/cystanford/Titanic_Data)其中数据集格式为 csv，一共有两个文件：train.csv 是训练数据集，包含特征信息和存活与否的标签；test.csv: 测试数据集，只包含特征信息。现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。在训练集中，包括了以下字段，它们具体为：![1585397401155-e052cf89-fe96-49b3-b3fe-37539d7a67ae.png](https://cdn.nlark.com/yuque/0/2020/png/1072113/1586919895980-6bd3d51e-cf8d-4a92-a444-f540fcba423a.png#align=left&display=inline&height=370&margin=%5Bobject%20Object%5D&name=1585397401155-e052cf89-fe96-49b3-b3fe-37539d7a67ae.png&originHeight=370&originWidth=466&size=41457&status=done&style=none&width=466) #### 生存预测的关键流程 我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：![](https://cdn.nlark.com/yuque/0/2020/png/1072113/1585397402088-8ce15112-7300-4b68-b34c-0bc6a0c960c0.png#align=left&display=inline&height=1470&margin=%5Bobject%20Object%5D&originHeight=1470&originWidth=3202&size=0&status=done&style=none&width=3202)**准备阶段**：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；**分类阶段**：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。下面，我分别对这些模块进行介绍。**模块 1**：数据探索数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。那么如何进行数据探索呢？这里有一些函数你需要了解：使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；使用 head 查看前几行数据（默认是前 5 行）；使用 tail 查看后几行数据（默认是最后 5 行）。我们可以使用 Pandas 便捷地处理这些问题：1234567891011121314151617181920import pandas as pdtrain_data = pd.read_csv('train.csv')print(train_data.info())print(\"===========================================\")print(train_data.describe())print(\"===========================================\")print(train_data.describe(include=['O']))print(\"===========================================\")print(train_data.head())print(\"===========================================\")print(train_data.tail())运行结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KBNone=========================================== PassengerId Survived Pclass ... SibSp Parch Farecount 891.000000 891.000000 891.000000 ... 891.000000 891.000000 891.000000mean 446.000000 0.383838 2.308642 ... 0.523008 0.381594 32.204208std 257.353842 0.486592 0.836071 ... 1.102743 0.806057 49.693429min 1.000000 0.000000 1.000000 ... 0.000000 0.000000 0.00000025% 223.500000 0.000000 2.000000 ... 0.000000 0.000000 7.91040050% 446.000000 0.000000 3.000000 ... 0.000000 0.000000 14.45420075% 668.500000 1.000000 3.000000 ... 1.000000 0.000000 31.000000max 891.000000 1.000000 3.000000 ... 8.000000 6.000000 512.329200[8 rows x 7 columns]=========================================== Name Sex Ticket Cabin Embarkedcount 891 891 891 204 889unique 891 2 681 147 3top Wick, Miss. Mary Natalie male 347082 B96 B98 Sfreq 1 577 7 4 644=========================================== PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]=========================================== PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]模块 2：数据清洗通过数据探索，我们发现 Age 和 Cabin 这三个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐；具体实现的代码如下：12train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。首先观察下 Embarked 字段的取值，方法如下：首先观察下 Embarked 字段的取值，方法如下：1print(train_data['Embarked'].value_counts())结果如下：123S 644C 168Q 77我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：1train_data['Embarked'].fillna('S', inplace=True)模块 3：特征选择特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？通过数据探索我们发现，PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；Cabin 字段缺失值太多，可以放弃； Ticket 字段为船票号码，杂乱无章且无规律，可以放弃。其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。12345# 特征选择features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']train_features = train_data[features]train_labels = train_data['Survived']test_features = test_data[features]特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。那该如何操作呢?我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下：123from sklearn.feature_extraction import DictVectorizerdvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record'))你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：12print(dvec.feature_names_)['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']你可以看到原本是一列的 Embarked，变成了“Embarked=C”“Embarked=Q”“Embarked=S”三列。Sex 列变成了“Sex=female”“Sex=male”两列。这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。模块 4：决策树模型刚才我们已经讲了如何使用 sklearn 中的决策树模型。现在我们使用 ID3 算法，即在创建 DecisionTreeClassifier 时，设置 criterion=‘entropy’，然后使用 fit 进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。12345from sklearn.tree import DecisionTreeClassifier# 构造ID3决策树clf = DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels)模块 5：模型预测 &amp; 评估在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labels：123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels = clf.predict(test_features)在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的 score 函数计算下得到的结果：123# 得到决策树准确率acc_decision_tree = round(clf.score(train_features, train_labels), 6)print(u'score准确率为 %.4lf' % acc_decision_tree)运行结果：1score准确率为 0.9820你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。这是为什么呢？因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比(test.csv数据中没有Survived)。如果我们使用 score 函数对训练集的准确率进行统计，正确率会接近于 100%（如上结果为 98.2%），无法对分类器的在实际环境下做准确率的评估。那么有什么办法，来统计决策树分类器的准确率呢？这里可以使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。K 折交叉验证的原理是这样的：将数据集平均分割成 K 个等份；使用 1 份数据作为测试数据，其余作为训练数据；计算测试准确率；使用不同的测试集，重复 2、3 步骤。在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score 和 cross_val_score 两种函数的正确率的评估结果：1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证 统计决策树准确率print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))1cross_val_score准确率为 0.7835你可以看到，score 函数的准确率为 0.9820，cross_val_score 准确率为 0.7835。这里很明显，对于不知道测试集实际结果的，要使用 K 折交叉验证才能知道模型的准确率。模块 6：决策树可视化sklearn 的决策树模型对我们来说，还是比较抽象的。我们可以使用 Graphviz 可视化工具帮我们把决策树呈现出来。安装 Graphviz 库需要下面的几步：安装 graphviz 工具，这里是它的下载地址；http://www.graphviz.org/download/将 Graphviz 添加到环境变量 PATH 中；需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。这样你就可以在程序里面使用 Graphviz 对决策树模型进行呈现，最后得到一个决策树可视化的 PDF 文件，可视化结果文件 Source.gv.pdf 你可以在 GitHub 上下载：https://github.com/cystanford/Titanic_Data决策树模型使用技巧总结今天我用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。我上面讲了泰坦尼克乘客生存预测的六个关键模块，请你用 sklearn 中的决策树模型独立完成这个项目，对测试集中的乘客是否生存进行预测，并给出模型准确率评估。数据从 GitHub 上下载即可。最后给你留一个思考题吧，我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，那么这两个函数有什么区别?项目完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import pandas as pdfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.feature_extraction import DictVectorizertrain_data = pd.read_csv('train.csv')test_data = pd.read_csv('test.csv')train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\"\"\"将空值用此列最多值来补齐index 获取索引值。 [num] 下标获取返回值 ，ascending=True 降序排列\"\"\"train_data['Embarked'].fillna(train_data['Embarked'].value_counts().index[0], inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().index[0], inplace=True)features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']train_features = train_data[features]train_labels = train_data['Survived']test_features = test_data[features]dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record'))# 构造ID3决策树clf = DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels)test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测# pred_labels = clf.predict(test_features)# # 得到决策树准确率# acc_decision_tree = round(clf.score(train_features, train_labels), 6)# print(u'score准确率为 %.4lf' % acc_decision_tree)import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证 统计决策树准确率print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树项目-用户流失预警","slug":"数据分析-决策树项目-用户流失预警","date":"2018-07-14T11:19:49.000Z","updated":"2020-05-01T14:18:27.501Z","comments":true,"path":"2018/07/14/数据分析-决策树项目-用户流失预警/","link":"","permalink":"cpeixin.cn/2018/07/14/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E9%A1%B9%E7%9B%AE-%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E8%AD%A6/","excerpt":"","text":"https://www.zhihu.com/question/340500005https://zhuanlan.zhihu.com/p/26332219https://zhuanlan.zhihu.com/p/45435103音乐网站用户流失 ** https://zhuanlan.zhihu.com/p/29598241","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-CART","slug":"数据分析-决策树分类算法-CART","date":"2018-07-13T15:12:49.000Z","updated":"2020-05-01T14:18:55.201Z","comments":true,"path":"2018/07/13/数据分析-决策树分类算法-CART/","link":"","permalink":"cpeixin.cn/2018/07/13/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-CART/","excerpt":"","text":"CART算法上节课我们讲了决策树，基于信息度量的不同方式，我们可以把决策树分为 ID3 算法、C4.5 算法和 CART 算法。今天我来带你学习 CART 算法。CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。那么你首先需要了解的是，什么是分类树，什么是回归树呢？我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。不管是分类，还是回归，其本质是一样的，都是对输入做出预测，并且都是监督学习。说白了，就是根据特征，分析输入的内容，判断它的类别，或者预测其值。分类问题输出的是物体所属的类别，回归问题输出的是物体的值分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。CART 分类树的工作流程通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。在属性选择上，我们是通过统计“不纯度”来做判断的，ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。我们接下来详解了解一下基尼系数。基尼系数不好懂，你最好跟着例子一起手动计算下。假设 t 为节点，那么该节点的 GINI 系数的计算公式为：这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：集合 1：6 个都去打篮球；集合 2：3 个去打篮球，3 个不去打篮球。针对集合 1，所有人都去打篮球，所以 p(Ck|t)=1，因此 GINI(t)=1-1=0。针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5_0.5+0.5_0.5）=0.5。通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到：所以在属性 A 的划分下，节点 D 的基尼系数为：节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。如何使用 CART 算法来创建分类树通过上面的讲解你可以知道，CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。下面，我们来用 CART 分类树，给 iris 数据集构造一棵分类决策树。iris 这个数据集，我在 Python 可视化中讲到过，实际上在 sklearn 中也自带了这个数据集。基于 iris 数据集，构造 CART 分类树的代码如下：1234567891011121314151617181920212223# encoding=utf-8from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_iris# 准备数据集iris=load_iris()# 获取特征集和分类标识features = iris.datalabels = iris.target# 随机抽取33%的数据作为测试集，其余为训练集train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)# 创建CART分类树clf = DecisionTreeClassifier(criterion='gini')# 拟合构造CART分类树clf = clf.fit(train_features, train_labels)# 用CART分类树做预测test_predict = clf.predict(test_features)# 预测结果与测试集结果作比对score = accuracy_score(test_labels, test_predict)print(\"CART分类树准确率 %.4lf\" % score)CART分类树准确率 0.9600如果我们把决策树画出来，可以得到下面的图示：首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。使用 clf = DecisionTreeClassifier(criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。使用 clf.fit(train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。使用 clf.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。最后使用 accuracy_score(test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。我们能看到 sklearn 帮我们做了 CART 分类树的使用封装，使用起来还是很方便的。CART 回归树的工作流程CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。其中差值的绝对值为样本值减去样本均值的绝对值：方差为每个样本值减去样本均值的平方和除以样本个数：所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。我们可以通过一个例子来看下如何创建一棵 CART 回归树来做预测。如何使用 CART 回归树做预测这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。根据这些指标，我们使用 CART 回归树对波士顿房价进行预测，代码如下：123456789101112131415161718192021222324# encoding=utf-8from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor# 准备数据集boston=load_boston()# 探索数据print(boston.feature_names)# 获取特征集和房价features = boston.dataprices = boston.target# 随机抽取33%的数据作为测试集，其余为训练集train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)# 创建CART回归树dtr=DecisionTreeRegressor()# 拟合构造CART回归树dtr.fit(train_features, train_price)# 预测测试集中的房价predict_price = dtr.predict(test_features)# 测试集的结果评价print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price))运行结果（每次运行结果可能会有不同）：[‘CRIM’ ‘ZN’ ‘INDUS’ ‘CHAS’ ‘NOX’ ‘RM’ ‘AGE’ ‘DIS’ ‘RAD’ ‘TAX’ ‘PTRATIO’ ‘B’ ‘LSTAT’]回归树二乘偏差均值: 23.80784431137724回归树绝对值偏差均值: 3.040119760479042如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）：我们来看下这个例子，首先加载了波士顿房价数据集，得到特征集和房价。然后通过 train_test_split 帮助我们把数据集抽取一部分作为测试集，其余作为训练集。使用 dtr=DecisionTreeRegressor() 初始化一棵 CART 回归树。使用 dtr.fit(train_features, train_price) 函数，将训练集的特征值和结果作为参数进行拟合，得到 CART 回归树。使用 dtr.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到预测结果 predict_price。最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。我们能看到 CART 回归树的使用和分类树类似，只是最后求得的预测值是个连续值。CART 决策树的剪枝CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。总结今天我给你讲了 CART 决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。最后我们来整理下三种决策树之间在属性选择标准上的差异：ID3 算法，基于信息增益做判断；C4.5 算法，基于信息增益率做判断；CART 算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。实际上这三个指标也是计算“不纯度”的三种计算方式。在工具使用上，我们可以使用 sklearn 中的 DecisionTreeClassifier 创建 CART 分类树，通过 DecisionTreeRegressor 创建 CART 回归树。你可以用代码自己跑一遍我在文稿中举到的例子。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-C4.5","slug":"数据分析-决策树分类算法-C4.5","date":"2018-07-12T06:12:39.000Z","updated":"2020-05-01T14:19:02.521Z","comments":true,"path":"2018/07/12/数据分析-决策树分类算法-C4.5/","link":"","permalink":"cpeixin.cn/2018/07/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-C4.5/","excerpt":"","text":"在 ID3 算法上进行改进的 C4.5 算法那么 C4.5 都在哪些方面改进了 ID3 呢？采用信息增益率因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。采用悲观剪枝ID3构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。离散化处理连续属性C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。处理缺失值针对数据集不完整的情况，C4.5 也可以进行处理。假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。温度 = 高：D1={2-,3+,4+} ；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。这里 + 号代表打篮球，- 号代表不打篮球。比如 ID=2 时，决策是不打篮球，我们可以记录为 2-。针对将属性选择为温度的信息增益为：Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208属性熵 =1.459, 信息增益率 Gain_ratio(D′, 温度)=0.208/1.459=0.1426。下图是计算过程：D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重比例为 6/7，所以 Gain(D′，温度) 所占权重比例为 6/7，所以：Gain_ratio(D, 温度)=6/7*0.1426=0.122。这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。Cart 算法在这里不做介绍，我会在下一讲给你讲解这个算法。现在我们总结下 ID3 和 C4.5 算法。首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。总结前面我们讲了两种决策树分类算法 ID3 和 C4.5，了解了它们的数学原理。你可能会问，公式这么多，在实际使用中该怎么办呢？实际上，我们可以使用一些数据挖掘工具使用它们，比如 Python 的 sklearn，或者是 Weka（一个免费的数据挖掘工作平台），它们已经集成了这两种算法。只是我们在了解了这两种算法之后，才能更加清楚这两种算法的优缺点。我们总结下，这次都讲到了哪些知识点呢？首先我们采用决策树分类，需要了解它的原理，包括它的构造原理、剪枝原理。另外在信息度量上，我们需要了解信息度量中的纯度和信息熵的概念。在决策树的构造中，一个决策树包括根节点、子节点、叶子节点。在属性选择的标准上，度量方法包括了信息增益和信息增益率。在算法上，我讲解了两种算法：ID3 和 C4.5，其中 ID3 是基础的决策树算法，C4.5 在它的基础上进行了改进，也是目前决策树中应用广泛的算法。然后在了解这些概念和原理后，强烈推荐你使用工具，具体工具的使用我会在后面进行介绍。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-决策树分类算法-ID3","slug":"数据分析-决策树分类算法-ID3","date":"2018-07-11T11:12:49.000Z","updated":"2020-05-01T14:18:48.874Z","comments":true,"path":"2018/07/11/数据分析-决策树分类算法-ID3/","link":"","permalink":"cpeixin.cn/2018/07/11/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-ID3/","excerpt":"","text":"在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，你会发现它实际上是一个树状图，这就是我们今天要讲的决策树决策树的工作原理决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去打篮球？还是不去？上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：构造和剪枝。构造什么是构造呢？构造就是生成一棵完整的决策树。简单来说，构造的过程就是选择什么属性作为节点的过程，那么在构造过程中，会存在三种节点：根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；叶节点：就是树最底部的节点，也就是决策结果。节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：选择哪个属性作为根节点；选择哪些属性作为子节点；什么时候停止并得到目标状态，即叶节点。剪枝决策树构造出来之后是不是就万事大吉了呢？也不尽然，我们可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止“过拟合”（Overfitting）现象的发生。“过拟合”这个概念你一定要理解，它指的就是模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果“太好“，反而在实际应用过程中会导致分类错误。造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。fitting:拟合，就是说这个曲线能不能很好的描述这个样本，有比较好的泛化能力过拟合（OverFititing）：太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平。欠拟合(UnderFitting)：样本不够或者算法不精确，测试样本特性没有学到，不具泛化性，拿到新样本后没有办法去准确的判断泛化能力指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。既然要对决策树进行剪枝，具体有哪些方法呢？一般来说，剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）**。预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。如何判断要不要去打篮球？我给你准备了打篮球的数据集，训练数据如下：我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：纯度和信息熵。先来说一下纯度。你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。我在这里举个例子，假设有 3 个集合：集合 1：6 次都去打篮球；集合 2：4 次去打篮球，2 次不去打篮球；集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1&gt; 集合 2&gt; 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。然后我们再来介绍信息熵（entropy）的概念，它表示了信息的不确定度**在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：大写Σ用于数学上的总和符号，比如：∑Pi，其中i=1,2,…,T，即为求P1 + P2 + … + PT的和。小写σ用于统计学上的标准差。这种写法表示的就是∑j=1+2+3+…+n。下图：其中i表示下界，n表示上界， k从i开始取数，一直取到n,全部加起来。p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。我举个简单的例子，假设有 2 个集合集合 1：5 次去打篮球，1 次不去打篮球；集合 2：3 次去打篮球，3 次不去打篮球。在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入上述信息熵公式可以计算得出：同样，集合 2 中，也是一共 6 次决策，其中类别 1 中“打篮球”的次数是 3，类别 2“不打篮球”的次数也是 3，那么信息熵为多少呢？我们可以计算得出：从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。ID3算法我们先看下 ID3 算法。ID3 算法计算的是信息增益，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。假设天气 = 晴的时候，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。那么 a 代表节点的属性，即天气 = 晴。你可以在下面的图例中直观地了解这几个概念。比如针对图上这个例子，D 作为节点的信息增益为：也就是 D 节点的信息熵 -2 个子节点的归一化信息熵。2 个子节点归一化信息熵 =3/10 的 D1 信息熵 +7/10 的 D2 信息熵。我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，于是我们可以用下面的方式来记录 D1，D2，D3：D1(天气 = 晴天)={1-,2-,6+}D2(天气 = 阴天)={3+,7-}D3(天气 = 小雨)={4+,5-}我们先分别计算三个叶子节点的信息熵：因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（父节点）中的概率是 3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是 2/7。那么作为子节点的归一化信息熵 = 3/70.918+2/71.0+2/7*1.0=0.965。因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。天气作为属性节点的信息增益为，Gain(D , 天气)=0.985-0.965=0.020。同理我们可以计算出其他属性作为根节点的信息增益，它们分别为 ：Gain(D , 温度)=0.128Gain(D , 湿度)=0.020Gain(D , 刮风)=0.020下图为计算Gain(D , 温度)过程：我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：然后我们要将上图中第一个叶节点，也就是 D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：Gain(D , 湿度)=1Gain(D , 天气)=1Gain(D , 刮风)=0.3115我们能看到湿度，或者天气为 D1 的节点都可以得到最大的信息增益，这里我们选取湿度作为节点的属性划分。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下：于是我们通过 ID3 算法得到了一棵决策树。ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3 算法倾向于选择取值比较多的属性。这样，如果我们把“编号”作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。所以 ID3 有一个缺陷就是，有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"}]},{"title":"数据分析-使用sklearn优雅地进行数据挖掘","slug":"数据分析-使用sklearn优雅地进行数据挖掘","date":"2018-07-10T11:12:49.000Z","updated":"2020-05-01T14:17:46.353Z","comments":true,"path":"2018/07/10/数据分析-使用sklearn优雅地进行数据挖掘/","link":"","permalink":"cpeixin.cn/2018/07/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%BD%BF%E7%94%A8sklearn%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","excerpt":"","text":"https://www.cnblogs.com/jasonfreak/p/5448462.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"cpeixin.cn/tags/sklearn/"}]},{"title":"数据分析-特征选择","slug":"数据分析-特征选择","date":"2018-07-09T15:12:49.000Z","updated":"2020-05-01T14:17:11.676Z","comments":true,"path":"2018/07/09/数据分析-特征选择/","link":"","permalink":"cpeixin.cn/2018/07/09/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"","text":"数据分析-特征选择特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。根据特征选择的形式又可以将特征选择方法分为3种：Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。我们可以使用sklearn中的feature_selection库来进行特征选择。Filter方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：12345from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data)相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：123456789101112from sklearn.feature_selection import SelectKBestfrom minepy import MINE #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5 def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5)#选择K个最好的特征，返回特征选择后的数据SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)WrapperWrapper递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：1234567from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression#递归特征消除法，返回特征选择后的数据#参数estimator为基模型#参数n_features_to_select为选择的特征个数RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)Embedded基于惩罚项的特征选择法使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：12345from sklearn.feature_selection import SelectFromModelfrom sklearn.linear_model import LogisticRegression#带L1惩罚项的逻辑回归作为基模型的特征选择SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)实际上，L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn.linear_model import LogisticRegressionclass LR(LogisticRegression): def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1): #权值相近的阈值 self.threshold = threshold LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) #使用同样的参数创建L2逻辑回归 self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) def fit(self, X, y, sample_weight=None): #训练L1逻辑回归 super(LR, self).fit(X, y, sample_weight=sample_weight) self.coef_old_ = self.coef_.copy() #训练L2逻辑回归 self.l2.fit(X, y, sample_weight=sample_weight) cntOfRow, cntOfCol = self.coef_.shape #权值系数矩阵的行数对应目标值的种类数目 for i in range(cntOfRow): for j in range(cntOfCol): coef = self.coef_[i][j] #L1逻辑回归的权值系数不为0 if coef != 0: idx = [j] #对应在L2逻辑回归中的权值系数 coef1 = self.l2.coef_[i][j] for k in range(cntOfCol): coef2 = self.l2.coef_[i][k] #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0 if abs(coef1-coef2) &lt; self.threshold and j != k and self.coef_[i][k] == 0: idx.append(k) #计算这一类特征的权值系数均值 mean = coef / len(idx) self.coef_[i][idx] = mean return self 使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：from sklearn.feature_selection import SelectFromModel #带L1和L2惩罚项的逻辑回归作为基模型的特征选择#参数threshold为权值系数之差的阈值SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)基于树模型的特征选择法树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：123456from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifier#GBDT作为基模型的特征选择SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)降维当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。主成分分析法（PCA）使用decomposition库的PCA类选择特征的代码如下：12345from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(iris.data)线性判别分析法（LDA）使用lda库的LDA类选择特征的代码如下：12345from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(iris.data, iris.target)总结再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-特征预处理","slug":"数据分析-特征预处理","date":"2018-07-08T12:12:49.000Z","updated":"2020-05-01T14:17:03.540Z","comments":true,"path":"2018/07/08/数据分析-特征预处理/","link":"","permalink":"cpeixin.cn/2018/07/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"","text":"特征工程华盛顿大学教授、《终极算法》（The Master Algorithm）的作者佩德罗·多明戈斯曾在 Communications of The ACM 第 55 卷第 10 期上发表了一篇名为《机器学习你不得不知的那些事》（A Few Useful Things to Know about Machine Learning）的小文，介绍了 12 条机器学习中的“金科玉律”，其中的 7/8 两条说的就是对数据的作用的认识。多明戈斯的观点是：数据量比算法更重要。即使算法本身并没有什么精巧的设计，但使用大量数据进行训练也能起到填鸭的效果，获得比用少量数据训练出来的聪明算法更好的性能。这也应了那句老话：数据决定了机器学习的上限，而算法只是尽可能逼近这个上限。但多明戈斯嘴里的数据可不是硬件采集或者软件抓取的原始数据，而是经过特征工程处理之后的精修数据，在他看来，特征工程（feature engineering）才是机器学习的关键**。通常来说，原始数据并不直接适用于学习，而是特征筛选、构造和生成的基础。一个好的预测模型与高效的特征提取和明确的特征表示息息相关，如果通过特征工程得到很多独立的且与所属类别相关的特征，那学习过程就变成小菜一碟。特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码。机器学习中的很多具体算法都可以归纳到特征工程的范畴之中，比如使用 L1 正则化项的 LASSO 回归，就是通过将某些特征的权重系数缩小到 0 来实现特征的过滤；再比如主成分分析，将具有相关性的一组特征变换为另一组线性无关的特征。这些方法本质上完成的都是特征工程的任务。本质上讲，特征工程是一个表示和展现数据的过程；实际工作中，特征工程的目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。特征工程的重要性有以下几点：特征越好，灵活性越强。好的特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易和维护。特征越好，构建的模型越简单。好的特征可以在参数不是最优的情况，依然得到很好的性能，减少调参的工作量和时间，也就可以大大降低模型复杂度。特征越好，模型的性能越出色。特征工程的目的本来就是为了提升模型的性能。但是今天，我将不会讨论这些，而是把关注点放在算法之外，看一看在特征工程之前，数据的特征需要经过哪些必要的预处理（preprocessing）。特征预处理通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：不属于同一量纲即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。信息冗余对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。定性特征不能直接使用某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。存在缺失值：缺失值需要补充。信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。首先介绍数据预处理中，比较简单的部分：处理缺失值数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确。缺失值产生的原因信息暂时无法获取，或者获取信息的代价太大。信息被遗漏，人为的输入遗漏或者数据采集设备的遗漏。属性不存在，在某些情况下，缺失值并不意味着数据有错误，对一些对象来说某些属性值是不存在的，如未婚者的配偶姓名、儿童的固定收入等。缺失值的影响数据挖掘建模将丢失大量的有用信息。数据挖掘模型所表现出的不确定性更加显著，模型中蕴含的规律更难把握。包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。缺失值的处理方法直接使用含有缺失值的特征：当仅有少量样本缺失该特征的时候可以尝试使用；删除含有缺失值的特征：这个方法一般适用于大多数样本都缺少该特征，且仅包含少量有效值是有效的；插值补全缺失值插值补全缺失值最常使用的还是第三种插值补全缺失值的做法，这种做法又可以有多种补全方法。均值/中位数/众数补全如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。同类均值/中位数/众数补全对样本进行分类后，根据同类其他样本该属性的均值补全缺失值，当然同第一种方法类似，如果均值不可行，可以尝试众数或者中位数等统计数据来补全。固定值补全利用固定的数值补全缺失的属性值。建模预测利用机器学习方法，将缺失属性作为预测目标进行预测，具体为将样本根据是否缺少该属性分为训练集和测试集，然后采用如回归、决策树等机器学习算法训练模型，再利用训练得到的模型预测测试集中样本的该属性的数值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。高维映射将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。多重插补多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。压缩感知和矩阵补全压缩感知通过利用信号本身所具有的稀疏性，从部分观测样本中回复原信号。压缩感知分为感知测量和重构恢复两个阶段。感知测量：此阶段对原始信号进行处理以获得稀疏样本表示。常用的手段是傅里叶变换、小波变换、字典学习、稀疏编码等重构恢复：此阶段基于稀疏性从少量观测中恢复原信号。这是压缩感知的核心矩阵补全可以查看知乎上的问题手动补全除了手动补全方法，其他插值补全方法只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。但这种方法需要对问题领域有很高的认识和理解，要求比较高，如果缺失数据较多，会比较费时费力。最近邻补全寻找与该样本最接近的样本，使用其该属性数值来补全。处理异常值异常值分析是检验数据是否有录入错误以及含有不合常理的数据。忽视异常值的存在是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响。异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也称为离群点，异常值分析也称为离群点分析。异常值检测简单统计比如利用pandas库的describe()方法观察数据的统计性描述，或者简单使用散点图也能观察到异常值的存在，如下图所示：3∂原则这个原则有个条件：数据需要服从正态分布。在 3∂ 原则下，异常值如超过 3 倍标准差，那么可以将其视为异常值。正负3∂ 的概率是 99.7%，那么距离平均值 3∂ 之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。如下图所示：箱型图这种方法是利用箱型图的四分位距（IQR）对异常值进行检测，也叫Tukey‘s test。箱型图的定义如下：四分位距(IQR)就是上四分位与下四分位的差值。而我们通过IQR的1.5倍为标准，规定：超过上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离的点为异常值。下面是Python中的代码实现，主要使用了numpy的percentile方法。1234Percentile = np.percentile(df['length'],[0,25,50,75,100])IQR = Percentile[3] - Percentile[1]UpLimit = Percentile[3]+ageIQR*1.5DownLimit = Percentile[1]-ageIQR*1.5也可以使用seaborn的可视化方法boxplot来实现：123f,ax=plt.subplots(figsize=(10,8))sns.boxplot(y='length',data=df,ax=ax)plt.show()上面三种方法是比较简单的异常值检测方法，接下来是一些较复杂的异常值检测方法，因此这里简单介绍下这些方法的基本概念。基于模型预测顾名思义，该方法会构建一个概率分布模型，并计算对象符合该模型的概率，将低概率的对象视为异常点。如果模型是簇的组合，则异常点是不在任何簇的对象；如果模型是回归，异常点是远离预测值的对象(就是第一个方法的图示例子)。优缺点：有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。基于近邻度的离群点检测一个对象的离群点得分由到它的 k-最近邻（KNN）的距离给定。这里需要注意 k 值的取值会影响离群点得分，如果 k 太小，则少量的邻近离群点可能会导致较低的离群点得分；如果 k 太大，则点数少于 k 的簇中所有的对象可能都成了离群点。为了增强鲁棒性，可以采用 k 个最近邻的平均距离。优缺点：简单;基于邻近度的方法需要 O(m2) 时间，大数据集不适用；k 值的取值导致该方法对参数的选择也是敏感的；不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。基于密度的离群点检测一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。优缺点：给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；与基于距离的方法一样，这些方法必然具有 O(m2) 的时间复杂度。对于低维数据使用特定的数据结构可以达到 O(mlogm) ；参数选择是困难的。虽然 LOF 算法通过观察不同的 k 值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。基于聚类的离群点检测一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。这也是 k-means 算法的缺点，对离群点敏感。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。优缺点：基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；簇的定义通常是离群点的补集，因此可能同时发现簇和离群点；产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。专门的离群点检测除了以上提及的方法，还有两个专门用于检测异常点的方法比较常用：One Class SVM和Isolation Forest异常值处理删除含有异常值的记录：直接将含有异常值的记录删除；视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；平均值修正：可用前后两个观测值的平均值修正该异常值；不处理：直接在具有异常值的数据集上进行数据挖掘；将含有异常值的记录直接删除的方法简单易行，但缺点也很明显，在观测值很少的情况下，这种删除会造成样本量不足，可能会改变变量的原有分布，从而造成分析结果的不准确。视为缺失值处理的好处是可以利用现有变量的信息，对异常值（缺失值）进行填补。在很多情况下，要先分析异常值出现的可能原因，在判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。处理类别不平衡问题什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。那么如何解决类别不平衡问题呢？这里介绍八大解决办法。扩充数据集首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。尝试其他评价指标一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。其他可以推荐的评价指标有以下几种混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量；精确度(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，P=TP / (TP+FP)召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，R = TP / (TP+FN)F1 得分(F1 Score)：精确度和召回率的加权平均，F1=2PR / (P+R)Kappa (Cohen kappa)ROC 曲线(ROC Curves):常被用于评价一个二值分类器的优劣，而且对于正负样本分布变化的时候，ROC 曲线可以保持不变，即不受类别不平衡的影响。其中 TP、FP、TN、FN 分别表示正确预测的正类、错误预测的正类、预测正确的负类以及错误预测的负类。图例如下：对数据集进行重采样可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。对小类的数据样本进行采样来增加小类的数据样本个数，即过采样（over-sampling ，采样的个数大于该类样本的个数）。对大类的数据样本进行采样来减少该类数据样本的个数，即欠采样（under-sampling，采样的次数少于该类样本的个素）。采样算法往往很容易实现，并且其运行速度快，并且效果也不错。 一些经验法则：考虑对大类下的样本（超过 1 万、十万甚至更多）进行欠采样，即删除部分样本；考虑对小类下的样本（不足 1万甚至更少）进行过采样，即添加部分样本的副本；考虑尝试随机采样与非随机采样两种采样方法；考虑对各类别尝试不同的采样比例，比一定是 1:1，有时候 1:1 反而不好，因为与现实情况相差甚远；考虑同时使用过采样与欠采样。尝试人工生成数据样本一种简单的人工样本数据产生的方法便是，对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样。你可以使用基于经验对属性值进行随机采样而构造新的人工样本，或者使用类似朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。有一个系统的构造人工数据样本的方法 SMOTE(Synthetic Minority Over-sampling Technique)。SMOTE 是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。它基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本，然后对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。python 实现的 SMOTE 算法代码地址如下，它提供了多种不同实现版本，以及多个重采样算法。https://github.com/scikit-learn-contrib/imbalanced-learn尝试不同分类算法强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。决策树往往在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。尝试对模型进行惩罚你可以使用相同的分类算法，但使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 penalized-SVM 和 penalized-LDA 算法。如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用惩罚模型来解决不平衡性问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。尝试一个新的角度理解问题从一个新的角度来理解问题，比如我们可以将小类的样本作为异常点，那么问题就变成异常点检测与变化趋势检测问题。异常点检测：即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。变化趋势检测：类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。尝试创新仔细对问题进行分析和挖掘，是否可以将问题划分为多个更小的问题，可以尝试如下方法：将你的大类压缩成小类；使用 One Class 分类器（将小类作为异常点）；使用集成方式，训练多个分类器，然后联合这些分类器进行分类；对于类别不平衡问题，还是需要具体问题具体分析，如果有先验知识可以快速挑选合适的方法来解决，否则最好就是逐一测试每一种方法，然后挑选最好的算法。最重要的还是多做项目，多积累经验，这样遇到一个新的问题，也可以快速找到合适的解决方法。以上针对异常值，缺失数据，分类不均问题做了详细的描述，那么接下来，就要转化数据，使之成为有效的特征。常用的方法是标准化，归一化，特征的离散化等。无量纲化量纲：就是单位，特征的单位不一致，特征就不能放在一起比较。无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。标准化比方说有一些数字的单位是千克，有一些数字的单位是克，这个时候需要统一单位。如果没有标准化，两个变量混在一起搞，那么肯定就会不合适。0-1标准化是对原始数据进行线性变换，将特征值映射成区间为［0，1］的标准值中：Z标准化基于特征值的均值（mean）和标准差（standard deviation）进行数据的标准化。它的计算公式为：标准化的代码如下：123456789101112131415161718import pandasdata = pandas.read_csv('路径.csv')#（一）Min-Max 标准化from sklearn.preprocessing import MinMaxScaler#初始化一个scaler对象scaler = MinMaxScaler()#调用scaler的fit_transform方法，把我们要处理的列作为参数传进去data['标准化后的A列数据'] = scaler.fit_transform(data['A列数据'])data['标准化后的B列数据'] = scaler.fit_transform(data['B列数据'])#（二）Z-Score标准化 （可在scale中直接实现）from sklearn.preprocessing import scaledata['标准化后的A列数据'] = scale(data['A列数据'])data['标准化后的B列数据'] = scale(data['B列数据'])区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data)归一化归一化是因为在特征会在不同的尺度下有不同的表现形式，归一化会使得各个特征能够同时以恰当的方式表现。比方说某个专辑的点击播放率一般不会超过0.2，但是专辑的播放次数可能会达到几千次，所以说为了能够在模型里面得到更合适结果，需要先把一些特征在尺度上进行归一化，然后进行模型训练。与标准化的区别，简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：使用preproccessing库的Normalizer类对数据进行归一化的代码如下：12345from sklearn.preprocessing import Normalizerscaler = Normalizer()#归一化可以同时处理多个列，所以[0]第一个进行赋值data['归一化后的A列数据'] = scaler.fit_transform(data['A列数据'])[0]data['归一化后的B列数据'] = scaler.fit_transform(data['B列数据'])[0]对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：使用preproccessing库的Binarizer类对数据进行二值化的代码如下：1234from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data)特征的离散化在进行建模时，变量中经常会有一些变量为离散型变量，例如性别。这些变量我们一般无法直接放到模型中去训练模型。因此在使用之前，我们往往会对此类变量进行处理。一般是对离散变量进行one-hot编码。下面具体介绍通过python对离散变量进行one-hot的方法。离散化是把连续型的数值型特征分段，每一段内的数据都可以当做成一个新的特征。具体又可分为等步长方式离散化和等频率的方式离散化，等步长的方式比较简单，等频率的方式更加精准，会跟数据分布有很大的关系。 代码层面，可以用pandas中的cut方法进行切分。总之，离散化的特征能够提高模型的运行速度以及准确率。比方说年龄特征是一个连续的特征，但是把年龄层分成5－18岁（中小学生），19－23岁（大学生），24－29岁（工作前几年），30－40岁（成家立业），40－60岁（中年人）从某些层面来说比连续的年龄数据（比如说某人年龄是20岁1月3日之类的）更容易理解不同年龄层人的特性。典型的离散化步骤：对特征做排序－&gt; 选择合适的分割点－&gt; 作出区间的分割 －&gt; 作出区间分割－&gt; 查看是否能够达到停止条件。分桶1.离散化的常用方法是分桶：2.分桶的数量和边界通常需要人工指定。一般有两种方法：根据业务领域的经验来指定。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。年收入超过 260 万元，则为分桶 4 。根据模型指定。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。3.选择分桶大小时，有一些经验指导：分桶大小必须足够小，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。分桶大小必须足够大，使每个桶内都有足够的样本。如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。每个桶内的样本尽量分布均匀。离散特征的两种数据类型离散特征的取值之间有大小的意义：例如：尺寸（L、XL、XXL）离散特征的取值之间没有大小的意义：例如：颜色（Red、Blue、Green）离散特征值有大小意义的虚拟变量处理离散特征的取值之间有大小意义的处理函数，我们只需要把大小值以字典的方式，作为第一个参数传入即可；(1) dict映射的字典pandas.Series.map(dict)离散特征值没有大小意义的虚拟变量处理离散特征的取值之间没有大小意义的处理方法，我们可以使用get_dummies方法处理，它有6个常用的参数(1) data 要处理的DataFrame(2) prefix 列名的前缀，在多个列有相同的离散项时候使用(3) prefix_sep 前缀和离散值的分隔符，默认为下划线，默认即可(4) dummy_na 是否把NA值，作为一个离散值进行处理，默认不处理(5) columns 要处理的列名，如果不指定该列，那么默认处理所有列(6) drop_first 是否从备选项中删第一个，建模的时候为避免共线性使用pandas.getdummies(data,prefix=None,prefix_sep=’‘,dummy_na=False,columns=None,drop_first=False)举例：123456import pandas#有些朋友也可能是encoding='utf8'或其他data=pandas.read_csv('file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK')print(data)其实，虚拟变量的实质就是要把离散型的数据转化为连续型的数据，因为第1列年龄已经是连续值，所以我们就不需要处理了。我们看看如何处理学历和性别？因为不同学历之间是有高低之分的，因此我们要使用Map方法来处理这种类型的离散型数据；第1步： 首先我们要处理不同学历之间的大小值我们使用drop_duplicates方法，来看看数据列都有哪些学历12#查看学历去重之后的情况data['学历'].drop_duplicates()第2步：理解数据值背后的意义，作出我们自己的解析，对每个学历进行评分#构建学历字典123educationLevelDict=&#123;'博士':4,'硕士':3,'大学':2,'大专':1&#125;#调用Map方法进行虚拟变量的转换data['Education Level Map']=data['Education Level'].map(educationLevelDict)第3步 对于性别这种没有大小比较的离散变量，我们使用get_dummies方法，来进行调用处理即可；1234567dummies=pandas.get_dummies(data,columns=['性别'],prefix=['性别'],prefix_sep='_',dummy_na=False,drop_first=False)sklearn 处理离散变量preprocessing.LabelEncoder：用于标签，将分类转换为分类数值 一般就是一维的preprocessing.OrdinalEncoder ：特征专用，要求至少为二维矩阵preprocessing.OneHotEncoder虽然上面已经将文字类型转换成了数值类型，但是考虑下舱门这特征Embarked（S,C,Q），表示成了[0,1,2]这样合适吗?这三个数字在算法看来，是连续且可以计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所以算法会把舱门，学历这样的分类特征，都误会成是体重这样的分类特征。这是说，我们把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模。类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息：1234567#独热编码 创建哑变量from sklearn.preprocessing import OneHotEncoderdata_ = data.copy()encoder=OneHotEncoder()encoder.fit(data_.iloc[:,1:-1])res=encoder.transform(data_.iloc[:,1:-1]).toarray()#这里要转换成数组，否则得到的是一个对象resscikit DictVectorizer 热编码（只处理类别型变量）123456789101112# 数据预处理df.transpose().to_dict().values()feature = df.iloc[:, :-1]feature# 热编码from sklearn.feature_extraction import DictVectorizerdvec = DictVectorizer(sparse=False)X = dvec.fit_transform(feature.transpose().to_dict().values())pd.DataFrame(X, columns=dvec.get_feature_names())哑编码我们针对类别型的特征，通常采用哑编码（One_Hot Encodin）的方式。所谓的哑编码，直观的讲就是用N个维度来对N个类别进行编码，并且对于每个类别，只有一个维度有效，记作数字1 ；其它维度均记作数字0。但有时使用哑编码的方式，可能会造成维度的灾难，所以通常我们在做哑编码之前，会先对特征进行Hash处理，把每个维度的特征编码成词向量。以上为大家介绍了几种较为常见、通用的数据预处理方式，但只是浩大特征工程中的冰山一角。往往很多特征工程的方法需要我们在项目中不断去总结积累比如：针对缺失值的处理，在不同的数据集中，用均值填充、中位数填充、前后值填充的效果是不一样的；对于类别型的变量，有时我们不需要对全部的数据都进行哑编码处理；对于时间型的变量有时我们有时会把它当作是离散值，有时会当成连续值处理等。所以很多情况下，我们要根据实际问题，进行不同的数据预处理。参考：https://mp.weixin.qq.com/s/BnTXjzHSb5-4s0O0WuZYlg","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-特征提取","slug":"数据分析-特征提取","date":"2018-07-07T14:07:12.000Z","updated":"2020-05-01T14:17:20.871Z","comments":true,"path":"2018/07/07/数据分析-特征提取/","link":"","permalink":"cpeixin.cn/2018/07/07/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","excerpt":"","text":"特征工程当数据预处理完成后，我们就要开始进行特征工程了。主要包含以下几个方面：特征提取特征创造特征选择特征提取和特征选择的区别特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。特征分类下图就是将我们所能遇到的特征数据进行一个分类：首先是基本特征，而后统计和复杂特征层层递进。其中针对图像语音等抽提特征有专用的知识方法掌握了这套特征设计的思路，在复杂数据上几乎可以设计出无穷无尽的特征。而怎么在最短的时间内，把数据中最有价值的特征提炼出来，就要考验数据挖掘工程师的功底。特征提取特征抽取或者特征提取大概可以分为；字典特征抽取，应用DiceVectorizer实现对类别特征进行数值化、离散化文本特征抽取，应用CounterVertorize/TfIdfVectorize实现对文本特征数值化图像特征抽取(深度学习)对于以上不同类别数据的特征提取，这里不一一介绍，等以后遇到了对应问题，再详细的举例用哪些相应的算法来处理。特征提取也可以说是将任意数据（文本或者图像）转化成适用于机器学习的数字特征应用 sklearn.feature_extraction可以轻松完成上述任务","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-数据转换","slug":"数据分析-数据转换","date":"2018-07-06T15:26:15.000Z","updated":"2020-05-01T14:17:29.058Z","comments":true,"path":"2018/07/06/数据分析-数据转换/","link":"","permalink":"cpeixin.cn/2018/07/06/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2/","excerpt":"","text":"数据变换。如果一个人在百分制的考试中得了 95 分，你肯定会认为他学习成绩很好，如果得了 65 分，就会觉得他成绩不好。如果得了 80 分呢？你会觉得他成绩中等，因为在班级里这属于大部分人的情况。为什么会有这样的认知呢？这是因为我们从小到大的考试成绩基本上都会满足正态分布的情况。什么是正态分布呢？正态分布也叫作常态分布，就是正常的状态下，呈现的分布情况。比如你可能会问班里的考试成绩是怎样的？这里其实指的是大部分同学的成绩如何。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。正态分布的另一个好处就是，如果你知道了自己的成绩，和整体的正态分布情况，就可以知道自己的成绩在全班中的位置。另一个典型的例子就是，美国 SAT 考试成绩也符合正态分布。而且美国本科的申请，需要中国高中生的 GPA 在 80 分以上（百分制的成绩），背后的理由也是默认考试成绩属于正态分布的情况。为了让成绩符合正态分布，出题老师是怎么做的呢？他们通常可以把考题分成三类：第一类：基础题，占总分 70%，基本上属于送分题；第二类：灵活题，基础范围内 + 一定的灵活性，占 20%；第三类：难题，涉及知识面较广的难题，占 10%；那么，你想下，如果一个出题老师没有按照上面的标准来出题，而是将第三类难题比重占到了 70%，也就是我们说的“超纲”，结果会是怎样呢？你会发现，大部分人成绩都“不及格”，最后在大家激烈的讨论声中，老师会将考试成绩做规范化处理，从而让成绩满足正态分布的情况。因为只有这样，成绩才更具有比较性。所以正态分布的成绩，不仅可以让你了解全班整体的情况，还能了解每个人的成绩在全班中的位置。数据变换在数据分析中的角色我们再来举个例子，假设 A 考了 80 分，B 也考了 80 分，但前者是百分制，后者 500 分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。你现在可以理解为什么 80% 的工作时间会花在前期的数据准备上了吧。那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。我来介绍下这些常见的变换方法：数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。在这些变换方法中，最简单易用的就是对数据进行规范化处理。下面我来给你讲下如何对数据进行规范化处理。数据规范化的几种方法Min-max 规范化Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。Z-Score 规范化假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义。那么如何用相同的标准来比较 A 与 B 的成绩呢？Z-Score 就是用来可以解决这一问题的。我们定义：新数值 =（原数值 - 均值）/ 标准差。假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为 400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值 =(80-400)/100=-3.2。那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。我们能看到 Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。小数定标规范化小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。上面这三种是数值规范化中常用的几种方式。Python的SciKit-Learn库使用SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。我现在来讲下如何使用 SciKit-Learn 进行数据规范化。Min-max 规范化我们可以让原始数据投射到指定的空间[min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。默认情况下[min,max]是[0,1]，也就是把原始数据投放到[0,1]范围内。我们来看下下面这个例子：123456789101112# coding:utf-8from sklearn import preprocessingimport numpy as np# 初始化数据，每一行表示一个样本，每一列表示一个特征x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 将数据进行[0,1]规范化min_max_scaler = preprocessing.MinMaxScaler()minmax_x = min_max_scaler.fit_transform(x)print minmax_x结果：1234[[0. 0. 0.66666667] [1. 1. 1. ] [0. 1. 0. ]]Z-Score 规范化在 SciKit-Learn 库中使用 preprocessing.scale() 函数，可以直接将给定数据进行 Z-Score 规范化。12345678910from sklearn import preprocessingimport numpy as np# 初始化数据x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 将数据进行Z-Score规范化scaled_x = preprocessing.scale(x)print scaled_x结果：1234[[-0.70710678 -1.41421356 0.26726124] [ 1.41421356 0.70710678 1.06904497] [-0.70710678 0.70710678 -1.33630621]]1这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。我们看到 Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。小数定标规范化我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。这里我们看下运行代码：123456789101112# coding:utf-8from sklearn import preprocessingimport numpy as np# 初始化数据x = np.array([[ 0., -3., 1.], [ 3., 1., 2.], [ 0., 1., -1.]])# 小数定标规范化j = np.ceil(np.log10(np.max(abs(x))))scaled_x = x/(10**j)print scaled_x结果：1234[[ 0. -0.3 0.1] [ 0.3 0.1 0.2] [ 0. 0.1 -0.1]]数据挖掘中数据变换比算法选择更重要在考试成绩中，我们都需要让数据满足一定的规律，达到规范性的要求，便于进行挖掘。这就是数据变换的作用。如果不进行变换的话，要不就是维数过多，增加了计算的成本，要不就是数据过于集中，很难找到数据之间的特征。在数据变换中，重点是如何将数值进行规范化，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 Z-Score 规范化可以直接将数据转化为正态分布的情况，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。在最后我给大家推荐了 Python 的 sklearn 库，它和 NumPy, Pandas 都是非常有名的 Python 库，在数据统计工作中起了很大的作用。SciKit-Learn 不仅可以用于数据变换，它还提供了分类、聚类、预测等数据挖掘算法的 API 封装。后面我会详细给你讲解这些算法，也会教你如何使用 SciKit-Learn 工具来完成数据挖掘算法的工作。Q&amp;A数据规范化、归一化、标准化是同一个概念么？数据规范化是更大的概念，它指的是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。数据归一化和数据标准化都是数据规范化的方式。不同点在于数据归一化会让数据在一个[0,1]或者[-1,1]的区间范围内。而数据标准化会让规范化的数据呈现正态分布的情况，所以你可以这么记：归一化的“一”，是让数据在[0,1]的范围内。而标准化，目标是让数据呈现标准的正态分布。什么时候会用到数据规范化（Min-max、Z-Score 和小数定标）？刚才提到了，进行数据规范化有两个作用：一是让数据之间具有可比较性，二是加快后续算法的迭代收敛速度。实际上你能看到 Min-max、Z-Score 和小数定标规范化都是一种线性映射的关系，将原来的数值投射到新的空间中。这样变换的好处就是可以看到在特定空间内的数值分布情况，比如通过 Min-max 可以看到数据在[0,1]之间的分布情况，Z-Score 可以看到数值的正态分布情况等。不论是采用哪种数据规范化方法，规范化后的数值都会在同一个数量的级别上，这样方便后续进行运算。那么回过头来看，在数据挖掘算法中，是否都需要进行数据规范化呢？一般情况下是需要的，尤其是针对距离相关的运算，比如在 K-Means、KNN 以及聚类算法中，我们需要有对距离的定义，所以在做这些算法前，需要对数据进行规范化。另外还有一些算法用到了梯度下降作为优化器，这是为了提高迭代收敛的效率，也就是提升找到目标函数最优解的效率。我们也需要进行数据规范化，比如逻辑回归、SVM 和神经网络算法。在这些算法中都有目标函数，需要对目标函数进行求解。梯度下降的目标是寻找到目标函数的最优解，而梯度的方法则指明了最优解的方向，如下图所示。当然不是所有的算法都需要进行数据规范化。在构造决策树的时候，可以不用提前做数据规范化，因为我们不需要关心特征值的大小维度，也没有使用到梯度下降来做优化，所以数据规范化对决策树的构造结果和构造效率影响不大。除此之外，还是建议你在做数据挖掘算法前进行数据规范化。如何使用 Z-Score 规范化，将分数变成正态分布？假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。这里假设 A 和 B 的考试成绩都是成正态分布，可以直接采用 Z-Score 的线性化规范化方法。在专栏的讨论区中，有个同学提出了“Z-Score”的非线性计算方式，大家可以一起了解下：先按公式计算出百分等级。百分等级（年级）=100-(100x 年级名次 -50)/ 有效参加考试人数。这里百分等级是每个学生在该批学生中的相对位置，其中百分等级是按照正态分布图的所占面积比例求得的；按照百分等级数去标准正态分布表中查询得出 Z-Score 值，这样最终得出的 Z 分便是标准的正态分布，能够将偏态转化成标准正态。因为在很多情况下，数值如果不是正态分布，而是偏态分布，直接使用 Z-Score 的线性计算方式无法将分数转化成正态分布。采用以上的方法可以解决这一个问题，大家可以了解下。这里偏态分布指的是非对称分布的偏斜状态，包括了负偏态，也就是左偏态分布，以及正偏态，也就是右偏态分布。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"数据分析-数据清洗","slug":"数据分析-数据清洗","date":"2018-07-05T15:53:22.000Z","updated":"2020-05-01T14:17:36.976Z","comments":true,"path":"2018/07/05/数据分析-数据清洗/","link":"","permalink":"cpeixin.cn/2018/07/05/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/","excerpt":"","text":"数据分析80%时间都花费在了数据清洗任务上？做完数据采集就可以直接进行挖掘了吗？肯定不是的。就拿做饭打个比方吧，对于很多人来说，热油下锅、掌勺翻炒一定是做饭中最过瘾的环节，但实际上炒菜这个过程只占做饭时间的 20%，剩下 80% 的时间都是在做准备，比如买菜、择菜、洗菜等等。在数据挖掘中，数据清洗就是这样的前期准备工作。对于数据科学家来说，我们会遇到各种各样的数据，在分析前，要投入大量的时间和精力把数据“整理裁剪”成自己想要或需要的样子。为什么呢？因为我们采集到的数据往往有很多问题。我们先看一个例子，假设老板给你以下的数据，让你做数据分析，你看到这个数据后有什么感觉呢？你刚看到这些数据可能会比较懵，因为这些数据缺少标注。我们在收集整理数据的时候，一定要对数据做标注，数据表头很重要。比如这份数据表，就缺少列名的标注，这样一来我们就不知道每列数据所代表的含义，无法从业务中理解这些数值的作用，以及这些数值是否正确。我简单解释下上图这些数据代表的含义。这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。列坐标中，第 0 列代表的是序号，第 1 列代表的会员的姓名，第 2 列代表年龄，第 3 列代表体重，第 46 列代表男性会员的三围尺寸，第 79 列代表女性会员的三围尺寸。了解含义以后，我们再看下中间部分具体的数据，你可能会想，这些数据怎么这么“脏乱差”啊，有很多值是空的（NaN），还有空行的情况。是的，这还仅仅是一家商店的部分会员数据，我们一眼看过去就能发现一些问题。日常工作中的数据业务会复杂很多，通常我们要统计更多的数据维度，比如 100 个指标，数据量通常都是超过 TB、EB 级别的，所以整个数据分析的处理难度是呈指数级增加的。这个时候，仅仅通过肉眼就很难找到问题所在了。我举了这样一个简单的例子，带你理解在数据分析之前为什么要有数据清洗这个重要的准备工作。有经验的数据分析师都知道，好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。但在实际工作中，也可能像这个案例一样，数据是缺少标注的。但是大多数的情况下，数据表头都是存在的，即使不存在，我们也会首先的想倒要去处理表头信息，例如：如果数据来源是python爬虫获取的，那么在爬取之前，我们就会定义好，解析获得到的每个字段是什么意思。如果数据源是日志采集而来的，那么在我们接收到前端传输过来数据流，一般都是带有字段信息的，例如，前端埋点数据打到 kafka消息队 列中，是json格式，那么字段信息则为key值，如果埋点数据并不是标准的json格式，那么前端人员也是会给一份数据字典与之对应的。数据源来自某个业务数据库，那么，数据表头信息则是数据库中数据的列名。数据质量的准则在上面这个服装店会员数据的案例中，一看到这些数据，你肯定能发现几个问题。你是不是想知道，有没有一些准则来规范这些数据的质量呢？准则肯定是有的。不过如果数据存在七八种甚至更多的问题，我们很难将这些规则都记住。有研究说一个人的短期记忆，最多可以记住 7 条内容或信息，超过 7 条就记不住了。而数据清洗要解决的问题，远不止 7 条，我们万一漏掉一项该怎么办呢？有没有一种方法，我们既可以很方便地记住，又能保证我们的数据得到很好的清洗，提升数据质量呢？在这里，我将数据清洗规则总结为以下 4 个关键点，统一起来叫“完全合一”，下面我来解释下。完整性：单条数据是否存在空值，统计的字段是否完善。全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。拿接触过的实际项目举例，检查数据质量是有多么的重要，工作初期，经常会使用hive，spark做一些数据报表，面对几百GB的数据，经常是直接忽略过数据质量审查的这一步骤，所以导致的结果就是，在程序计算过程中，会出现很对关于数值的错误异常，还有空值异常，极大异常值等，例如 前端生日栏位出现bug,用户自定义输入生日可以为‘3000-14-03’,用户性别值，前端居然给了‘未知选项’等等让人哭笑不得的数值错误，反而就是因为没有去检查数据质量，所以会导致在后面的编程过程中，要花费更多的时间去DEBUG，去写更多的容错逻辑。在很多数据挖掘的教学中，数据准则通常会列出来 7~8 项，在这里我们归类成了“完全合一” 4 项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。清洗数据一一击破了解了数据质量准则之后，我们针对上面服装店会员数据案例中的问题进行一一击破。这里你就需要 Python 的 Pandas 工具了。这个工具我们之前介绍过。它是基于 NumPy 的工具，专门为解决数据分析任务而创建。Pandas 纳入了大量库，我们可以利用这些库高效地进行数据清理工作。这里我补充说明一下，如果你对 Python 还不是很熟悉，但是很想从事数据挖掘、数据分析相关的工作，那么花一些时间和精力来学习一下 Python 是很有必要的。Python 拥有丰富的库，堪称数据挖掘利器。当然了，数据清洗的工具也还有很多，这里我们只是以 Pandas 为例，帮你应用数据清洗准则，带你更加直观地了解数据清洗到底是怎么回事儿。下面，我们就依照“完全合一”的准则，使用 Pandas 来进行清洗。完整性问题 1：缺失值在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：删除：删除数据缺失的记录；均值：使用当前列的均值；高频：使用当前列出现频率最高的数据。问题 2：空行我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。全面性问题：列数据的单位不统一观察 weight 列的数值，我们能发现 weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）合理性问题：非 ASCII 字符我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法唯一性问题 1：一列有多个参数在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname 两个字段。我们使用 Python 的 split 方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name 列删除。问题 2：重复数据我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。养成数据审核的习惯现在，你是不是能感受到数据问题不是小事，上面这个简单的例子里都有 6 处错误。所以我们常说，现实世界的数据是“脏的”，需要清洗。第三方的数据要清洗，自有产品的数据，也需要数据清洗。比如美团自身做数据挖掘的时候，也需要去除爬虫抓取，作弊数据等。可以说没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有“数据审核”的“职业病”。这就好比编辑非常在意文章中的错别字、语法一样。数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。当然了，这也是一门需要不断修炼的功夫。终有一天，你会进入这样一种境界：看一眼数据，差不多 7 秒钟的时间，就能知道这个数据是否存在问题。为了这一眼的功力，我们要做很多练习。刚开始接触数据科学工作的时候，一定会觉得数据挖掘是件很酷、很有价值的事。确实如此，不过今天我还要告诉你，再酷炫的事也离不开基础性的工作，就像我们今天讲的数据清洗工作。对于这些基础性的工作，我们需要耐下性子，一个坑一个坑地去解决。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据清洗","slug":"数据清洗","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"}]},{"title":"数据分析-如何采集数据","slug":"数据分析-如何采集数据","date":"2018-07-04T15:12:49.000Z","updated":"2020-05-01T14:17:54.345Z","comments":true,"path":"2018/07/04/数据分析-如何采集数据/","link":"","permalink":"cpeixin.cn/2018/07/04/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A6%82%E4%BD%95%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE/","excerpt":"","text":"如何对用户画像建模，而建模之前我们都要进行数据采集。数据采集是数据挖掘的基础，没有数据，也没有下一阶段的数据挖掘。很多时候，我们拥有多少数据源，多少数据量，以及数据质量如何，将决定我们挖掘产出的成果会怎样。有一句话 垃圾进，垃圾出， 指的就是你利用一批垃圾数据，那么挖掘出来的结论，也是如同垃圾一样，毫无价值。举个例子，你做量化投资，基于大数据预测未来股票的波动，根据这个预测结果进行买卖。你当前能够拿到以往股票的所有历史数据，是否可以根据这些数据做出一个预测率高的数据分析系统呢？实际上，如果你只有股票历史数据，你仍然无法理解股票为什么会产生大幅的波动。比如，当时可能是爆发了 SARS 疫情，或者某地区发生了战争等。这些重大的社会事件对股票的影响也是巨大的。因此我们需要考虑到，一个数据的走势，是由多个维度影响的。我们需要通过多源的数据采集，收集到尽可能多的数据维度，同时保证数据的质量，这样才能得到高质量的数据挖掘结果。数据源分类那么，从数据采集角度来说，都有哪些数据源呢？我将数据源分成了以下的四类这四类数据源包括了：开放数据源、爬虫抓取、传感器和日志采集。开放数据源一般是针对行业的数据库。比如美国人口调查局开放了美国的人口信息、地区分布和教育情况数据。除了政府外，企业和高校也会开放相应的大数据，这方面北美相对来说做得好一些。国内，贵州做了不少大胆尝试，搭建了云平台，逐年开放了旅游、交通、商务等领域的数据量。要知道很多研究都是基于开放数据源进行的，否则每年不会有那么多论文发表，大家需要相同的数据集才能对比出算法的好坏。爬虫抓取，一般是针对特定的网站或 App。如果我们想要抓取指定的网站数据，比如购物网站上的购物评价, 以及微博实时热点等，就需要我们做特定的爬虫抓取。第三类数据源是传感器，也就是目前比较火的IOT， 例如小米智能手环，智能体重秤，街边摄像头录下来的录像，它基本上采集的是物理信息。最后是日志采集，这个是统计用户的操作。我们可以在前端进行埋点，在后端进行脚本收集、统计，来分析网站的访问情况，以及使用瓶颈等。数据采集知道了有四类数据源，那如何采集到这些数据呢？收集开源数据我们先来看下开放数据源，教你个方法，开放数据源可以从两个维度来考虑，一个是单位的维度，比如政府、企业、高校；一个就是行业维度，比如交通、金融、能源等领域。这方面，国外的开放数据源比国内做得好一些，当然近些年国内的政府和高校做开放数据源的也越来越多。一方面服务社会，另一方面自己的影响力也会越来越大。比如，下面这张表格列举的就是单位维度的数据源。所以如果你想找某个领域的数据源，比如金融领域，你基本上可以看下政府、高校、企业是否有开放的数据源。当然你也可以直接搜索金融开放数据源。爬虫采集使用爬虫做抓取爬虫抓取应该属于最常见的需求，也是我最常用的获取数据的方式。比如你想要餐厅的评价数据。当然这里要注重版权问题，而且很多网站也是有反爬机制的。最直接的方法就是使用 Python 编写爬虫代码，当然前提是你需要会 Python 的基本语法。除此之外，PHP 也可以做爬虫，只是功能不如 Python 完善，尤其是涉及到多线程的操作。在 Python 爬虫中，基本上会经历三个过程。发请求，获取内容。使用 Requests 爬取内容。我们可以使用 Requests 库来抓取网页信息。Requests 库可以说是 Python 爬虫的利器，也就是 Python 的 HTTP 库，通过这个库爬取网页中的数据，非常方便，可以帮我们节约大量的时间。同时也可以使用目前比较成熟的爬虫框架 scrapy 来进行数据爬取。解析内容BeautifulSoup 是爬虫必学的技能。BeautifulSoup最主要的功能是从网页解析数据,其次还可以使用 XPath 解析内容。XPath 是 XML Path 的缩写，也就是 XML 路径语言。它是一种用来确定 XML 文档中某部分位置的语言，在开发中经常用来当作小型查询语言。XPath 可以通过元素和属性进行位置索引。保存数据根据你所爬取的数据量，可以选择不同的存储方式。如果你爬取的只是单次小型数据集，那么可以直接存储到txt,csv等文件中，也可以存储到，mysql数据库中，方便以后分析使用。如果你爬取的是大型数据集，可以使用 Pandas 保存数据。Pandas 是让数据分析工作变得更加简单的高级数据结构，我们可以用 Pandas 保存爬取的数据。最后通过 Pandas 再写入到 XLS 或者 MySQL 等数据库中。如果你的爬虫任务是长期执行的，那么你就要考虑好你要用什么存储工具来存储数据了 （eg: mongoDB??? elasticsearch ????）当然做 Python 爬虫还有很多利器，比如 Selenium，PhantomJS，或者用 Puppteteer 这种无头模式, 以上所说的三个工具，都是对付那些反爬虫比较复杂的数据源。日志采集为什么要做日志采集呢？日志采集最大的作用，就是通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，也可以方便技术人员基于用户实际的访问情况进行优化。日志采集也是运维人员的重要工作之一，那么日志都包括哪些呢，又该如何对日志进行采集呢？日志就是日记的意思，它记录了用户访问网站的全过程：哪些人在什么时间，通过什么渠道（比如搜索引擎、网址输入）来过，都执行了哪些操作；系统是否产生了错误；甚至包括用户的 IP、HTTP 请求的时间，用户代理等。这些日志数据可以被写在一个日志文件中，也可以分成不同的日志文件，比如访问日志、错误日志等。日志采集可以分两种形式通过 Web 服务器采集，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的 Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。自定义采集用户行为，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。埋点是什么？？埋点是日志采集的关键步骤，那什么是埋点呢？埋点就是在有需要的位置采集相应的信息，进行上报。比如某页面的访问情况，包括用户信息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。那我们要如何进行埋点呢？埋点就是在你需要统计数据的地方植入统计代码，当然植入代码可以自己写，也可以使用第三方统计工具。我之前讲到“不重复造轮子”的原则，一般来说需要自己写的代码，一般是主营核心业务，对于埋点这类监测性的工具，市场上已经比较成熟，这里推荐你使用第三方的工具，比如友盟、Google Analysis、Talkingdata 等。他们都是采用前端埋点的方式，然后在第三方工具里就可以看到用户的行为数据。但如果我们想要看到更深层的用户操作行为，就需要进行自定义埋点。总结一下，日志采集有助于我们了解用户的操作数据，适用于运维监控、安全审计、业务数据分析等场景。一般 Web 服务器会自带日志功能，也可以使用 Flume 从不同的服务器集群中采集、汇总和传输大容量的日志数据。当然我们也可以使用第三方的统计工具或自定义埋点得到自己想要的统计内容。传感器采集基本上是基于特定的设备，将设备采集的信息通过进行收集即可总结数据采集是数据分析的关键，很多时候我们会想到 Python 网络爬虫，实际上数据采集的方法、渠道很广，有些可以直接使用开放的数据源，比如想获取比特币历史的价格及交易数据，可以直接从 Kaggle 上下载，不需要自己爬取。另一方面根据我们的需求，需要采集的数据也不同，比如交通行业，数据采集会和摄像头或者测速仪有关。对于运维人员，日志采集和分析则是关键。所以我们需要针对特定的业务场景，选择适合的采集工具。今天我讲了数据采集的不同渠道以及相关的工具。给你留一个思考题，假如你想预测比特币的未来走势，都需要哪些维度的数据源呢？怎样收集到它们呢？","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据采集","slug":"数据采集","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"}]},{"title":"数据分析-用户画像项目","slug":"数据分析-用户画像项目","date":"2018-07-03T14:32:06.000Z","updated":"2020-05-01T14:16:35.499Z","comments":true,"path":"2018/07/03/数据分析-用户画像项目/","link":"","permalink":"cpeixin.cn/2018/07/03/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"用户标签： 性别，年龄，职业，星座，生活城市，家乡，用户注册渠道，邀请码消费标签： 存款，领取优惠金额，参与活动，会员星级，使用信用卡次数，储蓄卡使用次数， 数字货币使用次数，购买装备金额行为标签： 登录次数，游戏次数，在线时长，游戏时长，行为路径，用户点击热点，（搜索补充）内容标签： 论坛留言，浏览优惠活动查看，游戏聊天（搜索补充）针对单个用户的画像分析。针对某一属性群体的画像分析。平均值，最大值，最小值，众数，中位数，四分之三位数，六分之一位数 等数值在在实际中代表的意义。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"}]},{"title":"数据分析-用户画像标签","slug":"数据分析-用户画像标签","date":"2018-07-02T14:31:58.000Z","updated":"2020-05-01T14:16:55.619Z","comments":true,"path":"2018/07/02/数据分析-用户画像标签/","link":"","permalink":"cpeixin.cn/2018/07/02/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E6%A0%87%E7%AD%BE/","excerpt":"","text":"王兴说过，我们已经进入到互联网的下半场。在上半场，也就是早期的互联网时代，你永远不知道在对面坐的是什么样的人。那个年代大部分人还是 QQ 的早期用户。在下半场，互联网公司已经不新鲜了，大部分公司已经互联网化。他们已经在用网络进行产品宣传，使用电商销售自己的商品。这两年引领下半场发展的是那些在讲 “大数据”“赋能”的企业，他们有数据，有用户。通过大数据告诉政府该如何智慧地管理交通，做城市规划。通过消费数据分析，告诉企业该在什么时间生产什么产品，以最大化地满足用户的需求。通过生活大数据告诉我们餐饮企业，甚至房地产企业该如何选址。如果说互联网的上半场是粗狂运营，因为有流量红利不需要考虑细节。那么在下半场，精细化运营将是长久的主题。有数据，有数据分析能力才能让用户得到更好的体验。所以，用户是根本，也是数据分析的出发点。假如你进入到一家卖羊肉串的餐饮公司，老板说现在竞争越来越激烈，要想做得好就要明白顾客喜欢什么。于是上班第一天，老板问你：“你能不能分析下用户数据，给咱们公司的业务做个赋能啊？”听到这，你会怎么想？你说：“老板啊，咱们是卖羊肉串的，做数据挖掘没用啊。”估计老板听后，晚上就把你给开了。那该怎么办呢？如果你感觉一头懵，没关系，我们今天就来讲讲怎么一步步分析用户数据。用户画像的准则首先就是将自己企业的用户画像做个白描，告诉他这些用户“都是谁”“从哪来”“要去哪”。你可以这么和老板说：“老板啊，用户画像建模是个系统的工程，我们要解决三个问题。第一呢，就是用户从哪里来，这里我们需要统一标识用户 ID，方便我们对用户后续行为进行跟踪。我们要了解这些羊肉串的用户从哪里来，也就是所谓的用户来源渠道。他们是为了聚餐，还是自己吃宵夜，这些场景我们都要做统计分析。第二呢，这些用户是谁？这里指的是 用户生日，职业，性别，年龄，收入等基本信息， 我们需要对这些用户进行标签化，方便我们对用户行为进行理解。第三呢，就是用户要到哪里去？我们要将这些用户画像与我们的业务相关联，提升我们的转化率，或者降低我们的流失率。”听到这，老板给你竖起了大拇指，说：“不错，都需要什么资源，随时找我就行。”刚才说的这三个步骤，下面我一一给你做个梳理。首先，为什么要设计唯一标识？用户唯一标识是整个用户画像的核心。我们以一个 App 为例，它把“从用户开始使用 APP 到下单到售后整个所有的用户行为”进行串联，因为一个用户在这个APP中没操作一项，都可能会产生一条单独的数据，所以在后续的分析过程中，需要利用用户的唯一标识去抓数据，统计数据。这样就可以更好地去跟踪和分析一个用户的特征。设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。其次，给用户打标签。你可能会想，标签有很多，且不同的产品，标签的选择范围也不同，这么多的标签，怎样划分才能既方便记忆，又能保证用户画像的全面性呢？这里我总结了八个字，叫“用户消费行为分析”。我们可以从这 4 个维度来进行标签划分。用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。最后，当你有了用户画像，可以为企业带来什么业务价值呢？我们可以从用户生命周期的三个阶段来划分业务价值，包括：拉新、用户粘度和留存：如何进行拉新，通过更精准的营销获取客户。用户粘度：个性化推荐，搜索排序，场景运营等。留存：流失率预测，分析关键节点降低流失率。如果按照数据流处理的阶段来划分用户画像建模的过程，可以分为数据层、算法层和业务层。你会发现在不同的层，都需要打上不同的标签。数据层指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录。算法层指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识。业务层指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果。所以这个标签化的流程，就是通过数据层的“事实标签”，在算法层进行计算，打上“模型标签”的分类结果，最后指导业务层，得出“预测标签”。美团外卖的用户画像该设计刚才讲的是用户画像的三个阶段，以及每个阶段的准则。下面，我们来使用这些准则做个练习。如果你是美团外卖的数据分析师，你该如何制定用户标识 ID，制定用户画像，以及基于用户画像可以做哪些业务关联？首先，我们先回顾下美团外卖的产品背景。美团已经和大众点评进行了合并，因此在大众点评和美团外卖上都可以进行外卖下单。另外美团外卖针对的是高频 O2O 的场景，美团外卖是美团的核心产品，基本上有一半的市值都是由外卖撑起来的。基于用户画像实施的三个阶段，我们首先需要统一用户的唯一标识，那么究竟哪个字段可以作为用户标识呢？我们先看下美团和大众点评都是通过哪些方式登录的。我们看到，美团采用的是手机号、微信、微博、美团账号的登录方式。大众点评采用的是手机号、微信、QQ、微博的登录方式。这里面两个 APP 共同的登录方式都是手机号、微信和微博。那么究竟哪个可以作为用户的唯一标识呢？当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。当然，大家知道在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，尤其是多条产品线，在数据整合的过程中，会出现各种各样的数据格式问题，数据精度，数据的空值处理等。所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。然后我们思考下，有了用户，用户画像都可以统计到哪些标签。我们按照“用户消费行为分析”的准则来进行设计。用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。当你有了“用户消费行为分析”的标签之后，你就可以更好地理解业务了。比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。有了这些数据，我们就可以预测用户的行为。比如一个用户购买了“月子餐”后，更有可能购买婴儿水，同样婴儿相关的产品比如婴儿湿巾等的购买概率也会增大。具体在业务层上，我们都可以基于标签产生哪些业务价值呢？在拉新上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。在用户粘度上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。在留存上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。锻炼自己的抽象能力，将繁杂的事务简单化上面我们讲到的“用户消费行为标签”都是基于一般情况考虑的，除此之外，用户的行为也会随着营销的节奏产生异常值，比如双十一的时候，如果商家都在促销就会产生突发的大量订单。因此在做用户画像的时候，还要考虑到异常值的处理。总之，数据量是庞大的，会存在各种各样的使用情况。光是分析 EB 级别的大数据，我们就要花很长的时间。但我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。对数据的标签化能让我们快速理解一个用户，一个商品，乃至一个视频内容的特征，从而方便我们去理解和使用数据。对数据的标签化其实考验的是我们的抽象能力，在日常工作中，我们也要锻炼自己的抽象能力，它可以让我们很快地将一个繁杂的事物简单化，不仅方便理解，还有益后续的使用。我们今天讲了用户画像的流程，其中很重要的一个步骤就是给用户打标签，那么你不妨想想，如果给羊肉串连锁店进行用户画像分析，都可以从哪些角度进行标签化？最后，我们从现实生活中出发，打开你的手机，翻翻看你的微信通讯录，分析下你的朋友圈，都有哪些用户画像？如果你来给它设计标签，都有哪些种类需要统计呢。为了方便后续使用，你是如何将他们归类分组的？关于朋友圈的用户画像，我们按照 “用户消费行为习惯” 来设定标签的话，大体可以参考如下：用户标签:用户性别、用户年龄、用户所处位置、用户家乡、用户学历、用户角色、用户来源渠道消费标签：朋友圈广告点击情况、用户参与活动、使用小程序的类型行为标签：朋友圈发布频次、朋友圈可见设置、朋友圈点赞次数、朋友圈评论次数、朋友圈文字浏览次数、朋友圈权限设置内容分析：浏览文字类型、文字转发类型","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"}]},{"title":"数据分析-基本概念","slug":"数据分析-基本概念","date":"2018-07-01T14:31:58.000Z","updated":"2020-05-01T14:19:10.225Z","comments":true,"path":"2018/07/01/数据分析-基本概念/","link":"","permalink":"cpeixin.cn/2018/07/01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"前言美国明尼苏达州一家 Target 百货被客户投诉，这名客户指控 Target 将婴儿产品优惠券寄给他的女儿，而他女儿还是一名高中生。但没多久这名客户就来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。Target 百货寄送婴儿产品优惠券绝非偶然之举，他们发现妇女在怀孕的情况下，购买的物品会发生变化，比如护手霜会从有香味的改成无味的，此外还会购买大量维生素等保健品。通过类似的关联分析，Target 构建了一个“怀孕预测指数”，通过这个指数预测到了顾客已经怀孕的情况，并把优惠券寄送给她。那么顾客怀孕与商品之间的关联关系是如何被发现的呢？实际上他们都是用的 Apriori 算法，该算法是由美国学者 Agrawal 在 1994 年提出的。他通过分析购物篮中的商品集合，找出商品之间的关联关系。利用这种隐性关联关系，商家就可以强化这类购买行为，从而提升销售额。这就是数据分析的力量，人们总是从数据分析中得到有价值的信息，啤酒和尿布的故事也是个经典的案例。如今在超市中，我们还能看到不少组合的套装打包在一起卖，比如宝洁的产品：飘柔洗发水 + 玉兰油沐浴露、海飞丝洗发水 + 舒肤佳沐浴露等等。商品的捆绑销售是个很有用的营销方式，背后都是数据分析在发挥作用。商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系开头中的百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。所以我们能在这个场景里看到三个重要的概念：商业智能、数据仓库和数据挖掘。商业智能的英文是 Business Intelligence，缩写是 BI。相比于数据仓库、数据挖掘，它是一个更大的概念。商业智能可以说是基于数据仓库，经过了数据挖掘后，得到了商业价值的过程。所以说数据仓库是个金矿，数据挖掘是炼金术，而商业报告则是黄金。数据仓库的英文是 Data Warehouse，缩写是 DW。它可以说是 BI 这个房子的地基，搭建好 DW 这个地基之后，才能进行分析使用，最后产生价值。数据仓库可以说是数据库的升级概念。数据仓库是企业级别的，而数据库则是业务系统和单个项目中所应用到的技术，从逻辑上理解，数据库和数据仓库没有什么区别，都是通过数据库技术来存储数据的。不过从数量上来讲，数据仓库的量更庞大，适用于数据挖掘和数据分析。数据库可以理解是一项技术。数据仓库将原有的多个数据来源中的数据进行汇总、整理而得。数据进入数据仓库前，必须消除数据中的不一致性，方便后续进行数据分析和挖掘。数据挖掘的英文是 Data Mining，缩写是 DM。在商业智能 BI 中经常会使用到数据挖掘技术。数据挖掘的核心包括分类、聚类、预测、关联分析等任务，通过这些炼金术，我们可以从数据仓库中得到宝藏，比如商业报告。很多时候，企业老板总是以结果为导向，他们认为商业报告才是他们想要的，但是这也是需要经过地基 DW、搬运工 ETL、科学家 DM 等共同的努力才得到的。元数据 VS 数据元我们前面提到了数据仓库，在数据仓库中，还有一类重要的数据是元数据，那么它和数据元有什么区别呢？元数据（MetaData）：描述其它数据的数据，也称为“中介数据”。数据元（Data Element）：就是最小数据单元。在生活中，只要有一类事物，就可以定义一套元数据。举个例子，比如一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。在图书这个元数据中，书名、作者、出版社就是数据元。你可以理解是最小的数据单元。元数据最大的好处是使信息的描述和分类实现了结构化，让机器处理起来很方便。元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息。而且整个数据仓库的运行都是基于元数据的，比如抽取调度数据、获取历史数据等。通过元数据，可以很方便地帮助我们管理数据仓库。数据挖掘的流程聊完了数据仓库，我们再来谈谈数据挖掘。数据挖掘不是凭空产生的，它与数据库技术的发展分不开。数据挖掘的一个英文解释叫 Knowledge Discovery in Database，简称 KDD，也就是数据库中的知识发现。在数据挖掘中，有几个非常重要的任务，就是分类、聚类、预测和关联分析。我来解释下这些概念。分类就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。这里需要说明下训练集和测试集的概念。一般来说数据可以划分为训练集和测试集。训练集是用来给机器做训练的，通常是人们整理好训练数据，以及这些数据对应的分类标识。通过训练，机器就产生了自我分类的模型，然后机器就可以拿着这个分类模型，对测试集中的数据进行分类预测。同样如果测试集中，人们已经给出了测试结果，我们就可以用测试结果来做验证，从而了解分类器在测试环境下的表现。聚类人以群分，物以类聚。聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。预测顾名思义，就是通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。关联分析就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。比如我们开头提到的那个案例。数据挖掘要怎么完成这些任务它需要将数据库中的数据经过一系列的加工计算，最终得出有用的信息。这个过程可以用以下步骤来描述。首先，输入我们收集到的数据，然后对数据进行预处理。预处理通常是将数据转化成我们想要的格式，然后我们再对数据进行挖掘，最后通过后处理得到我们想要的信息。那你可能想问，为什么不直接进行数据挖掘，还要进行数据预处理呢？因为在这个过程中，输入的数据通常是从不同渠道采集而来的，所以数据的格式以及质量是参差不齐的，所以我们需要对数据进行预处理。数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。数据清洗主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。数据集成是将多个数据源中的数据存放在一个统一的数据存储中。数据变换就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 01 之间。我会在后面的几节课给你讲解如何对数据进行预处理。数据后处理是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到的是 01 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"NLP word vector - word2vec","slug":"NLP-word-vector-word2vec","date":"2018-06-25T14:07:35.000Z","updated":"2020-04-04T11:05:38.947Z","comments":true,"path":"2018/06/25/NLP-word-vector-word2vec/","link":"","permalink":"cpeixin.cn/2018/06/25/NLP-word-vector-word2vec/","excerpt":"","text":"word vectorFirstly, in recent years, word vector is the basic knowledge of NLP, which is also very important.Among them, words or phrases from vocabulary are mapped to vectors of real Numbers, which is to translate human language symbols into Numbers that can be calculated by machines, which can improve the quality of machine translation.For example, word_1 is expressed as a vector [0,0,1] in articles and statements. There are many models or tools for word-to-vector transformation. Eg: one-hot, n-gram, word2vecword2vecWord2vec is a toolWord2vec contains two models skp-gram and CBOW, as well as two efficient training methods negative sampling and hiearchical softmax. Why introduce word2vec separately, because it can well express the similarity and analogy between different words顺便说说这两个语言模型。统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。","categories":[{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"cpeixin.cn/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"}]},{"title":"【转载】MySQL索引原理","slug":"【转载】MySQL索引原理","date":"2018-06-16T16:36:22.000Z","updated":"2020-09-07T03:28:03.028Z","comments":true,"path":"2018/06/17/【转载】MySQL索引原理/","link":"","permalink":"cpeixin.cn/2018/06/17/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91MySQL%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/","excerpt":"","text":"背景MySQL凭借着出色的性能、低廉的成本、丰富的资源，已经成为绝大多数互联网公司的首选关系型数据库。虽然性能出色，但所谓“好马配好鞍”，如何能够更好的使用它，已经成为开发工程师的必修课，我们经常会从职位描述上看到诸如“精通MySQL”、“SQL语句优化”、“了解数据库原理”等要求。我们知道一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，所以查询语句的优化显然是重中之重。本人从2013年7月份起，一直在美团核心业务系统部做慢查询的优化工作，共计十余个系统，累计解决和积累了上百个慢查询案例。随着业务的复杂性提升，遇到的问题千奇百怪，五花八门，匪夷所思。本文旨在以开发工程师的角度来解释数据库索引的原理和如何优化慢查询。一个慢查询引发的思考12345678910select count(*) from task where status&#x3D;2 and operator_id&#x3D;20839 and operate_time&gt;1371169729 and operate_time&lt;1371174603 and type&#x3D;2;系统使用者反应有一个功能越来越慢，于是工程师找到了上面的SQL。并且兴致冲冲的找到了我，“这个SQL需要优化，给我把每个字段都加上索引”。我很惊讶，问道：“为什么需要每个字段都加上索引？”“把查询的字段都加上索引会更快”，工程师信心满满。“这种情况完全可以建一个联合索引，因为是最左前缀匹配，所以operate_time需要放到最后，而且还需要把其他相关的查询都拿来，需要做一个综合评估。”“联合索引？最左前缀匹配？综合评估？”工程师不禁陷入了沉思。多数情况下，我们知道索引能够提高查询效率，但应该如何建立索引？索引的顺序如何？许多人却只知道大概。其实理解这些概念并不难，而且索引的原理远没有想象的那么复杂。MySQL索引原理索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。磁盘IO与预读前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：various-system-software-hardware-latencies考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。索引的数据结构前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。详解b+树b+树如上图，是一颗b+树，关于b+树的定义可以参见B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。b+树的查找过程如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。b+树性质1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。慢查询优化关于MySQL索引原理是比较枯燥的东西，大家只需要有一个感性的认识，并不需要理解得非常透彻和深入。我们回头来看看一开始我们说的慢查询，了解完索引原理之后，大家是不是有什么想法呢？先总结一下索引的几大基本原则：建索引的几大原则1.最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。3.尽量选择区分度高的列作为索引，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。回到开始的慢查询根据最左匹配原则，最开始的sql语句的索引应该是status、operator_id、type、operate_time的联合索引；其中status、operator_id、type的顺序可以颠倒，所以我才会说，把这个表的所有相关查询都找到，会综合分析；比如还有如下查询：1select * from task where status &#x3D; 0 and type &#x3D; 12 limit 10;1select count(*) from task where status &#x3D; 0 ;那么索引建立成(status,type,operator_id,operate_time)就是非常正确的，因为可以覆盖到所有情况。这个就是利用了索引的最左匹配的原则查询优化神器 - explain命令关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。慢查询优化基本步骤0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）3.order by limit 形式的sql语句让排序的表优先查4.了解业务方使用场景5.加索引时参照建索引的几大原则6.观察结果，不符合预期继续从0分析几个慢查询案例下面几个例子详细解释了如何分析和优化慢查询。复杂语句写法很多情况下，我们写SQL只是为了实现功能，这只是第一步，不同的语句书写方式对于效率往往有本质的差别，这要求我们对mysql的执行计划和索引原则有非常清楚的认识，请看下面的语句：12345678910111213141516171819202122232425262728select distinct cert.emp_id from cm_log cl inner join ( select emp.id as emp_id, emp_cert.id as cert_id from employee emp left join emp_certificate emp_cert on emp.id &#x3D; emp_cert.emp_id where emp.is_deleted&#x3D;0 ) cert on ( cl.ref_table&#x3D;&#39;Employee&#39; and cl.ref_oid&#x3D; cert.emp_id ) or ( cl.ref_table&#x3D;&#39;EmpCertificate&#39; and cl.ref_oid&#x3D; cert.cert_id ) where cl.last_upd_date &gt;&#x3D;&#39;2013-11-07 15:03:00&#39; and cl.last_upd_date&lt;&#x3D;&#39;2013-11-08 16:00:00&#39;;0.先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢153 rows in set (1.87 sec)1.explain12345678+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+| 1 | PRIMARY | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where; Using temporary || 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 63727 | Using where; Using join buffer || 2 | DERIVED | emp | ALL | NULL | NULL | NULL | NULL | 13317 | Using where || 2 | DERIVED | emp_cert | ref | emp_certificate_empid | emp_certificate_empid | 4 | meituanorg.emp.id | 1 | Using index |+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+简述一下执行计划，首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows = 1表示，每个关联都只锁定了一条记录，效率比较高。获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来，注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union all了，因为使用union需要去重的动作，会影响SQL性能。优化过的语句如下：12345678910111213141516171819202122232425262728select emp.id from cm_log cl inner join employee emp on cl.ref_table &#x3D; &#39;Employee&#39; and cl.ref_oid &#x3D; emp.id where cl.last_upd_date &gt;&#x3D;&#39;2013-11-07 15:03:00&#39; and cl.last_upd_date&lt;&#x3D;&#39;2013-11-08 16:00:00&#39; and emp.is_deleted &#x3D; 0 unionselect emp.id from cm_log cl inner join emp_certificate ec on cl.ref_table &#x3D; &#39;EmpCertificate&#39; and cl.ref_oid &#x3D; ec.id inner join employee emp on emp.id &#x3D; ec.emp_id where cl.last_upd_date &gt;&#x3D;&#39;2013-11-07 15:03:00&#39; and cl.last_upd_date&lt;&#x3D;&#39;2013-11-08 16:00:00&#39; and emp.is_deleted &#x3D; 04.不需要了解业务场景，只需要改造的语句和改造之前的语句保持结果一致5.现有索引可以满足，不需要建索引6.用改造后的语句实验一下，只需要10ms 降低了近200倍！1234567891011+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+| 1 | PRIMARY | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where || 1 | PRIMARY | emp | eq_ref | PRIMARY | PRIMARY | 4 | meituanorg.cl.ref_oid | 1 | Using where || 2 | UNION | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where || 2 | UNION | ec | eq_ref | PRIMARY,emp_certificate_empid | PRIMARY | 4 | meituanorg.cl.ref_oid | 1 | || 2 | UNION | emp | eq_ref | PRIMARY | PRIMARY | 4 | meituanorg.ec.emp_id | 1 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | |+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+53 rows in set (0.01 sec)明确应用场景举这个例子的目的在于颠覆我们对列的区分度的认知，一般上我们认为区分度越高的列，越容易锁定更少的记录，但在一些特殊的情况下，这种理论是有局限性的。1234567891011select * from stage_poi sp where sp.accurate_result&#x3D;1 and ( sp.sync_status&#x3D;0 or sp.sync_status&#x3D;2 or sp.sync_status&#x3D;4 );0.先看看运行多长时间,951条数据6.22秒，真的很慢。1951 rows in set (6.22 sec)1.先explain，rows达到了361万，type = ALL表明是全表扫描。12345+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | sp | ALL | NULL | NULL | NULL | NULL | 3613155 | Using where |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+2.所有字段都应用查询返回记录数，因为是单表查询 0已经做过了951条。3.让explain的rows 尽量逼近951。看一下accurate_result = 1的记录数：12345678select count(*),accurate_result from stage_poi group by accurate_result;+----------+-----------------+| count(*) | accurate_result |+----------+-----------------+| 1023 | -1 || 2114655 | 0 || 972815 | 1 |+----------+-----------------+我们看到accurate_result这个字段的区分度非常低，整个表只有-1,0,1三个值，加上索引也无法锁定特别少量的数据。再看一下sync_status字段的情况：1234567select count(*),sync_status from stage_poi group by sync_status;+----------+-------------+| count(*) | sync_status |+----------+-------------+| 3080 | 0 || 3085413 | 3 |+----------+-------------+同样的区分度也很低，根据理论，也不适合建立索引。问题分析到这，好像得出了这个表无法优化的结论，两个列的区分度都很低，即便加上索引也只能适应这种情况，很难做普遍性的优化，比如当sync_status 0、3分布的很平均，那么锁定记录也是百万级别的。4.找业务方去沟通，看看使用场景。业务方是这么来使用这个SQL语句的，每隔五分钟会扫描符合条件的数据，处理完成后把sync_status这个字段变成1,五分钟符合条件的记录数并不会太多，1000个左右。了解了业务方的使用场景后，优化这个SQL就变得简单了，因为业务方保证了数据的不平衡，如果加上索引可以过滤掉绝大部分不需要的数据。5.根据建立索引规则，使用如下语句建立索引1alter table stage_poi add index idx_acc_status(accurate_result,sync_status);6.观察预期结果,发现只需要200ms，快了30多倍。1952 rows in set (0.20 sec)我们再来回顾一下分析问题的过程，单表查询相对来说比较好优化，大部分时候只需要把where条件里面的字段依照规则加上索引就好，如果只是这种“无脑”优化的话，显然一些区分度非常低的列，不应该加索引的列也会被加上索引，这样会对插入、更新性能造成严重的影响，同时也有可能影响其它的查询语句。所以我们第4步调差SQL的使用场景非常关键，我们只有知道这个业务场景，才能更好地辅助我们更好的分析和优化查询语句。无法优化的语句12345678910111213141516171819202122232425262728293031323334353637select c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c inner join contact_branch cb on c.id &#x3D; cb.contact_id inner join branch_user bu on cb.branch_id &#x3D; bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id &#x3D; bu.user_id and oei.node_left &gt;&#x3D; 2875 and oei.node_right &lt;&#x3D; 10802 and oei.org_category &#x3D; - 1 order by c.created_time desc limit 0 , 10;还是几个步骤。0.先看语句运行多长时间，10条记录用了13秒，已经不可忍受。110 rows in set (13.06 sec)1.explain12345678+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+| 1 | SIMPLE | oei | ref | idx_category_left_right,idx_data_id | idx_category_left_right | 5 | const | 8849 | Using where; Using temporary; Using filesort || 1 | SIMPLE | bu | ref | PRIMARY,idx_userid_status | idx_userid_status | 4 | meituancrm.oei.data_id | 76 | Using where; Using index || 1 | SIMPLE | cb | ref | idx_branch_id,idx_contact_branch_id | idx_branch_id | 4 | meituancrm.bu.branch_id | 1 | || 1 | SIMPLE | c | eq_ref | PRIMARY | PRIMARY | 108 | meituancrm.cb.contact_id | 1 | |+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+从执行计划上看，mysql先查org_emp_info表扫描8849记录，再用索引idx_userid_status关联branch_user表，再用索引idx_branch_id关联contact_branch表，最后主键关联contact表。rows返回的都非常少，看不到有什么异常情况。我们在看一下语句，发现后面有order by + limit组合，会不会是排序量太大搞的？于是我们简化SQL，去掉后面的order by 和 limit，看看到底用了多少记录来排序。12345678910111213141516171819202122232425select count(*)from contact c inner join contact_branch cb on c.id &#x3D; cb.contact_id inner join branch_user bu on cb.branch_id &#x3D; bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id &#x3D; bu.user_id and oei.node_left &gt;&#x3D; 2875 and oei.node_right &lt;&#x3D; 10802 and oei.org_category &#x3D; - 1 +----------+| count(*) |+----------+| 778878 |+----------+1 row in set (5.19 sec)发现排序之前居然锁定了778878条记录，如果针对70万的结果集排序，将是灾难性的，怪不得这么慢，那我们能不能换个思路，先根据contact的created_time排序，再来join会不会比较快呢？于是改造成下面的语句，也可以用straight_join来优化：12345678910111213141516171819202122232425262728293031323334353637383940414243select c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c where exists ( select 1 from contact_branch cb inner join branch_user bu on cb.branch_id &#x3D; bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id &#x3D; bu.user_id and oei.node_left &gt;&#x3D; 2875 and oei.node_right &lt;&#x3D; 10802 and oei.org_category &#x3D; - 1 where c.id &#x3D; cb.contact_id ) order by c.created_time desc limit 0 , 10;验证一下效果 预计在1ms内，提升了13000多倍！110 rows in set (0.00 sec)本以为至此大工告成，但我们在前面的分析中漏了一个细节，先排序再join和先join再排序理论上开销是一样的，为何提升这么多是因为有一个limit！大致执行过程是：mysql先按索引排序得到前10条记录，然后再去join过滤，当发现不够10条的时候，再次去10条，再次join，这显然在内层join过滤的数据非常多的时候，将是灾难的，极端情况，内层一条数据都找不到，mysql还傻乎乎的每次取10条，几乎遍历了这个数据表！用不同参数的SQL试验下：1234567891011121314151617181920212223242526272829303132333435363738394041424344select sql_no_cache c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c where exists ( select 1 from contact_branch cb inner join branch_user bu on cb.branch_id &#x3D; bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id &#x3D; bu.user_id and oei.node_left &gt;&#x3D; 2875 and oei.node_right &lt;&#x3D; 2875 and oei.org_category &#x3D; - 1 where c.id &#x3D; cb.contact_id ) order by c.created_time desc limit 0 , 10;Empty set (2 min 18.99 sec)2 min 18.99 sec！比之前的情况还糟糕很多。由于mysql的nested loop机制，遇到这种情况，基本是无法优化的。这条语句最终也只能交给应用系统去优化自己的逻辑了。通过这个例子我们可以看到，并不是所有语句都能优化，而往往我们优化时，由于SQL用例回归时落掉一些极端情况，会造成比原来还严重的后果。所以，第一：不要指望所有语句都能通过SQL优化，第二：不要过于自信，只针对具体case来优化，而忽略了更复杂的情况。慢查询的案例就分析到这儿，以上只是一些比较典型的案例。我们在优化过程中遇到过超过1000行，涉及到16个表join的“垃圾SQL”，也遇到过线上线下数据库差异导致应用直接被慢查询拖死，也遇到过varchar等值比较没有写单引号，还遇到过笛卡尔积查询直接把从库搞死。再多的案例其实也只是一些经验的积累，如果我们熟悉查询优化器、索引的内部原理，那么分析这些案例就变得特别简单了。写在后面的话本文以一个慢查询案例引入了MySQL索引原理、优化慢查询的一些方法论;并针对遇到的典型案例做了详细的分析。其实做了这么长时间的语句优化后才发现，任何数据库层面的优化都抵不上应用系统的优化，同样是MySQL，可以用来支撑Google/FaceBook/Taobao应用，但可能连你的个人网站都撑不住。套用最近比较流行的话：“查询容易，优化不易，且写且珍惜！”转载自美团技术团队","categories":[{"name":"DataBases","slug":"DataBases","permalink":"cpeixin.cn/categories/DataBases/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"cpeixin.cn/tags/mysql/"}]},{"title":"Spark SQL Join实现原理","slug":"Spark-SQL-Join实现原理","date":"2018-06-08T09:56:03.000Z","updated":"2020-09-08T09:57:32.705Z","comments":true,"path":"2018/06/08/Spark-SQL-Join实现原理/","link":"","permalink":"cpeixin.cn/2018/06/08/Spark-SQL-Join%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"Join背景当前SparkSQL支持三种join算法：Shuffle Hash Join、Broadcast Hash Join以及Sort Merge Join。其中前两者归根到底都属于Hash Join，只不过在Hash Join之前需要先Shuffle还是先Broadcast。其实，Hash Join算法来自于传统数据库，而Shuffle和Broadcast是大数据在分布式情况下的概念，两者结合的产物。因此可以说，大数据的根就是传统数据库。Hash Join是内核。Spark Join的分类和实现机制上图是Spark Join的分类和使用。Hash Join先来看看这样一条SQL语句：select * from order,item where item.id = order.i_id，参与join的两张表是order和item，join key分别是item.id以及order.i_id。现在假设Join采用的是hash join算法，整个过程会经历三步：确定Build Table以及Probe Table：这个概念比较重要，Build Table会被构建成以join key为key的hash table，而Probe Table使用join key在这张hash table表中寻找符合条件的行，然后进行join链接。Build表和Probe表是Spark决定的。通常情况下，小表会被作为Build Table，较大的表会被作为Probe Table。构建Hash Table：依次读取Build Table(item)的数据，对于每一条数据根据Join Key(item.id)进行hash，hash到对应的bucket中(类似于HashMap的原理)，最后会生成一张HashTable，HashTable会缓存在内存中，如果内存放不下会dump到磁盘中。匹配：生成Hash Table后，在依次扫描Probe Table(order)的数据，使用相同的hash函数(在spark中，实际上就是要使用相同的partitioner)在Hash Table中寻找hash(join key)相同的值，如果匹配成功就将两者join在一起。Broadcast Hash Join当Join的一张表很小的时候，使用broadcast hash join。Broadcast Hash Join的条件有以下几个：被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的信息，默认是10M；基表不能被广播，比如left outer join时，只能广播右表。broadcast hash join可以分为两步：broadcast阶段：将小表广播到所有的executor上，广播的算法有很多，最简单的是先发给driver，driver再统一分发给所有的executor，要不就是基于bittorrete的p2p思路；hash join阶段：在每个executor上执行 hash join，小表构建为hash table，大表的分区数据匹配hash table中的数据。Sort Merge Join当两个表都非常大时，SparkSQL采用了一种全新的方案来对表进行Join，即Sort Merge Join。这种方式不用将一侧数据全部加载后再进行hash join，但需要在join前将数据进行排序。首先将两张表按照join key进行重新shuffle，保证join key值相同的记录会被分在相应的分区，分区后对每个分区内的数据进行排序，排序后再对相应的分区内的记录进行连接。可以看出，无论分区有多大，Sort Merge Join都不用把一侧的数据全部加载到内存中，而是即用即丢；因为两个序列都有有序的，从头遍历，碰到key相同的就输出，如果不同，左边小就继续取左边，反之取右边。从而大大提高了大数据量下sql join的稳定性。整个过程分为三个步骤：shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理sort阶段：对单个分区节点的两表数据，分别进行排序merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则继续取更小一边的key。经过上文的分析，很明显可以得出这几种join的代价关系：cost(Broadcast Hash Join)&lt; cost(Shuffle Hash Join) &lt; cost(Sort Merge Join)，数据仓库设计时最好避免大表与大表的join查询，SparkSQL也可以根据内存资源、带宽资源适量将参数spark.sql. autoBroadcastJoinThreshold调大，让更多join实际执行为Broadcast Hash Join。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"python - 优雅的程序设计","slug":"优雅的程序设计-python「工匠」","date":"2018-05-24T15:01:20.000Z","updated":"2020-04-04T12:00:22.701Z","comments":true,"path":"2018/05/24/优雅的程序设计-python「工匠」/","link":"","permalink":"cpeixin.cn/2018/05/24/%E4%BC%98%E9%9B%85%E7%9A%84%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-python%E3%80%8C%E5%B7%A5%E5%8C%A0%E3%80%8D/","excerpt":"","text":"今天是优秀博客的搬运工😂记录两篇看完很爽的文章，编程也是一门手艺，也可以说是一门艺术，编程的人要把自己当成工匠，对自己的作品不断的打磨，打磨成一个可以供人观赏的艺术品。Python 工匠：善用变量来改善代码质量Python 工匠：编写条件分支代码的技巧","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"Mac 安装 redis","slug":"Mac-安装-redis","date":"2018-04-30T13:53:24.000Z","updated":"2020-05-22T15:22:30.207Z","comments":true,"path":"2018/04/30/Mac-安装-redis/","link":"","permalink":"cpeixin.cn/2018/04/30/Mac-%E5%AE%89%E8%A3%85-redis/","excerpt":"","text":"Redis安装下载软件包访问 https://redis.io/download 下载 Stable 版本或者1wget http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-6.0.3.tar.gz解压软件包1tar -zxvf redis-6.0.3.tar.gz移动到指定目录1sudo mv redis-6.0.3 &#x2F;usr&#x2F;local&#x2F;编译redis1sudo make编译test1sudo make test安装1sudo make install运行1redis-server配置后台启动找到redis.conf 并修改 daemonize no 为 daemonize yes ，这样就可以默认启动就后台运行1redis-server &#x2F;usr&#x2F;local&#x2F;redis-6.0.3&#x2F;redis.confRedis Desktop Manager安装下载 Redis Desktop Manager可以去官网下载，也可以去RedisDesktopManager下载免费tar包自己编译，而我选择了百度网盘下载😄https://pan.baidu.com/s/1tpnvkE9R63U9VVMfw5xodQ提取码：zd7y","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"}]},{"title":"Hive 数据倾斜&优化","slug":"Hive-数据倾斜-优化","date":"2018-04-15T14:29:56.000Z","updated":"2020-06-15T14:32:25.992Z","comments":true,"path":"2018/04/15/Hive-数据倾斜-优化/","link":"","permalink":"cpeixin.cn/2018/04/15/Hive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C-%E4%BC%98%E5%8C%96/","excerpt":"","text":"数据倾斜的原因注意的SQL操作关键词情形后果Join其中一个表较小，但是key集中分发到某一个或几个Reduce上的数据远高于平均值大表与大表，但是分桶的判断字段0值或空值过多这些空值都由一个reduce处理，灰常慢group bygroup by 维度过小，某值的数量过多处理某值的reduce灰常耗时Count Distinct某特殊值过多处理此特殊值的reduce耗时原因1)、key分布不均匀2)、业务数据本身的特性3)、建表时考虑不周4)、某些SQL语句本身就有数据倾斜表现任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长。数据倾斜的解决方案参数调节1hive.map.aggr=trueMap 端部分聚合，相当于Combiner1hive.groupby.skewindata=true有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。SQL语句调节如何Join关于驱动表的选取，选用join key分布最均匀的表作为驱动表做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。大小表Join使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.大表Join大表把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。count distinct大量相同特殊值count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。group by维度过小采用sum() group by的方式来替换count(distinct)完成计算。特殊情况特殊处理**在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理。最后union回去。典型的业务场景空值产生的数据倾斜场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。解决方法1：** user_id为空的不参与关联（红色字体为修改后）1234567select * from log a join users b on a.user_id is not null and a.user_id = b.user_idunion allselect * from log a where a.user_id is null;解决方法2 ：赋与空值分新的key值1234select * from log a left outer join users b on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id;结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 , ’’, null 等) 产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。**不同数据类型关联产生数据倾斜场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。解决方法：把数字类型转换成字符串类型123select * from users a left outer join logs b on a.usr_id = cast(b.user_id as string)**小表不小不大，怎么用 map join 解决倾斜问题使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:123select * from log a left outer join users b on a.user_id = b.user_id;users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。解决方法：12345678select /*+mapjoin(x)*/* from log a left outer join ( select /*+mapjoin(c)*/d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id ) x on a.user_id = b.user_id;假如，log里user_id有上百万个，这就又回到原来map join问题。所幸，每日的会员uv不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。所以这个方法能解决很多场景下的数据倾斜问题。GROUP BY替代COUNT(DISTINCT)达到优化效果计算 uv 的时候，经常会用到 COUNT(DISTINCT)，但在数据比较倾斜的时候 COUNT(DISTINCT) 会比较慢。这时可以尝试用 GROUP BY 改写代码计算 uv。12INSERT OVERWRITE TABLE s_dw_tanx_adzone_uv PARTITION (ds=20120329) SELECT 20120329 AS thedate,adzoneid,COUNT(DISTINCT acookie) AS uv FROM s_ods_log_tanx_pv t WHERE t.ds=20120329 GROUP BY adzoneid关于COUNT(DISTINCT)的数据倾斜问题不能一概而论，要依情况而定，下面是我测试的一组数据：测试数据：169857条*统计每日IP *12CREATE TABLE ip_2014_12_29 AS SELECT COUNT(DISTINCT ip) AS IP FROM logdfs WHERE logdate='2014_12_29'; 耗时：24.805 seconds*统计每日IP（改造） *12CREATE TABLE ip_2014_12_29 AS SELECT COUNT(1) AS IP FROM (SELECT DISTINCT ip from logdfs WHERE logdate='2014_12_29') tmp; 耗时：46.833 seconds测试结果表名：明显改造后的语句比之前耗时，这是因为改造后的语句有2个SELECT，多了一个job，这样在数据量小的时候，数据不会存在倾斜问题。解决Hive对UNION ALL优化的短板Hive 对 union all 的优化的特性：对 union all 优化只局限于非嵌套查询。消灭子查询内的 group by示例 1：子查询内有 group by123SELECT * FROM (SELECT * FROM t1 GROUP BY c1,c2,c3 UNION ALL SELECT * FROM t2 GROUP BY c1,c2,c3)t3 GROUP BY c1,c2,c3从业务逻辑上说，子查询内的 GROUP BY 怎么都看显得多余（功能上的多余，除非有 COUNT(DISTINCT)），如果不是因为 Hive Bug 或者性能上的考量（曾经出现如果不执行子查询 GROUP BY，数据得不到正确的结果的 Hive Bug）。所以这个 Hive 按经验转换成如下所示：1SELECT * FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t2)t3 GROUP BY c1,c2,c3调优结果：经过测试，并未出现 union all 的 Hive Bug，数据是一致的。MapReduce 的 作业数由 3 减少到 1。t1 相当于一个目录，t2 相当于一个目录，对 Map/Reduce 程序来说，t1，t2 可以作为 Map/Reduce 作业的 mutli inputs。这可以通过一个 Map/Reduce 来解决这个问题。Hadoop 的 计算框架，不怕数据多，就怕作业数多。但如果换成是其他计算平台如 Oracle，那就不一定了，因为把大的输入拆成两个输入， 分别排序汇总后 merge（假如两个子排序是并行的话），是有可能性能更优的（比如希尔排 序比冒泡排序的性能更优）。消灭子查询内的 COUNT(DISTINCT)，MAX，MIN1234SELECT * FROM (SELECT * FROM t1 UNION ALL SELECT c1,c2,c3 COUNT(DISTINCT c4) FROM t2 GROUP BY c1,c2,c3) t3 GROUP BY c1,c2,c3;由于子查询里头有 COUNT(DISTINCT)操作，直接去 GROUP BY 将达不到业务目标。这时采用 临时表消灭 COUNT(DISTINCT)作业不但能解决倾斜问题，还能有效减少 jobs。123456INSERT t4 SELECT c1,c2,c3,c4 FROM t2 GROUP BY c1,c2,c3; SELECT c1,c2,c3,SUM(income),SUM(uv) FROM (SELECT c1,c2,c3,income,0 AS uv FROM t1 UNION ALL SELECT c1,c2,c3,0 AS income,1 AS uv FROM t2) t3 GROUP BY c1,c2,c3;job 数是 2，减少一半，而且两次 Map/Reduce 比 COUNT(DISTINCT)效率更高。调优结果：千万级别的类目表，member 表，与 10 亿级得商品表关联。原先 1963s 的任务经过调整，1152s 即完成。消灭子查询内的 JOIN123SELECT * FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t4 UNION ALL SELECT * FROM t2 JOIN t3 ON t2.id=t3.id) x GROUP BY c1,c2;上面代码运行会有 5 个 jobs。加入先 JOIN 生存临时表的话 t5，然后 UNION ALL，会变成 2 个 jobs。123INSERT OVERWRITE TABLE t5 SELECT * FROM t2 JOIN t3 ON t2.id=t3.id; SELECT * FROM (t1 UNION ALL t4 UNION ALL t5);调优结果显示：针对千万级别的广告位表，由原先 5 个 Job 共 15 分钟，分解为 2 个 job 一个 8-10 分钟，一个3分钟。总结使map的输出数据更均匀的分布到reduce中去，是我们的最终目标。由于Hash算法的局限性，按key Hash会或多或少的造成数据倾斜。大量经验表明数据倾斜的原因是人为的建表疏忽或业务逻辑可以规避的。在此给出较为通用的步骤：1、采样log表，哪些user_id比较倾斜，得到一个结果表tmp1。由于对计算框架来说，所有的数据过来，他都是不知道数据分布情况的，所以采样是并不可少的。2、数据的分布符合社会学统计规则，贫富不均。倾斜的key不会太多，就像一个社会的富人不多，奇特的人不多一样。所以tmp1记录数会很少。把tmp1和users做map join生成tmp2,把tmp2读到distribute file cache。这是一个map过程。3、map读入users和log，假如记录来自log,则检查user_id是否在tmp2里，如果是，输出到本地文件a,否则生成&lt;user_id,value&gt;的key,value对，假如记录来自member,生成&lt;user_id,value&gt;的key,value对，进入reduce阶段。4、最终把a文件，把Stage3 reduce阶段输出的文件合并起写到hdfs。如果确认业务需要这样倾斜的逻辑，考虑以下的优化方案：1、对于join，在判断小表不大于1G的情况下，使用map join2、对于group by或distinct，设定 hive.groupby.skewindata=true3、尽量使用上述的SQL语句调节进行优化hadoop处理数据的过程，有几个显著的特征:不怕数据多，就怕数据倾斜。对jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，没半小时是跑不完的。map reduce作业初始化的时间是比较长的。对sum，count来说，不存在数据倾斜问题。对count(distinct ),效率较低，数据量一多，准出问题，如果是多count(distinct )效率更低。优化可以从几个方面着手好的模型设计事半功倍。解决数据倾斜问题。减少job数。设置合理的map reduce的task数，能有效提升性能。(比如，10w+级别的计算，用160个reduce，那是相当的浪费，1个足够)。自己动手写sql解决数据倾斜问题是个不错的选择。set hive.groupby.skewindata=true;这是通用的算法优化，但算法优化总是漠视业务，习惯性提供通用的解决方法。 Etl开发人员更了解业务，更了解数据，所以通过业务逻辑解决倾斜的方法往往更精确，更有效。对count(distinct)采取漠视的方法，尤其数据大的时候很容易产生倾斜问题，不抱侥幸心理。自己动手，丰衣足食。对小文件进行合并，是行至有效的提高调度效率的方法，假如我们的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的影响。优化时把握整体，单个作业最优不如整体最优。细节上就是**去除查询中不需要的columnWhere条件判断等在TableScan阶段就进行过滤利用Partition信息，只读取符合条件的PartitionMap端join，以大表作驱动，小表载入所有mapper内存中调整Join顺序，确保以大表作为驱动表对于数据分布不均衡的表Group by时，为避免数据集中到少数的reducer上，分成两个map-reduce阶段。第一个阶段先用Distinct列进行shuffle，然后在reduce端部分聚合，减小数据规模，第二个map-reduce阶段再按group-by列聚合。在map端用hash进行部分聚合，减小reduce端数据处理规模。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"OLTP与OLAP：有何区别？","slug":"OLTP与OLAP：有何区别？","date":"2018-04-10T14:25:18.000Z","updated":"2020-05-11T05:14:44.588Z","comments":true,"path":"2018/04/10/OLTP与OLAP：有何区别？/","link":"","permalink":"cpeixin.cn/2018/04/10/OLTP%E4%B8%8EOLAP%EF%BC%9A%E6%9C%89%E4%BD%95%E5%8C%BA%E5%88%AB%EF%BC%9F/","excerpt":"","text":"什么是OLAP？联机分析处理，一类软件工具，可为业务决策提供数据分析。OLAP系统允许用户一次分析来自多个数据库系统的数据库信息。主要目标是数据分析而不是数据处理。什么是OLTP？在线事务处理（简称OLTP）在3层体系结构中支持面向事务的应用程序。OLTP管理组织的日常事务。主要目标是数据处理而不是数据分析OLAP示例任何数据仓库系统都是OLAP系统。OLAP的用途如下公司可能会将9月的手机销售与10月的销售进行比较，然后将这些结果与可能存储在正确数据库中的另一个位置进行比较。亚马逊分析其客户的购买情况，以提供个性化的主页，其中包含其客户可能感兴趣的产品。OLTP系统示例OLTP系统的一个示例是ATM中心。假设一对夫妇在银行有一个联名账户。一天，两者都同时在精确的同一时间到达不同的ATM中心，并希望提取其银行帐户中的总金额。但是，首先完成身份验证过程的人将能够赚钱。在这种情况下，OLTP系统确保提款金额永远不会超过银行中存在的金额。这里要注意的关键是OLTP系统针对事务优势进行了优化，而不是数据分析。OLTP系统的其他示例是：网上银行业务网上机票预订发送短信订单输入将书添加到购物车使用OLAP服务的好处OLAP为所有类型的业务分析需求（包括计划，预算，预测和分析）创建一个平台。OLAP的主要好处是信息和计算的一致性。轻松对用户和对象施加安全限制，以符合法规并保护敏感数据。OLTP方法的好处它管理组织的日常交易。OLTP通过简化单个流程来扩大组织的客户基础。OLAP服务的缺点实施和维护依赖于IT专业人员，因为传统的OLAP工具需要复杂的建模过程。OLAP工具需要各个部门人员之间的合作才能有效，而这通常是不可能的。OLTP方法的缺点如果OLTP系统面临硬件故障，则在线交易会受到严重影响。OLTP系统允许多个用户同时访问和更改同一数据，这多次创造了前所未有的局面。OLTP和OLAP之间的区别参量OLTPOLAP处理这是一个在线交易系统。它管理数据库修改。OLAP是一个在线分析和数据检索过程。特性它的特点是大量的短期在线交易。它的特点是数据量大。功能性OLTP是一个在线数据库修改系统。OLAP是一个在线数据库查询管理系统。方法OLTP使用传统的DBMS。OLAP使用数据仓库。询问从数据库中插入，更新和删除信息。主要是选择操作表OLTP数据库中的表已标准化。OLAP数据库中的表未规范化。资源OLTP及其事务是数据源。不同的OLTP数据库成为OLAP的数据源。数据的完整性OLTP数据库必须维护数据完整性约束。OLAP数据库不会经常修改。因此，数据完整性不是问题。响应时间它的响应时间以毫秒为单位。响应时间以秒为单位。资料品质OLTP数据库中的数据始终是详细和组织的。OLAP流程中的数据可能没有组织。有用性它有助于控制和运行基本业务任务。它有助于计划，问题解决和决策支持。运作方式允许读/写操作。只读，很少写。听众这是一个面向市场的过程。这是一个以客户为导向的过程。查询类型此过程中的查询是标准化且简单的。涉及聚合的复杂查询。后备完整的数据备份与增量备份相结合。OLAP仅需要不时备份。与OLTP相比，备份并不重要设计数据库设计是面向应用程序的。示例：数据库设计随零售，航空公司，银行等行业的变化而变化数据库设计是面向主题的。示例：数据库设计随销售，市场营销，采购等主题而变化。用户类型数据关键用户（如业务员，DBA和数据库专业人员）使用它。由数据知识用户（例如工人，经理和CEO）使用。目的专为实时业务运营而设计。设计用于按类别和属性分析业务度量。绩效指标事务吞吐量是性能指标查询吞吐量是性能指标。用户数这种数据库用户允许成千上万的用户。这种数据库仅允许数百个用户。生产率它有助于提高用户的自助服务和生产率帮助提高业务分析师的生产率。挑战从历史上看，数据仓库一直是一个开发项目，可能证明构建成本很高。OLAP多维数据集不是开放的SQL Server数据仓库。因此，技术知识和经验对于管理OLAP服务器至关重要。处理它为日常使用的数据提供了快速的结果。它确保对查询的响应更快，更一致。特性它易于创建和维护。它使用户可以在电子表格的帮助下创建视图。样式OLTP被设计为具有快速响应时间，低数据冗余并已标准化。数据仓库是唯一创建的，因此它可以集成不同的数据源以构建统一的数据库关键区别在线分析处理（OLAP）是一类软件工具，可以分析存储在数据库中的数据，而在线事务处理（OLTP）支持3层体系结构中面向事务的应用程序。OLAP为所有类型的业务分析需求（包括计划，预算，预测和分析）创建一个平台，而OLTP对管理组织的日常事务很有用。OLAP的特点是数据量大，而OLTP的特点是大量的短时间在线交易。在OLAP中，数据仓库是唯一创建的，因此它可以集成不同的数据源以构建统一的数据库，而OLTP使用传统的DBMS。","categories":[{"name":"DataBase","slug":"DataBase","permalink":"cpeixin.cn/categories/DataBase/"}],"tags":[{"name":"OLAP","slug":"OLAP","permalink":"cpeixin.cn/tags/OLAP/"}]},{"title":"MySQL中如何实现事务","slug":"MySQL中如何实现事务","date":"2018-03-10T14:04:36.000Z","updated":"2020-05-10T14:06:56.435Z","comments":true,"path":"2018/03/10/MySQL中如何实现事务/","link":"","permalink":"cpeixin.cn/2018/03/10/MySQL%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"事务（Transaction），一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。事务其实就是并发控制的基本单位；特性事务是恢复和并发控制的基本单位。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。原子性（atomicity）。一个事务是一个不可分割的工作单位，事务中包括的操作要么都做，要么都不做。一致性（consistency）。事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。隔离性（isolation）。一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。持久性（durability）。持久性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。MySQL怎么实现事务的？事务是MySQL等关系型数据库区别于NoSQL的重要方面，是保证数据一致性的重要手段。MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你既需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！MySQL在架构方面分为三层：第一层：处理客户端连接、授权认证等。第二层：服务器层，负责查询语句的解析、优化、缓存以及内置函数的实现、存储过程等。第三层：存储引擎，负责MySQL中数据的存储和提取。MySQL中服务器层不管理事务，事务是由存储引擎实现的。**MySQL支持事务的存储引擎有InnoDB、BDB Cluster等，其中InnoDB的使用最为广泛；其他存储引擎不支持事务，如MyIsam、Memory等。实现原理 #### 原子性如果事务不具备原子性，那么就没办法保证同一个事务中的所有操作都被执行或者未被执行了，整个数据库系统就既不可用也不可信。想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚。在 MySQL 中，恢复机制是通过_回滚日志_（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。回滚日志除了能够在发生错误或者用户执行 ROLLBACK 时提供回滚相关的信息，它还能够在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志逻辑地将数据库中的修改撤销掉看，可以理解为，我们在事务中使用的每一条 INSERT 都对应了一条 DELETE，每一条 UPDATE 也都对应一条相反的 UPDATE 语句。持久性事务的持久性就体现在，一旦事务被提交，那么数据一定会被写入到数据库中并持久存储起来。与原子性一样，事务的持久性也是通过日志来实现的，MySQL 使用重做日志（redo log）实现事务的持久性，重做日志由两部分组成，一是内存中的重做日志缓冲区，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在磁盘上的重做日志文件，它是持久的。当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上，在 InnoDB 中，重做日志都是以 512 字节的块的形式进行存储的，同时因为块的大小与磁盘扇区大小相同，所以重做日志的写入可以保证原子性，不会由于机器断电导致重做日志仅写入一半并留下脏数据。在MySQL中还存在binlog(二进制日志)也可以记录写操作并用于数据的恢复，但二者是有着根本的不同的：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。隔离性与原子性、持久性侧重于研究事务本身不同，隔离性研究的是不同事务之间的相互影响。**隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们仅考虑最简单的读操作和写操作(暂时不考虑带锁读等特殊操作)，那么隔离性的探讨，主要可以分为两个方面：(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性概括来说，InnoDB实现的重复读，通过锁机制、数据的隐藏列、undo log和类next-key lock，实现了一定程度的隔离性，可以满足大多数场景的需要。一致性一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。**可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。实现一致性的措施包括：保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致","categories":[{"name":"DataBase","slug":"DataBase","permalink":"cpeixin.cn/categories/DataBase/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"cpeixin.cn/tags/mysql/"}]},{"title":"Docker three elements","slug":"Docker-three-elements","date":"2018-01-02T13:01:05.000Z","updated":"2020-04-04T12:00:32.855Z","comments":true,"path":"2018/01/02/Docker-three-elements/","link":"","permalink":"cpeixin.cn/2018/01/02/Docker-three-elements/","excerpt":"","text":"whatbuild, ship and run any app anywhereDocker is designed to avoid a series of problems caused by inconsistencies between development and production environments during the process of program delivery and deployment between development engineers and operations engineers. Through docker, the development engineer packages the program code, running system, software version, description document and so on in a image file, and passes the image file to the operation and maintenance engineer for deployment. For example, if you buy a fish tank with water in it from a pet store, the fish can live in your home without any other operation.At present, there is no need to discuss the struggle between docker and virtual machine, because major Internet companies are using docker technology, and can expand capacity in a short time, we can know that in the current high concurrency business scenario, from practicality and convenience, virtual machine is unable to defeat docker.Simply, docker’s centos image is only 200MB, while the virtual machine image is about 2GB. Docker makes use of the resources of the host and only copies the Linux kernel, and docker is no need Hypervisor ,which can be said to be a stripped-down version of the virtual machinesoftware architectureFocus on understanding the three elementsHowDocker has many commands, which are not complicated. Just like Linux commands, it is recommended to find them on the official website and practice them a lotdocker imagesdocker run [options]docker build [options]and so on …Docker imagebase on UnionFSimage = fs1 + fs2 + … + fnIt looks like an image is a file system, but it’s actually an image that’s made up of layers of file systemsAn available image file is made up of multiple base image layers superimposed, all read-onlyFor example,Tomcat image = kernel image + centos image + jdk image + tomcat imageDocker containerdocker container = docker run -it imageWhen docker image is launched, a docker container will be instantiated. In principle, a layer of writable files is added to the top layer of multiple read-only docker images superimposed together, thus generating the running docker containerDocker volumes is very important and you can check the official website for detailed usageDockerfileDockerfile is similar to a key/value configuration file, which is composed of the construction word in the following figure. Starting From ‘From’, other construction words are listed according to requirements to tell the Dockerfile what to do and what image to generate.Note the difference between CMD and ENTRYPOINT build wordsDockerfile Docker image Docker container， So what is the relationship between these threeDockerfile–&gt;(build)–&gt;Docker image–&gt;(run)–&gt;Docker containerDocker RegistryA Repository is a place to store images. Like Node’s NPM; Python PyPi. Currently, Docker officially maintains a public repository, and most of the requirements can be implemented by directly downloading the image from the Docker Hub. Similar to GitHub, in the process of making image or Dockerfile, we draw the base image, i.e. From [base image], From the public library.","categories":[{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"}]},{"title":"Docker install and first Dockerfile","slug":"Docker-install-and-first-Dockerfile","date":"2018-01-01T12:00:05.000Z","updated":"2020-04-04T12:00:27.785Z","comments":true,"path":"2018/01/01/Docker-install-and-first-Dockerfile/","link":"","permalink":"cpeixin.cn/2018/01/01/Docker-install-and-first-Dockerfile/","excerpt":"","text":"prefaceThis article only explains the installation of Docker and the creation of Dockerfile. Dockerfile is composed of different commands in different requirements, which can be understood as a configuration file. However, only a demo is shown here to show the basic usage of DockerServer environmentlinux centos 7.6installstep 1If you have installed an older version of docker, you need to uninstall it first12345678910sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-enginestep 2install dependencies123sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2step 3set stable Repository123sudo yum-config-manager \\ --add-repo \\ https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;centos&#x2F;docker-ce.repostep 4install docker1sudo yum install docker-ce docker-ce-cli containerd.iostep 5start docker service1sudo systemctl start dockerstep 6Run a demo to see if the installation was successful: The hello-world image is not in your local, but when you run the command, docker will pulls the helloworld image from the remote repository1sudo docker run hello-worldcreate a demostep 1create dockerfileChoose any directory and create a dockerfile.It is suggested to name ‘Dockerfile’, because by default docker will run the file called ‘Dockerfile’ in the current directory. If you give it a different name, add the -f parameter and the path of the dockerfile12345678vim DockerfileFROM nginxMAINTAINER author &lt;email&gt;RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;&#x2F;h1&gt;&#39; &gt; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html:wqstep 2build image1docker build -t test&#x2F;hello-world: .-t is to set the repository and imagetest is repository namehello-world as the image namestep 3run image1docker run --name hello -d -p 8080:80 test&#x2F;hellostep 4Browser input http://localhost:8080/finalThis is a simple example of using Dockerfile to build the image and run the container!","categories":[{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"}]},{"title":"Spark DataFrame withColumn","slug":"Spark-DataFrame-withColumn","date":"2017-10-03T05:53:39.000Z","updated":"2020-06-14T05:58:24.744Z","comments":true,"path":"2017/10/03/Spark-DataFrame-withColumn/","link":"","permalink":"cpeixin.cn/2017/10/03/Spark-DataFrame-withColumn/","excerpt":"","text":"Spark withColumn()函数用于重命名，更改值，转换现有DataFrame列的数据类型，还可以用于创建新列，在本文中，我将通过Scala和Pyspark示例向您介绍常用的DataFrame列操作。首先，让我们创建一个要使用的DataFrame。123456789101112131415val data = Seq(Row(Row(\"James \",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), Row(Row(\"Michael \",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), Row(Row(\"Robert \",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), Row(Row(\"Maria \",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), Row(Row(\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\"))val schema = new StructType() .add(\"name\",new StructType() .add(\"firstname\",StringType) .add(\"middlename\",StringType) .add(\"lastname\",StringType)) .add(\"dob\",StringType) .add(\"gender\",StringType) .add(\"salary\",StringType)val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)Spark withColumn –更改列的数据类型通过在DataFrame上使用Spark withColumn并在列上使用强制转换功能，我们可以更改DataFrame列的数据类型。下面的语句将“工资”列的数据类型从字符串更改为整数。1df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))更改现有列的值withColumn() 函数也可以用于更新现有列的值。为了更改值，将现有的列名作为第一个参数传递，将值分配为第二个列。请注意，第二个参数应为Columntype。1df.withColumn(\"salary\",col(\"salary\")*100)此代码段将“ salary”的值乘以100，并将其值更新回“ salary”列。从现有列派生新列要创建新列，请使用您希望新列使用的名称指定第一个参数，并通过对现有列进行操作来使用第二个参数来分配值。1df.withColumn(\"CopiedColumn\",col(\"salary\")* -1)此代码段通过将“工资”列乘以值-1来创建新列“ CopiedColumn”。如果派生出来的列需要复杂的计算，则可以使用udf来进行转换12345678910111213141516// 利用withColumn方法，新增列的过程包含在udf函数中val to_date_udf: (Long =&gt; String) = (timestamps: Long) =&gt; &#123; var sdf: SimpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd\") var date: String = sdf.format(new Date(timestamps*1000L)) date&#125;val timestamps_2_date: UserDefinedFunction = udf(to_date_udf)val user_behavior_dataframe: DataFrame = spark.read.format(\"csv\") .option(\"header\", \"false\") // 文件中的第一行是否为列的名称 .option(\"mode\", \"FAILFAST\") // 是否快速失败 .option(\"inferSchema\", \"true\") // 是否自动推断 schema .schema(user_behavior_Schema) .load(\"/Users/cpeixin/IdeaProjects/code_warehouse/data/UserBehavior_5000.csv\") .withColumn(\"date\", timestamps_2_date(col(\"timestamps\")))添加一个新列要创建新列，请将所需的列名传递给withColumn()转换函数的第一个参数。确保此新列尚未出现在DataFrame上（如果显示的话）会更新该列的值。在下面的代码片段中，lit（）函数用于将常量值添加到DataFrame列。我们还可以链接以添加多个列。1234df.withColumn(\"Country\", lit(\"USA\"))//chaining to operate on multiple columnsdf.withColumn(\"Country\", lit(\"USA\")) .withColumn(\"anotherColumn\",lit(\"anotherValue\"))重命名DataFrame列名要重命名现有列，请在DataFrame上使用“ withColumnRenamed ”功能。1df.withColumnRenamed(\"gender\",\"sex\")从Spark DataFrame删除一列使用“放置”功能从数据框中放置特定的列。1df.drop(\"CopiedColumn\")将列拆分为多列尽管此示例未使用withColumn()函数，但我仍然觉得用map()转换函数将一个DataFrame列拆分为多个列还是很好的解释。123456789101112131415import spark.implicits._val columns = Seq(\"name\",\"address\")val data = Seq((\"Robert, Smith\", \"1 Main st, Newark, NJ, 92537\"), (\"Maria, Garcia\",\"3456 Walnut st, Newark, NJ, 94732\"))var dfFromData = spark.createDataFrame(data).toDF(columns:_*)dfFromData.printSchema()val newDF = dfFromData.map(f=&gt;&#123;val nameSplit = f.getAs[String](0).split(\",\")val addSplit = f.getAs[String](1).split(\",\") (nameSplit(0),nameSplit(1),addSplit(0),addSplit(1),addSplit(2),addSplit(3)) &#125;)val finalDF = newDF.toDF(\"First Name\",\"Last Name\", \"Address Line1\",\"City\",\"State\",\"zipCode\")finalDF.printSchema()finalDF.show(false)此代码段将“名称”列拆分为“名字”，“姓氏”，并将“地址”列拆分为“地址行1”，“城市”，“州”和“邮政编码”。产量低于产出：12345678910111213root |-- First Name: string (nullable &#x3D; true) |-- Last Name: string (nullable &#x3D; true) |-- Address Line1: string (nullable &#x3D; true) |-- City: string (nullable &#x3D; true) |-- State: string (nullable &#x3D; true) |-- zipCode: string (nullable &#x3D; true)+----------+---------+--------------+-------+-----+-------+|First Name|Last Name|Address Line1 |City |State|zipCode|+----------+---------+--------------+-------+-----+-------+|Robert | Smith |1 Main st | Newark| NJ | 92537 ||Maria | Garcia |3456 Walnut st| Newark| NJ | 94732 |+----------+---------+--------------+-------+-----+-------+注意：请注意，所有这些函数在应用函数后都将返回新的DataFrame，而不是更新DataFrame。Spark withColumn完整示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.sparkbyexamples.spark.dataframeimport org.apache.spark.sql.&#123;Row, SparkSession&#125;import org.apache.spark.sql.types.&#123;ArrayType, IntegerType, MapType, StringType, StructType&#125;import org.apache.spark.sql.functions._object WithColumn &#123; def main(args:Array[String]):Unit= &#123; val spark: SparkSession = SparkSession.builder() .master(\"local[1]\") .appName(\"SparkByExamples.com\") .getOrCreate() val arrayStructureData = Seq( Row(Row(\"James \",\"\",\"Smith\"),\"1\",\"M\",3100,List(\"Cricket\",\"Movies\"),Map(\"hair\"-&gt;\"black\",\"eye\"-&gt;\"brown\")), Row(Row(\"Michael \",\"Rose\",\"\"),\"2\",\"M\",3100,List(\"Tennis\"),Map(\"hair\"-&gt;\"brown\",\"eye\"-&gt;\"black\")), Row(Row(\"Robert \",\"\",\"Williams\"),\"3\",\"M\",3100,List(\"Cooking\",\"Football\"),Map(\"hair\"-&gt;\"red\",\"eye\"-&gt;\"gray\")), Row(Row(\"Maria \",\"Anne\",\"Jones\"),\"4\",\"M\",3100,null,Map(\"hair\"-&gt;\"blond\",\"eye\"-&gt;\"red\")), Row(Row(\"Jen\",\"Mary\",\"Brown\"),\"5\",\"M\",3100,List(\"Blogging\"),Map(\"white\"-&gt;\"black\",\"eye\"-&gt;\"black\")) ) val arrayStructureSchema = new StructType() .add(\"name\",new StructType() .add(\"firstname\",StringType) .add(\"middlename\",StringType) .add(\"lastname\",StringType)) .add(\"id\",StringType) .add(\"gender\",StringType) .add(\"salary\",IntegerType) .add(\"Hobbies\", ArrayType(StringType)) .add(\"properties\", MapType(StringType,StringType)) val df2 = spark.createDataFrame( spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema) //Change the column data type df2.withColumn(\"salary\",df2(\"salary\").cast(\"Integer\")) //Derive a new column from existing val df4=df2.withColumn(\"CopiedColumn\",df2(\"salary\")* -1) //Transforming existing column val df5 = df2.withColumn(\"salary\",df2(\"salary\")*100) //You can also chain withColumn to change multiple columns //Renaming a column. val df3=df2.withColumnRenamed(\"gender\",\"sex\") df3.printSchema() //Droping a column val df6=df4.drop(\"CopiedColumn\") println(df6.columns.contains(\"CopiedColumn\")) //Adding a literal value df2.withColumn(\"Country\", lit(\"USA\")).printSchema() //Retrieving df2.show(false) df2.select(\"name\").show(false) df2.select(\"name.firstname\").show(false) df2.select(\"name.*\").show(false) val df8 = df2.select(col(\"*\"),explode(col(\"hobbies\"))) df8.show(false) //Splitting one column to multiple columns import spark.implicits._ val columns = Seq(\"name\",\"address\") val data = Seq((\"Robert, Smith\", \"1 Main st, Newark, NJ, 92537\"), (\"Maria, Garcia\",\"3456 Walnut st, Newark, NJ, 94732\")) var dfFromData = spark.createDataFrame(data).toDF(columns:_*) dfFromData.printSchema() val newDF = dfFromData.map(f=&gt;&#123; val nameSplit = f.getAs[String](0).split(\",\") val addSplit = f.getAs[String](1).split(\",\") (nameSplit(0),nameSplit(1),addSplit(0),addSplit(1),addSplit(2),addSplit(3)) &#125;) val finalDF = newDF.toDF(\"First Name\",\"Last Name\", \"Address Line1\",\"City\",\"State\",\"zipCode\") finalDF.printSchema() finalDF.show(false) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"HBase图解","slug":"HBase图解","date":"2017-08-09T17:33:43.000Z","updated":"2020-07-20T01:56:01.413Z","comments":true,"path":"2017/08/10/HBase图解/","link":"","permalink":"cpeixin.cn/2017/08/10/HBase%E5%9B%BE%E8%A7%A3/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"}]},{"title":"HashMap 总结","slug":"HashMap-总结","date":"2017-08-07T13:29:53.000Z","updated":"2020-08-07T13:30:58.302Z","comments":true,"path":"2017/08/07/HashMap-总结/","link":"","permalink":"cpeixin.cn/2017/08/07/HashMap-%E6%80%BB%E7%BB%93/","excerpt":"","text":"简介HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。HashMap最多只允许一条记录的键为HashMap非线程安全，即任一时刻可以有多个线程同时写HashMapConcurrentHashMap: 满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。底层组成结构结构HashMap是数组+链表+红黑树，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组Node(int hash, K key, V value, Node&lt;K,V&gt; next) 存储着hash //用来定位数组索引位置keyvalueNode&lt;K,V&gt; next //链表的下一个node默认初始参数从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化int threshold; // 所能容纳的key-value对极限final float loadFactor; // 负载因子int modCount;int size;Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考HashMap常用方法根据key获取哈希桶数组索引位置在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。因为一般情况下，HashMap长度不会太大，所以如果采用取模计算情况下，h &amp; (table.length -1)的得到的二进制数，实际有效位有限，一般都是低16位，这样高16位就等于完全浪费了。HashMap Put流程①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容；②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。扩容通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。HashMap的size达到threshold大小，则会扩容扩容算法：每一次的扩容，都是原size的两倍，具体是根据上一次的table size进行一次位移运算得到的。那么为什么不直接乘以2来得到新的长度呢？主要是CPU支持乘法运算，都是以加法的方式来实现的，位运算的话会更简洁高效。扩容后，原数组拷贝到新数组：我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图：这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞红黑树扩展一棵合格的红黑树需要满足这样几个要求：根节点是黑色的；每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点。两个非常重要的操作，左旋（rotate left）、右旋（rotate right）。左旋全称其实是叫围绕某个节点的左旋，那右旋的全称估计你已经猜到了，就叫围绕某个节点的右旋。红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。除此之外，其他情况都会违背红黑树的定义，于是我们就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。红黑树的平衡调整过程是一个迭代的过程。我们把正在处理的节点叫做关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。新节点插入之后，如果红黑树的平衡被打破，那一般会有下面三种情况。我们只需要根据每种情况的特点，不停地调整，就可以让红黑树继续符合定义，也就是继续保持平衡。为什么要引入红黑树：解决Hash冲突的情况下，链化严重，如果需要查找的元素在链表的末尾，则时间复杂度退化为O(1)了扩展散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash 表”映射方法就叫作散列函数（或“Hash 函数”“哈希函数”），而散列函数计算得到的值就叫作散列值散列表的查询效率并不能笼统地说成是 O(1)。它跟散列函数、装载因子、散列冲突等都有关系散列函数设计总结了三点散列函数设计的基本要求：散列函数计算得到的散列值是一个非负整数；如果 key1 = key2，那 hash(key1) == hash(key2)；3.如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。散列函数的设计不能太复杂。影响到散列表的性能。散列函数生成的值要尽可能随机并且均匀分布散列函数的设计方法直接寻址法、平方取中法、折叠法、随机数法等即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突开放寻址法(缺点：开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。)线性探测（Linear Probing）二次探测（Quadratic probing）双重散列（Double hashing）（使用一组散列函数）链表法基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。装载因子当散列表的装载因子超过某个阈值时，就需要进行扩容。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"cpeixin.cn/tags/HashMap/"}]},{"title":"Spark Streaming offset 管理","slug":"Spark-Streaming-offset-管理","date":"2017-08-02T08:03:44.000Z","updated":"2020-05-07T08:13:24.491Z","comments":true,"path":"2017/08/02/Spark-Streaming-offset-管理/","link":"","permalink":"cpeixin.cn/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/","excerpt":"","text":"越来越多的实时项目需求，感觉好像各个业务线，产品都想让自己的数据动起来，并且配合上数据可视化展示出来。那么在使用Spark Streaming过程中，肯定不能避免一个问题，那就是，你全天24小时运行的实时程序，如果在某一时刻因为各种原因，停掉。当你发现实时程序已经因为故障停止运行了1个小时，或者产品运营中数据的使用者打电话给你，通知你最近一个小时的数据没有显示（尴尬😅），或者实时程序要临时升级，需要添加新的业务逻辑重新部署。那么此时，在你还没有做offset管理的时候，你准备怎么办呢？是”auto.offset.reset” -&gt; “latest” 从最新的offset位移数据读起，放弃那1个小时未读取到的数据，还是”auto.offset.reset” -&gt; “earliest”, 从最早的位移数据读取，重新处理一遍所有topic数据。思考一下，这两种情况都不是一个好的办法，所以，我们要在有这个需求的程序中，来进行offset的管理，避免数据的丢失以及重复计算的问题。将offset存储在外部数据存储中CheckpointsHBaseZooKeeperKafkaredis不管理offset上图描述了在Spark Streaming应用程序中管理offset的一般流程。offset可以通过几种方式进行管理，但通常遵循以下通用步骤。在Direct DStream初始化后，可以指定每个主题分区的offset映射，以了解Direct DStream应该从哪个分区开始读取。指定的偏移量与下面的第4步写入的位置相同。然后可以读取和处理这批消息。处理后，结果和offset都可以存储。_存储结果和提交偏移量_动作周围的虚线只是突出显示了一系列步骤，如果需要特殊的交付语义更严格的情况，用户可能需要进一步检查。这可能包括检查幂等运算或将结果及其偏移量存储在原子运算中。最后，任何外部持久数据存储（例如HBase，Kafka，HDFS和ZooKeeper）都可以用来跟踪已处理的消息。根据业务需求，可以将不同的方案合并到上述步骤中。Spark的编程灵活性允许用户进行细粒度的控制，以在处理的周期性阶段之前或之后存储offset。考虑发生以下情况的应用程序：Spark Streaming应用程序正在从Kafka读取消息，针对HBase数据执行转换操作，然后将操作后的消息发布到另一个topic中或单独的系统（例如，其他消息传递系统，到HBase，Solr，DBMS等）。在这种情况下，只有将消息成功发布到辅助系统后，我们才将其视为已处理。外部存储offset在本节中，我们探索用于在持久数据存储区中将offset持久保存在外部的不同选项。对于本节中提到的方法，如果使用spark-streaming-kafka-0-10_2.**库，建议用户将enable.auto.commit 设置为false。此配置仅适用于此版本，将enable.auto.commit 设置为true意味着offset将以config auto.commit.interval.ms控制的频率自动提交。在Spark Streaming中，将此值设置为true会在从Kafka读取消息时自动向Kafka提交偏移量，这不一定意味着Spark已完成对这些消息的处理。要启用精确的偏移量控制，请将Kafka参数enable.auto.commit 设置为 false。Spark Streaming checkpoints启用Spark Streaming的是存储checkpoints的最简单方法，因为它在Spark的框架中很容易获得。checkpoint是专门为保存应用程序的状态（一般情况下保存在HDFS）而设计的，以便可以在出现故障时将其恢复。对Kafka流进行检查点将导致偏移范围存储在检查点中。如果出现故障，Spark Streaming应用程序可以开始从检查点偏移范围读取消息。但是，Spark Streaming检查点无法在Spark应用程序升级之后恢复，因此不是很可靠，尤其是当您将这种机制用于关键的生产应用程序时。我们不建议通过Spark检查点管理偏移量。在HBase中存储offsetHBase可用作外部数据存储，以可靠的方式保留偏移范围。通过在外部存储偏移量范围，它使Spark Streaming应用程序能够从任意时间点重新启动和重播消息，只要消息在Kafka中仍然有效。借助HBase的通用设计，该应用程序能够利用rowkey和column family 来处理跨同一表中的多个Spark Streaming应用程序和Kafka topic 存储偏移范围。在此示例中，是使用包含topic，group id和Spark Streaming 的 batchTime.milliSeconds 组合作为行键来区分写入表的每个条目。新记录将累积在我们在以下设计中配置的表格中，以在30天后自动过期。下面是HBase表的DDL和结构。ddl1create &#39;stream_kafka_offsets&#39;, &#123;NAME&#x3D;&gt;&#39;offsets&#39;, TTL&#x3D;&gt;2592000&#125;RowKey 设计1234row: &lt;TOPIC_NAME&gt;:&lt;GROUP_ID&gt;:&lt;EPOCH_BATCHTIME_MS&gt;column family: offsetsqualifier: &lt;PARTITION_ID&gt;value: &lt;OFFSET_ID&gt;下面直接给出在HBase中，offset的管理设计流程代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162import kafka.utils.ZkUtilsimport org.apache.hadoop.hbase.filter.PrefixFilterimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.&#123;TableName, HBaseConfiguration&#125;import org.apache.hadoop.hbase.client.&#123;Scan, Put, ConnectionFactory&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.kafka010.ConsumerStrategies._import org.apache.spark.streaming.kafka010.&#123;OffsetRange, HasOffsetRanges, KafkaUtils&#125;import org.apache.spark.streaming.kafka010.LocationStrategies._import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkContext, SparkConf&#125;/** * Created by gmedasani on 6/10/17. */object KafkaOffsetsBlogStreamingDriver &#123; def main(args: Array[String]) &#123; if (args.length &lt; 6) &#123; System.err.println(\"Usage: KafkaDirectStreamTest &lt;batch-duration-in-seconds&gt; &lt;kafka-bootstrap-servers&gt; \" + \"&lt;kafka-topics&gt; &lt;kafka-consumer-group-id&gt; &lt;hbase-table-name&gt; &lt;kafka-zookeeper-quorum&gt;\") System.exit(1) &#125; val batchDuration = args(0) val bootstrapServers = args(1).toString val topicsSet = args(2).toString.split(\",\").toSet val consumerGroupID = args(3) val hbaseTableName = args(4) val zkQuorum = args(5) val zkKafkaRootDir = \"kafka\" val zkSessionTimeOut = 10000 val zkConnectionTimeOut = 10000 val sparkConf = new SparkConf().setAppName(\"Kafka-Offset-Management-Blog\") .setMaster(\"local[4]\")//Uncomment this line to test while developing on a workstation val sc = new SparkContext(sparkConf) val ssc = new StreamingContext(sc, Seconds(batchDuration.toLong)) val topics = topicsSet.toArray val topic = topics(0) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; bootstrapServers, \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; consumerGroupID, \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) /* Create a dummy process that simply returns the message as is. */ def processMessage(message:ConsumerRecord[String,String]):ConsumerRecord[String,String]=&#123; message &#125; /* Save Offsets into HBase */ def saveOffsets(TOPIC_NAME:String,GROUP_ID:String,offsetRanges:Array[OffsetRange],hbaseTableName:String, batchTime: org.apache.spark.streaming.Time) =&#123; val hbaseConf = HBaseConfiguration.create() hbaseConf.addResource(\"src/main/resources/hbase-site.xml\") val conn = ConnectionFactory.createConnection(hbaseConf) val table = conn.getTable(TableName.valueOf(hbaseTableName)) val rowKey = TOPIC_NAME + \":\" + GROUP_ID + \":\" + String.valueOf(batchTime.milliseconds) val put = new Put(rowKey.getBytes) for(offset &lt;- offsetRanges)&#123; put.addColumn(Bytes.toBytes(\"offsets\"),Bytes.toBytes(offset.partition.toString), Bytes.toBytes(offset.untilOffset.toString)) &#125; table.put(put) conn.close() &#125; /* Returns last committed offsets for all the partitions of a given topic from HBase in following cases. - CASE 1: SparkStreaming job is started for the first time. This function gets the number of topic partitions from Zookeeper and for each partition returns the last committed offset as 0 - CASE 2: SparkStreaming is restarted and there are no changes to the number of partitions in a topic. Last committed offsets for each topic-partition is returned as is from HBase. - CASE 3: SparkStreaming is restarted and the number of partitions in a topic increased. For old partitions, last committed offsets for each topic-partition is returned as is from HBase as is. For newly added partitions, function returns last committed offsets as 0 */ def getLastCommittedOffsets(TOPIC_NAME:String,GROUP_ID:String,hbaseTableName:String,zkQuorum:String, zkRootDir:String, sessionTimeout:Int,connectionTimeOut:Int):Map[TopicPartition,Long] =&#123; val hbaseConf = HBaseConfiguration.create() hbaseConf.addResource(\"src/main/resources/hbase-site.xml\") val zkUrl = zkQuorum+\"/\"+zkRootDir val zkClientAndConnection = ZkUtils.createZkClientAndConnection(zkUrl,sessionTimeout,connectionTimeOut) val zkUtils = new ZkUtils(zkClientAndConnection._1, zkClientAndConnection._2,false) val zKNumberOfPartitionsForTopic = zkUtils.getPartitionsForTopics(Seq(TOPIC_NAME)).get(TOPIC_NAME).toList.head.size //Connect to HBase to retrieve last committed offsets val conn = ConnectionFactory.createConnection(hbaseConf) val table = conn.getTable(TableName.valueOf(hbaseTableName)) val startRow = TOPIC_NAME + \":\" + GROUP_ID + \":\" + String.valueOf(System.currentTimeMillis()) val stopRow = TOPIC_NAME + \":\" + GROUP_ID + \":\" + 0 val scan = new Scan() val scanner = table.getScanner(scan.setStartRow(startRow.getBytes).setStopRow(stopRow.getBytes).setReversed(true)) val result = scanner.next() var hbaseNumberOfPartitionsForTopic = 0 //Set the number of partitions discovered for a topic in HBase to 0 if (result != null)&#123; //If the result from hbase scanner is not null, set number of partitions from hbase to the number of cells hbaseNumberOfPartitionsForTopic = result.listCells().size() &#125; val fromOffsets = collection.mutable.Map[TopicPartition,Long]() if(hbaseNumberOfPartitionsForTopic == 0)&#123; // initialize fromOffsets to beginning for (partition &lt;- 0 to zKNumberOfPartitionsForTopic-1)&#123; fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; 0)&#125; &#125; else if(zKNumberOfPartitionsForTopic &gt; hbaseNumberOfPartitionsForTopic)&#123; // handle scenario where new partitions have been added to existing kafka topic for (partition &lt;- 0 to hbaseNumberOfPartitionsForTopic-1)&#123; val fromOffset = Bytes.toString(result.getValue(Bytes.toBytes(\"offsets\"),Bytes.toBytes(partition.toString))) fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; fromOffset.toLong)&#125; for (partition &lt;- hbaseNumberOfPartitionsForTopic to zKNumberOfPartitionsForTopic-1)&#123; fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; 0)&#125; &#125; else &#123; //initialize fromOffsets from last run for (partition &lt;- 0 to hbaseNumberOfPartitionsForTopic-1 )&#123; val fromOffset = Bytes.toString(result.getValue(Bytes.toBytes(\"offsets\"),Bytes.toBytes(partition.toString))) fromOffsets += (new TopicPartition(TOPIC_NAME,partition) -&gt; fromOffset.toLong)&#125; &#125; scanner.close() conn.close() fromOffsets.toMap &#125; val fromOffsets= getLastCommittedOffsets(topic,consumerGroupID,hbaseTableName,zkQuorum,zkKafkaRootDir, zkSessionTimeOut,zkConnectionTimeOut) val inputDStream = KafkaUtils.createDirectStream[String, String](ssc,PreferConsistent,Assign[String, String]( fromOffsets.keys,kafkaParams,fromOffsets)) /* For each RDD in a DStream apply a map transformation that processes the message. */ inputDStream.foreachRDD((rdd,batchTime) =&gt; &#123; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges offsetRanges.foreach(offset =&gt; println(offset.topic, offset.partition, offset.fromOffset,offset.untilOffset)) val newRDD = rdd.map(message =&gt; processMessage(message)) newRDD.count() saveOffsets(topic,consumerGroupID,offsetRanges,hbaseTableName,batchTime) //save the offsets to HBase &#125;) println(\"Number of messages processed \" + inputDStream.count()) ssc.start() ssc.awaitTermination() &#125;&#125;在ZooKeeper中存储offset用户可以将偏移范围存储在ZooKeeper中，这可以类似地提供一种可靠的方法，以在最后停止的Kafka流上开始流处理。在这种情况下，启动时，Spark Streaming作业将从ZooKeeper中检索每个主题分区的最新处理过的偏移量。如果找到了一个以前在ZooKeeper中未管理过的新分区，则默认将其最新处理的偏移量从头开始。处理完每批后，用户可以存储第一个或最后一个处理过的偏移量。此外，在ZooKeeper中存储偏移量的znode位置使用与旧Kafka使用者API相同的格式。因此，用于跟踪或监视存储在ZooKeeper中的Kafka偏移量的任何工具仍然可以使用。初始化ZooKeeper连接，以获取和存储到ZooKeeper的偏移量：12val zkClientAndConnection = ZkUtils.createZkClientAndConnection(zkUrl, sessionTimeout, connectionTimeout)val zkUtils = new ZkUtils(zkClientAndConnection._1, zkClientAndConnection._2, false)检索存储在使用者组和主题列表的ZooKeeper中的最后偏移量的方法：1234567891011121314151617181920212223242526272829303132def readOffsets(topics: Seq[String], groupId:String): Map[TopicPartition, Long] = &#123; val topicPartOffsetMap = collection.mutable.HashMap.empty[TopicPartition, Long] val partitionMap = zkUtils.getPartitionsForTopics(topics) // /consumers/&lt;groupId&gt;/offsets/&lt;topic&gt;/ partitionMap.foreach(topicPartitions =&gt; &#123; val zkGroupTopicDirs = new ZKGroupTopicDirs(groupId, topicPartitions._1) topicPartitions._2.foreach(partition =&gt; &#123; val offsetPath = zkGroupTopicDirs.consumerOffsetDir + \"/\" + partition try &#123; val offsetStatTuple = zkUtils.readData(offsetPath) if (offsetStatTuple != null) &#123; LOGGER.info(\"retrieving offset details - topic: &#123;&#125;, partition: &#123;&#125;, offset: &#123;&#125;, node path: &#123;&#125;\", Seq[AnyRef](topicPartitions._1, partition.toString, offsetStatTuple._1, offsetPath): _*) topicPartOffsetMap.put(new TopicPartition(topicPartitions._1, Integer.valueOf(partition)), offsetStatTuple._1.toLong) &#125; &#125; catch &#123; case e: Exception =&gt; LOGGER.warn(\"retrieving offset details - no previous node exists:\" + \" &#123;&#125;, topic: &#123;&#125;, partition: &#123;&#125;, node path: &#123;&#125;\", Seq[AnyRef](e.getMessage, topicPartitions._1, partition.toString, offsetPath): _*) topicPartOffsetMap.put(new TopicPartition(topicPartitions._1, Integer.valueOf(partition)), 0L) &#125; &#125;) &#125;) topicPartOffsetMap.toMap&#125;使用特定的偏移量初始化Kafka Direct Dstream以开始处理：1val inputDStream = KafkaUtils.createDirectStream(ssc, PreferConsistent, ConsumerStrategies.Subscribe[String,String](topics, kafkaParams, fromOffsets))将一组可恢复的偏移量持久保存到ZooKeeper的方法。注意：_offsetPath_是一个ZooKeeper位置，表示为/ consumers / [groupId] / offsets / topic / [partitionId]，用于存储偏移值123456789101112131415161718def persistOffsets(offsets: Seq[OffsetRange], groupId: String, storeEndOffset: Boolean): Unit = &#123; offsets.foreach(or =&gt; &#123; val zkGroupTopicDirs = new ZKGroupTopicDirs(groupId, or.topic); val acls = new ListBuffer[ACL]() val acl = new ACL acl.setId(ANYONE_ID_UNSAFE) acl.setPerms(PERMISSIONS_ALL) acls += acl val offsetPath = zkGroupTopicDirs.consumerOffsetDir + \"/\" + or.partition; val offsetVal = if (storeEndOffset) or.untilOffset else or.fromOffset zkUtils.updatePersistentPath(zkGroupTopicDirs.consumerOffsetDir + \"/\" + or.partition, offsetVal + \"\", JavaConversions.bufferAsJavaList(acls)) LOGGER.debug(\"persisting offset details - topic: &#123;&#125;, partition: &#123;&#125;, offset: &#123;&#125;, node path: &#123;&#125;\", Seq[AnyRef](or.topic, or.partition.toString, offsetVal.toString, offsetPath): _*) &#125;)&#125;在kafka中管理offset在Apache Spark 2.1.x的Cloudera发行版中，spark-streaming-kafka-0-10使用了新的Consumer api，它公开了commitAsync API。使用commitAsync API，使用方可以在知道输出已存储后将偏移量提交给Kafka。新的使用者api根据使用者的_group.id_唯一地将偏移提交回Kafka 。Persist Offsets in Kafka123456789101112131415161718192021222324252627282930313233343536def createKafkaRDD(ssc: StreamingContext, config: Source) = &#123; var SparkDStream: InputDStream[ConsumerRecord[String, String]] = null try &#123; SparkDStream = &#123; val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; config.servers, \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; config.group, \"auto.offset.reset\" -&gt; config.offset )/* \"enable.auto.commit\" -&gt; config.getString(\"kafkaSource.enable.auto.commit\"))*/ // val subscribeTopics = config.getStringList(\"kafkaSource.topics\").toIterable import scala.collection.JavaConversions._ val kafkaStream = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](config.topic.toList, kafkaParams) ) kafkaStream &#125; &#125; catch &#123; case e: Throwable =&gt; &#123; throw new Exception(\"Couldn't init Spark stream processing\", e) &#125; &#125; SparkDStream &#125;var inputDStream: InputDStream[ConsumerRecord[String, String]] = createKafkaRDD（）inputDStream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // 更新 Offset 值 inputDStream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges) &#125;有关详细信息，请访问– http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#kafka-itself注意：commitAsync（）是Spark Streaming和Kafka Integration的kafka-0-10版本的一部分。如Spark文档所述，此集成仍处于试验阶段，API可能会发生变化。在Redis中存储offset123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010._import scala.collection.JavaConverters._import scala.util.Tryobject KafkaOffsetsBlogStreamingDriver &#123; /** * 根据groupId保存offset * @param ranges * @param groupId */ def storeOffset(ranges: Array[OffsetRange], groupId: String): Unit = &#123; for (o &lt;- ranges) &#123; val key = s\"bi_kafka_offset_$&#123;groupId&#125;_$&#123;o.topic&#125;_$&#123;o.partition&#125;\" val value = o.untilOffset JedisUtil.set(key, value.toString) &#125; &#125; /** * 根据topic，groupid获取offset * @param topics * @param groupId * @return */ def getOffset(topics: Array[String], groupId: String): (Map[TopicPartition, Long], Int) = &#123; val fromOffSets = scala.collection.mutable.Map[TopicPartition, Long]() topics.foreach(topic =&gt; &#123; val keys = JedisUtil.getKeys(s\"bi_kafka_offset_$&#123;groupId&#125;_$&#123;topic&#125;*\") if (!keys.isEmpty) &#123; keys.asScala.foreach(key =&gt; &#123; val offset = JedisUtil.get(key) val partition = Try(key.split(s\"bi_kafka_offset_$&#123;groupId&#125;_$&#123;topic&#125;_\").apply(1)).getOrElse(\"0\") fromOffSets.put(new TopicPartition(topic, partition.toInt), offset.toLong) &#125;) &#125; &#125;) if (fromOffSets.isEmpty) &#123; (fromOffSets.toMap, 0) &#125; else &#123; (fromOffSets.toMap, 1) &#125; &#125; /** * 创建InputDStream，如果auto.offset.reset为latest则从redis读取 * @param ssc * @param topic * @param kafkaParams * @return */ def createStreamingContextRedis(ssc: StreamingContext, topic: Array[String], kafkaParams: Map[String, Object]): InputDStream[ConsumerRecord[String, String]] = &#123; var kafkaStreams: InputDStream[ConsumerRecord[String, String]] = null val groupId = kafkaParams.get(\"group.id\").get val (fromOffSet, flag) = getOffset(topic, groupId.toString) val offsetReset = kafkaParams.get(\"auto.offset.reset\").get if (flag == 1 &amp;&amp; offsetReset.equals(\"latest\")) &#123; kafkaStreams = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe(topic, kafkaParams, fromOffSet)) &#125; else &#123; kafkaStreams = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe(topic, kafkaParams)) &#125; kafkaStreams &#125; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"offSet Redis\").setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(60)) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"group.id\" -&gt; \"binlog.test.rpt_test_1min\", \"auto.offset.reset\" -&gt; \"latest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean), \"session.timeout.ms\" -&gt; \"20000\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer] ) val topic = Array(\"weibo_keyword\") val groupId = \"test\" val lines = createStreamingContextRedis(ssc, topic, kafkaParams) lines.foreachRDD(rdds =&gt; &#123; if (!rdds.isEmpty()) &#123; println(\"##################:\" + rdds.count()) &#125; storeOffset(rdds.asInstanceOf[HasOffsetRanges].offsetRanges, groupId) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;import java.utilimport com.typesafe.config.ConfigFactoryimport org.apache.kafka.common.serialization.StringDeserializerimport redis.clients.jedis.&#123;HostAndPort, JedisCluster, JedisPool, JedisPoolConfig&#125;object JedisUtil &#123; private val config = ConfigFactory.load(\"realtime-etl.conf\") private val redisHosts: String = config.getString(\"redis.server\") private val port: Int = config.getInt(\"redis.port\") private val hostAndPortsSet: java.util.Set[HostAndPort] = new util.HashSet[HostAndPort]() redisHosts.split(\",\").foreach(host =&gt; &#123; hostAndPortsSet.add(new HostAndPort(host, port)) &#125;) private val jedisConf: JedisPoolConfig = new JedisPoolConfig() jedisConf.setMaxTotal(5000) jedisConf.setMaxWaitMillis(50000) jedisConf.setMaxIdle(300) jedisConf.setTestOnBorrow(true) jedisConf.setTestOnReturn(true) jedisConf.setTestWhileIdle(true) jedisConf.setMinEvictableIdleTimeMillis(60000l) jedisConf.setTimeBetweenEvictionRunsMillis(3000l) jedisConf.setNumTestsPerEvictionRun(-1) lazy val redis = new JedisCluster(hostAndPortsSet, jedisConf) def get(key: String): String = &#123; try &#123; redis.get(key) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() null &#125; &#125; def set(key: String, value: String) = &#123; try &#123; redis.set(key, value) &#125; catch &#123; case e: Exception =&gt; &#123; e.printStackTrace() &#125; &#125; &#125; def hmset(key: String, map: java.util.Map[String, String]): Unit = &#123; // val redis=pool.getResource try &#123; redis.hmset(key, map) &#125;catch &#123; case e:Exception =&gt; e.printStackTrace() &#125; &#125; def hset(key: String, field: String, value: String): Unit = &#123; // val redis=pool.getResource try &#123; redis.hset(key, field, value) &#125; catch &#123; case e: Exception =&gt; &#123; e.printStackTrace() &#125; &#125; &#125; def hget(key: String, field: String): String = &#123; try &#123; redis.hget(key, field) &#125;catch &#123; case e:Exception =&gt; e.printStackTrace() null &#125; &#125; def hgetAll(key: String): java.util.Map[String, String] = &#123; try &#123; redis.hgetAll(key) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() null &#125; &#125;&#125; #### 其他方法 值得一提的是，您还可以将偏移量存储在HDFS之类的存储系统中。与上述选项相比，在HDFS中存储偏移量不太受欢迎，因为与其他系统（如ZooKeeper和HBase）相比，HDFS具有更高的延迟。此外，如果管理不当，则在HDFS中为每个批次编写offsetRanges可能会导致文件较小的问题。不管理offset当然，Spark Streaming应用程序并不是必须的去管理offset。对当前业务考虑好是否需要对offset进行保存。本文参考如下：Offset Management For Apache Kafka With Apache Spark Streaming","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 广播变量和累加器","slug":"Spark-广播变量和累加器","date":"2017-06-03T13:13:20.000Z","updated":"2020-09-03T13:43:56.310Z","comments":true,"path":"2017/06/03/Spark-广播变量和累加器/","link":"","permalink":"cpeixin.cn/2017/06/03/Spark-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%92%8C%E7%B4%AF%E5%8A%A0%E5%99%A8/","excerpt":"","text":"共享变量是 Spark 中进阶特性之一，一共有两种：广播变量累加器这两种变量可以认为是在用算子定义的数据管道外的两个全局变量，供所有计算任务使用。在 Spark 作业中，用户编写的高阶函数会在集群中的 Executor 里执行，这些 Executor 可能会用到相同的变量，这些变量被复制到每个 Executor 中，而 Executor 对变量的更新不会传回 Driver。在计算任务中支持通用的可读写变量一般是低效的，即便如此，Spark 还是提供了两类共享变量：广播变量（broadcast variable）与累加器（accumulator）。当然，对于分布式变量，如果不加限制会出现一致性的问题，所以共享变量是两种非常特殊的变量。广播变量：只读；累加器：只能增加。**广播变量广播变量类似于 MapReduce 中的 DistributeFile，通常来说是一份不大的数据集，一旦广播变量在 Driver 中被创建，整个数据集就会在集群中进行广播，能让所有正在运行的计算任务以只读方式访问。广播变量支持一些简单的数据类型，如整型、集合类型等，也支持很多复杂数据类型，如一些自定义的数据类型。广播变量为了保证数据被广播到所有节点，使用了很多办法。这其实是一个很重要的问题，我们不能期望 100 个或者 1000 个 Executor 去连接 Driver，并拉取数据，这会让 Driver 不堪重负。Executor 采用的是通过 HTTP 连接去拉取数据，类似于 BitTorrent 点对点传输。这样的方式更具扩展性，避免了所有 Executor 都去向 Driver 请求数据而造成 Driver 故障。Spark 广播机制运作方式是这样的：Driver 将已序列化的数据切分成小块，然后将其存储在自己的块管理器 BlockManager 中。BlockManager是spark自己的存储系统，RDD-Cache、 Shuffle-output、broadcast 等的实现都是基于BlockManager来实现的，BlockManager也是分布式结构，在driver和所有executor上都会有blockmanager节点，每个节点上存储的block信息都会汇报给driver端的blockManagerMaster作统一管理，BlockManager对外提供get和set数据接口，可将数据存储在memory, disk, off-heap当Executor 开始运行时，每个 Executor 首先从自己的内部块管理器中试图获取广播变量，如果以前广播过，那么直接使用；如果没有，Executor 就会从 Driver 或者其他可用的 Executor 去拉取数据块。一旦拿到数据块，就会放到自己的块管理器中。供自己和其他需要拉取的 Executor 使用。这就很好地防止了 Driver 单点的性能瓶颈，如下图所示。在 Spark 作业中创建、使用广播变量。代码如下：12345678910111213scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] atparallelize at &lt;console&gt;:25 scala&gt; val i = 5 i: Int = 5scala&gt; val bi = sc.broadcast(i)bi: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(147)scala&gt; bi.valueres166: Int = 5scala&gt; rdd_one.take(5)res164: Array[Int] = Array(1, 2, 3)scala&gt; rdd_one.map(j =&gt; j + bi.value).take(5)res165: Array[Int] = Array(6, 7, 8)在用户定义的高阶函数中，可以直接使用广播变量的引用。下面看一个集合类型的广播变量：1234567891011scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3)) rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[109] atparallelize at &lt;console&gt;:25scala&gt; val m = scala.collection.mutable.HashMap(1 -&gt; 2, 2 -&gt; 3, 3 -&gt; 4) m: scala.collection.mutable.HashMap[Int,Int] = Map(2 -&gt; 3, 1 -&gt; 2, 3 -&gt; 4)scala&gt; val bm = sc.broadcast(m)bm:org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[Int,Int]] = Broadcast(178)scala&gt; rdd_one.map(j =&gt; j * bm.value(j)).take(5)res191: Array[Int] = Array(2, 6, 12)该例中，元素乘以元素对应值得到最后结果。广播变量会持续占用内存，当我们不需要的时候，可以用 unpersist 算子将其移除，这时，如果计算任务又用到广播变量，那么就会重新拉取数据，如下：12345678910111213 ...scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] atparallelize at &lt;console&gt;:25scala&gt; val k = 5k: Int = 5scala&gt; val bk = sc.broadcast(k)bk: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(163)scala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)res184: Array[Int] = Array(6, 7, 8)scala&gt; bk.unpersistscala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)res186: Array[Int] = Array(6, 7, 8)你还可以使用 destroy 方法彻底销毁广播变量，调用该方法后，如果计算任务中又用到广播变量，则会抛出异常：12345678910111213141516171819scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] atparallelize at &lt;console&gt;:25scala&gt; val k = 5k: Int = 5scala&gt; val bk = sc.broadcast(k)bk: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(163)scala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)res184: Array[Int] = Array(6, 7, 8)scala&gt; bk.destroyscala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)17/05/27 14:07:28 ERROR Utils: Exception encounteredorg.apache.spark.SparkException: Attempted to use Broadcast(163) after itwas destroyed (destroy at &lt;console&gt;:30)at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)atorg.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeObject$1.apply$mcV$sp(TorrentBroadcast.scala:202)at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$wri示例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546import org.apache.spark.sql.SparkSessionobject BigRDDJoinSmallRDD &#123; def main(args: Array[String]): Unit = &#123; val sparkSession = SparkSession.builder().master(\"local[3]\").appName(\"BigRDD Join SmallRDD\").getOrCreate() val sc = sparkSession.sparkContext val list1 = List((\"jame\",23), (\"wade\",3), (\"kobe\",24)) val list2 = List((\"jame\", 13), (\"wade\",6), (\"kobe\",16)) val bigRDD = sc.makeRDD(list1) val smallRDD = sc.makeRDD(list2) println(bigRDD.getNumPartitions) println(smallRDD.getNumPartitions) // driver端rdd不broadcast广播smallRDD到各executor，RDD不能被broadcast，需要转换成数组array val smallRDDB= sc.broadcast(smallRDD.collect()) val joinedRDD = bigRDD.mapPartitions(partition =&gt; &#123; val smallRDDBV = smallRDDB.value // 各个executor端的task读取广播value partition.map(element =&gt; &#123; //println(joinUtil(element, smallRDDBV)) joinUtil(element, smallRDDBV) &#125;) &#125;) joinedRDD.foreach(x =&gt; println(x)) &#125;/** * join操作：对两个rdd中的相同key的value1和value2进行聚合，即(key,value1).join(key,value2)得到(key,(value1, vlaue2)) * 如果bigRDDEle的key和smallRDD的某个key一致，那么返回(key,(value1, vlaue2)) * 该方法会在各executor的task上执行 * */ def joinUtil(bigRDDEle:(String,Int), smallRDD: Array[(String, Int)]): (String, (Int,Int)) = &#123; var joinEle:(String, (Int, Int)) = null // 遍历数组smallRDD smallRDD.foreach(smallRDDEle =&gt; &#123; if(smallRDDEle._1.equals(bigRDDEle._1))&#123; // 如果bigRDD中某个元素的key和数组smallRDD的key一致，返回join结果 joinEle = (bigRDDEle._1, (bigRDDEle._2, smallRDDEle._2)) &#125; &#125;) joinEle &#125;&#125;这样，相当于先将小表进行广播，广播到每个 Executor 的内存中，供 map 函数使用，这就避免了 Shuffle，虽然语义上还是 join（小表放内存），但无论是资源消耗还是执行时间，都要远优于前面两种方式。累加器与广播变量只读不同，累加器是一种只能进行增加操作的共享变量。如果你想知道记录中有多少错误数据，一种方法是针对这种错误数据编写额外逻辑，另一种方式是使用累加器。用法如下：1234567891011121314151617 ...scala&gt; val acc1 = sc.longAccumulator(\"acc1\")acc1: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 10355,name: Some(acc1), value: 0)scala&gt; val someRDD = tableRDD.map(x =&gt; &#123;acc1.add(1); x&#125;)someRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[99] at map at&lt;console&gt;:29scala&gt; acc1.valueres156: Long = 0 /*there has been no action on the RDD so accumulator didnot get incremented*/scala&gt; someRDD.countres157: Long = 351scala&gt; acc1.valueres158: Long = 351scala&gt; acc1res145: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 10355,name: Some(acc1), value: 351)上面这个例子用 SparkContext 初始化了一个长整型的累加器。LongAccumulator 方法会将累加器变量置为 0。行动算子 count 触发计算后，累加器在 map 函数中被调用，其值会一直增加，最后定格为 351。Spark 内置的累加器有如下几种。LongAccumulator：长整型累加器，用于求和、计数、求均值的 64 位整数。DoubleAccumulator：双精度型累加器，用于求和、计数、求均值的双精度浮点数。CollectionAccumulator[T]：集合型累加器，可以用来收集所需信息的集合。所有这些累加器都是继承自 AccumulatorV2，如果这些累加器还是不能满足用户的需求，Spark 允许自定义累加器。如果需要某两列进行汇总，无疑自定义累加器比直接编写逻辑要方便很多，例如：这个表只有两列，需要统计 A 列与 B 列的汇总值。下面来看看根据上面的逻辑如何实现一个自定义累加器。代码如下：12345678910111213141516171819202122232425262728293031323334353637383940import org.apache.spark.util.AccumulatorV2import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.SparkConf // 构造一个保存累加结果的类case class SumAandB(A: Long, B: Long) class FieldAccumulator extends AccumulatorV2[SumAandB,SumAandB] &#123;private var A:Long = 0Lprivate var B:Long = 0L // 如果A和B同时为0，则累加器值为0 override def isZero: Boolean = A == 0 &amp;&amp; B == 0L // 复制一个累加器 override def copy(): FieldAccumulator = &#123; val newAcc = new FieldAccumulator newAcc.A = this.A newAcc.B = this.B newAcc &#125; // 重置累加器为0 override def reset(): Unit = &#123; A = 0 ; B = 0L &#125; // 用累加器记录汇总结果 override def add(v: SumAandB): Unit = &#123; A += v.A B += v.B &#125; // 合并两个累加器 override def merge(other: AccumulatorV2[SumAandB, SumAandB]): Unit = &#123; other match &#123; case o: FieldAccumulator =&gt; &#123; A += o.A B += o.B&#125; case _ =&gt; &#125; &#125; // 当Spark调用时返回结果 override def value: SumAandB = SumAandB(A,B)&#125;凡是有关键字 override 的方法，均是重载实现自己逻辑的方法。累加器调用方式如下：123456789101112131415161718192021package com.spark.examples.rdd import org.apache.spark.SparkConfimport org.apache.spark.SparkContext class Driver extends App&#123; val conf = new SparkConf val sc = new SparkContext(conf) val filedAcc = new FieldAccumulator sc.register(filedAcc, \" filedAcc \") // 过滤掉表头 val tableRDD = sc.textFile(\"table.csv\").filter(_.split(\",\")(0) != \"A\") tableRDD.map(x =&gt; &#123; val fields = x.split(\",\") val a = fields(1).toInt val b = fields(2).toLong filedAcc.add(SumAandB (a, b)) x &#125;).count&#125;总结本课时主要介绍了 Spark 的两种共享变量，注意体会广播变量最后介绍的 map 端 join 的场景，这在实际使用中非常普遍。另外广播变量的大小，按照我的经验，要根据 Executor 和 Worker 资源来确定，几十兆、一个 G 的广播变量在大多数情况不会有什么问题，如果资源充足，那么1G~10G 以内问题也不大。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"python - 协程","slug":"python-协程","date":"2017-05-31T12:16:32.000Z","updated":"2020-04-04T11:03:54.388Z","comments":true,"path":"2017/05/31/python-协程/","link":"","permalink":"cpeixin.cn/2017/05/31/python-%E5%8D%8F%E7%A8%8B/","excerpt":"","text":"深入理解Python异步编程推荐推荐推荐！！！！协程，Python asyncio异步编程 只此一篇足矣，一览众山小！链接","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 生成器 2","slug":"python-生成器-2","date":"2017-05-30T11:08:35.000Z","updated":"2020-04-04T17:14:25.836Z","comments":true,"path":"2017/05/30/python-生成器-2/","link":"","permalink":"cpeixin.cn/2017/05/30/python-%E7%94%9F%E6%88%90%E5%99%A8-2/","excerpt":"","text":"生成器进化成协程生成器是由迭代器进化而来，所以生成器对象有 iter 和 next 方法，可以使用 for 循环获得值，注意这里所说的 “获得值” 指的是下文代码块里 yield 语句中 yield 关键字后面的 i 。这是在 Python 2.5 时出现的特性，在 Python 3.3 中出现 yield from 语法之前，生成器没有太大用途。但此时 yield 关键字还是实现了一些特性，且至关重要，就是生成器对象有 send 、throw 和 close 方法。这三个方法的作用分别是发送数据给生成器并赋值给 yield 语句、向生成器中抛入异常由生成器内部处理、终止生成器。这三个方法使得生成器进化成协程。协程有四种存在状态：GEN_CREATED 创建完成，等待执行GEN_RUNNING 解释器正在执行（这个状态在下面的示例程序中无法看到）GEN_SUSPENDED 在 yield 表达式处暂停GEN_CLOSE 执行结束，生成器停止可以使用 inspect.getgeneratorstate 方法查看协程的当前状态，举例如下：inspect模块用于收集python对象的信息，可以获取类或函数的参数的信息，源码，解析堆栈，对对象进行类型检查等等12345678910111213141516171819202122import inspectdef generator(): i = '激活生成器' while True: try: value = yield i except ValueError: print('OVER') i = valueg = generator() # 1inspect.getgeneratorstate(g) # 2print(next(g)) # 3inspect.getgeneratorstate(g)print(g.send('Hello Shiyanlou')) # 4g.throw(ValueError) # 5g.close() # 6inspect.getgeneratorstate(g)代码说明如下：1、创建生成器2、查看生成器状态3、这步操作叫做预激生成器（或协程），这是必须做的。在生成器创建完成后，需要将其第一次运行到 yield 语句处暂停4、暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值，也就是 yield 表达式等号前面的 value 变量的值变成 ‘Hello Shiyanlou’，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i5、向生成器抛入异常，异常会被 try except 捕获，作进一步处理6、close 方法终止生成器，异常不会被抛出因为生成器的调用方也就是程序员自己可以控制生成器的启动、暂停、终止，而且可以向生成器内部传入数据，所以这种生成器又叫做协程，generator 函数既可以叫做生成器函数，也可以叫协程函数，这是生成器向协程的过渡阶段。yield -&gt; yield from在 Python 3.3 中新增了 yield from 语法，如果将yield理解成“返回”，那么yield from就是“从什么（生成器）里面返回”,这是全新的语言结构，是 yield 的升级版。相比 yield ，该语法有两大优势，我们来举例说明它的用法。区别示例123456789101112def generator(): yield 'a' yield 'b' yield 'c' yield from generator1() #yield from iterable本质上等于 for item in iterable: yield item的缩写版 yield from [11,22,33,44] yield from (12,23,34) yield from range(3) for i in generator(): print(i,end=' , ')避免潜逃循环yield:12345678910def chain(*args): for iter_obj in args: for i in iter_obj: yield ichain(&#123;'one', 'two'&#125;, list('ace'))for i in c: print(i)yield from:123456789def chain(*args): for iter_obj in args: yield from iter_objc = chain(&#123;'one', 'two'&#125;, list('ace'))for i in c: print(i)可以看到 yield from 语句可以替代 for 循环，避免了嵌套循环。同 yield 一样，yield from 语句也只能出现在函数体内部，有 yield from 语句的函数叫做协程函数或生成器函数。yield from 后面接收一个可迭代对象，例如上面代码中的 iter_obj 变量，在协程中，可迭代对象往往是协程对象，这样就形成了嵌套协程。转移控制权123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import timefrom faker import Fakerfrom functools import wraps# 预激协程装饰器def coroutine(func): @wraps(func) def wrapper(*args, **kw): g = func(*args, **kw) next(g) return g return wrapper# 子生成器函数，这个生成器是真正做事的生成器def sub_coro(): l = [] # 创建空列表 while True: # 无限循环 value = yield # 调用方使用 send 方法发生数据并赋值给 value 变量 if value == 'CLOSE': # 如果调用方发生的数据是 CLOSE ，终止循环 break l.append(value) # 向列表添加数据 return sorted(l) # 返回排序后的列表# 使用预激协程装饰器# 带有 yield from 语句的父生成器函数@coroutinedef dele_coro(): # while True 可以多次循环，每次循环会创建一个新的子生成器 sub_coro() # 这里 while 只循环一次，这是由调用方，也就是 main 函数决定的 # while 循环可以捕获函数本身创建的父生成器终止时触发的 StopIteration 异常 while True: # yield from 会自动预激子生成器 sub_coro() # 所以 sub_coro 在定义时不可以使用预激协程装饰器 # yield from 将捕获子生成器终止时触发的 StopIteration 异常 # 并将异常的 value 属性值赋值给等号前面的变量 l # 也就是 l 变量的值等于 sub_coro 函数的 return 值 # yield from 还实现了一个重要功能 # 就是父生成器的 send 方法将发送值给子生成器 # 并赋值给子生成器中 yield 语句等号前面的变量 value l = yield from sub_coro() print('排序后的列表：', l) print('------------------')# 调用父生成器的函数，也叫调用方def main(): # 生成随机国家代号的方法 fake = Faker().country_code # 嵌套列表，每个子列表中有三个随机国家代号(字符串) nest_country_list = [[fake() for i in range(3)] for j in range(3)] for country_list in nest_country_list: print('国家代号列表：', country_list) c = dele_coro() # 创建父生成器 for country in country_list: c.send(country) # 父生成器的 send 方法将国家代号发送给子生成器 # CLOSE 将终止子生成器中的 while 循环 # 子生成器的 return 值赋值给父生成器 yield from 语句中等号前面的变量 l c.send('CLOSE')if __name__ == '__main__': main()所谓 “转移控制权” 就是 yield from 语法可以将子生成器的控制权交给调用方 main 函数，在 main 函数内部创建父生成器 c ，控制 c.send 方法传值给子生成器。这是一个巨大的进步，在此基础上，Python 3.4 新增了创建协程的装饰器，这样非生成器函数的协程函数就正式出现了。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 生成器","slug":"python-生成器","date":"2017-05-28T12:07:35.000Z","updated":"2020-04-04T17:13:37.299Z","comments":true,"path":"2017/05/28/python-生成器/","link":"","permalink":"cpeixin.cn/2017/05/28/python-%E7%94%9F%E6%88%90%E5%99%A8/","excerpt":"","text":"前言在开始讲解协程和生成器之前，说明一下协程和生成器之间的关系是很有必要的，生成器在学习python的时候，并没有深入的学习，所以感觉很抽象很难懂，和普通函数单向调用的逻辑思维并不一样。协程则是和线程，进程是同一类别的概念。首先要认识yield关键字，是yield关键词，yield放在函数中可以使得函数变成生成器，也可以变成协程。在生成器中, yield 只对外产出值，在协程中，yield能对外产出值，而且能接收通过send()方法传入值yielld构造的生成器可以作为协程使用，协程是指一个过程，这个过程与调用方协作，由调用方提供的值，来计算并产出。纯粹的生，这样可以交接给for调用。成器只输出值，和迭代有关协程与函数的区别，函数是一种上下级调用关系，而协程是通过_yield_方式转移执行权，对称而平级的调用对方，典型的有生产者和消费者。从调试过程中理解区别于其他教程，前几段都是云里雾里的讲概念，但是对于生成器，一上来就看概念，真的很难懂。所以我准备了三个实例，建议上来先通过打断点，分步调试过程中，通过看调用顺序和返回值来初步了解生成器的原理，接下来在看概念解释，则比较容易理解。1234567891011121314151617# encoding:UTF-8def yield_test(n): for i in range(n): yield call(i) print(\"i=\", i) # 做一些其它的事情 print(\"do something.\") print(\"end.\")def call(i): return i * 2# 使用for循环for i in yield_test(5): print(i, \",\")下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c)下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示12345678910def h(): print 'Wen Chuan', m = yield 5 print m d = yield 12 print 'We are together!'c = h()m = c.__next__() c.send('Fighting!')下图是打断点调试的过程，其中程序注释中会标注，运行步骤顺序，用step x来表示经过上面的三个程序的调试步骤后，下面开始带着心中初步了解的程序运行顺序来看生成器相关的原理生成器what生成器是一次生成一个值的特殊类型函数。可以将其视为可恢复函数。调用该函数将返回一个可用于生成连续 x 值的生成【Generator】，简单的说就是在函数的执行过程中，yield语句会把你需要的值返回给调用生成器的地方，然后退出函数，下一次调用生成器函数的时候又从上次中断的地方开始执行，而生成器内的所有变量参数都会被保存下来供下一次使用。why列表所有数据都在内存中，如果有海量数据的话将会非常耗内存。如：仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。如果列表元素按照某种算法推算出来，那我们就可以在循环的过程中不断推算出后续的元素，这样就不必创建完整的list，从而节省大量的空间。简单一句话：我又想要得到庞大的数据，又想让它占用空间少，那就用生成器！How方法一123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;方法二如果一个函数中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。调用函数就是创建了一个生成器（generator）对象。yield关键字yield 的用法起源于对一般 function 中 return 的扩展。在一个 function 中，必须有一个返回值列于 return 之后，可以返回数字也可以返回空值，但是必须要有一个返回值，标志着这个 function 的结束。一旦它结束，那么这个 function 中产生的一切变量将被统统抛弃，有什么可以使一个 function 暂停下来，并且返回当前所在地方的值，当接收到继续的命令时可以继续前进呢？换个说法，就是 return 返回一个值，并且记住这个返回的位置。这个操作有点儿像Python2.5以前，yield是一个语句，但现在2.5中，yield是一个表达式(Expression)，比如：1m = yield 5大家不要认为，m值为5，而是表达式(yield 5)的返回值将赋值给m，那么yield 5的返回值是什么呢？yield 5的返回值是下面将要提到的send(msg)方法传递过来的msg参数。生成器的工作原理生成器(generator)能够迭代的关键是它有一个 next() 方法 。 工作原理就是通过重复调用next()方法，直到捕获一个异常。带有 yield 的函数不再是一个普通函数，而是一个生成器generator。可用next()调用生成器对象来取值。next 两种方式 t.next() | next(t)。 可用for 循环获取返回值（每执行一次，取生成器里面一个值） （基本上不会用next()来获取下一个返回值，而是直接使用for循环来迭代）。yield相当于 return 返回一个值，_迭代一次遇到yield时就返回yield后面(右边)的值_。并且记住这个返回的位置，下次迭代时，代码从yield的_下一条_语句开始执行。send() 和next()一样，都能让生成器继续往下走一步（下次遇到yield停），send()能传一个值，这个值作为yield表达式等号左边的值。——换句话说，就是send可以强行修改上一个yield表达式值。比如函数中有一个yield赋值，a = yield第一次迭代到这里会返回5，a还没有赋值。第二次迭代时，使用.send(10)，那么，就是强行修改yield 5表达式的值为10，本来是5的，那么a=10send(msg)与next()都有返回值，它们的返回值是当前迭代遇到yield时，yield后面表达式的值，其实就是当前迭代中yield后面的参数。感受下yield返回值的过程（关注点：每次停在哪，下次又开始在哪）及send()传参的通讯过程生成器还可以使用 next 方法迭代。生成器会在 yield 语句处暂停，这是至关重要的，未来协程中的 IO 阻塞就出现在这里。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 多进程","slug":"python-多进程","date":"2017-05-26T15:10:35.000Z","updated":"2020-04-04T11:05:03.269Z","comments":true,"path":"2017/05/26/python-多进程/","link":"","permalink":"cpeixin.cn/2017/05/26/python-%E5%A4%9A%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"多线程： 同一进程中，创建多个线程，执行添加的任务列表多进程： 创建任意个进程，各自执行添加的任务列表假如cpu只有一个（早期的计算机确实如此），也能保证支持（伪）并发的能力，将一个单独的cpu变成多个虚拟的cpu（多道技术：时间多路复用和空间多路复用+硬件上支持隔离），没有进程的抽象，现代计算机将不复存在。对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。如果我们并没有在程序中加入进程，线程，协程等，程序都是执行单任务的进程，也就是只有一个线程。如果我们要同时执行多个任务怎么办？有两种解决方案：一种是启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。还有一种方法是启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。当然还有第三种方法，就是启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。总结一下就是，多任务的实现有3种方式：多进程模式；多线程模式；多进程+多线程模式。同时执行多个任务通常各个任务之间并不是没有关联的，而是需要相互通信和协调，有时，任务1必须暂停等待任务2完成后才能继续执行，有时，任务3和任务4又不能同时执行，所以，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。什么时候用多进程1.多线程使用场景：IO密集型2.多进程使用场景：CPU密集型涉及并发的场景，大家想到使用多线程或多进程解决并发问题;一般情况下，解决多并发场景问题，多数语言采用多线程编程模式(线程是轻量级的进程，共用一份进程空间)。也同样适用于Python多并发处理吗? 不是的，针对并发处理，Python多线程和多进程是有很大差异的!Python多线程和多进程差异Python多线程不能使用CPU多核资源，即同一时刻，只有一个线程使用CPU资源，所以使用Python多线程不能算是并发。如果想要充分利用CPU多核资源，做到多并发，这就需要Python多进程的了!也就是说：只有Python多进程才能利用CPU多核资源，做到真正的多并发!Python多线程和多进程应用场景Python多线程适用于I/O密集型场景，如解决网络IO、磁盘IO阻塞问题，例如文件读写、网络数据传输等;而Python多进程更适用于计算密集型场景，多并发，大量计算任务等。注意：Python多线程和多进程在平时开发过程中，需要注意使用，如果使用Python多线程方式处理计算密集型任务，它比实际单进程处理性能还要慢!所以要注意，看场景类型。多进程-fork()Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程：123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid))结果：123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876.由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的，推荐大家用Mac学Python！有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。多进程-multiprocessing进程基础版本：1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.')执行结果如下：1234Parent process 928.Process will start.Run child process test (929)...Process end.进程池：如果要启动大量的子进程，可以用进程池的方式批量创建子进程：12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.')代码解读：对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：p = Pool(5)就可以同时跑5个进程。由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。进程池实例方法：apply(func[, args[, kwds]])：同步进程池apply_async(func[, args[, kwds[, callback[, error_callback]]]]) ：异步进程池Lock互斥锁当多个进程需要访问共享资源的时候，Lock可以用来避免访问的冲突进程之间数据隔离，但是共享一套文件系统，因而可以通过文件来实现进程直接的通信，但问题是必须自己加锁处理。注意：加锁的目的是为了保证多个进程修改同一块数据时，同一时间只能有一个修改，即串行的修改，没错，速度是慢了，牺牲了速度而保证了数据安全1234567891011121314151617181920212223242526272829303132333435363738import jsonimport timeimport randomimport osfrom multiprocessing import Process,Lockdef chakan(): dic = json.load(open('piao',)) # 先查看票数，也就是打开那个文件 print('剩余票数：%s' % dic['count']) # 查看剩余的票数def buy(): dic = json.load(open('piao',)) if dic['count']&gt;0: #如果还有票 dic['count']-=1 #就修改里面的值-1 time.sleep(random.randint(1,3)) #执行里面买票的一系列操作就先不执行了，让睡一会代替（并且随机的睡） json.dump(dic,open('piao','w')) print('%s 购票成功' % os.getpid()) # 当前的那个id购票成功def task(mutex): #抢票 # 第一种加锁： # mutex.acquire() #加锁 # chakan() # 因为查看的时候大家都可以看到，不需要加锁 # buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买 # mutex.release() #取消锁 # 第二种加锁： #with表示自动打开自动释放锁 with mutex: chakan() # 因为查看的时候大家都可以看到，不需要加锁 buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买if __name__ == '__main__': mutex = Lock() for i in range(50):#让50个人去访问那个票数 p = Process(target=task,args=(mutex,)) p.start()进程池加锁还是上面抢票的例子，这次使用了进程池，由于进程之间不共享内存，所以进程之间的通信不能像线程之间直接引用，使用进程池异步对共享变量进行操作，异步操作lock锁，会引起冲突，因而需要采取一些策略来完成进程之间的数据通信。所以要引入进程Manager来完成进程间通信的方式这里举两个例子：大家自行创建 piao 文件，内容为 {“count”:320}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import jsonimport timeimport randomimport osfrom multiprocessing import Process,Lock,Pooldef chakan(): dic = json.load(open('piao',)) # 先查看票数，也就是打开那个文件 print('剩余票数：%s' % dic['count']) # 查看剩余的票数def buy(): dic = json.load(open('piao',)) if dic['count']&gt;0: #如果还有票 dic['count']-=1 #就修改里面的值-1 time.sleep(random.randint(1,3)) #执行里面买票的一系列操作就先不执行了，让睡一会代替（并且随机的睡） json.dump(dic,open('piao','w')) print('%s 购票成功' % os.getpid()) # 当前的那个id购票成功def task(mutex): #抢票 # mutex.acquire() #加锁 # chakan() # 因为查看的时候大家都可以看到，不需要加锁 # buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买 # mutex.release() #取消锁 with mutex: chakan() # 因为查看的时候大家都可以看到，不需要加锁 buy() #买的时候必须一个一个的买，先等一个人买完了，后面的人在买if __name__ == '__main__': mutex = Lock() pool = Pool(20) \"\"\"进程池加锁 \"\"\" from multiprocessing import Pool, Manager, Lock manager = Manager() mutex = manager.Lock() for i in range(50):#让50个人去访问那个票数 pool.apply_async(task,args=(mutex, )) pool.close() pool.join()12345678910111213141516171819202122232425from multiprocessing import Process,Managerimport os# 这里实现的就是多个进程之间共享内存，并修改数据# 这里不需要加锁，因为manager已经默认给你加锁了def f(d,l): d[1] = '1' d['2'] = 2 d[0.25] = None l.append(os.getpid()) print(l)if __name__ == '__main__': with Manager() as manager: d = manager.dict() #生成一个字典 l = manager.list(range(5)) #生成一个列表 p_list = [] for i in range(10): p = Process(target=f,args=(d,l)) p.start() p_list.append(p) for res in p_list: res.join() print(d) print(l)","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"python - 多线程","slug":"python-多线程","date":"2017-05-24T15:08:35.000Z","updated":"2020-04-04T11:04:52.986Z","comments":true,"path":"2017/05/24/python-多线程/","link":"","permalink":"cpeixin.cn/2017/05/24/python-%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"巴拉巴拉最近在搞爬虫项目，架构和程序设计都是由我来决定和设计，所以发挥空间还是很自由的。程序语言选择的是python，在语言选型上也考虑过golang，golang的效率和速度肯定是要比python好很多的。但是在爬虫领域，python的易用性，超多的第三方扩展库，而且python支持协程后，效率速度方面上的表现，也是很不错的。那么在python做爬虫的过程中，多线程，多进程，协程，异步等肯定都是逃不过的。所以抽出一些时间，写一写线程，进程，协程，异步等方法在爬虫中的表现。多进程和多线程我们常见的 Linux、Windows、Mac OS 操作系统，都是支持多进程的多核操作系统。所谓多进程，就是系统可以同时运行多个任务。例如我们的电脑上运行着 QQ、浏览器、音乐播放器、影音播放器等。在操作系统中，每个任务就是一个进程。每个进程至少做一件事，多数进程会做很多事，例如影音播放器，要播放画面，同时要播放声音，在一个进程中，就有很多线程，每个线程做一件事，在一个进程中有多个线程运行就是多线程。可以在实验环境终端执行 ps -ef 命令来查看当前系统中正在运行的进程。计算机的两大核心为运算器和存储器。常说的手机配置四核、八核，指的就是 CPU 的数量，它决定了手机的运算能力；128G、256G 超大存储空间，指的就是手机存储数据的能力。当我们运行一个程序来计算 3 + 5，计算机操作系统会启动一个进程，并要求运算器派过来一个 CPU 来完成任务；当我们运行一个程序来打开文件，操作系统会启动存储器的功能将硬盘中的文件数据导入到内存中。一个 CPU 在某一时刻只能做一项任务，即在一个进程（或线程）中工作，当它闲置时，会被系统派到其它进程中。单核计算机也可以实现多进程，原理是第 1 秒的时间段内运行 A 进程，其它进程等待：第 2 秒的时间段内运行 B 进程，其它进程等待。。。第 5 秒的时间段内又运行 A 进程，往复循环。当然实际上 CPU 在各个进程间的切换是极快的，在毫秒（千分之一）、微秒（百万分之一）级，以至于我们看起来这些程序就像在同时运行。现代的计算机都是多核配置，四核八核等，但计算机启动的瞬间，往往就有几十上百个进程在运行了，所以进程切换是一定会发生的，CPU 在忙不迭停地到处赶场。注意，什么时候进行进程 、线程切换是由操作系统决定的，无法人为干预。线程安全我们都知道在 MySQL 中有 “原子操作” 的概念，打个比方：韩梅向李红转账 100 块钱，在 MySQL 中需要两步操作：韩梅账户减少 100 元，李红账户增加 100 元。如果第一步操作完成后，意外情况导致第二步没有做，这是不允许发生的，如何保证其不允许发生呢？将两步操作设计成一个事务，事务里可以有多个步骤，其中任何一步出现问题，事务都将失败，前面的步骤全部回滚，就像什么事都没发生。这种操作就叫做原子操作，这种特性就叫做原子性。在 Python 多线程中，变量是共享的，这也是相较多进程的一个优点，线程占用资源要少得多，但也导致多个 CPU 同时操作多个线程时会引起结果无法预测的问题，也就是说 Python 的线程不安全。在多线程与多进程的时候，因为一般情况下都是各自完成各自的任务，各个子线程或者各个子进程之前并没有太多的联系，如果需要通信的话我会使用队列或者数据库来完成，但是最近我在写一些多线程与多进程的代码时，发现如果它们需要用到共享变量的话，需要有一些注意的地方接下来用实例讲解一下线程共享的问题标准数据类型在线程间共享下面的实例，在主线程中创建变量d,在5个子线程中引用。123456789101112131415# coding:utf-8import threadingdef test(name, data): print(\"in thread &#123;&#125; name is &#123;&#125;\".format(threading.current_thread(), name)) print(\"data is &#123;&#125; id(data) is &#123;&#125;\".format(data, id(data)))if __name__ == '__main__': d = 5 name = \"cpeixin\" for i in range(5): th = threading.Thread(target=test, args=(name, d)) th.start()下面的结果中显示，5个子线程中打印出变量d的id相同，表示引用的同一个变量，所以说明在主线程中创建了变量d，在子线程中是可以共享的，在子线程中对共享元素的改变是会影响到其它线程的，所以如果要对共享变量进行修改时，也就是线程不安全的，需要加锁。1234567891011/Users/cpeixin/venv/pythonCode/bin/python /Users/cpeixin/PycharmProjects/pythonCode/thread/variable_thread.pyin thread &lt;Thread(Thread-1, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-2, started 123145524641792)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-3, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-4, started 123145519386624)&gt; name is cpeixindata is 5 id(data) is 4304846080in thread &lt;Thread(Thread-5, started 123145524641792)&gt; name is cpeixindata is 5 id(data) is 4304846080自定义类型对象在线程间共享如果我们要自定义一个类呢，将一个对象作为变量在子线程中传递呢？会是什么效果呢？123456789101112131415161718192021222324252627# coding:utf-8import threadingclass Data: def __init__(self, data=None): self.data = data def get(self): return self.data def set(self, data): self.data = datadef test(name, data): print(\"in thread &#123;&#125; name is &#123;&#125;\".format(threading.current_thread(), name)) print(\"data is &#123;&#125; id(data) is &#123;&#125;\".format(data.get(), id(data)))if __name__ == '__main__': d = Data(10) name = \"cpeixin\" print(\"in main thread id(data) is &#123;&#125;\".format(id(d))) for i in range(5): th = threading.Thread(target=test, args=(name, d)) th.start()1234567891011in main thread id(data) is 4348194152in thread &lt;Thread(Thread-1, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-2, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-3, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-4, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152in thread &lt;Thread(Thread-5, started 123145427701760)&gt; name is cpeixindata is 10 id(data) is 4348194152我们看到，在主线程和子线程中，这个对象的id是一样的，说明它们用的是同一个对象。无论是标准数据类型还是复杂的自定义数据类型，它们在多线程之间是共享同一个的以上就是在多线程中，变量共享的码上说明GIL 全局解释器锁如何解决线程安全问题？CPython 解释器使用了加锁的方法。每个进程有一把锁，启动线程先加锁，结束线程释放锁。打个比方，进程是一个厂房，厂房大门是开着的，门内有锁，工人进入大门后可以在内部上锁。厂房里面有 10 个车间对应 10 个线程，每个 CPU 就是一个工人。GIL（Global Interpreter Lock）全局锁就相当于厂房规定：工人要到车间工作，从厂房大门进去后要在里面反锁，完成工作后开锁出门，下一个工人再进门上锁。也就是说，任意时刻厂房里只能有一个工人，但这样就保证了工作的安全性，这就是 GIL 的原理。当然了，GIL 的存在有很多其它益处，包括简化 CPython 解释器和大量扩展的实现。根据上面的例子可以看出 GIL 实现了线程操作的安全性，但多线程的效率被大打折扣，一个工厂里只能有一个工人干活，很难想象。这也是 David Beazley（《Python 参考手册》和《Python Cookbook》的作者）说 “Python 线程毫无用处” 的原因。注意，GIL 不是语言特性，而是解释器的设计特点，有些 Python 解释器例如 JPython 就没有 GIL ，除了 Python 其它语言也有 GIL 设计，例如 Ruby 。线程锁为什么需要线程锁?多个线程对同一个数据进行修改时， 可能会出现不可预料的情况.例如实现银行转账功能，money += 1 这句其实有三个步骤 money; money+1; money=money+1;假如这三步骤还没完成money-=1的线程就开始执行了，后果可想而知，money的值肯定时乱的如何实现线程锁?实例化一个锁对象;lock = threading.Lock()操作变量之前进行加锁lock.acquire()操作变量之后进行解锁lock.release()12345678910111213141516171819202122232425262728293031323334353637import threading# 银行存钱和取钱def add(lock): global money # 生命money为全局变量 for i in range(1000000): # 2. 操作变量之前进行加锁 lock.acquire() money += 1 # money; money+1; money=money+1; # 3. 操作变量之后进行解锁 lock.release()def reduce(lock): global money for i in range(1000000): # 2. 操作变量之前进行加锁 lock.acquire() money -= 1 # 3. 操作变量之后进行解锁 lock.release()if __name__ == '__main__': money = 0 # 1. 实例化一个锁对象; lock = threading.Lock() t1 = threading.Thread(target=add, args=(lock,)) t2 = threading.Thread(target=reduce, args=(lock,)) t1.start() t2.start() t1.join() t2.join() print(\"当前金额:\", money)多线程提高工作效率实际情况并非上面讲得那么惨，Python 多线程可以成倍提高程序的运行速度，而且在多数情况下都是有效的。接着上面的例子说，一个工厂里同一时刻只能有一个工人在工作，如果这个工厂里各个车间的自动化程度极高且任务耦合度极低，工人进去只是按几下按钮，就可以等待机器完成其余工作，那情况就不一样了，这种场景下一个工人可以管理好多个车间，而且大多数时间都是等，甚至还能抽空打打羽毛球看场电影。比如爬虫程序爬取页面数据这个场景中，CPU 做的事就是发起页面请求和处理响应数据，这两步是极快的，中间网络传输数据的过程是耗时且不占用 CPU 的。一个工人可以在吃完早饭后一分钟内快速到 1000 个车间按下发起请求的按钮，吃完午饭睡一觉，日薄西山时差不多收到网络传回的数据，又用一分钟处理数据，整个程序完成。上面的场景中，CPU 再多也没有用处，一个 CPU 抽空就能完成整个任务了，毕竟程序中需要 CPU 做的事并不多。这就涉及复杂程序的分类：CPU 密集型和 IO 密集型。爬虫程序就是 IO 密集型程序。CPU 密集型程序全是手工操作，工人一刻也不能停歇，这种情况下 Python 多线程就真可以说是毫无用处了。我们可以使用 time.sleep 方法模拟 IO 操作来写一段程序证明多线程可以提高程序的运行效率：12345678910111213141516171819202122232425262728293031323334353637# File Name: thread.pyimport threadingimport timeimport requestsdef crawl_url(): # 假设这是爬虫程序，爬取一个 URL time.sleep(0.02) # 模拟 IO 操作 # res = requests.get(\"http://www.ip111.cn\").status_code # print(res)def main1(): # 单线程程序 for i in range(100): crawl_url()def main2(): # 多线程程序 thread_list = [] for i in range(100): t = threading.Thread(target=crawl_url) t.start() thread_list.append(t) for t in thread_list: t.join()if __name__ == '__main__': start = time.time() main1() end = time.time() print('单线程耗时：&#123;:.4f&#125;s'.format(end - start)) start = time.time() main2() end = time.time() print('多线程耗时：&#123;:.4f&#125;s'.format(end - start))12单线程耗时：2.4027s多线程耗时：0.0323s理论上，main1 的耗时是 main2 的 100 倍，考虑到 main2 创建多线程、线程切换的开销，这个结果也是相当可观的，IO 操作耗时越长，多线程的威力越大。线程池ThreadPool在使用多线程处理任务时也不是线程越多越好，由于在切换线程的时候，需要切换上下文环境，依然会造成cpu的大量开销。为解决这个问题，线程池的概念被提出来了。预先创建好一个较为优化的数量的线程，让过来的任务立刻能够使用，就形成了线程池。1234567891011121314# coding: utf-8from concurrent.futures import ThreadPoolExecutorimport timedef spider(page): time.sleep(page) print(f\"crawl task&#123;page&#125; finished\") return pagewith ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 task1 = t.submit(spider, 1) task2 = t.submit(spider, 2) # 通过submit提交执行的函数到线程池中 task3 = t.submit(spider, 3)使用 with 语句 ，通过 ThreadPoolExecutor 构造实例，同时传入 max_workers 参数来设置线程池中最多能同时运行的线程数目。使用 submit 函数来提交线程需要执行的任务到线程池中，并返回该任务的句柄（类似于文件、画图），注意 submit() 不是阻塞的，而是立即返回。multiprocessing.dummy.Poolmultiprocessing.dummy.Pool 是个什么东东？看起来像一个进程池的样子啊～～看了源码中的代码和注释123456789# Support for the API of the multiprocessing package using threads## multiprocessing/dummy/__init__.pyclass DummyProcess(threading.Thread)def Pool(processes=None, initializer=None, initargs=()): from ..pool import ThreadPool return ThreadPool(processes, initializer, initargs)这只是是以multiprocessing相同API实现的多线程模块。继承了Thread，内部是封装调用了ThreadPool，使用起来更加方便。异步和同步，阻塞和非阻塞上文的模拟爬虫示例代码中，main1 中的 for 循环运行 100 次爬取网页的操作，前一个完成后才能运行下一个，这就是同步的概念，在 crawl_url 函数内部的 IO 操作为阻塞操作，线程无法向下执行。main2 中的第一个 for 循环，_创建 100 个线程并启动，这步操作是非阻塞的_，不会等一个线程运行完成才创建下一个线程，它会一气儿创建 100 个线程；第二个 for 循环将主线程挂起，直到全部子线程完成，此时的主线程就是阻塞的。这种程序运行方式叫做异步，CPU 在遇到 IO 阻塞时不会站在那儿傻等，而是被操作系统派往其它线程中看看有什么事可做。所谓的异步，就是 CPU 在当前线程阻塞时可以去其它线程中工作，不管怎么设计，在一个线程内部代码都是顺序执行的，遇到 IO 都得阻塞，所谓的非阻塞，是遇到当前线程阻塞时，CPU 去其它线程工作。","categories":[{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"}]},{"title":"Spark 单元测试","slug":"Spark-单元测试","date":"2017-05-19T09:52:52.000Z","updated":"2020-05-19T09:57:56.873Z","comments":true,"path":"2017/05/19/Spark-单元测试/","link":"","permalink":"cpeixin.cn/2017/05/19/Spark-%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","excerpt":"","text":"Spark 单元测试相比于传统代码，Spark是比较难调试的。程序运行在集群中，每次修改代码后，都要上传到集群进行测试，代价非常大，所以优先在本地进行单元测试，可以减少小模块的逻辑错误。ScalaTest 测试框架ScalaTest是比JUnit和TestNG更加高阶的测试编写工具，这个Scala应用在JVM上运行，可以测试Scala以及Java代码。ScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。FunSuite的方式较为灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包。而FlatSpec和FunSpec则通过提供诸如it、should、describe等方法，来规定书写测试的一种模式。Maven中引入：1234567&lt;!-- Test Dependency --&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalatest&lt;/groupId&gt; &lt;artifactId&gt;scalatest_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;首先定义好我们要测试的函数，下面的程序中，我们需要对 count(rdd:RDD[String]): RDD[(String,Int)] 进行测试1234567891011121314151617181920212223242526package unit_testimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDobject WordCount extends Serializable&#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setAppName(\"test_rdd\").setMaster(\"local\") val sc = new SparkContext(sparkConf) val make_rdd: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\")) val result_rdd: RDD[(String, Int)] = count(make_rdd) result_rdd.foreach(println) &#125; def count(rdd:RDD[String]): RDD[(String,Int)]=&#123; val wordcount_rdd: RDD[(String, Int)] =rdd.map((word: String) =&gt;(word,1)) .reduceByKey((_: Int) + (_: Int)) wordcount_rdd &#125;&#125;创建测试类：12345678910111213141516171819202122232425262728293031323334353637383940import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.scalatest.&#123;BeforeAndAfter, FlatSpec&#125;import unit_test.WordCount//引入scalatest建立一个单元测试类，混入特质BeforeAndAfter，在before和after中分别初始化sc和停止sc，//初始化SparkContext时只需将Master设置为local(local[N],N表示线程)即可，无需本地配置或搭建集群，class WordCountTests extends FlatSpec with BeforeAndAfter&#123; val master=\"local\" //sparkcontext的运行master var sc:SparkContext=_ \"wordcount_class\" should \"map word ,1\" in&#123; //其中参数为rdd或者dataframe可以通过通过简单的手动构造即可 val seq=Seq(\"Brent\",\"HayLee\",\"Henry\") val rdd=sc.parallelize(seq) val wordCounts=WordCount.count(rdd) wordCounts.map(p=&gt;&#123; p._1 match &#123; case \"Brent\"=&gt; assert(p._2==1)// 断言 case \"HayLee\"=&gt; assert(p._2==1) case \"Henry\"=&gt; assert(p._2==1) case _=&gt; None &#125; &#125;).foreach(_=&gt;()) &#125; //这里before和after中分别进行sparkcontext的初始化和结束，如果是SQLContext也可以在这里面初始化 before&#123; val conf=new SparkConf() .setAppName(\"test\").setMaster(master) sc=new SparkContext(conf) &#125; after&#123; if(sc!=null)&#123; sc.stop() &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Streaming + ELK + HBase","slug":"Spark-Streaming-ELK-HBase","date":"2017-04-22T15:26:15.000Z","updated":"2020-05-06T16:57:17.655Z","comments":true,"path":"2017/04/22/Spark-Streaming-ELK-HBase/","link":"","permalink":"cpeixin.cn/2017/04/22/Spark-Streaming-ELK-HBase/","excerpt":"","text":"在Spark Streaming的业务场景中，大多数的业务需求是针对实时数据做数据统计，网站数据，App数据的监控分析，或者是对抓取的实时数据做ETL，再进行展示，以及实时的推荐系统等。目前公司正在做的项目，针对公司内部运营部门的需求，一方面统计时尚，教育，科技等行业的最新动态，二则是对公司编辑人员提供目前行业热点素材，以热点图的方式展示，点击相应的关键词，并提取出相应的素材。那么在整体的架构上：数据源来自Python的定向爬虫消息队列采用Kafka数据处理采用Spark Streaming，对搜集的素材做ETL以及关键词提取等数据存储采用HBase，Elasticsearch， HBase存储全量数据，做历史数据的备份，方便于后续的数据分析，Elasticsearch搭配HBase做二级索引方案，同时存储部分数据，对戒Kibana做数据可视化。下面对上面的业务场景做一个简单的代码实例，其中没有写入项目的数据逻辑，那自己电脑中的数据进行了模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package writeimport java.security.MessageDigestimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.hadoop.hbase.TableNameimport org.apache.hadoop.hbase.client.&#123;Connection, Put, Table&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.elasticsearch.spark.streaming.EsSparkStreamingimport utils.HBaseUtilimport scala.util.Tryobject streaming_to_hbase_1 &#123; val logger:Logger = Logger.getRootLogger Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"spark streaming window\") .setMaster(\"local[2]\") .set(\"spark.es.nodes\", \"localhost\") .set(\"spark.es.port\", \"9200\") .set(\"es.index.auto.create\", \"true\") val ssc = new StreamingContext(conf, Seconds(5)) val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest，latest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: DStream[(String, String)] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) .map((x: ConsumerRecord[String, String]) =&gt; &#123; val json_data: JSONObject = JSON.parseObject(x.value()) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString (date_time, keywordList) &#125;) // ES 数据写入部分 val es_dstream: DStream[String] = kafkaStream.map((x: (String, String)) =&gt; &#123; val date_time: String = x._1 val keywordList: String = x._2 val data_es_json: JSONObject = new JSONObject() data_es_json.put(\"date_time\", date_time) data_es_json.put(\"keyword_list\", keywordList) data_es_json.put(\"rowkey\", MD5Encode(date_time)) data_es_json.toJSONString &#125;) EsSparkStreaming.saveJsonToEs(es_dstream,\"weibo_keyword-2017-04-25/default\") //HBase 数据写入部分 kafkaStream.foreachRDD((rdd: RDD[(String, String)]) =&gt; &#123; rdd.foreachPartition((partitionRecords: Iterator[(String, String)]) =&gt; &#123;//循环分区 try &#123; val connection: Connection = HBaseUtil.getHBaseConn //获取HBase连接,分区创建一个连接，分区不跨节点，不需要序列化 partitionRecords.foreach((s: (String, String)) =&gt; &#123; val tableName: TableName = TableName.valueOf(\"t_weibo_keyword\") val table: Table = connection.getTable(tableName)//获取表连接 var date_time: String = s._1 val keywordList: String = s._2 val put = new Put(Bytes.toBytes(MD5Encode(date_time))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"keywordLisr\"), Bytes.toBytes(keywordList)) Try(table.put(put)).getOrElse(table.close())//将数据写入HBase，若出错关闭table// table.close()//分区数据写入HBase后关闭连接 &#125;) &#125; catch &#123; case e: Exception =&gt; logger.info(e) logger.info(\"写入HBase失败\") &#125; &#125;) &#125;) ssc.start() ssc.awaitTermination() &#125; def MD5Encode(input: String): String = &#123; // 指定MD5加密算法 val md5: MessageDigest = MessageDigest.getInstance(\"MD5\") // 对输入数据进行加密,过程是先将字符串中转换成byte数组,然后进行随机哈希 val encoded: Array[Byte] = md5.digest(input.getBytes) // 将加密后的每个字节转化成十六进制，一个字节8位，相当于2个16进制，不足2位的前面补0 encoded.map(\"%02x\".format(_: Byte)).mkString &#125;&#125;es在maven中引入的依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;&#x2F;groupId&gt; &lt;artifactId&gt;elasticsearch-spark-20_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;6.7.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;kibana中的结果展示","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Streaming 进阶","slug":"Spark-Streaming-进阶","date":"2017-04-14T14:22:12.000Z","updated":"2020-05-05T16:34:55.494Z","comments":true,"path":"2017/04/14/Spark-Streaming-进阶/","link":"","permalink":"cpeixin.cn/2017/04/14/Spark-Streaming-%E8%BF%9B%E9%98%B6/","excerpt":"","text":"初始化要初始化Spark Streaming程序，必须创建StreamingContext对象，该对象是所有Spark Streaming功能的主要入口点。1234val conf: SparkConf = new SparkConf() .setAppName(\"your application name\") .setMaster(\"local[2]\")val ssc = new StreamingContext(conf, Seconds(5))maven依赖：123456&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;/dependency&gt;该appName参数是您的应用程序在集群UI上显示的名称。 master是Spark，Mesos，Kubernetes或YARN群集URL或特殊的“ local []”字符串，以本地模式运行。实际工作中，程序部署、运行在集群上，所以并不希望master在程序中进行硬编码，而是在提交程序的spark-submit –master * 命令中来指定。如果只是本地IDEA运行，则可指定 local。在初始化的代码中，我们要设置每个批处理的时间间隔，上面代码中 Seconds(5)，也就是5秒划分一个批次。我们打开源码，可以看到，StreamingContext（）第二个参数还有其他选择，最终都是将时间转换成毫秒。**DstreamDStream由一系列连续的RDD表示，这是Spark对不可变的分布式数据集的抽象。DStream中的每个RDD都包含来自特定间隔的数据。这里，我们来看一下下面的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package streamingimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object streaming_case &#123; // 设置日志级别 Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"Kafka Streaming\") .setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(10)) ssc.checkpoint(\"/Users/cpeixin/IdeaProjects/code_warehouse/spark_streaming/src/main/scala/streaming/\") val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) kafkaStream.foreachRDD((x: RDD[ConsumerRecord[String, String]]) =&gt;println(x)) ssc.start() ssc.awaitTermination() &#125; def change_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;结果：1KafkaRDD[0] at createDirectStream at streaming_case.scala:39上面的代码，在42行foreachRDD的中，我们打印DStream中的RDD，结果中我们看到，第一个批次中，只有一个RDD，这里我想说的是，在上面的这种读取数据代码中，一个 batch Interval中，DStream 只有一个RDD，当一个新的时间窗口（batchInterval)开始时，此时产生一个空的block，此后在这个窗口内接受到的数据都会累加到这个block上，当这个时间窗口结束时，停止累加，这个block对应的数据就是这个时间窗口对应的RDD包含的数据这里我们还可以深入 slideDuration：Duration来看，和后面要讲的窗口函数windiw（）中，RDD的区别。batch interval关于Spark Streaming的批处理时间设置是非常重要的，Spark Streaming在不断接收数据的同时，需要处理数据的时间，所以如果设置过段的批处理时间，会造成数据堆积，即未完成的batch数据越来越多，从而发生阻塞。另外值得注意的是，batchDuration本身也不能设置为小于500ms，这会导致Spark Streaming进行频繁地提交作业，造成额外的开销，减少整个系统的吞吐量；相反如果将batchDuration时间设置得过长，又会影响整个系统的吞吐量。如何设置一个合理的批处理时间，需要根据应用本身、集群资源情况，以及关注和监控Spark Streaming系统的运行情况来调整，重点关注Spark Web UI监控界面中的Total Delay，来进行调整。CheckPoint我们所编写的实时计算程序大多数都是24小时全天候生产环境运行的，因此必须对与应用程序逻辑无关的故障（例如，系统故障，JVM崩溃等）具有弹性。为此，Spark Streaming需要将足够的信息检查点指向容错存储系统，以便可以从故障中恢复。检查点有两种类型的数据。元数据检查点-将定义流计算的信息保存到HDFS等容错存储中。这用于从运行流应用程序的驱动程序的节点的故障中恢复。元数据包括：配置 用于创建流应用程序的配置。DStream操作 -定义流应用程序的DStream操作集。不完整的批次 -作业排队但尚未完成的批次。数据检查点 将生成的RDD保存到可靠的存储中。在一些有状态转换中，这需要跨多个批次合并数据，这是必需的。在此类转换中，生成的RDD依赖于先前批次的RDD，这导致依赖项链的长度随时间不断增加。为了避免恢复时间的这种无限制的增加（与依赖关系链成比例），有状态转换的中间RDD定期 检查点到可靠的存储（例如HDFS）以切断依赖关系链。总而言之，metadata checkpointing主要还是从drvier失败中恢复，而Data Checkpoing用于对有状态的transformation操作进行checkpointingCheckpoint和persist从根本上是不一样的：1、Cache or persist:Cache or persist保存了RDD的血统关系，假如有部分cache的数据丢失可以根据血缘关系重新生成。2、Checkpoint会将RDD数据写到hdfs这种安全的文件系统里面，并且抛弃了RDD血缘关系的记录。即使persist存储到了磁盘里面，在driver停掉之后会被删除，而checkpoint可以被下次启动使用。何时启用检查点必须为具有以下任一要求的应用程序启用检查点：有状态转换的用法 -如果在应用程序中使用updateStateByKey或reduceByKeyAndWindow（带有反函数），则必须提供检查点目录以允许定期进行RDD检查点。从运行应用程序的驱动程序故障中恢复 -元数据检查点用于恢复进度信息。注意，没有前述状态转换的简单流应用程序可以在不启用检查点的情况下运行。在这种情况下，从驱动程序故障中恢复也将是部分的（某些已接收但未处理的数据可能会丢失）。这通常是可以接受的，并且许多都以这种方式运行Spark Streaming应用程序。预计将来会改善对非Hadoop环境的支持。如何配置检查点**可以通过在容错，可靠的文件系统（例如，HDFS，S3等）中设置目录来启用检查点，将检查点信息保存到该目录中。这是通过使用完成的streamingContext.checkpoint(checkpointDirectory)。这将允许您使用前面提到的有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流应用程序以具有以下行为。程序首次启动时，它将创建一个新的StreamingContext，设置所有流，然后调用start（）。失败后重新启动程序时，它将根据检查点目录中的检查点数据重新创建StreamingContext。代码如下：123456789101112131415161718def functionToCreateContext(): StreamingContext = &#123; val ssc = new StreamingContext(...) // new context val lines = ssc.socketTextStream(...) // create DStreams ... ssc.checkpoint(checkpointDirectory) // set checkpoint directory ssc&#125;// Get StreamingContext from checkpoint data or create a new oneval context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)// Do additional setup on context that needs to be done,// irrespective of whether it is being started or restartedcontext. ...// Start the contextcontext.start()context.awaitTermination()请注意，RDD的检查点会导致保存到可靠存储的成本。这可能会导致RDD获得检查点的那些批次的处理时间增加。因此，需要仔细设置检查点的间隔。在小批量（例如1秒）时，每批检查点可能会大大降低操作吞吐量。相反，检查点太不频繁会导致沿袭和任务规模增加，这可能会产生不利影响。对于需要RDD检查点的有状态转换，默认间隔为批处理间隔的倍数，至少应为10秒。可以使用设置 dstream.checkpoint(checkpointInterval)。通常，DStream的5-10个滑动间隔的检查点间隔是一个很好的尝试设置。checkpoint时机在spark Streaming中，JobGenerator用于生成每个batch对应的jobs，它有一个定时器，定时器 的周期即初始化StreamingContext时设置batchDuration。这个周期一到，JobGenerator将调用generateJobs方法来生成并提交jobs，这之后调用doCheckpoint方法来进行checkpoint。doCheckpoint方法中，会判断 当前时间与streaming application start的时间只差是否是 checkpoint duration的倍数，只有在是的情况下才进行checkpoint。具体应用实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119package org.apache.spark.examples.streamingimport java.io.Fileimport java.nio.charset.Charsetimport com.google.common.io.Filesimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.broadcast.Broadcastimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext, Time&#125;import org.apache.spark.util.&#123;IntParam, LongAccumulator&#125;/** * Use this singleton to get or register a Broadcast variable. */object WordBlacklist &#123; @volatile private var instance: Broadcast[Seq[String]] = null def getInstance(sc: SparkContext): Broadcast[Seq[String]] = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; val wordBlacklist = Seq(\"a\", \"b\", \"c\") instance = sc.broadcast(wordBlacklist) &#125; &#125; &#125; instance &#125;&#125;/** * Use this singleton to get or register an Accumulator. */object DroppedWordsCounter &#123; @volatile private var instance: LongAccumulator = null def getInstance(sc: SparkContext): LongAccumulator = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; instance = sc.longAccumulator(\"WordsInBlacklistCounter\") &#125; &#125; &#125; instance &#125;&#125;object RecoverableNetworkWordCount &#123; def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String) : StreamingContext = &#123; // If you do not see this printed, that means the StreamingContext has been loaded // from the new checkpoint println(\"Creating new context\") val outputFile = new File(outputPath) if (outputFile.exists()) outputFile.delete() val sparkConf = new SparkConf().setAppName(\"RecoverableNetworkWordCount\") // Create the context with a 1 second batch size val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(checkpointDirectory) // Create a socket stream on target ip:port and count the // words in input stream of \\n delimited text (eg. generated by 'nc') val lines = ssc.socketTextStream(ip, port) val words = lines.flatMap(_.split(\" \")) val wordCounts = words.map((_, 1)).reduceByKey(_ + _) wordCounts.foreachRDD &#123; (rdd: RDD[(String, Int)], time: Time) =&gt; // Get or register the blacklist Broadcast val blacklist = WordBlacklist.getInstance(rdd.sparkContext) // Get or register the droppedWordsCounter Accumulator val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext) // Use blacklist to drop words and use droppedWordsCounter to count them val counts = rdd.filter &#123; case (word, count) =&gt; if (blacklist.value.contains(word)) &#123; droppedWordsCounter.add(count) false &#125; else &#123; true &#125; &#125;.collect().mkString(\"[\", \", \", \"]\") val output = s\"Counts at time $time $counts\" println(output) println(s\"Dropped $&#123;droppedWordsCounter.value&#125; word(s) totally\") println(s\"Appending to $&#123;outputFile.getAbsolutePath&#125;\") Files.append(output + \"\\n\", outputFile, Charset.defaultCharset()) &#125; ssc &#125; def main(args: Array[String]): Unit = &#123; if (args.length != 4) &#123; System.err.println(s\"Your arguments were $&#123;args.mkString(\"[\", \", \", \"]\")&#125;\") System.err.println( \"\"\" |Usage: RecoverableNetworkWordCount &lt;hostname&gt; &lt;port&gt; &lt;checkpoint-directory&gt; | &lt;output-file&gt;. &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark | Streaming would connect to receive data. &lt;checkpoint-directory&gt; directory to | HDFS-compatible file system which checkpoint data &lt;output-file&gt; file to which the | word counts will be appended | |In local mode, &lt;master&gt; should be 'local[n]' with n &gt; 1 |Both &lt;checkpoint-directory&gt; and &lt;output-file&gt; must be absolute paths \"\"\".stripMargin ) System.exit(1) &#125; val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args val ssc = StreamingContext.getOrCreate(checkpointDirectory, () =&gt; createContext(ip, port, outputPath, checkpointDirectory)) ssc.start() ssc.awaitTermination() &#125;&#125;UpdateStateByKey流处理主要有3种应用场景：无状态操作、window操作、状态操作。updateStateByKey就是典型的状态操作。下面是针对updateStateByKey举的实例，主要功能是针对流数据中的关键词进行统计，并且是根据历史状态持续统计。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package streamingimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject streaming_updatastatebykey &#123; Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"Kafka Streaming\") .setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(2)) ssc.checkpoint(\"/Users/cpeixin/IdeaProjects/code_warehouse/spark_streaming/src/main/scala/streaming/\") val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"latest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) val wordcount_dstream: DStream[(String, Int)] = kafkaStream .map((x: ConsumerRecord[String, String]) =&gt; &#123; change_data(x.value()) &#125;) .flatMap((_: String).split(\",\")) .map((x: String) =&gt; (x, 1)) val sum_dstream: DStream[(String, Int)] = wordcount_dstream.updateStateByKey((seq: Seq[Int], state: Option[Int]) =&gt; &#123; var sum: Int = state.getOrElse(0)+seq.sum Option(sum) &#125;) sum_dstream.foreachRDD((keywordFormat_rdd: RDD[(String, Int)]) =&gt; &#123; val sort_rdd: Array[(Int, String)] = keywordFormat_rdd.map((x: (String, Int)) =&gt; &#123;(x._2, x._1)&#125;).sortByKey().top(10) sort_rdd.foreach(println) println(\"====================\") &#125;) ssc.start() ssc.awaitTermination() &#125; def change_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;打印统计信息：12345678910111213141516171819202122&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;(8,剪头)(5,黑人抬棺队长称将环游世界)(5,黑人)(5,队长)(5,环游世界)(4,野餐)(4,这野餐也太实在了吧)(3,郑钧低空飞行)(3,郑钧)(3,森林)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;(8,剪头)(7,森林)(7,挪威)(7,伍佰 挪威的森林)(7,伍佰)(6,肤色)(6,状态)(6,今年夏天的肤色状态)(6,今年夏天)(5,黑人抬棺队长称将环游世界)注意：类似updateStateByKey和mapWithState等有状态转换算子，程序中必须要指定checkpoint检查点。窗口函数Spark Streaming还提供了_窗口计算_，可让您在数据的滑动窗口上应用转换。下图说明了此滑动窗口。如该图所示，每当窗口_滑动_在源DSTREAM，落入窗口内的源RDDS被组合及操作以产生RDDS的窗DSTREAM。在这种特定情况下，该操作将应用于数据的最后3个时间单位，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数。窗口长度 - _窗口_的持续时间。滑动间隔 -进行窗口操作的间隔。这两个参数必须是源DStream的批处理间隔的倍数下面给出实例代码，描述的场景是每10秒统计一次过去30秒期间，关键词出现次数的top 51234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package streamingimport com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.&#123;Duration, Seconds, StreamingContext&#125;object streaming_window &#123; Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"spark streaming window\") .setMaster(\"local[2]\") val sc = new StreamingContext(conf, Seconds(5)) val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"latest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](sc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) val wordcount_dstream: DStream[(String, Int)] = kafkaStream .map((x: ConsumerRecord[String, String]) =&gt; &#123; get_data(x.value()) &#125;) .flatMap((_: String).split(\",\")) .map((x: String) =&gt; (x, 1)) val window_dstream: DStream[(String, Int)] = wordcount_dstream.reduceByKeyAndWindow((x: Int,y: Int)=&gt;x+y,Seconds(30), Seconds(10)) val result: DStream[(String, Int)] = window_dstream.transform((rdd: RDD[(String, Int)]) =&gt;&#123; val top3: Array[(String, Int)] = rdd.map((x: (String, Int)) =&gt;(x._2,x._1)).sortByKey(ascending = false).map((x: (Int, String)) =&gt;(x._2,x._1)).take(5) sc.sparkContext.makeRDD(top3) &#125;) result.print() sc.start() sc.awaitTermination() &#125; def get_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;结果：1234567891011121314151617181920212223242526272829303132333435-------------------------------------------Time: 1588694330000 ms-------------------------------------------(台版,6)(黑人,6)(台版101模仿黑人抬棺,6)(训练,2)(听起来很厉害的专业术语,2)-------------------------------------------Time: 1588694340000 ms-------------------------------------------(野餐,10)(野餐还没拍好照就被牛吃了,10)(台版,6)(黑人,6)(台版101模仿黑人抬棺,6)-------------------------------------------Time: 1588694350000 ms-------------------------------------------(姐姐,10)(野餐,10)(野餐还没拍好照就被牛吃了,10)(乘风破浪的姐姐们,10)(台版,6)-------------------------------------------Time: 1588694360000 ms-------------------------------------------(姐姐,10)(野餐,10)(野餐还没拍好照就被牛吃了,10)(乘风破浪的姐姐们,10)(叶冲太难了,9)transform操作，应用在DStream上时，可以用于执行任意的RDD到RDD的转换操作；它可以用于实现，DStream API中所没有提供的操作；比如说，DStream API中，并没有提供将一个DStream中的每个batch，与一个特定的RDD进行join的操作。但是我们自己就可以使用transform操作来实现该功能。这里看 reduceByKeyAndWindow（）函数的第二个参数和第三个参数，分别代表的意义就是，窗口的长度和窗口滑动间隔。这里还需要知道一点，Dstream中的RDD也可以调用persist()方法保存在内存当中，但是基于window和state的操作，reduceByWindow,reduceByKeyAndWindow,updateStateByKey等它们已经在源码中默认persist了**","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Streaming 讲解","slug":"Spark-Streaming-讲解","date":"2017-04-10T14:22:12.000Z","updated":"2020-05-04T14:25:36.548Z","comments":true,"path":"2017/04/10/Spark-Streaming-讲解/","link":"","permalink":"cpeixin.cn/2017/04/10/Spark-Streaming-%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"简述Spark Streaming是核心Spark API的扩展，近实时计算框架。特点可伸缩，高吞吐量，容错流处理。而我们之前讲的Spark SQL是负责处理离线数据。既然是计算框架，那么实战的过程中还是三个步骤，读取数据源、计算数据、数据存储：数据源：可以从Kafka, Flume, Kinesis, or TCP sockets等数据源读入数据计算：同样可以使用同样的map，reduce，join等算子进行数据计算，还有Spark Streaming 特有的window窗口函数。数据存储：可以将处理后的数据推送到文件系统HDFS，数据库的话比较常用的是MySQL，Mongo DB和HBase等，对于实时数据的展示和监控方面，我们比较常用的是将数据写入Elasticsearch中，并且使用Kibana或者Grafana来进行展示。Spark Streaming 提供一个对于流数据的抽象 DStream。DStream 可以由来自 Apache Kafka、Flume 或者 HDFS 的流数据生成，也可以由别的 DStream 经过各种转换操作得来，底层 DStream 也是由很多个序列化的 RDD 构成，按时间片（比如一秒）切分成的每个数据单位都是一个 RDD。然后，Spark 核心引擎将对 DStream 的 Transformation 操作变为针对 Spark 中对 RDD 的 Transformation 操作，将 RDD 经过操作变成中间结果保存在内存中。之前的 DataFrame 和 DataSet 也是同样基于 RDD，所以说 RDD 是 Spark 最基本的数据抽象。就像 Java 里的基本数据类型（Primitive Type）一样，所有的数据都可以用基本数据类型描述。也正是因为这样，无论是 DataFrame，还是 DStream，都具有 RDD 的不可变性、分区性和容错性等特质。所以，Spark 是一个高度统一的平台，所有的高级 API 都有相同的性质，它们之间可以很容易地相互转化。Spark 的野心就是用这一套工具统一所有数据处理的场景。由于 Spark Streaming 将底层的细节封装起来了，所以对于开发者来说，只需要操作 DStream 就行。接下来，让我们一起学习 DStream 的结构以及它支持的转换操作对比一般在讲述Spark Streaming的时候，其他博主都会列一个表格，将Storm，Spark Streaming，Flink三个实时计算框架进行对比，这里我就不进行三者之间的比较了，直接根据我这几年使用的经验来分享一下直接的结果，Storm是一个早期的流式计算框架，可以做到毫秒级别的计算，但是编程语言使用的是Java，代码量很多，编程复杂度会高一些，并且目前很少有公司使用Storm，我实习期后，在第一家公司的第一个任务就是改写现有的Storm任务，迁移到Spark Streaming。Spark Streaming的流式计算，准确的来讲，可以说是近实时或者微批计算，所以在一般情况下，对于时间要求不是太严格的情况下，Spark Streaming可以满足大部分的场景了。支持Java，Scala， Python三种语言。但是Spark Streaming有一个缺点，就是某些需求中，我们的计算结果应该是基于event time数据产生时间，但是在Spark Streaming中，有时会因为延迟的原因，计算结果会基于process time 数据到达的时间。针对这个问题，我在后面的Structured Streaming也会讲到。Flink可以说是流式计算的后起之秀，并且就是为了流计算而诞生，它采用了基于操作符（Operator）的连续流模型，可以做到微秒级别的延迟。同样支持Java，Scala，Python三种语言，在我还认认真真写Spark代码的时候，每天都能看到铺天盖地的Flink新闻，Flink 用流处理去模拟批处理的思想，比 Spark 用批处理去模拟流处理的思想扩展性更好，所以我相信将来 Flink 会发展的越来越好，生态和社区各方面追上 Spark。比如，阿里巴巴就基于 Flink 构建了公司范围内全平台使用的数据处理平台 Blink，美团、饿了么等公司也都接受 Flink 作为数据处理解决方案。实例这里用一个实例来展示一下实际工作中，Spark Streaming的大概流程：数据的采集或者网站数据收集—&gt;Kafka—&gt;Spark Streaming—&gt;show data or save data下面这端python代码是数据采集或者收集，打到Kafka的阶段。我本想写个爬虫来做实时数据的，但是我想起来我的Elasticsearch集群中正在实时的爬取某博数据，所以就直接间隔调用ES，来打入Kafka中。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-import timefrom kafka import KafkaProducerimport jsonfrom elasticsearch import Elasticsearchproducer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8'),bootstrap_servers=['localhost:9092'])def getEsArticle(): \"\"\"动态链接\"\"\" es = Elasticsearch( ['xxx.xxx.xxx.xxx:9205'], # 在做任何操作之前，先进行嗅探 sniff_on_start=True, # 节点没有响应时，进行刷新，重新连接 sniff_on_connection_fail=True, # 每 60 秒刷新一次 sniffer_timeout=10 ) # 模糊查询 query = &#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;, \"sort\": &#123; \"datetime\": &#123; \"order\": \"desc\" # 降序 &#125; &#125;, \"size\": 10 &#125; esResult = es.search(index=\"article_warehouse*\", body=query) for hit in esResult['hits']['hits']: data = &#123;'datetime': hit[\"_source\"][\"datetime\"], 'keywordList': hit[\"_source\"][\"keywordList\"]&#125; print(data) producer.send('weibo_keyword', data)def main(): while True: getEsArticle() time.sleep(10)if __name__ == '__main__': main()打入数据的格式：12345678910111213&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:42&#39;, &#39;keywordList&#39;: &#39;特朗普,得志,领头羊,治国,爱心,美国,本领&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:56&#39;, &#39;keywordList&#39;: &#39;车型,整体,长安,手动挡,自动,便利性,新手,平顺,发动机,版本,市区,积炭,编想,小编,驻车,速手,次顶,小伙伴,车师,省心&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:53&#39;, &#39;keywordList&#39;: &#39;同桌,一个男孩,高三,人才,感觉&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:53&#39;, &#39;keywordList&#39;: &#39;玩家,段位,落地,钢枪,游戏,新人,机器人,成盒,开局,军事基地,低端,小学生,精英,高水平,惩罚,谢谢,字数,高端,优质,战场&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:52&#39;, &#39;keywordList&#39;: &#39;公话,情侣,青春校园,放学,校园,同学,印象,家人&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:52&#39;, &#39;keywordList&#39;: &#39;阿萨德,表哥,经济命脉,皇亲国戚,叙利亚,动手,国家&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:51&#39;, &#39;keywordList&#39;: &#39;集训,高带,体工队,大牌,劲旅,球员,南亚,浪费,印度,时代,国家&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:51&#39;, &#39;keywordList&#39;: &#39;图片,老师,全校,同桌,爱慕,姨妈,手册,男孩,青春,窗户,主权,我会,毕业&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:51&#39;, &#39;keywordList&#39;: &#39;段位,玩家,分会,分数,战场&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:50&#39;, &#39;keywordList&#39;: &#39;杯子,盒子,自习室,专四,保送生,作文,单词,礼物,伤心,考试,耳朵,距离,姑娘&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:29:50&#39;, &#39;keywordList&#39;: &#39;边路传,有球,曼联,全员,出力&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:30:07&#39;, &#39;keywordList&#39;: &#39;T恤,女生,身材,时尚,女装,印花,袖口,条纹,小编,板型,潘领,蓬蓬裙,踝款,百褶裙,碎花,拜拜,黑白相间,牛仔,牛仔裤,深色&#39;&#125;&#123;&#39;datetime&#39;: &#39;2017-04-10 18:30:06&#39;, &#39;keywordList&#39;: &#39;周梅森,经济&#39;&#125;下面的spark streaming代码比较简单，只是读取数据，从json格式中解析出来，随后打印，这里我没有直接写入的数据库或者es中，主要是给出一个spark streaming的程序框架。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package streamingimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;HashPartitioner, SparkConf&#125;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import org.apache.log4j.&#123;Level, Logger&#125;object streaming_weibo &#123; // 设置日志级别 Logger.getLogger(\"org\").setLevel(Level.ERROR) def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf() .setAppName(\"Kafka Streaming\") .setMaster(\"local[2]\") val ssc = new StreamingContext(conf, Seconds(10)) ssc.checkpoint(\"/Users/cpeixin/IdeaProjects/code_warehouse/spark_streaming/src/main/scala/streaming/\") val kafkaParams: Map[String, Object] = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"kafka_spark_streaming\", \"auto.offset.reset\" -&gt; \"earliest\", // earliest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"weibo_keyword\") val kafkaStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils .createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams)) kafkaStream.map((x: ConsumerRecord[String, String]) =&gt; &#123; get_data(x.value()) &#125;).foreachRDD((x: RDD[String]) =&gt; x.foreach(println)) ssc.start() ssc.awaitTermination() &#125; def get_data(string_data: String): String = &#123; val json_data: JSONObject = JSON.parseObject(string_data) val date_time: String = json_data.get(\"datetime\").toString val keywordList: String = json_data.get(\"keywordList\").toString keywordList &#125;&#125;12345678910111213141516手机,安卓,像素,摄像头,国产品牌,品牌,华为,小米,旗舰,英寸,屏幕,电池容量,对焦,逆光,音质,系统,后置,机身,光线,光学电场线,本子,样子,铃响,心形,卷子,猫儿,异性,男朋友,板凳,电荷,我会,桌子,小学范畴,心情,能力,东西车型,整体,长安,手动挡,自动,便利性,新手,平顺,发动机,版本,市区,积炭,编想,小编,驻车,速手,次顶,小伙伴,车师,省心高尔夫,大众,领速,高嘉,技术实力,试车,性价比,舒适性,内饰,亮点,亲戚,重庆,消费者,动力,空间,人士,原因,产品,情况同桌,一个男孩,高三,人才,感觉方向机,漆面,大叔,差点,图片公话,情侣,青春校园,放学,校园,同学,印象,家人棒棒,东忘西,太久,方言,老汉,老头,老师,时间图片,老师,全校,同桌,爱慕,姨妈,手册,男孩,青春,窗户,主权,我会,毕业刹车,啊啊啊,篮球场,转圈,脸红,张开,手臂,汉子,口气,朋友段位,玩家,分会,分数,战场法学专家谈拒绝加班被判赔1.8万,被判赔,法学,专家杯子,盒子,自习室,专四,保送生,作文,单词,礼物,伤心,考试,耳朵,距离,姑娘思域,品味周梅森,经济","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 性能优化方向","slug":"Spark-性能优化方向","date":"2017-04-05T15:48:43.000Z","updated":"2020-04-29T13:25:12.537Z","comments":true,"path":"2017/04/05/Spark-性能优化方向/","link":"","permalink":"cpeixin.cn/2017/04/05/Spark-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91/","excerpt":"","text":"目前大数据业务中，Spark开发任务是很多的，一周一个小任务，两三个月一个大项目，根据不同公司数据量的大小，那么Spark任务需要优化的程度也就有所不同，一般公司中的大数据集群计算资源都是有限的，所以如果某个Spark任务所占用的资源过大，则会影响Yarn队列中，其他任务的运行，尤其是在生产环境下，这种情况还是要避免发生的。在大数据使用、开发过程的性能优化一般可以从以下角度着手进行。SQL 语句优化使用关系数据库的时候，SQL 优化是数据库优化的重要手段，因为实现同样功能但是不同的 SQL 写法可能带来的性能差距是数量级的。我们知道在大数据分析时，由于数据量规模巨大，所以 SQL 语句写法引起的性能差距就更加巨大。典型的就是 Hive 的 MapJoin 语法，如果 join 的一张表比较小，比如只有几 MB，那么就可以用 MapJoin 进行连接，Hive 会将这张小表当作 Cache 数据全部加载到所有的 Map 任务中，在 Map 阶段完成 join 操作，无需 shuffle。数据倾斜处理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。淘宝的产品经理曾经讲过一个案例，他想把用户日志和用户表通过用户 ID 进行 join，但是日志表有几亿条 记录的用户 ID 是 null，Hive 把 null 当作一个字段值 shuffle 到同一个 Reduce，结果这个 Reduce 跑了两 天也没跑完，SQL 当然也执行不完。像这种情况的数据倾斜，因为 null 字段没有意义，所以可以在 where 条件里加一个 userID != null 过滤掉就可以了。定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子： distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。我们可以通过Spark Web UI，查看各个task的运行时间异常，或者从yarn日志中查看task的运行信息，是否存在某个task内存溢出的情况。查看导致数据倾斜的key的数据分布情况在确定了数据倾斜的情况下，下一步就是查看锁定位置的key分布，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。数据倾斜解决方案使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。过滤少数导致倾斜的key方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。提高shuffle操作的并行度方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。两阶段聚合（局部聚合+全局聚合）方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。采样倾斜key并分拆join操作方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“map join”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。使用随机前缀和扩容RDD进行join方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。方案实现思路： 首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。代码优化了解 Spark 的工作原理，了解要处理的数据的特点，了解要计算的目标，设计合理的代码处理逻辑，RDD lineage设计、算子的合理使用，shuffle调优等，使用良好的编程方法开发大数据应用，是大数据应用性能优化的重要手段，也是大数据开发工程师的重要职责。避免创建重复的RDD我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。对多次使用的RDD进行持久化Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。尽量使用可以在map端进行shuffle类算子如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以在map端进行聚合的算子这里拿reduceByKey和groupByKey进行举例reduceByKey(func, numPartitions=None)reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。groupByKey(numPartitions=None)groupByKey也是对每个key进行操作，但只生成一个sequence。如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。最后reduceByKey和groupByKey都是要经过shuffle的，groupByKey在方法shuffle之间不会合并原样进行shuffle，reduceByKey进行shuffle之前会先做合并,这样就减少了shuffle的io传送，所以效率高一点。其他算子的优化使用mapPartitions替代普通map使用reduceByKey/aggregateByKey替代groupByKey使用foreachPartitions替代foreach使用filter之后进行coalesce操作使用repartitionAndSortWithinPartitions替代repartition与sort类操作巧用广播变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： * 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 * 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 * 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）123456val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .getOrCreate() ### 配置参数优化根据公司数据特点，为部署的大数据产品以及运行的作业选择合适的配置参数，是公司大数据平台性能优化最主要的手段，也是大数据工程师或者大数据运维工程师的主要职责。比如 Yarn 的每个 Container 包含的 CPU 个数和内存数目、HDFS 数据块的大小和复制数等，每个大数据产品都有很多配置参数，这些参数会对大数据运行时的性能产生重要影响。num-executors参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。executor-memory参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。参数调优建议：每个Executor进程的内存设置4G8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/31/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。executor-cores参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。参数调优建议：Executor的CPU core数量设置为24个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/31/2左右比较合适，也是避免影响其他同学的作业运行。driver-memory参数说明：该参数用于设置Driver进程的内存。参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。spark.default.parallelism参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。参数调优建议：Spark作业的默认task数量为5001000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的23倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。spark.storage.memoryFraction参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。spark.shuffle.memoryFraction参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。123456789./bin/spark-submit \\ --master yarn-cluster \\ --num-executors 100 \\ --executor-memory 6G \\ --executor-cores 4 \\ --driver-memory 1G \\ --conf spark.default.parallelism=1000 \\ --conf spark.storage.memoryFraction=0.5 \\ --conf spark.shuffle.memoryFraction=0.3 \\在性能优化过程中，**CPU、内存、网络、磁盘**这四种主要计算资源的使用和 Spark 的计算阶段密切相关。Spark 性能优化可以分解为下面几步。性能测试，观察 Spark 性能特性和资源（CPU、Memory、Disk、Net）利用情况。分析、寻找资源瓶颈。分析系统架构、代码，发现资源利用关键所在，思考优化策略。代码、架构、基础设施调优，优化、平衡资源利用。性能测试，观察系统性能特性，是否达到优化目的，以及寻找下一个瓶颈点。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark SQL 源码解析","slug":"Spark-SQL-源码解析","date":"2017-04-01T15:48:43.000Z","updated":"2020-04-29T12:09:17.572Z","comments":true,"path":"2017/04/01/Spark-SQL-源码解析/","link":"","permalink":"cpeixin.cn/2017/04/01/Spark-SQL-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"Spark SQL是让Spark应用程序拥有高效性、高可容错性和丰富生态的“幕后英雄”。在学习Spark SQL 底层解析原理前，先看下面三张架构图。架构图中运行的流程，以及过程中涉及到的组件。两张图片互为补充的去看。运行流程首先梳理运行流程，在spark中执行SQL语句或者DataFrame的算子操作，在集群主要经过两大过程LogicalPlan（逻辑计划），理解为树型数据结构，逻辑算子树PhysicalPlan（物理计划），理解为物理算子树其次，两个主要过程中有包含几个子过程：逻辑计划（LogicalPlan）:**未解析的逻辑算子树（Unresolved LogicalPlan），仅仅是数据结构，不包含任何数据信息解析后逻辑算子树（Analyzed LogicalPlan），节点中绑定各种信息，可以看到，这个过程中有catalog参与优化后逻辑算子树（Optimized LogicalPlan），应用各种优化规则对一些低效的逻辑计划进行转换物理计划（PhysicalPlan）：**根据逻辑算子树，生成物理算子树的列表（Iterator[PhysicalPlan]）,同样逻辑算子树可能对应多个物理算子树。（上图可以看到，PhysicalPlan阶段的多个绿色五边形）从列表中按照一定策略选取最优的物理算子树（SparkPlan）对选取的物理算子树做提交前的准备工作，例如确保分区操作正确，物理算子节点重用，执行代码生成等。经过准备后的物理算子（Prepared SparkPlan）最终物理算子树生成的RDD执行action操作，即可提交执行。流程说明逻辑执行计划主要是一系列 _抽象的转换过程_，它并不涉及executors或者drivers，它仅仅是将用户的代码表达式转换成一个优化的版本。用户的代码首先会被转换成 未解决的逻辑计划(unresolved logical plan) ，之所以将之称作 未解决的 , 是因为它并一定是正确的，它所引用到的表名或者列名可能存在，也可能不存在。Spark之后会使用 calalog_，一个含有所有table和DataFrame的元数据仓库，在 _分析器(analyser) 中来解析校对所引用的表名或者列名。假如 unresolved logical plan 通过了验证，这时的计划我们称之为 已解决的逻辑计划(resolved logical plan) 。它会传递到 Catalyst Optimizer 优化器，然后被经过一些列优化生成最优逻辑计划(Optimized logical plan)AST -&gt; Unresolved Logical Plan实质：SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan首先在架构图中，最前端我们可以是在Spark中指定SQL语句，或者是Dataframe的算子操作，那么接下来，就直接进入了逻辑计划生成阶段：spark.sql() 这种方式的话会涉及到sql的解析，解析之后就会生成逻辑计划如果是直接在DataFrame的api上直接操作的话，使用的api将直接生成逻辑计划这里，我们从spark.sql()为入口，在源码里面一步一步的去探索底层逻辑是如何实现的。1spark.sql(\"select * from t_user\")sql入口⬇️123456789/** * Executes a SQL query using Spark, returning the result as a `DataFrame`. * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'. * * @since 2.0.0 */ def sql(sqlText: String): DataFrame = &#123; Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText)) &#125;⬇️12345678trait ParserInterface &#123; /** * Parse a string to a [[LogicalPlan]]. */ @throws[ParseException](\"Text cannot be parsed to a LogicalPlan\") def parsePlan(sqlText: String): LogicalPlan.......&#125;这里看到 parsePlan 方法返回为LogicalPlan，就是在这里，将一个sql语句通过ast解析成unresolved logicalPlan⬇️123456789/** Creates LogicalPlan for a given SQL string. */override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) &#123; parser =&gt; astBuilder.visitSingleStatement(parser.singleStatement()) match &#123; case plan: LogicalPlan =&gt; plan case _ =&gt; val position = Origin(None, None) throw new ParseException(Option(sqlText), \"Unsupported SQL statement\", position, position) &#125;&#125;ANTLR4是JAVA写的语言识别工具，它用来声明语言的语法。它的语法识别分为两个阶段词法分析阶段：理解为把符号分成组或者符号类解析阶段：根据词法，解析成一棵语法分析树astBuilder类主要功能是把Antlr4的解析树 转换为catalyst表达式astBuilder是SparkSqlAstBuilder的实例，在将Antlr中的匹配树转换成unresolved logical plan中，它起着桥梁作用。singleStatement()方法构建整棵语法树，然后通过 astBuilder.visitSingleStatement使用visitor模式，开始匹配SqlBase.g4中sql的入口匹配规则，（SqlBase.g4是sparksql的语法文件）递归的遍历statement，以及其后的各个节点。在匹配过程中，碰到叶子节点，就将构造Logical Plan中对应的TreeNode。⬇️12345678910111213141516171819202122232425262728293031323334353637383940protected def parse[T](command: String)(toResult: SqlBaseParser =&gt; T): T = &#123; logDebug(s\"Parsing command: $command\") val lexer = new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command))) lexer.removeErrorListeners() lexer.addErrorListener(ParseErrorListener) val tokenStream = new CommonTokenStream(lexer) val parser = new SqlBaseParser(tokenStream) parser.addParseListener(PostProcessor) parser.removeErrorListeners() parser.addErrorListener(ParseErrorListener) try &#123; try &#123; // first, try parsing with potentially faster SLL mode parser.getInterpreter.setPredictionMode(PredictionMode.SLL) toResult(parser) &#125; catch &#123; case e: ParseCancellationException =&gt; // if we fail, parse with LL mode tokenStream.seek(0) // rewind input stream parser.reset() // Try Again. parser.getInterpreter.setPredictionMode(PredictionMode.LL) toResult(parser) &#125; &#125; catch &#123; case e: ParseException if e.command.isDefined =&gt; throw e case e: ParseException =&gt; throw e.withCommand(command) case e: AnalysisException =&gt; val position = Origin(e.line, e.startPosition) throw new ParseException(Option(command), e.message, position, position) &#125; &#125;先后实例化了分词解析SqlBaseLexer和语法解析SqlBaseParser类，最后将antlr的语法解析器，然后一次尝试用不同的模式去进行解析。最终将antlr的语法解析器parser，传入parsePlan。UnresolvedLogicalPlan在经历上面过程之后，此时的未解析的逻辑树，只是一个简单逻辑，还没有绑定数据，没有任何数据信息。那么接下来发生的事情就是如何在UnresolvedLogicalPlan基础上绑定每个节点信息？Unresolved Logical Plan -&gt; Logical Plan实质：Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；流程图中，我们看到Analysis主要职责就是将通过Sql Parser未能Resolved的Logical Plan给Resolved掉。我们从下面开始：123456789/** * Executes a SQL query using Spark, returning the result as a `DataFrame`. * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'. * * @since 2.0.0 */ def sql(sqlText: String): DataFrame = &#123; Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText)) &#125;在上一阶段我们分析到sessionState.sqlParser.parsePlan(sqlText)方式是逻辑计划的开始，解析生成Unresolved Logical Plan，这时Unresolved Logical Plan作为参数，传入到ofRows()方法中。1234567891011121314151617package org.apache.spark.sqlprivate[sql] object Dataset &#123; def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = &#123; val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]]) // Eagerly bind the encoder so we verify that the encoder matches the underlying // schema. The user will get an error if this is not the case. dataset.deserializer dataset &#125; def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = &#123; val qe = sparkSession.sessionState.executePlan(logicalPlan) qe.assertAnalyzed() new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema)) &#125;&#125;这里首先创建了queryExecution类对象，QueryExecution中定义了sql执行过程中的关键步骤，是sql执行的关键类，返回一个dataframe类型的对象。QueryExecution类中的成员都是lazy的，被调用时才会执行。只有等到程序中出现action算子时，才会调用 queryExecution类中的executedPlan成员，1def executePlan(plan: LogicalPlan): QueryExecution = createQueryExecution(plan)创建QueryExecution123456789101112131415161718class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) &#123; // TODO: Move the planner an optimizer into here from SessionState. protected def planner = sparkSession.sessionState.planner def assertAnalyzed(): Unit = analyzed def assertSupported(): Unit = &#123; if (sparkSession.sessionState.conf.isUnsupportedOperationCheckEnabled) &#123; UnsupportedOperationChecker.checkForBatch(analyzed) &#125; &#125; lazy val analyzed: LogicalPlan = &#123; SparkSession.setActiveSession(sparkSession) sparkSession.sessionState.analyzer.executeAndCheck(logical) &#125; .......QueryExecution是Spark用来执行关系型查询的主要工作流。它是被设计用来为开发人员提供更方便的对查询执行中间阶段的访问。QueryExecution中最重要的是成员变量，它的成员变量几乎都是lazy variable，它的方法大部分是为了提供给这些lazy variable去调用的，调用analyzer解析器,对Unresolved LogicalPlan进行元数据绑定生成的Resolved LogicalPlan123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package org.apache.spark.sql.catalyst.analysis/** * Provides a logical query plan analyzer, which translates [[UnresolvedAttribute]]s and * [[UnresolvedRelation]]s into fully typed objects using information in a [[SessionCatalog]]. */class Analyzer( catalog: SessionCatalog, conf: SQLConf, maxIterations: Int) extends RuleExecutor[LogicalPlan] with CheckAnalysis &#123; def this(catalog: SessionCatalog, conf: SQLConf) = &#123; this(catalog, conf, conf.optimizerMaxIterations) &#125; def executeAndCheck(plan: LogicalPlan): LogicalPlan = &#123; val analyzed = execute(plan) try &#123; checkAnalysis(analyzed) EliminateBarriers(analyzed) &#125; catch &#123; case e: AnalysisException =&gt; val ae = new AnalysisException(e.message, e.line, e.startPosition, Option(analyzed)) ae.setStackTrace(e.getStackTrace) throw ae &#125; &#125; override def execute(plan: LogicalPlan): LogicalPlan = &#123; AnalysisContext.reset() try &#123; executeSameContext(plan) &#125; finally &#123; AnalysisContext.reset() &#125; &#125; private def executeSameContext(plan: LogicalPlan): LogicalPlan = super.execute(plan) def resolver: Resolver = conf.resolver protected val fixedPoint = FixedPoint(maxIterations) /** * Override to provide additional rules for the \"Resolution\" batch. */ val extendedResolutionRules: Seq[Rule[LogicalPlan]] = Nil /** * Override to provide rules to do post-hoc resolution. Note that these rules will be executed * in an individual batch. This batch is to run right after the normal resolution batch and * execute its rules in one pass. */ val postHocResolutionRules: Seq[Rule[LogicalPlan]] = Nil lazy val batches: Seq[Batch] = Seq( Batch(\"Hints\", fixedPoint, new ResolveHints.ResolveBroadcastHints(conf), ResolveHints.RemoveAllHints), Batch(\"Simple Sanity Check\", Once, LookupFunctions), Batch(\"Substitution\", fixedPoint, CTESubstitution, WindowsSubstitution, EliminateUnions, new SubstituteUnresolvedOrdinals(conf)), Batch(\"Resolution\", fixedPoint, ResolveTableValuedFunctions :: ResolveRelations :: ResolveReferences :: ResolveCreateNamedStruct :: ResolveDeserializer :: ResolveNewInstance :: ResolveUpCast :: ResolveGroupingAnalytics :: ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ResolveSubqueryColumnAliases :: ResolveWindowOrder :: ResolveWindowFrame :: ResolveNaturalAndUsingJoin :: ExtractWindowExpressions :: GlobalAggregates :: ResolveAggregateFunctions :: TimeWindowing :: ResolveInlineTables(conf) :: ResolveTimeZone(conf) :: ResolvedUuidExpressions :: TypeCoercion.typeCoercionRules(conf) ++ extendedResolutionRules : _*), Batch(\"Post-Hoc Resolution\", Once, postHocResolutionRules: _*), Batch(\"View\", Once, AliasViewChild(conf)), Batch(\"Nondeterministic\", Once, PullOutNondeterministic), Batch(\"UDF\", Once, HandleNullInputsForUDF), Batch(\"FixNullability\", Once, FixNullability), Batch(\"Subquery\", Once, UpdateOuterReferences), Batch(\"Cleanup\", fixedPoint, CleanupAliases) )其中val analyzed: LogicalPlan= analyzer.execute(logical)，logical就是sqlparser解析出来的unresolved logical plan，analyzed就是analyzed logical plan。那么exectue究竟是这么样的过程呢？super.execute(plan)方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124package org.apache.spark.sql.catalyst.rulesobject RuleExecutor &#123; protected val queryExecutionMeter = QueryExecutionMetering() /** Dump statistics about time spent running specific rules. */ def dumpTimeSpent(): String = &#123; queryExecutionMeter.dumpTimeSpent() &#125; /** Resets statistics about time spent running specific rules */ def resetMetrics(): Unit = &#123; queryExecutionMeter.resetMetrics() &#125;&#125;abstract class RuleExecutor[TreeType &lt;: TreeNode[_]] extends Logging &#123; /** * An execution strategy for rules that indicates the maximum number of executions. If the * execution reaches fix point (i.e. converge) before maxIterations, it will stop. */ abstract class Strategy &#123; def maxIterations: Int &#125; /** A strategy that only runs once. */ case object Once extends Strategy &#123; val maxIterations = 1 &#125; /** A strategy that runs until fix point or maxIterations times, whichever comes first. */ case class FixedPoint(maxIterations: Int) extends Strategy /** A batch of rules. */ protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*) /** Defines a sequence of rule batches, to be overridden by the implementation. */ protected def batches: Seq[Batch] /** * Defines a check function that checks for structural integrity of the plan after the execution * of each rule. For example, we can check whether a plan is still resolved after each rule in * `Optimizer`, so we can catch rules that return invalid plans. The check function returns * `false` if the given plan doesn't pass the structural integrity check. */ protected def isPlanIntegral(plan: TreeType): Boolean = true /** * Executes the batches of rules defined by the subclass. The batches are executed serially * using the defined execution strategy. Within each batch, rules are also executed serially. */ def execute(plan: TreeType): TreeType = &#123; var curPlan = plan val queryExecutionMetrics = RuleExecutor.queryExecutionMeter batches.foreach &#123; batch =&gt; val batchStartPlan = curPlan var iteration = 1 var lastPlan = curPlan var continue = true // Run until fix point (or the max number of iterations as specified in the strategy. while (continue) &#123; curPlan = batch.rules.foldLeft(curPlan) &#123; case (plan, rule) =&gt; val startTime = System.nanoTime() val result = rule(plan) val runTime = System.nanoTime() - startTime if (!result.fastEquals(plan)) &#123; queryExecutionMetrics.incNumEffectiveExecution(rule.ruleName) queryExecutionMetrics.incTimeEffectiveExecutionBy(rule.ruleName, runTime) logTrace( s\"\"\" |=== Applying Rule $&#123;rule.ruleName&#125; === |$&#123;sideBySide(plan.treeString, result.treeString).mkString(\"\\n\")&#125; \"\"\".stripMargin) &#125; queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime) queryExecutionMetrics.incNumExecution(rule.ruleName) // Run the structural integrity checker against the plan after each rule. if (!isPlanIntegral(result)) &#123; val message = s\"After applying rule $&#123;rule.ruleName&#125; in batch $&#123;batch.name&#125;, \" + \"the structural integrity of the plan is broken.\" throw new TreeNodeException(result, message, null) &#125; result &#125; iteration += 1 if (iteration &gt; batch.strategy.maxIterations) &#123; // Only log if this is a rule that is supposed to run more than once. if (iteration != 2) &#123; val message = s\"Max iterations ($&#123;iteration - 1&#125;) reached for batch $&#123;batch.name&#125;\" if (Utils.isTesting) &#123; throw new TreeNodeException(curPlan, message, null) &#125; else &#123; logWarning(message) &#125; &#125; continue = false &#125; if (curPlan.fastEquals(lastPlan)) &#123; logTrace( s\"Fixed point reached for batch $&#123;batch.name&#125; after $&#123;iteration - 1&#125; iterations.\") continue = false &#125; lastPlan = curPlan &#125; if (!batchStartPlan.fastEquals(curPlan)) &#123; logDebug( s\"\"\" |=== Result of Batch $&#123;batch.name&#125; === |$&#123;sideBySide(batchStartPlan.treeString, curPlan.treeString).mkString(\"\\n\")&#125; \"\"\".stripMargin) &#125; else &#123; logTrace(s\"Batch $&#123;batch.name&#125; has no effect.\") &#125; &#125; curPlan &#125;&#125;此函数实现了针对analyzer类中定义的每一个batch（类别），按照batch中定义的fix point(策略)和rule（规则）对Unresolved的逻辑计划进行解析。这里提到的batch，可以在上面的class Analyzer中看到，lazy val batches: Seq[Batch]，这里定义了一组转换规则。Analyzer中使用的Rules，具体的Rule实现是通过RuleExecutor完成 ,定义了batches，由多个batch构成，每个batch又有不同的rule构成，如Resolution由ResolveReferences 、ResolveRelations、ResolveSortReferences 、NewRelationInstances等构成；每个rule又有自己相对应的处理函数，可以具体参看Analyzer中的ResolveReferences 、ResolveRelations、ResolveSortReferences 、NewRelationInstances函数；同时要注意的是，不同的rule应用次数是不同的：如CaseInsensitiveAttributeReferences这个batch中rule只应用了一次（Once），而Resolution这个batch中的rule应用了多次（fixedPoint = FixedPoint(100)，也就是说最多应用100次，除非前后迭代结果一致退出）。1234567891011121314/** * 输入为旧的plan，输出为新的plan，仅此而已。 * 所以真正的逻辑在各个继承实现的rule里，analyze的过程也就是执行各个rule的过程 */abstract class Rule[TreeType &lt;: TreeNode[_]] extends Logging &#123; /** Name for this rule, automatically inferred based on class name. */ val ruleName: String = &#123; val className = getClass.getName if (className endsWith \"$\") className.dropRight(1) else className &#125; def apply(plan: TreeType): TreeType&#125;rule(plan)，这里的参数plan是应用rule.apply转化里面的TreeNode分析后，每张表对应的字段集，字段类型，数据存储位置都已确定。Project 与 Filter 操作的字段类型以及在表中的位置也已确定。有了这些信息，已经可以直接将该 LogicalPlan 转换为 Physical Plan 进行执行。但是由于不同用户提交的 SQL 质量不同，直接执行会造成不同用户提交的语义相同的不同 SQL 执行效率差距甚远。换句话说，如果要保证较高的执行效率，用户需要做大量的 SQL 优化，使用体验大大降低。为了尽可能保证无论用户是否熟悉 SQL 优化，提交的 SQL 质量如何， Spark SQL 都能以较高效率执行，还需在执行前进行 LogicalPlan 优化。Logical Plan -&gt; Optimized Logical Plan作用：Optimizer再通过各种基于规则的优化策略对Logical Plan进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解Optimizer的主要职责是将Analyzer给Resolved的Logical Plan根据不同的优化策略Batch，来对语法树进行优化，优化逻辑计划节点(Logical Plan)以及表达式(Expression)，也是转换成物理执行计划的前置。它的工作原理和analyzer一致，也是通过其下的batch里面的Rule[LogicalPlan]来进行处理的。123456789101112131415161718192021222324252627282930313233343536373839404142package org.apache.spark.sql.execution/** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) &#123; // TODO: Move the planner an optimizer into here from SessionState. protected def planner = sparkSession.sessionState.planner def assertAnalyzed(): Unit = analyzed def assertSupported(): Unit = &#123; if (sparkSession.sessionState.conf.isUnsupportedOperationCheckEnabled) &#123; UnsupportedOperationChecker.checkForBatch(analyzed) &#125; &#125; lazy val analyzed: LogicalPlan = &#123; SparkSession.setActiveSession(sparkSession) sparkSession.sessionState.analyzer.executeAndCheck(logical) &#125;// 如果缓存中有查询结果，则直接替换为缓存的结果，逻辑不复杂，这里不再展开讲了。 lazy val withCachedData: LogicalPlan = &#123; assertAnalyzed() assertSupported() sparkSession.sharedState.cacheManager.useCachedData(analyzed) &#125;// 对Logical Plan 优化 lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData) lazy val sparkPlan: SparkPlan = &#123; SparkSession.setActiveSession(sparkSession) // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(optimizedPlan)).next() &#125;.......看到Optimizer也是继承自RuleExecutor，和Analyzer一个套路，也是遍历tree，并对每个节点应用rule。Optimizer里的batches包含了3类优化策略：1、Combine Limits 合并Limits 2、ConstantFolding 常量合并 3、Filter Pushdown 过滤器下推,每个Batch里定义的优化伴随对象都定义在Optimizer里了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package org.apache.spark.sql.catalyst.optimizer/** * Abstract class all optimizers should inherit of, contains the standard batches (extending * Optimizers can override this. */abstract class Optimizer(sessionCatalog: SessionCatalog) extends RuleExecutor[LogicalPlan] &#123; // Check for structural integrity of the plan in test mode. Currently we only check if a plan is // still resolved after the execution of each rule. override protected def isPlanIntegral(plan: LogicalPlan): Boolean = &#123; !Utils.isTesting || plan.resolved &#125; protected def fixedPoint = FixedPoint(SQLConf.get.optimizerMaxIterations) def batches: Seq[Batch] = &#123; val operatorOptimizationRuleSet = Seq( // Operator push down PushProjectionThroughUnion, ReorderJoin, EliminateOuterJoin, PushPredicateThroughJoin, PushDownPredicate, LimitPushDown, ColumnPruning, InferFiltersFromConstraints, // Operator combine CollapseRepartition, CollapseProject, CollapseWindow, CombineFilters, CombineLimits, CombineUnions, // Constant folding and strength reduction NullPropagation, ConstantPropagation, FoldablePropagation, OptimizeIn, ConstantFolding, ReorderAssociativeOperator, LikeSimplification, BooleanSimplification, SimplifyConditionals, RemoveDispensableExpressions, SimplifyBinaryComparison, PruneFilters, EliminateSorts, SimplifyCasts, SimplifyCaseConversionExpressions, RewriteCorrelatedScalarSubquery, EliminateSerialization, RemoveRedundantAliases, RemoveRedundantProject, SimplifyCreateStructOps, SimplifyCreateArrayOps, SimplifyCreateMapOps, CombineConcats) ++ extendedOperatorOptimizationRules val operatorOptimizationBatch: Seq[Batch] = &#123; val rulesWithoutInferFiltersFromConstraints = operatorOptimizationRuleSet.filterNot(_ == InferFiltersFromConstraints) Batch(\"Operator Optimization before Inferring Filters\", fixedPoint, rulesWithoutInferFiltersFromConstraints: _*) :: Batch(\"Infer Filters\", Once, InferFiltersFromConstraints) :: Batch(\"Operator Optimization after Inferring Filters\", fixedPoint, rulesWithoutInferFiltersFromConstraints: _*) :: Nil &#125; (Batch(\"Eliminate Distinct\", Once, EliminateDistinct) :: // Technically some of the rules in Finish Analysis are not optimizer rules and belong more // in the analyzer, because they are needed for correctness (e.g. ComputeCurrentTime). // However, because we also use the analyzer to canonicalized queries (for view definition), // we do not eliminate subqueries or compute current time in the analyzer. Batch(\"Finish Analysis\", Once, EliminateSubqueryAliases, EliminateView, ReplaceExpressions, ComputeCurrentTime, GetCurrentDatabase(sessionCatalog), RewriteDistinctAggregates, ReplaceDeduplicateWithAggregate) :: ////////////////////////////////////////////////////////////////////////////////////////// // Optimizer rules start here ////////////////////////////////////////////////////////////////////////////////////////// // - Do the first call of CombineUnions before starting the major Optimizer rules, // since it can reduce the number of iteration and the other rules could add/move // extra operators between two adjacent Union operators. // - Call CombineUnions again in Batch(\"Operator Optimizations\"), // since the other rules might make two separate Unions operators adjacent. Batch(\"Union\", Once, CombineUnions) :: Batch(\"Pullup Correlated Expressions\", Once, PullupCorrelatedPredicates) :: Batch(\"Subquery\", Once, OptimizeSubqueries) :: Batch(\"Replace Operators\", fixedPoint, ReplaceIntersectWithSemiJoin, ReplaceExceptWithFilter, ReplaceExceptWithAntiJoin, ReplaceDistinctWithAggregate) :: Batch(\"Aggregate\", fixedPoint, RemoveLiteralFromGroupExpressions, RemoveRepetitionFromGroupExpressions) :: Nil ++ operatorOptimizationBatch) :+ Batch(\"Join Reorder\", Once, CostBasedJoinReorder) :+ Batch(\"Decimal Optimizations\", fixedPoint, DecimalAggregates) :+ Batch(\"Object Expressions Optimization\", fixedPoint, EliminateMapObjects, CombineTypedFilters) :+ Batch(\"LocalRelation\", fixedPoint, ConvertToLocalRelation, PropagateEmptyRelation) :+ // The following batch should be executed after batch \"Join Reorder\" and \"LocalRelation\". Batch(\"Check Cartesian Products\", Once, CheckCartesianProducts) :+ Batch(\"RewriteSubquery\", Once, RewritePredicateSubquery, ColumnPruning, CollapseProject, RemoveRedundantProject) &#125;Optimizer的优化策略不仅有对plan进行transform的，也有对expression进行transform的，究其原理就是遍历树，然后应用优化的Rule，但是注意一点，对Logical Plantransfrom的是先序遍历(pre-order)，而对Expression transfrom的时候是后序遍历(post-order)：batch的执行和analyzer一样是通过RuleExecutor的execute方法依次遍历，这里不再解析Catalyst以上的三个过程都是属于Catalyst阶段，SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；此时，Optimizer再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan。Physical Plan从Optimizer LogicalPlan传入SparkSQL物理计划，并提交，整个PhysicalPlan经历了三个阶段转换为Iterator[PhysicalPlan]SparkPlanPrepared SparkPlanSparkPlanner把Optimizer LogicalPlan转换为PhysicalPlan列表。SparkPlanner主要是通过物理计划策略（Strategy）作用于Optimizer LogicalPlan上，从而生成SparkPlan列表即Iterator[PhysicalPlan]，此时的一个Optimizer LogicalPlan会对应多个SparkPlan。在Iterator[PhysicalPlan]中选取一个最佳的SparkPlanSparkPlan通过 prepareForExecution方法调用若干规则（ Rule ）进行转换为Prepared SparkPlan为能在各个节点上正确执行。Spark SQL中有多于65种以上的SparkPlan实现，涉及到数据源RDD的创建和各种数据处理等。12345678910111213141516171819202122232425262728293031323334353637package org.apache.spark.sql.executionlazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData) lazy val sparkPlan: SparkPlan = &#123; SparkSession.setActiveSession(sparkSession) // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(optimizedPlan)).next() &#125; // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan) /** Internal version of the RDD. Avoids copies and has no schema */ lazy val toRdd: RDD[InternalRow] = executedPlan.execute() /** * Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal * row format conversions as needed. */ protected def prepareForExecution(plan: SparkPlan): SparkPlan = &#123; preparations.foldLeft(plan) &#123; case (sp, rule) =&gt; rule.apply(sp) &#125; &#125; /** A sequence of rules that will be applied in order to the physical plan before execution. */ protected def preparations: Seq[Rule[SparkPlan]] = Seq( python.ExtractPythonUDFs, PlanSubqueries(sparkSession), EnsureRequirements(sparkSession.sessionState.conf), CollapseCodegenStages(sparkSession.sessionState.conf), ReuseExchange(sparkSession.sessionState.conf), ReuseSubquery(sparkSession.sessionState.conf))......一条SQL经历了Parser，Analyzed，Optimizer之后再转换为最终能在Spark框架中实际执行的SparkPlan的物理算子树。物理算子树中，叶子类型SparkPlan是创建一个RDD开始，而往上遍历Tree过程中每个非叶子节点做一次Transformation，通过execute函数转换成新的RDD，最终会执行Action算子把结果返回给用户。SparkPlan除了给RDD做Transformation的操作之外，还做分区，排序；同时除了执行execute方法之外，还会执行executeBroadcase方法，将数据直接广播到集群上。SparkPlan分为三大块：1，每个SparkPlan都会记住其元数据(Metadata)与指标(Metric)的信息。2，RDD做Transformation操作时候还同时做了分区和排序3，最后SparkPlan作为物理计划提交到Spark集群中执行，其中以execute为主，executeBroadcast为辅。至此，将用户程序中的SQL/Dataset/DataFrame经过一系列操作，最终转化为Spark系统中执行的RDD。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark SQL 入门","slug":"Spark-SQL-入门","date":"2017-03-25T15:48:43.000Z","updated":"2020-04-22T04:19:49.614Z","comments":true,"path":"2017/03/25/Spark-SQL-入门/","link":"","permalink":"cpeixin.cn/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/","excerpt":"","text":"程序起点在Spark2.0之前， Spark程序必须做的第一件事是创建一个SparkContext对象，该对象告诉Spark如何访问集群。要创建一个SparkContext您首先需要构建一个SparkConf对象，其中包含有关您的应用程序的信息。每个JVM只能激活一个SparkContext。12345val sparkConf: SparkConf = new SparkConf() .setAppName(\"transformation_func\") .setMaster(\"local\")val sc = new SparkContext(sparkConf)在Spark2.0之后， SparkSession类是Spark中所有功能的入口点。为了引入dataframe和dataset的API，要创建一个基本的SparkSession，只需使用SparkSession.builder()。SparkConf、SparkContext和SQLContext都已经被封装在SparkSession当中，不需要显示的创建。并且提供了对Hive功能的内置支持，下图是SparkSession的源码定义：SparkSession创建**12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession .builder() .appName(\"Spark SQL basic example\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._创建DataFrame使用SparkSession，应用程序可以从现有的RDD，Hive表的或Spark数据源创建DataFrame 。基于RDD转化DataFrame：1234567891011121314151617181920212223package readimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object sparksql_rdd &#123; case class Person(user_name: String, sex: String) def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"function_case\") .master(\"local\") .getOrCreate() val rdd_ss: RDD[(String, String)] = spark.sparkContext.makeRDD(List((\"brent\",\"male\"),(\"haylee\",\"female\"),(\"vicky\",\"male\"))) import spark.implicits._ val df2: DataFrame = rdd_ss.map((x: (String, String)) =&gt;&#123;Person(x._1,x._2)&#125;).toDF() &#125;&#125;基于JSON文件的内容创建一个DataFrame：1234567891011121314151617package spark_sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object sparksql_1 &#123; def main(args: Array[String]): Unit =&#123; val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .getOrCreate() val df_json: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/user_data.json\") df_json.show(5) &#125;&#125;基于Hive表内容创建一个DataFrame：12345678910111213141516171819package spark_sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object sparksql_hive &#123; def main(args: Array[String]): Unit = &#123; val warehouseLocation=\"hdfs://localhost:8020/user/hive/warehouse\" val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .config(\"spark.sql.warehouse.dir\",warehouseLocation) .enableHiveSupport() .getOrCreate() val df_databases: DataFrame = spark.sql(\"show databases\") df_databases.show() &#125;&#125;注意： Spark读取Hive是需要两三个步骤的如果你是在集群上运行，需要注意，要将hive-site.xml复制一份到spark目录下的conf文件夹中，如果你是在本地连接集群中的Hive，那么请将hive-site.xml复制一份到你IDEA中，resources目录下。如果你是在集群上运行，需要注意，要将mysql-connector-java-8.0.19.jar复制一份到spark目录下的jars文件夹中，如果你是在本地连接集群中的Hive，那么请在pom文件中不要忘记引入mysql-connector-java在本地运行，可能会遇到/tmp/hive的权限问题，请用chmod修改/tmp权限为777**基于HBase内容创建一个DataFrame：**1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package spark_sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.execution.datasources.hbase.HBaseTableCatalogimport scala.collection.immutableobject sparksql_hbase &#123; def main(args: Array[String]): Unit = &#123; case class Record(col0: Int, col1: Int, col2: Boolean) val spark: SparkSession = SparkSession .builder() .appName(\"Spark HBase Example\") .master(\"local[4]\") .getOrCreate() def catalog: String = // 这里，我们在读取数据的过程中，无论什么类型的数据，type字段统一指定成 string 即可。否则读取报错 s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"default\", \"name\":\"t_user\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"&#125;, |\"col1\":&#123;\"cf\":\"cf1\", \"col\":\"user_name\", \"type\":\"string\"&#125;, |\"col2\":&#123;\"cf\":\"cf1\", \"col\":\"customer_id\", \"type\":\"string\"&#125;, |\"col3\":&#123;\"cf\":\"cf1\", \"col\":\"age\", \"type\":\"string\"&#125;, |\"col4\":&#123;\"cf\":\"cf1\", \"col\":\"birthday\", \"type\":\"string\"&#125;, |\"col5\":&#123;\"cf\":\"cf1\", \"col\":\"deposit_amount\", \"type\":\"string\"&#125;, |\"col6\":&#123;\"cf\":\"cf1\", \"col\":\"last_login_time\", \"type\":\"string\"&#125;, |\"col7\":&#123;\"cf\":\"cf1\", \"col\":\"flag\", \"type\":\"string\"&#125; |&#125; |&#125;\"\"\".stripMargin // read val df: DataFrame = spark .read .option(HBaseTableCatalog.tableCatalog, catalog) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .load() df.show() &#125;&#125;注意：如果我们对于读取和写入HBase的场景很频繁的话，就需要考虑性能的问题，内置的读取数据源是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；TableInputFormat 中不支持 BulkGet；不能享受到 Spark SQL 内置的 catalyst 引擎的优化。基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。但是对于使用SHC，目前还是有些麻烦的，网上的maven依赖可能是因为版本的原因，程序引入找不到org.apache.spark.sql.execution.datasources.hbase.HBaseTableCatalog类，这里推荐自己下载源码，进行编译成jar文件或者编译后上传到自己的maven库中进行使用下载源码 https://github.com/hortonworks-spark/shc，选择相应低于或者等于spark，hbase的版本本地中打开，点击程序根目录下的pom文件，注释掉distributionManagement，直接点击install，将jar包生成到你本地的maven库中，当然你也可以上传到你远程的私有Maven 库中。pom文件中，引入下面依赖，就可以使用了（注意 version 版本号）12345&lt;dependency&gt; &lt;groupId&gt;com.hortonworks&lt;/groupId&gt; &lt;artifactId&gt;shc-core&lt;/artifactId&gt; &lt;version&gt;1.1.2-2.2-s_2.11&lt;/version&gt;&lt;/dependency&gt;DataFrame操作上篇文章中，我们讲到DataFrame每一列并不存储类型信息，所以在编译时并不能发现类型错误，所以在这里我们也可以叫做** 无类型的数据集操作。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270package functionimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;object sparksql_function &#123; def main(args: Array[String]): Unit =&#123; val spark: SparkSession = SparkSession .builder() .appName(\"function_case\") .master(\"local\") .config(\"spark.sql.crossJoin.enabled\", \"true\") .getOrCreate() // 样例数据 /** * &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\": 22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; */ val df: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/user_data.json\") val rdd_row: RDD[Row] = spark.sparkContext .makeRDD(List((\"brent\", \"male\"), (\"haylee\", \"female\"), (\"vicky\", \"male\"))) .map((x: (String, String)) =&gt; Row(x._1, x._2)) // The schema is encoded in a string val schemaString = \"user_name sex\" // Generate the schema based on the string of schema val fields: Array[StructField] = schemaString.split(\" \") .map((fieldName: String) =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) val df2: DataFrame = spark.createDataFrame(rdd_row, schema) show_get_data(spark, df) map_data(spark, df) filter_data(spark, df) sort_data(spark, df) groupBy_data(spark, df) join_data(spark, df, df2) intersect_data(spark, df, df2) withColumn_rename_dataframe(spark, df) &#125; def show_get_data(spark: SparkSession, df: DataFrame): Unit = &#123; df.printSchema() // root // |-- age: long (nullable = true) // |-- birthday: string (nullable = true) // |-- customer_id: long (nullable = true) // |-- deposit_amount: double (nullable = true) // |-- last_login_time: string (nullable = true) // |-- user_name: string (nullable = true) df.show(5) //默认打印前20条结果 // +---+----------+-----------+--------------+-------------------+---------+ // |age| birthday|customer_id|deposit_amount| last_login_time|user_name| // +---+----------+-----------+--------------+-------------------+---------+ // | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent| // | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee| // | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky| // +---+----------+-----------+--------------+-------------------+---------+ // Select only the \"name\" column // 这个表达式不能进行计算操作 df.select(\"user_name\", \"age\").show() // +---------+ // |user_name| // +---------+ // | brent| // | haylee| // | vicky| // +---------+ // Select everybody, but increment the age by 1 // This import is needed to use the $-notation import spark.implicits._ df.select($\"user_name\", $\"age\" + 1 as \"new_age\").show() // +---------+-------+ // |user_name|new_age| // +---------+-------+ // | brent| 23| // | haylee| 24| // | vicky| 31| // +---------+-------+ import org.apache.spark.sql.functions._ df.select(col(\"customer_id\"), col(\"deposit_amount\")).show() df.limit(5).show() df.describe() &#125; def map_data(spark: SparkSession, df: DataFrame): Unit = &#123; import spark.implicits._ // 注意 这里是Row类型 df.map((x: Row) =&gt; &#123;\"name: \"+x.getAs[String](\"user_name\")&#125;).show() &#125; def filter_data(spark: SparkSession, df: DataFrame): Unit = &#123; import spark.implicits._ // 取等于时必须用=== df.filter($\"user_name\" === \"brent\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// +---+----------+-----------+--------------+-------------------+---------+ df.filter($\"age\" &gt; 25).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ df.filter(\"deposit_amount = 3000.0\").show() df.filter($\"deposit_amount\" &gt; 200 and $\"age\" &lt; 25).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// +---+----------+-----------+--------------+-------------------+---------+ df.filter(\"substring(user_name,0,1) = 'h'\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// +---+----------+-----------+--------------+-------------------+---------+// 在源码中可以看到，where算子，底层是filter实现的。 import org.apache.spark.sql.functions._ df.where(col(\"age\") &gt; 23).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ df.where(\"age&gt; 23\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ &#125; def sort_data(spark: SparkSession, df: DataFrame): Unit = &#123; import spark.implicits._ df.sort($\"age\".desc).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// +---+----------+-----------+--------------+-------------------+---------+ df.sort($\"age\".asc).show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ // 只能对数字类型和日期类型生效 df.orderBy($\"age\") df.orderBy(- df(\"age\")) df.orderBy(df(\"age\").desc) &#125; def groupBy_data(spark: SparkSession, df: DataFrame): Unit = &#123; df.groupBy(\"age\").count().show()// +---+-----+// |age|count|// +---+-----+// | 22| 1|// | 30| 1|// | 23| 1|// +---+-----+ // 只能作用于数值字段 df.groupBy(\"user_name\").max(\"deposit_amount\").show() df.groupBy(\"user_name\").min(\"deposit_amount\").show() df.groupBy(\"user_name\").mean(\"deposit_amount\").as(\"mean_deposit_amount\").show() df.groupBy(\"user_name\").sum(\"deposit_amount\").toDF(\"user_name\", \"sum_deposit_amount\").show()// +---------+------------------+// |user_name|sum_deposit_amount|// +---------+------------------+// | vicky| 200.4|// | haylee| 4000.56|// | brent| 3000.0|// +---------+------------------+ import org.apache.spark.sql.functions._ df.groupBy(\"user_name\", \"age\") .agg(min(\"deposit_amount\").as(\"min_deposit_amount\")) .show()// +---------+---+------------------+// |user_name|age|min_deposit_amount|// +---------+---+------------------+// | vicky| 30| 200.4|// | haylee| 23| 4000.56|// | brent| 22| 3000.0|// +---------+---+------------------+ //单独使用 agg df.agg(\"age\" -&gt; \"max\").show() &#125; def distinct_data(spark: SparkSession, df: DataFrame): Unit = &#123; // distinct 底层实现实则为 dropDuplicates（） df.distinct() df.dropDuplicates() &#125; def join_data(spark: SparkSession, df: DataFrame, df2: DataFrame): Unit = &#123; //笛卡尔积, spark2中默认不开启笛卡尔积，需添加\"spark.sql.crossJoin.enabled\", \"true\"配置 df.join(df2).show() df.join(df2, \"user_name\").show() df.join(df2, Seq(\"user_name\"), \"left\").show()// +---------+---+----------+-----------+--------------+-------------------+------+// |user_name|age| birthday|customer_id|deposit_amount| last_login_time| sex|// +---------+---+----------+-----------+--------------+-------------------+------+// | vicky| 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| male|// | haylee| 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00|female|// | brent| 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| male|// +---------+---+----------+-----------+--------------+-------------------+------+ &#125; def intersect_data(spark: SparkSession, df: DataFrame, df2: DataFrame): Unit = &#123; // 获取两个DataFrame中共有的记录 df.intersect(df2).show(false) &#125; def withColumn_rename_dataframe(spark: SparkSession, df: DataFrame): Unit = &#123; // 字段重命名 df.withColumnRenamed(\"deposit_amount\",\"withdraw_amount\").show() // 添加新列 import spark.implicits._ df.withColumn(\"next_year_age\", $\"age\"+1).show() &#125;&#125;以编程方式运行SQL查询12345678910111213141516171819202122232425262728293031323334package sqlimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object spark_use_sql &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"sql_case\") .master(\"local\") .getOrCreate() // 样例数据 /** * &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\": 22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; * &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; * &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; */ val df: DataFrame = spark.read.json(\"hdfs://localhost:8020/data/user_data.json\") df.createTempView(\"t_user\") spark.sql(\"select * from t_user\").show()// +---+----------+-----------+--------------+-------------------+---------+// |age| birthday|customer_id|deposit_amount| last_login_time|user_name|// +---+----------+-----------+--------------+-------------------+---------+// | 22|1993-04-05| 12031602| 3000.0|2017-03-10 14:55:22| brent|// | 23|1992-08-10| 12031603| 4000.56|2017-03-11 10:55:00| haylee|// | 30|2000-03-02| 12031604| 200.4|2017-03-10 09:10:00| vicky|// +---+----------+-----------+--------------+-------------------+---------+ import org.apache.spark.sql.functions._ spark.sql(\"select * from t_user\").groupBy(\"user_name\").agg(\"deposit_amount\"-&gt;\"sum\").show() &#125; &#125;DataSet创建12345678910111213141516171819202122232425262728293031323334353637383940import org.apache.spark.sql.&#123;Dataset, SparkSession&#125;object dataset &#123; case class Person(name: String, age: Long) def main(args: Array[String]): Unit =&#123; val spark: SparkSession = SparkSession .builder() .appName(\"dataset_case\") .master(\"local\") .getOrCreate() import spark.implicits._ // $example on:create_ds$ // Encoders are created for case classes val caseClassDS: Dataset[Person] = Seq(Person(\"Andy\", 32)).toDS() caseClassDS.show() // +----+---+ // |name|age| // +----+---+ // |Andy| 32| // +----+---+ // Encoders for most common types are automatically provided by importing spark.implicits._ val primitiveDS: Dataset[Int] = Seq(1, 2, 3).toDS() primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4) // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name val path = \"examples/src/main/resources/people.json\" val peopleDS: Dataset[Person] = spark.read.json(path).as[Person] peopleDS.show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ // $example off:create_ds$ &#125;&#125;数据存储文件123456789101112131415161718192021222324252627282930313233343536373839404142// $example on:generic_load_save_functions$ val usersDF = spark.read.load(\"examples/src/main/resources/users.parquet\") usersDF.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\") // $example off:generic_load_save_functions$ // $example on:manual_load_options$ val peopleDF = spark.read.format(\"json\").load(\"examples/src/main/resources/people.json\") peopleDF.select(\"name\", \"age\").write.format(\"parquet\").save(\"namesAndAges.parquet\") // $example off:manual_load_options$ // $example on:manual_load_options_csv$ val peopleDFCsv = spark.read.format(\"csv\") .option(\"sep\", \";\") .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .load(\"examples/src/main/resources/people.csv\") // $example off:manual_load_options_csv$ // $example on:manual_save_options_orc$ usersDF.write.format(\"orc\") .option(\"orc.bloom.filter.columns\", \"favorite_color\") .option(\"orc.dictionary.key.threshold\", \"1.0\") .option(\"orc.column.encoding.direct\", \"name\") .save(\"users_with_options.orc\") // $example off:manual_save_options_orc$ // $example on:direct_sql$ val sqlDF = spark.sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\") // $example off:direct_sql$ // $example on:write_sorting_and_bucketing$ peopleDF.write.bucketBy(42, \"name\").sortBy(\"age\").saveAsTable(\"people_bucketed\") // $example off:write_sorting_and_bucketing$ // $example on:write_partitioning$ usersDF.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\") // $example off:write_partitioning$ // $example on:write_partition_and_bucket$ usersDF .write .partitionBy(\"favorite_color\") .bucketBy(42, \"name\") .saveAsTable(\"users_partitioned_bucketed\") // $example off:write_partition_and_bucket$ spark.sql(\"DROP TABLE IF EXISTS people_bucketed\") spark.sql(\"DROP TABLE IF EXISTS users_partitioned_bucketed\")jdbc123456789101112131415161718192021222324252627282930313233343536373839404142434445package writeimport java.util.Propertiesimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object write_data &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession .builder() .appName(\"write_case\") .master(\"local\") .getOrCreate() // read val jdbcDF: DataFrame = spark.read .format(\"jdbc\") .option(\"url\", \"jdbc:postgresql:dbserver\") .option(\"dbtable\", \"schema.tablename\") .option(\"user\", \"username\") .option(\"password\", \"password\") .load() // write // Saving data to a JDBC source jdbcDF.write .format(\"jdbc\") .option(\"url\", \"jdbc:postgresql:dbserver\") .option(\"dbtable\", \"schema.tablename\") .option(\"user\", \"username\") .option(\"password\", \"password\") .mode(\"append\") .save() // or val properties=new Properties() properties.setProperty(\"user\",\"root\") properties.setProperty(\"password\",\"secret_password\") jdbcDF.write .mode(\"append\") .jdbc(\"jdbc:mysql://your_ip:3306/my_test?useUnicode=true&amp;characterEncoding=UTF-8\",\"t_result\",properties) &#125;&#125;hive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import java.io.Fileimport org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;case class Record(key: Int, value: String)// warehouseLocation points to the default location for managed databases and tablesval warehouseLocation = new File(\"spark-warehouse\").getAbsolutePathval spark = SparkSession .builder() .appName(\"Spark Hive Example\") .config(\"spark.sql.warehouse.dir\", warehouseLocation) .enableHiveSupport() .getOrCreate()import spark.implicits._import spark.sqlsql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")// Queries are expressed in HiveQLsql(\"SELECT * FROM src\").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.sql(\"SELECT COUNT(*) FROM src\").show()// +--------+// |count(1)|// +--------+// | 500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.val sqlDF = sql(\"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key\")// The items in DataFrames are of type Row, which allows you to access each column by ordinal.val stringsDS = sqlDF.map &#123; case Row(key: Int, value: String) =&gt; s\"Key: $key, Value: $value\"&#125;stringsDS.show()// +--------------------+// | value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a SparkSession.val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s\"val_$i\")))recordsDF.createOrReplaceTempView(\"records\")// Queries can then join DataFrame data with data stored in Hive.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// | 2| val_2| 2| val_2|// | 4| val_4| 4| val_4|// | 5| val_5| 5| val_5|// ...// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax// `USING hive`sql(\"CREATE TABLE hive_records(key int, value string) STORED AS PARQUET\")// Save DataFrame to the Hive managed tableval df = spark.table(\"src\")df.write.mode(SaveMode.Overwrite).saveAsTable(\"hive_records\")// After insertion, the Hive managed table has data nowsql(\"SELECT * FROM hive_records\").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Prepare a Parquet data directoryval dataDir = \"/tmp/parquet_data\"spark.range(10).write.parquet(dataDir)// Create a Hive external Parquet tablesql(s\"CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION '$dataDir'\")// The Hive external table should already have datasql(\"SELECT * FROM hive_bigints\").show()// +---+// | id|// +---+// | 0|// | 1|// | 2|// ... Order may vary, as spark processes the partitions in parallel.// Turn on flag for Hive Dynamic Partitioningspark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")// Create a Hive partitioned table using DataFrame APIdf.write.partitionBy(\"key\").format(\"hive\").saveAsTable(\"hive_part_tbl\")// Partitioned column `key` will be moved to the end of the schema.sql(\"SELECT * FROM hive_part_tbl\").show()// +-------+---+// | value|key|// +-------+---+// |val_238|238|// | val_86| 86|// |val_311|311|// ...spark.stop()hbase12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061object Application &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master(\"local\") .appName(\"normal\") .getOrCreate() spark.sparkContext.setLogLevel(\"warn\") val data = (0 to 255).map &#123; i =&gt; HBaseRecord(i, \"extra\")&#125; val df:DataFrame = spark.createDataFrame(data) df.write .mode(SaveMode.Overwrite) .options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog)) .format(\"org.apache.spark.sql.execution.datasources.hbase\") .save() &#125; def catalog = s\"\"\"&#123; |\"table\":&#123;\"namespace\":\"rec\", \"name\":\"user_rec\"&#125;, |\"rowkey\":\"key\", |\"columns\":&#123; |\"col0\":&#123;\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"&#125;, |\"col1\":&#123;\"cf\":\"t\", \"col\":\"col1\", \"type\":\"boolean\"&#125;, |\"col2\":&#123;\"cf\":\"t\", \"col\":\"col2\", \"type\":\"double\"&#125;, |\"col3\":&#123;\"cf\":\"t\", \"col\":\"col3\", \"type\":\"float\"&#125;, |\"col4\":&#123;\"cf\":\"t\", \"col\":\"col4\", \"type\":\"int\"&#125;, |\"col5\":&#123;\"cf\":\"t\", \"col\":\"col5\", \"type\":\"bigint\"&#125;, |\"col6\":&#123;\"cf\":\"t\", \"col\":\"col6\", \"type\":\"smallint\"&#125;, |\"col7\":&#123;\"cf\":\"t\", \"col\":\"col7\", \"type\":\"string\"&#125;, |\"col8\":&#123;\"cf\":\"t\", \"col\":\"col8\", \"type\":\"tinyint\"&#125; |&#125; |&#125;\"\"\".stripMargin&#125;case class HBaseRecord( col0: String, col1: Boolean, col2: Double, col3: Float, col4: Int, col5: Long, col6: Short, col7: String, col8: Byte)object HBaseRecord&#123; def apply(i: Int, t: String): HBaseRecord = &#123; val s = s\"\"\"row$&#123;\"%03d\".format(i)&#125;\"\"\" HBaseRecord(s, i % 2 == 0, i.toDouble, i.toFloat, i, i.toLong, i.toShort, s\"String$i: $t\", i.toByte) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"通过你的CPU主频，我们来谈谈“性能”究竟是什么？","slug":"通过你的CPU主频，我们来谈谈“性能”究竟是什么？","date":"2017-03-21T14:10:27.000Z","updated":"2020-10-21T14:12:52.184Z","comments":true,"path":"2017/03/21/通过你的CPU主频，我们来谈谈“性能”究竟是什么？/","link":"","permalink":"cpeixin.cn/2017/03/21/%E9%80%9A%E8%BF%87%E4%BD%A0%E7%9A%84CPU%E4%B8%BB%E9%A2%91%EF%BC%8C%E6%88%91%E4%BB%AC%E6%9D%A5%E8%B0%88%E8%B0%88%E2%80%9C%E6%80%A7%E8%83%BD%E2%80%9D%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"","text":"“性能”这个词，不管是在日常生活还是写程序的时候，都经常被提到。比方说，买新电脑的时候，我们会说“原来的电脑性能跟不上了”；写程序的时候，我们会说，“这个程序性能需要优化一下”。那么，你有没有想过，我们常常挂在嘴边的“性能”到底指的是什么呢？我们能不能给性能下一个明确的定义，然后来进行准确的比较呢？在计算机组成原理乃至体系结构中，“性能”都是最重要的一个主题。我在前面说过，学习和研究计算机组成原理，就是在理解计算机是怎么运作的，以及为什么要这么运作。“为什么”所要解决的事情，很多时候就是提升“性能”。什么是性能？时间的倒数计算机的性能，其实和我们干体力劳动很像，好比是我们要搬东西。对于计算机的性能，我们需要有个标准来衡量。这个标准中主要有两个指标。第一个是响应时间（Response time）或者叫执行时间（Execution time）。想要提升响应时间这个性能指标，你可以理解为让计算机“跑得更快”。图中是我们实际系统里性能监测工具 NewRelic 中的响应时间，代表了每个外部的 Web 请求的执行时间第二个是吞吐率（Throughput）或者带宽（Bandwidth），想要提升这个指标，你可以理解为让计算机“搬得更多”。所以说，响应时间指的就是，我们执行一个程序，到底需要花多少时间。花的时间越少，自然性能就越好。而吞吐率是指我们在一定的时间范围内，到底能处理多少事情。这里的“事情”，在计算机里就是处理的数据或者执行的程序指令。和搬东西来做对比，如果我们的响应时间短，跑得快，我们可以来回多跑几趟多搬几趟。所以说，缩短程序的响应时间，一般来说都会提升吞吐率。除了缩短响应时间，我们还有别的方法吗？当然有，比如说，我们还可以多找几个人一起来搬，这就类似现代的服务器都是 8 核、16 核的。人多力量大，同时处理数据，在单位时间内就可以处理更多数据，吞吐率自然也就上去了。提升吞吐率的办法有很多。大部分时候，我们只要多加一些机器，多堆一些硬件就好了。但是响应时间的提升却没有那么容易，因为 CPU 的性能提升其实在 10 年前就处于“挤牙膏”的状态了，所以我们得慎重地来分析对待。下面我们具体来看。我们一般把性能，定义成响应时间的倒数，也就是：性能 = 1/ 响应时间这样一来，响应时间越短，性能的数值就越大。同样一个程序，在 Intel 最新的 CPU Coffee Lake 上，只需要 30s 就能运行完成，而在 5 年前 CPU Sandy Bridge 上，需要 1min 才能完成。那么我们自然可以算出来，Coffee Lake 的性能是 1/30，Sandy Bridge 的性能是 1/60，两个的性能比为 2。于是，我们就可以说，Coffee Lake 的性能是 Sandy Bridge 的 2 倍。过去几年流行的手机跑分软件，就是把多个预设好的程序在手机上运行，然后根据运行需要的时间，算出一个分数来给出手机的性能评估。而在业界，各大 CPU 和服务器厂商组织了一个叫作 SPEC（Standard Performance Evaluation Corporation）的第三方机构，专门用来指定各种“跑分”的规则。一份 SPEC 报告通常包含了大量不同测试的评分SPEC 提供的 CPU 基准测试程序，就好像 CPU 届的“高考”，通过数十个不同的计算程序，对于 CPU 的性能给出一个最终评分。这些程序丰富多彩，有编译器、解释器、视频压缩、人工智能国际象棋等等，涵盖了方方面面的应用场景。计算机的计时单位：CPU 时钟虽然时间是一个很自然的用来衡量性能的指标，但是用时间来衡量时，有两个问题。第一个就是时间不“准”。如果用你自己随便写的一个程序，来统计程序运行的时间，每一次统计结果不会完全一样。有可能这一次花了 45ms，下一次变成了 53ms。为什么会不准呢？这里面有好几个原因。首先，我们统计时间是用类似于“掐秒表”一样，记录程序运行结束的时间减去程序开始运行的时间。这个时间也叫 Wall Clock Time 或者 Elapsed Time，就是在运行程序期间，挂在墙上的钟走掉的时间。但是，计算机可能同时运行着好多个程序，CPU 实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能 CPU 切换去运行别的程序了。而且，有些程序在运行的时候，可能要从网络、硬盘去读取数据，要等网络和硬盘把数据读出来，给到内存和 CPU。所以说，要想准确统计某个程序运行时间，进而去比较两个程序的实际性能，我们得把这些时间给刨除掉。那这件事怎么实现呢？Linux 下有一个叫 time 的命令，可以帮我们统计出来，同样的 Wall Clock Time 下，程序实际在 CPU 上到底花了多少时间。我们简单运行一下 time 命令。它会返回三个值，第一个是 real time，也就是我们说的 Wall Clock Time，也就是运行程序整个过程中流逝掉的时间；第二个是 user time，也就是 CPU 在运行你的程序，在用户态运行指令的时间；第三个是 sys time，是 CPU 在运行你的程序，在操作系统内核里运行指令的时间。而程序实际花费的 CPU 执行时间（CPU Time），就是 user time 加上 sys time。12345678$ time seq 1000000 | wc -l1000000real 0m0.101suser 0m0.031ssys 0m0.016s在我给的这个例子里，你可以看到，实际上程序用了 0.101s，但是 CPU time 只有 0.031+0.016 = 0.047s。运行程序的时间里，只有不到一半是实际花在这个程序上的。程序实际占用的 CPU 时间一般比 Elapsed Time 要少不少其次，即使我们已经拿到了 CPU 时间，我们也不一定可以直接“比较”出两个程序的性能差异。即使在同一台计算机上，CPU 可能满载运行也可能降频运行，降频运行的时候自然花的时间会多一些。除了 CPU 之外，时间这个性能指标还会受到主板、内存这些其他相关硬件的影响。所以，我们需要对“时间”这个我们可以感知的指标进行拆解，把程序的** CPU 执行时间变成 CPU 时钟周期数（CPU Cycles）和 时钟周期时间（Clock Cycle）的乘积。**程序的 CPU 执行时间 =CPU 时钟周期数×时钟周期时间我们先来理解一下什么是时钟周期时间。你在买电脑的时候，一定关注过 CPU 的主频。比如我手头的这台电脑就是 Intel Core-i7-7700HQ 2.8GHz，这里的 2.8GHz 就是电脑的主频（Frequency/Clock Rate）。这个 2.8GHz，我们可以先粗浅地认为，CPU 在 1 秒时间内，可以执行的简单指令的数量是 2.8G 条。如果想要更准确一点描述，这个 2.8GHz 就代表，我们 CPU 的一个“钟表”能够识别出来的最小的时间间隔。就像我们挂在墙上的挂钟，都是“滴答滴答”一秒一秒地走，所以通过墙上的挂钟能够识别出来的最小时间单位就是秒。而在 CPU 内部，和我们平时戴的电子石英表类似，有一个叫晶体振荡器（Oscillator Crystal）的东西，简称为晶振。我们把晶振当成 CPU 内部的电子表来使用。晶振带来的每一次“滴答”，就是时钟周期时间。在我这个 2.8GHz 的 CPU 上，这个时钟周期时间，就是 1/2.8G。我们的 CPU，是按照这个“时钟”提示的时间来进行自己的操作。主频越高，意味着这个表走得越快，我们的 CPU 也就“被逼”着走得越快。如果你自己组装过台式机的话，可能听说过“超频”这个概念，这说的其实就相当于把买回来的 CPU 内部的钟给调快了，于是 CPU 的计算跟着这个时钟的节奏，也就自然变快了。当然这个快不是没有代价的，CPU 跑得越快，散热的压力也就越大。就和人一样，超过生理极限，CPU 就会崩溃了。我们现在回到上面程序 CPU 执行时间的公式。程序的 CPU 执行时间 =CPU 时钟周期数×时钟周期时间最简单的提升性能方案，自然缩短时钟周期时间，也就是提升主频。换句话说，就是换一块好一点的 CPU。不过，这个是我们这些软件工程师控制不了的事情，所以我们就把目光挪到了乘法的另一个因子——CPU 时钟周期数上。如果能够减少程序需要的 CPU 时钟周期数量，一样能够提升程序性能。对于 CPU 时钟周期数，我们可以再做一个分解，把它变成“指令数×每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI）”。不同的指令需要的 Cycles 是不同的，加法和乘法都对应着一条 CPU 指令，但是乘法需要的 Cycles 就比加法要多，自然也就慢。在这样拆分了之后，我们的程序的 CPU 执行时间就可以变成这样三个部分的乘积。程序的 CPU 执行时间 = 指令数×CPI×Clock Cycle Time因此，如果我们想要解决性能问题，其实就是要优化这三者。时钟周期时间，就是计算机主频，这个取决于计算机硬件。我们所熟知的摩尔定律就一直在不停地提高我们计算机的主频。比如说，我最早使用的 80386 主频只有 33MHz，现在手头的笔记本电脑就有 2.8GHz，在主频层面，就提升了将近 100 倍。每条指令的平均时钟周期数 CPI，就是一条指令到底需要多少 CPU Cycle。在后面讲解 CPU 结构的时候，我们会看到，现代的 CPU 通过流水线技术（Pipeline），让一条指令需要的 CPU Cycle 尽可能地少。因此，对于 CPI 的优化，也是计算机组成和体系结构中的重要一环。指令数，代表执行我们的程序到底需要多少条指令、用哪些指令。这个很多时候就把挑战交给了编译器。同样的代码，编译成计算机指令时候，就有各种不同的表示方式。我们可以把自己想象成一个 CPU，坐在那里写程序。计算机主频就好像是你的打字速度，打字越快，你自然可以多写一点程序。CPI 相当于你在写程序的时候，熟悉各种快捷键，越是打同样的内容，需要敲击键盘的次数就越少。指令数相当于你的程序设计得够合理，同样的程序要写的代码行数就少。如果三者皆能实现，你自然可以很快地写出一个优秀的程序，你的“性能”从外面来看就是好的。总结对“性能”这个名词，你应该有了更清晰的认识。我主要对于“响应时间”这个性能指标进行抽丝剥茧，拆解成了计算机时钟周期、CPI 以及指令数这三个独立的指标的乘积，并且为你指明了优化计算机性能的三条康庄大道。也就是，提升计算机主频，优化 CPU 设计使得在单个时钟周期内能够执行更多指令，以及通过编译器来减少需要的指令数。","categories":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"cpeixin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"CPU","slug":"CPU","permalink":"cpeixin.cn/tags/CPU/"}]},{"title":"Spark SQL 讲解","slug":"Spark-SQL-讲解","date":"2017-03-17T13:15:45.000Z","updated":"2020-04-22T04:19:51.260Z","comments":true,"path":"2017/03/17/Spark-SQL-讲解/","link":"","permalink":"cpeixin.cn/2017/03/17/Spark-SQL-%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"Spark SQL是Apache Spark的用于处理结构化数据的模块。下图是Spark官网对Spark SQL的描述Spark SQL特点存在即合理，在数据分析查询领域中，虽然已经存在了底层运行MapReduce的Hive，但是人们越来越不满足于查询的速度，所以Spark SQL就应运而生。在我平常的工作中，Spark SQL的出场率真的超高～～集成123将SQL查询与Spark程序无缝混合。Spark SQL使您可以使用SQL或熟悉的DataFrame API在Spark程序中查询结构化数据。可在Java，Scala，Python和R中使用。统一数据访问123以相同的方式连接到任何数据源。DataFrame和SQL提供了一种访问各种数据源的通用方法，包括Hive，Avro，Parquet，ORC，JSON和JDBC。您甚至可以跨这些源联接数据。Hive整合12在现有仓库上运行SQL或HiveQL查询。Spark SQL支持HiveQL语法以及Hive SerDes和UDF，从而使您可以访问现有的Hive仓库。标准连接12通过JDBC或ODBC连接。服务器模式为商业智能工具提供了行业标准的JDBC和ODBC连接。Spark SQL架构Spark SQL 本质上是一个库。它运行在 Spark 的核心执行引擎之上。如上图所示，它提供类似于 SQL 的操作接口，允许数据仓库应用程序直接获取数据，允许使用者通过命令行操作来交互地查询数据，还提供两个编程抽象API：DataFrame API 和 DataSet API。Java、Python 和 Scala 的应用程序可以通过这两个 API 来读取和写入 RDD。此外，正如我们在RDD介绍的，应用程序还可以直接操作 RDD。使用 Spark SQL 会让开发者觉得好像是在操作一个关系型数据库一样，而不是在操作 RDD。这是它优于原生的 RDD API 的地方。与基本的 Spark RDD API 不同，Spark SQL 提供的接口为 Spark 提供了关于数据结构和正在执行的计算的更多信息。在内部，Spark SQL 使用这些额外的信息来执行额外的优化。虽然 Spark SQL 支持多种交互方式，但是在计算结果时均使用相同的执行引擎。这种统一意味着开发人员可以轻松地在不同的 API 之间来回切换，基于这些 API 提供了表达给定转换的最自然的方式。接下来让我们进一步了解 DataSet 和 DataFrame。DataSetDataSet，顾名思义，就是数据集的意思，它是 Spark 1.6 新引入的接口。同弹性分布式数据集类似，DataSet 也是不可变分布式的数据单元，它既有与 RDD 类似的各种转换和动作函数定义，而且还享受 Spark SQL 优化过的执行引擎，使得数据搜索效率更高。DataSet 支持的转换和动作也和 RDD 类似，比如 map、filter、select、count、show 及把数据写入文件系统中。同样地，DataSet 上的转换操作也不会被立刻执行，只是先生成新的 DataSet，只有当遇到动作操作，才会把之前的转换操作一并执行，生成结果。所以，DataSet 的内部结构包含了逻辑计划，即生成该数据集所需要的运算。当动作操作执行时，Spark SQL 的查询优化器会优化这个逻辑计划，并生成一个可以分布式执行的、包含分区信息的物理计划。那么，DataSet 和 RDD 的区别是什么呢？通过之前的叙述，我们知道 DataSet API 是 Spark SQL 的一个组件。那么，你应该能很容易地联想到，DataSet 也具有关系型数据库中表的特性。是的，DataSet 所描述的数据都被组织到有名字的列中，就像关系型数据库中的表一样。如上图所示，左侧的 RDD 虽然以 People 为类型参数，但 Spark 框架本身不了解 People 类的内部结构。所有的操作都以 People 为单位执行。而右侧的 DataSet 却提供了详细的结构信息与每列的数据类型。这让 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。也就是说，DataSet 提供数据表的 schema 信息。这样的结构使得 DataSet API 的执行效率更高。试想，如果我们要查询 People 的年龄信息，Spark SQL 执行的时候可以依靠查询优化器仅仅把需要的那一列取出来，其他列的信息根本就不需要去读取了。所以，有了这些信息以后在编译的时候能够做更多的优化。其次，由于 DataSet 存储了每列的数据类型。所以，在程序编译时可以执行类型检测。DataFrameDataFrame 可以被看作是一种特殊的 DataSet。它也是关系型数据库中表一样的结构化存储机制，也是分布式不可变的数据结构。但是，它的每一列并不存储类型信息，所以在编译时并不能发现类型错误。DataFrame 每一行的类型固定为 Row，他可以被当作 DataSet[Row]来处理，我们必须要通过解析才能获取各列的值。所以，对于 DataSet 我们可以用类似 people.name 来访问一个人的名字，而对于 DataFrame 我们一定要用类似 people.get As [String] (“name”) 来访问。RDD、DataFrame、DataSet 对比学习 Spark 到现在，我们已经接触了三种基本的数据结构：RDD、DataFrame 和 DataSet。接下来你的表格中，你可以看到它们的异同点，在实际工作中，经常会遇到 RDD-&gt;DataFrame-&gt;DataSet之间的相互转换，思考一下怎样在实际工程中选择。发展历史从发展历史上来看，RDD API 在第一代 Spark 中就存在，是整个 Spark 框架的基石。接下来，为了方便熟悉关系型数据库和 SQL 的开发人员使用，在 RDD 的基础上，Spark 创建了 DataFrame API。依靠它，我们可以方便地对数据的列进行操作。DataSet 最早被加入 Spark SQL 是在 Spark 1.6，它在 DataFrame 的基础上添加了对数据的每一列的类型的限制。在 Spark 2.0 中，DataFrame 和 DataSet 被统一。DataFrame 作为 DataSet[Row]存在。在弱类型的语言，如 Python 中，DataFrame API 依然存在，但是在 Java 中，DataFrame API 已经不复存在了。不变性与分区由于 DataSet 和 DataFrame 都是基于 RDD 的，所以它们都拥有 RDD 的基本特性，在此不做赘述。而且我们可以通过简单的 API 在 DataFrame 或 Dataset 与 RDD 之间进行无缝切换。性能DataFrame 和 DataSet 的性能要比 RDD 更好。Spark 程序运行时，Spark SQL 中的查询优化器会对语句进行分析，并生成优化过的 RDD 在底层执行。举个例子，如果我们想先对一堆数据进行 GroupBy 再进行 Filter 操作，这无疑是低效的，因为我们并不需要对所有数据都 GroupBy。如果用 RDD API 实现这一语句，在执行时它只会机械地按顺序执行。而如果用 DataFrame/DataSet API，Spark SQL 的 Catalyst 优化器会将 Filter 操作和 GroupBy 操作调换顺序，从而提高执行效率。下图反映了这一优化过程。详细的解析优化过程，我会单独用一篇文章来写的。错误检测RDD 和 DataSet 都是类型安全的，而 DataFrame 并不是类型安全的。这是因为它不存储每一列的信息如名字和类型。使用 DataFrame API 时，我们可以选择一个并不存在的列，这个错误只有在代码被执行时才会抛出。如果使用 DataSet API，在编译时就会检测到这个错误。小结DataFrame 和 DataSet 是 Spark SQL 提供的基于 RDD 的结构化数据抽象。它既有 RDD 不可变、分区、存储依赖关系等特性，又拥有类似于关系型数据库的结构化信息。所以，基于 DataFrame 和 DataSet API 开发出的程序会被自动优化，使得开发人员不需要操作底层的 RDD API 来进行手动优化，大大提升开发效率。但是 RDD API 对于非结构化的数据处理有独特的优势，比如文本流数据，而且更方便我们做底层的操作。所以在开发中，我们还是需要根据实际情况来选择使用哪种 API。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark Core 数据存储操作","slug":"Spark-Core-数据存储操作","date":"2017-03-15T14:41:50.000Z","updated":"2020-04-14T17:27:43.590Z","comments":true,"path":"2017/03/15/Spark-Core-数据存储操作/","link":"","permalink":"cpeixin.cn/2017/03/15/Spark-Core-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%93%8D%E4%BD%9C/","excerpt":"","text":"上文讲解了RDD的概念和转换算子、行动算子的操作内容，对于算子的操作，属于Spark针对数据进行计算的过程。回到现实场景，我们使用Spark来做任务的过程，其实就是三个阶段：读取数据源对源数据进行计算，业务逻辑的编写数据存储，数据落地在日常的工作中，Spark的任务可以分为两类，离线计算和实时计算。针对实时计算，几乎95%的情况下，我们是使用kafka作为消息队列与Spark进行数据对接。针对离线计算，数据源就多种多样了，可以是产品业务线的业务数据库，可以是大数据平台的Hive，HBase等，也可以是CEO办公室给出的报表数据。总结一下上面所讲述的，就是数据源多种多样，选择合适的组件和API将数据读进来就好。并且，在现实业务场景中，都是选择上层组件Spark SQL 和 Spark Streaming来读取数据，计算数据，存储数据。所以在这里就不复杂的数据操作了，只是针对在简单的业务场景，不需要Spark SQL 和 Spark Streaming组件，使用Spark Core处理简单的读取、计算和存储任务示例。数据落地至HDFS文件系统1234567891011121314151617181920212223242526272829package rddimport com.google.gson.&#123;JsonObject, JsonParser&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object process_json &#123; def main(args: Array[String]): Unit=&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"process_json\") .setMaster(\"local\") val sc = new SparkContext(sparkConf) \"\"\" data_demo: &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\":22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; \"\"\".stripMargin var json_rdd: RDD[String] = sc.textFile(\"hdfs://localhost:8020/data/user_data.json\") val result_rdd: RDD[JsonObject] = json_rdd.map((x: String) =&gt; &#123; val json = new JsonParser() val json_item = json.parse(x).asInstanceOf[JsonObject] json_item.addProperty(\"flag\", 1) json_item &#125;) result_rdd.saveAsTextFile(\"hdfs://localhost:8020/data/result/user_data\") &#125;&#125;数据落地至MySQL**1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package rddimport java.sql.&#123;Connection, DriverManager, PreparedStatement&#125;import com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object process_json &#123; def main(args: Array[String]): Unit=&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"process_json\") \"\"\" data_demo: &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\":22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; \"\"\".stripMargin val sc = new SparkContext(sparkConf) // 读取数据源 var json_rdd: RDD[String] = sc.textFile(\"hdfs://localhost:8020/data/user_data.json\") // 数据计算 val result_rdd: RDD[JSONObject] = json_rdd.map((x: String) =&gt; &#123; val json_item: JSONObject = JSON.parseObject(x) json_item.put(\"flag\", 1) json_item &#125;) val driverClassName = \"com.mysql.jdbc.Driver\" val url = \"jdbc:mysql://localhost:3306/test?characterEncoding=utf8&amp;useSSL=false\" val user = \"root\" val password = \"cpx726175\" result_rdd.foreachPartition((partition: Iterator[JSONObject]) =&gt; &#123; Class.forName(driverClassName) val connection: Connection = DriverManager.getConnection(url, user, password) val sql = \"insert into t_user(user_name, customer_id, age, birthday, deposit_amount, last_login_time,flag) values(?,?,?,?,?,?,?)\" val statement: PreparedStatement = connection.prepareStatement(sql) try &#123; partition.foreach &#123; json_data: JSONObject =&gt; &#123; statement.setString(1, json_data.getString(\"user_name\")) statement.setInt(2, json_data.getInteger(\"customer_id\")) statement.setInt(3, json_data.getInteger(\"age\")) statement.setString(4, json_data.getString(\"birthday\")) statement.setFloat(5, json_data.getFloat(\"deposit_amount\")) statement.setString(6, json_data.getString(\"last_login_time\")) statement.setInt(7, json_data.getInteger(\"flag\")) statement.executeUpdate() &#125; &#125; &#125;catch &#123; case e: Exception =&gt; println(e.printStackTrace()) &#125; finally &#123; if(statement!=null) statement.close() if(connection!=null) connection.close() &#125; connection.close() &#125; ) sc.stop() &#125;&#125;注：在使用mysql建表时，float字段，double字段需要指定小数点位数。否则将会按照四舍五入整数显示123456789CREATE TABLE &#96;t_user&#96; ( &#96;user_name&#96; varchar(50) DEFAULT NULL, &#96;customer_id&#96; int(50) DEFAULT NULL, &#96;age&#96; int(20) DEFAULT NULL, &#96;birthday&#96; varchar(50) DEFAULT NULL, &#96;deposit_amount&#96; float(20,3) DEFAULT NULL, &#96;last_login_time&#96; varchar(100) DEFAULT NULL, &#96;flag&#96; int(10) DEFAULT NULL) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4 COLLATE&#x3D;utf8mb4_0900_ai_ci落地数据如下：**数据落地至HBase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package rddimport java.langimport java.sql.&#123;Connection, DriverManager, PreparedStatement&#125;import org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableOutputFormat&#125;import org.apache.hadoop.hbase.client.Resultimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.mapreduce.Jobimport com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.mapred.JobConfimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object process_json &#123; def main(args: Array[String]): Unit=&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"process_json\") \"\"\" data_demo: &#123;\"user_name\":\"brent\",\"customer_id\":12031602,\"age\":22,\"birthday\":\"1993-04-05\",\"deposit_amount\":3000,\"last_login_time\":\"2017-03-10 14:55:22\"&#125; &#123;\"user_name\":\"haylee\",\"customer_id\":12031603,\"age\":23,\"birthday\":\"1992-08-10\",\"deposit_amount\":4000.56,\"last_login_time\":\"2017-03-11 10:55:00\"&#125; &#123;\"user_name\":\"vicky\",\"customer_id\":12031604,\"age\":30,\"birthday\":\"2000-03-02\",\"deposit_amount\":200.4,\"last_login_time\":\"2017-03-10 09:10:00\"&#125; \"\"\".stripMargin val sc = new SparkContext(sparkConf) var json_rdd: RDD[String] = sc.textFile(\"hdfs://localhost:8020/data/user_data.json\") val result_rdd: RDD[JSONObject] = json_rdd.map((x: String) =&gt; &#123; val json_item: JSONObject = JSON.parseObject(x) json_item.put(\"flag\", 1) json_item &#125;) var resultConf: Configuration = HBaseConfiguration.create() //设置zooKeeper集群地址，也可以通过将hbase-site.xml导入classpath，但是建议在程序里这样设置 resultConf.set(\"hbase.zookeeper.quorum\", \"localhost\") //设置zookeeper连接端口，默认2181 resultConf.set(\"hbase.zookeeper.property.clientPort\", \"2181\") //注意这里是output resultConf.set(TableOutputFormat.OUTPUT_TABLE, \"t_user\") var job: Job = Job.getInstance(resultConf) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[org.apache.hadoop.hbase.client.Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) val hbaseOut: RDD[(ImmutableBytesWritable, Put)] = result_rdd.map((json_data: JSONObject) =&gt; &#123; val put = new Put(Bytes.toBytes(json_data.getInteger(\"customer_id\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"user_name\"), Bytes.toBytes(json_data.getString(\"user_name\"))) //直接写入整型会以十六进制存储 put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"customer_id\"), Bytes.toBytes(json_data.get(\"customer_id\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"age\"), Bytes.toBytes(json_data.get(\"age\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"birthday\"), Bytes.toBytes(json_data.getString(\"birthday\"))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"deposit_amount\"), Bytes.toBytes(json_data.get(\"deposit_amount\").toString)) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"last_login_time\"), Bytes.toBytes(json_data.getString(\"last_login_time\"))) put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"flag\"), Bytes.toBytes(json_data.get(\"flag\").toString)) (new ImmutableBytesWritable, put) &#125;) hbaseOut.saveAsNewAPIHadoopDataset(job.getConfiguration) sc.stop() &#125;&#125;hbase建表语句：1create 't_user', 'cf1'落地数据如下：以上就是针对RDD，进行数据写入的操作。过程并不复杂，但是需要注意的是，在写入过程中，要对应好数据类型。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark - RDD讲解","slug":"Spark-RDD讲解","date":"2017-03-11T22:41:50.000Z","updated":"2020-05-18T06:36:24.984Z","comments":true,"path":"2017/03/12/Spark-RDD讲解/","link":"","permalink":"cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"What is RDD在讲RDD之前，先和大家说一下在Spark中，我们分析数据的过程，主要会碰到RDD，DataFrame，DataSet概念，这三种都是我们计算过程中的数据单元。在Spark 2.0之前，主要的数据操作都是操作RDD，而在Spark 2.0以后，官方开始呼吁大家迁移到基于DataSet，即使你是从Spark 2.0以后开始接触的spark，但是我也很负责人的告诉你，RDD你必须要懂～～定义弹性分布式数据集（Resilient Distributed Datasets : RDD），表示已被分区、不可变的，并能够被并行操作，可同错的数据集合。针对上面的定义，还说描述的很抽象，接下来根据每个RDD的属性，进行逐点说明分区分区这个概念在分布式计算中我们经常会看到，例如MapReduce中，数据在Map端写入到环形缓冲区，数据进行分区，reduce端读取相应的分区文件，还有在Kafka中，topic中的分区概念。在RDD中，本质上是一个只读的分区记录集合。也就是我们要处理的源数据的抽象。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同节点上，从而可以在集群中的不同节点上进行并行计算。这也是它可以被并行处理的前提。反向来思考的话，如果RDD不可分区，只是一个单独不可拆分的数据块，那么集群中的节点怎么对这个源数据进行分布式并行计算呢？逻辑上，我们可以认为 RDD 是一个大的数组。数组中的每个元素可以代表一个分区（Partition）。在物理存储中，每个分区指向一个存放在堆内内存和堆外内存或者磁盘中的数据块（Block），而这些数据块是独立的，它们可以被存放在系统中的不同节点。所以，RDD 只是抽象意义的数据集合，分区内部并不会存储具体的数据，仅保存了元数据信息。下图很好地展示了 RDD 的分区逻辑结构：RDD 中的每个分区存有它在该 RDD 中的 index。通过 RDD 的 ID 和分区的 index 可以唯一确定对应数据块的编号，从而通过底层存储层的接口中提取到数据进行处理。在集群中，各个节点上的数据块会尽可能地存放在内存中，只有当内存没有空间时才会存入硬盘。这样可以最大化地减少硬盘读写的开销。虽然 RDD 内部存储的数据是只读的，但是，我们可以去修改（例如通过 repartition 转换操作）并行计算单元的划分结构，也就是分区的数量。不可变性不可变性代表每一个 RDD 都是只读的，它所包含的分区信息不可以被改变。既然已有的 RDD 不可以被改变，我们只可以对现有的 RDD 进行转换（Transformation）操作，得到新的 RDD 作为中间计算的结果。上图也就是刚刚提到的针对RDD的Transformation操作中，包括的map算子，flatMap算子，filter算子，这些算子我们接下来的文章在一一讲解，这里我想给大家看的是，在举例的这三个算子实现方法中，我们可以看到都new MapPartitionsRDD（），也就是说，在对一个已有的RDD进行转换操作的过程中，并不是对这个RDD进行直接的修改，变换，而是读取父RDD，创建了一个新的RDD进行转换并且最后返回。那么这样会带来什么好处呢？显然，对于代表中间结果的 RDD，我们需要记录它是通过哪个 RDD 进行哪些转换操作得来，即依赖关系，而不用立刻去具体存储计算出的数据本身。这样做有助于提升 Spark 的计算效率，并且使错误恢复更加容易。试想，在一个有 N 步的计算模型中，如果记载第 N 步输出 RDD 的节点发生故障，数据丢失，我们可以从第 N-1 步的 RDD 出发，再次计算，而无需重复整个 N 步计算过程。这样的容错特性也是 RDD 为什么是一个“弹性”的数据集的原因之一。后边我们会提到 RDD 如何存储这样的依赖关系。并行操作由于单个 RDD 的分区特性，使得它天然支持并行操作，即不同节点上的数据可以被分别处理，然后产生一个新的 RDD。容错性为了保证RDD 中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。 相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的 特定数据转换（Transformation）操作（filter, map, join etc.)行为。当这个RDD的部分分区数据丢失时 ，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制 了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。另外，在RDD计算中，也通过checkpoint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是 logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通 过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算 生成丢失的分区数据。RDD 五大特性A list of partitionsRDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。A function for computing each split一个函数计算每一个分片，RDD的每个partition上面都会有function，也就是函数应用A list of dependencies on other RDDsRDD会记录它的依赖 ，依赖还具体分为宽依赖和窄依赖，RDD在计算的过程中，不断的转换，在内存中，不落地磁盘，如果某一环节出错，可以根据依赖来找回上一状态的RDD，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。Optionally,a Partitioner for Key-value RDDs可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面Optionally, a list of preferred locations to compute each split on我们的原则是移动计算，不移动数据，默认的是，磁盘中的数据是作为RDD加载到本机的内存中，但是，Spark这里给出了一个可选项，可以选择加载到指定的机器内存中，就是可以选择将数据放在那几台性能好的节点上RDD 结构通过上述讲解，我们了解了 RDD 的基本特性。而且，我们还提到每一个 RDD 里都会包括分区信息、所依赖的父 RDD 以及通过怎样的转换操作才能由父 RDD 得来等信息。实际上 RDD 的结构远比你想象的要复杂，让我们来看一个 RDD 的简易结构示意图：SparkContext 是所有 Spark 功能的入口，它代表了与 Spark 节点的连接，可以用来创建 RDD 对象以及在节点中的广播变量等。一个线程只有一个 SparkContext。SparkConf 则是一些参数配置信息。感兴趣的同学可以去阅读官方的技术文档，一些相对不重要的概念我就不再赘述了。Partitions 前文中我已经提到过，它代表 RDD 中数据的逻辑结构，每个 Partition 会映射到某个节点内存或硬盘的一个数据块。Partitioner 决定了 RDD 的分区方式，目前有两种主流的分区方式：Hash partitioner 和 Range partitioner。Hash，顾名思义就是对数据的 Key 进行散列分区，Range 则是按照 Key 的排序进行均匀分区。此外我们还可以创建自定义的 Partitioner。依赖关系Dependencies 是 RDD 中最重要的组件之一。如前文所说，Spark 不需要将每个中间计算结果进行数据复制以防数据丢失，因为每一步产生的 RDD 里都会存储它的依赖关系，即它是通过哪个 RDD 经过哪个转换操作得到的。细心的读者会问这样一个问题，父 RDD 的分区和子 RDD 的分区之间是否是一对一的对应关系呢？Spark 支持两种依赖关系：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）。窄依赖就是父 RDD 的分区可以一一对应到子 RDD 的分区，宽依赖就是父 RDD 的每个分区可以被多个子 RDD 的分区使用。显然，窄依赖允许子 RDD 的每个分区可以被并行处理产生，而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理。宽依赖本质就是shuffle,计算代价大,经过大量shuffle生成的RDD，建议进行缓存。这样避免失败后重新计算带来的开销如上图所示，一些转换操作如 map、filter 会产生窄依赖关系，而 Join、groupBy 则会生成宽依赖关系。这很容易理解，因为 map 是将分区里的每一个元素通过计算转化为另一个元素，一个分区里的数据不会跑到两个不同的分区。而 groupBy 则要将拥有所有分区里有相同 Key 的元素放到同一个目标分区，而每一个父分区都可能包含各种 Key 的元素，所以它可能被任意一个子分区所依赖。Spark 之所以要区分宽依赖和窄依赖是出于以下两点考虑：窄依赖可以支持在同一个节点上链式执行多条命令，例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。从失败恢复的角度考虑，窄依赖的失败恢复更有效，因为它只需要重新计算丢失的父分区即可，而宽依赖牵涉到 RDD 各级的多个父分区。DAG结合上篇spark运行原理和这篇RDD的讲述，我们来讲一下关于任务运行中，Job，Stage，Task的划分spark任务运行中，会存在一个或者多个Job，action算子的触发会生成一个Job, Job会提交给DAGScheduler,分解成Stage。上图是一个job被切割成三个Stage，每个stage中有包含不用个数的partition，每个partition在计算的时候对应一个task，影响程序的并行度。job分割stage的规则是从G端向前开始分割，遇到宽依赖，就分割一个stage.。F–&gt;G 切割 stage2 和 stage3A–&gt;B stage1上图，程序的运行最小单元是Task，就拿stage2来举例：stage2有4个Task: C端有2个partition，E端有两个partition。每个partition为开始，最终到F端，作为一个Task。 其中B阶段呢，属于一个程序级别的优化操作。一般分布式程序中，为了让程序能平稳的执行，就要做一些优化操作。在以后的Spark开发中，我们在Web UI中会经常看到类似于下图的工作流程，这里展示了一个Job的划分和对应操作的细节。How use RDD上面讲了很长篇幅的RDD概念和属性，那么我们该如何开始实操RDD呢？我们的第一个RDD来自哪里？上面我说过了，其实RDD就是我们要进行处理的源数据集合。在实际的业务场景中，对于离线数据分析，大多数的场景下，源数据可以是Spark从Hive、HBase中读取，转化成RDD，再细小一点的场景，源数据可以是一个csv报表数据，读取目标CSV进行RDD的转换。那么在下面的讲解中，我们无需对接上游生产环境的数据源，我们可以在IDEA中直接进行RDD的创建，随后再进行各种转换算子和行动算子的操作演示创建RDDRDD的创建有三种方法利用内存中集合中创建利用外部文件创建由其他RDD创建新的RDD从集合中创建可以使用parallelize() 和 makeRDD()方法创建我们可以看到，这两个方法需要我们传入的参数是 Seq[T] 集合，返回的都是RDD[T]。注意，makeRDD给了两种方法，这里先记为第一种makeRDD和第二种makeRDD这里，先把源码po出来可以清晰的看到，第一种makeRDD的源码实现中，实际上是调用了parallelize（），并且都可以指定分区数量，而且在注释中清晰的写明了，第一种makeRDD和parallelize是identical（完全相同的）。再来看第二种makeRDD，第二种实现可以为数据提供位置信息，并且不能指定RDD的分区数量，除此之外的实现和parallelize函数也是一致的创建示例：123456789val make_rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))val make_rdd_1: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\"))val make_rdd_2: RDD[Int] = sc.makeRDD(List(1,2,3,4),2)val make_rdd_3: RDD[String] = sc.parallelize(Array(\"Brent\",\"HayLee\",\"Henry\"),4)println(make_rdd.partitions.size)println(make_rdd_1.partitions.size)println(make_rdd_2.partitions.size)println(make_rdd_3.partitions.size)结果：12341124**外部文件创建****1234sc.textFile(\"hdfs://hadoop000:9000/xxx/data.txt\") // 读取hdfs文件sc.textFile(\"data.txt\") // 这里纯粹的本地文件是不推荐的， 因为这个文件访问是针对每一个Worker都要是能访问的 换言之,如果是本地文件,则必须保证每一个Worker的本地都有一份这个文件RDD 算子操作RDD 的数据操作分为两种：转换（Transformation）和动作（Action）。顾名思义，转换是用来把一个 RDD 转换成另一个 RDD，而动作则是通过计算返回一个结果。下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）Transformation（转换）Meaning（含义）map(func)返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中的元素应用一个函数 func 来生成。filter(func)返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中应用一个函数 func 且返回值为 true 的元素来生成。flatMap(func)与 map 类似，但是每一个输入的 item 可以被映射成 0 个或多个输出的 items（所以 func 应该返回一个 Seq 而不是一个单独的 item）。mapPartitions(func)与 map 类似，但是单独的运行在在每个 RDD 的 partition（分区，block）上，所以在一个类型为 T 的 RDD 上运行时 func 必须是 Iterator=&gt; Iterator 类型。mapPartitionsWithIndex(func)与 mapPartitions 类似，但是也需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 func_，所以在一个类型为 T 的 RDD 上运行时 _func 必须是 (Int, Iterator) =&gt; Iterator 类型。sample(withReplacement, fraction, seed)样本数据，设置是否放回（withReplacement），采样的百分比（_fraction_）、使用指定的随机数生成器的种子（seed）。union(otherDataset)返回一个新的 dataset，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的并集。intersection(otherDataset)返回一个新的 RDD，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的交集。distinct([_numTasks_]))返回一个新的 dataset，它包含了 source dataset（源数据集）中去重的元素。groupByKey([_numTasks_])在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable) .Note: 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 reduceByKey 或 aggregateByKey 来计算性能会更好.Note: 默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 numTasks 参数来设置不同的任务数。reduceByKey(func, [_numTasks_])在 (K, V) pairs 的 dataset 上调用时，返回 dataset of (K, V) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的函数 func 来进行聚合的，它必须是 type (V,V) =&gt; V 的类型。像 groupByKey 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。aggregateByKey(zeroValue)(seqOp, combOp, [_numTasks_])在 (K, V) pairs 的 dataset 上调用时，返回 (K, U) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的 combine 函数以及一个 neutral “0” 值来进行聚合的。允许聚合值的类型与输入值的类型不一样，同时避免不必要的配置。像 groupByKey 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。sortByKey([_ascending_], [_numTasks_])在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset，由 boolean 类型的 ascending 参数来指定。join(otherDataset, [_numTasks_])在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 leftOuterJoin, rightOuterJoin 和 fullOuterJoin 来实现。cogroup(otherDataset, [_numTasks_])在一个 (K, V) 和的 dataset 上调用时，返回一个 (K, (Iterable, Iterable)) tuples 的 dataset。这个操作也调用了 groupWith。cartesian(otherDataset)在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) pairs 类型的 dataset（所有元素的 pairs，即笛卡尔积）。pipe(command, [envVars])通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回。coalesce(numPartitions)Decrease（降低）RDD 中 partitions（分区）的数量为 numPartitions。对于执行过滤后一个大的 dataset 操作是更有效的。repartition(numPartitions)Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀。该操作总是通过网络来 shuffles 所有的数据。repartitionAndSortWithinPartitions(partitioner)根据给定的 partitioner（分区器）对 RDD 进行重新分区，并在每个结果分区中，按照 key 值对记录排序。这比每一个分区中先调用 repartition 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行。下表列出了一些 Spark 常用的 actions 操作。详细请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）。Action（动作）Meaning（含义）reduce(func)使用函数 func 聚合 dataset 中的元素，这个函数 func 输入为两个元素，返回为一个元素。这个函数应该是可交换（commutative）和关联（associative）的，这样才能保证它可以被并行地正确计算。collect()在 driver 程序中，以一个 array 数组的形式返回 dataset 的所有元素。这在过滤器（filter）或其他操作（other operation）之后返回足够小（sufficiently small）的数据子集通常是有用的。count()返回 dataset 中元素的个数。first()返回 dataset 中的第一个元素（类似于 take(1)。take(n)将数据集中的前 n 个元素作为一个 array 数组返回。takeSample(withReplacement, num, [_seed_])对一个 dataset 进行随机抽样，返回一个包含 num 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。takeOrdered(n, [ordering])返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 n 个元素。saveAsTextFile(path)将 dataset 中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录。saveAsSequenceFile(path)(Java and Scala)将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int，Double，String 等等)。saveAsObjectFile(path)(Java and Scala)使用 Java 序列化（serialization）以简单的格式（simple format）编写数据集的元素，然后使用 SparkContext.objectFile() 进行加载。countByKey()仅适用于（K,V）类型的 RDD。返回具有每个 key 的计数的（K , Int）pairs 的 hashmap。foreach(func)对 dataset 中每个元素运行函数 _func_。这通常用于副作用（side effects），例如更新一个 Accumulator（累加器）或与外部存储系统（external storage systems）进行交互。Note：修改除 foreach()之外的累加器以外的变量（variables）可能会导致未定义的行为（undefined behavior）。详细介绍请阅读 Understanding closures（理解闭包） 部分。该 Spark RDD API 还暴露了一些 actions（操作）的异步版本，例如针对 foreach 的 foreachAsync，它们会立即返回一个FutureAction 到调用者，而不是在完成 action 时阻塞。这可以用于管理或等待 action 的异步执行。部分Transformation算子操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package rddimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object transformation_func &#123; def main(args: Array[String]): Unit =&#123; val sparkConf: SparkConf = new SparkConf() .setAppName(\"transformation_func\") .setMaster(\"local\") val sc = new SparkContext(sparkConf) var original_rdd: RDD[(String, Int)] = sc.parallelize(Array((\"a\", 1), (\"b\", 1), (\"a\", 2),(\"c\",4),(\"c\",4)),2) var map_rdd: RDD[(String, Int)] = original_rdd.map(x =&gt;(x._1,x._2+1)) println(\"map操作：对original_rdd每个数据的第二个元素+1\") map_rdd.foreach(println) println(\"filter操作：过滤掉original_rdd中，第一个元素不为a的数据\") var filter_rdd: RDD[(String, Int)] = original_rdd.filter(x =&gt; x._1 == \"a\") filter_rdd.foreach(println) println(\"flatmap操作：对original_rdd做映射扁平化操作\") val flatmap_rdd: RDD[Char] = original_rdd.flatMap(x=&gt; x._1 + x._2) flatmap_rdd.foreach(println) println(\"mapPartitions操作：对original_rdd每个分区做相应操作\") // 效率要好于map 减少了发送到执行器的交互次数，但是可能会出现内存溢出 val mapPartitions_rdd: RDD[(String, Int)] = original_rdd.mapPartitions(x=&gt;&#123;x.map(item=&gt;(item._1,item._2+1))&#125;) mapPartitions_rdd.foreach(println) println(\"sample操作：提取样本\") val sample_rdd: RDD[(String, Int)] = original_rdd.sample(true, 0.25) sample_rdd.foreach(println) println(\"distinct操作：去重\") val distinct_rdd: RDD[(String, Int)] = original_rdd.distinct() distinct_rdd.foreach(println) println(\"groupbykey操作：分组聚合\") val groupByKey_rdd: RDD[(String, Iterable[Int])] = original_rdd.groupByKey() groupByKey_rdd.foreach(println) println(\"reduceByKey操作：聚合\") val reduceByKey_rdd: RDD[(String, Int)] = original_rdd.reduceByKey(_+_) reduceByKey_rdd.foreach(println) println(\"sortByKey操作：排序\") val sortByKey_rdd: RDD[(String, Int)] = original_rdd.sortByKey() sortByKey_rdd.foreach(println) &#125;&#125;RDD 的持久化（缓存）每当我们对 RDD 调用一个新的 action 操作时，整个 RDD 都会从头开始运算。因此，如果某个 RDD 会被反复重用的话，每次都从头计算非常低效，我们应该对多次使用的 RDD 进行一个持久化操作。Spark 的 persist() 和 cache() 方法支持将 RDD 的数据缓存至内存或硬盘中，这样当下次对同一 RDD 进行 Action 操作时，可以直接读取 RDD 的结果，大幅提高了 Spark 的计算效率。12345678rdd = sc.parallelize([1, 2, 3, 4, 5])rdd1 = rdd.map(lambda x: x+5)rdd2 = rdd1.filter(lambda x: x % 2 == 0)rdd2.persist()count = rdd2.count() // 3first = rdd2.first() // 6rdd2.unpersist()在文中的代码例子中你可以看到，我们对 RDD2 进行了多个不同的 action 操作。由于在第四行我把 RDD2 的结果缓存在内存中，所以 Spark 无需从一开始的 rdd 开始算起了（持久化处理过的 RDD 只有第一次有 action 操作时才会从源头计算，之后就把结果存储下来，所以在这个例子中，count 需要从源头开始计算，而 first 不需要）。在缓存 RDD 的时候，它所有的依赖关系也会被一并存下来。所以持久化的 RDD 有自动的容错机制。如果 RDD 的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算。持久化可以选择不同的存储级别。正如我们讲 RDD 的结构时提到的一样，有 MEMORY_ONLY，MEMORY_AND_DISK，DISK_ONLY 等。cache() 方法会默认取 MEMORY_ONLY 这一级别。RDD CheckpointCheckpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，但是在正常生产环境下放在 HDFS 上，这就天然的借助HDFS 高可靠的特征来完成最大化的可靠的持久化数据的方式。在进行 RDD 的 Checkpoint 的时候，其所依赖的所有 RDD 都会清空掉；官方建议如果要进行 checkpoint 时，必需先缓存在内存中。但实际可以考虑缓存在本地磁盘上或者是第三方组件，e.g. Taychon 上。在进行 checkpoint 之前需要通过 SparkConetxt 设置 checkpoint 的文件夹作为最佳实践，一般在进行 checkpoint 方法调用前都要进行 persists 来把当前 RDD 的数据持久化到内存或者是磁盘上，这是因为 checkpoint 是 lazy 级别，必需有 Job 的执行且在Job 执行完成后才会从后往前回溯哪个 RDD 进行了Checkpoint 标记，然后对该标记了要进行 Checkpoint 的 RDD 新启动一个Job 执行具体 Checkpoint 的过程RDD ShuffleSpark 里的某些操作会触发 shuffle。shuffle 是spark 重新分配数据的一种机制，使得这些数据可以跨不同的区域进行分组。这通常涉及在 executors 和 机器之间拷贝数据，这使得 shuffle 成为一个复杂的、代价高的操作。[Background](http://spark.apachecn.org/#/docs/4?id=background%ef%bc%88%e5%b9%95%e5%90%8e%ef%bc%89)为了明白 reduceByKey 操作的过程，我们以 reduceByKey 为例。reduceBykey 操作产生一个新的 RDD，其中 key 所有相同的的值组合成为一个 tuple - key 以及与 key 相关联的所有值在 reduce 函数上的执行结果。面临的挑战是，一个 key 的所有值不一定都在一个同一个 paritition 分区里，甚至是不一定在同一台机器里，但是它们必须共同被计算。在 spark 里，特定的操作需要数据不跨分区分布。在计算期间，一个任务在一个分区上执行，为了所有数据都在单个 reduceByKey 的 reduce 任务上运行，我们需要执行一个 all-to-all 操作。它必须从所有分区读取所有的 key 和 key对应的所有的值，并且跨分区聚集去计算每个 key 的结果 - 这个过程就叫做 shuffle。尽管每个分区新 shuffle 的数据集将是确定的，分区本身的顺序也是这样，但是这些数据的顺序是不确定的。如果希望 shuffle 后的数据是有序的，可以使用:mapPartitions 对每个 partition 分区进行排序，例如，.sortedrepartitionAndSortWithinPartitions 在分区的同时对分区进行高效的排序.sortBy 对 RDD 进行全局的排序触发的 shuffle 操作包括 repartition 操作，如 repartition 和 coalesce，‘ByKey 操作（除了 count 之外）像 groupByKey 和 reduceByKey，和 join 操作，像 cogroup 和 join.性能影响该 **Shuffle 是一个代价比较高的操作，它涉及磁盘 I/O、数据序列化、网络 I/O。为了准备 shuffle 操作的数据，Spark 启动了一系列的任务，map 任务组织数据，reduce 完成数据的聚合。这些术语来自 MapReduce，跟 Spark 的 map 操作和 reduce 操作没有关系。在内部，一个 map 任务的所有结果数据会保存在内存，直到内存不能全部存储为止。然后，这些数据将基于目标分区进行排序并写入一个单独的文件中。在 reduce 时，任务将读取相关的已排序的数据块。某些 shuffle 操作会大量消耗堆内存空间，因为 shuffle 操作在数据转换前后，需要在使用内存中的数据结构对数据进行组织。需要特别说明的是，reduceByKey 和 aggregateByKey 在 map 时会创建这些数据结构，&#39;ByKey 操作在 reduce 时创建这些数据结构。当内存满的时候，Spark 会把溢出的数据存到磁盘上，这将导致额外的磁盘 I/O 开销和垃圾回收开销的增加。shuffle 操作还会在磁盘上生成大量的中间文件。在 Spark 1.3 中，这些文件将会保留至对应的 RDD 不在使用并被垃圾回收为止。这么做的好处是，如果在 Spark 重新计算 RDD 的血统关系（lineage）时，shuffle 操作产生的这些中间文件不需要重新创建。如果 Spark 应用长期保持对 RDD 的引用，或者垃圾回收不频繁，这将导致垃圾回收的周期比较长。这意味着，长期运行 Spark 任务可能会消耗大量的磁盘空间。临时数据存储路径可以通过 SparkContext 中设置参数 spark.local.dir 进行配置。shuffle 操作的行为可以通过调节多个参数进行设置。详细的说明请看 Spark 配置指南 中的 “Shuffle 行为” 部分。Why use RDD首先，它的数据可以尽可能地存在内存中，从而大大提高的数据处理的效率；其次它是分区存储，所以天然支持并行处理；而且它还存储了每一步骤计算结果之间的依赖关系，从而大大提升了数据容错性和错误恢复的正确率，使 Spark 更加可靠。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 运行原理","slug":"Spark-运行原理","date":"2017-03-10T13:41:50.000Z","updated":"2020-09-26T06:31:57.842Z","comments":true,"path":"2017/03/10/Spark-运行原理/","link":"","permalink":"cpeixin.cn/2017/03/10/Spark-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/","excerpt":"","text":"spark的运行原理对于spark的学习尤为重要，如果不了解其运行原理，也就不会从根本上将spark的程序写好。这是一篇spark理论的文章，如果这篇文章并不能让你理解，可以先从实战入手，等你能简单的用spark写一个word count，需要提交任务在本地或者扔到服务器上的时候，再回来看看这篇文章，也许理解的程度就会更高一些。运行架构运行架构12345释义：• Cluster Manager：控制整个集群，监控worker。在standalone模式中即为Master主节点，在YARN模式中为ResourceManager• Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。• Driver： 运行Application 的main()函数• Executor：执行器，是为某个Application运行在worker node上的一个进程,Executor中有线程池任务运行流程构建Spark Application的运行环境，启动SparkContextSparkContext向资源管理器（Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackendExecutor向SparkContext申请TaskSparkContext将应用程序分发给ExecutorSparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行Task在Executor上运行，运行完释放所有资源运行特点**每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行Task。这种Application隔离机制是有优势的，无论是从调度角度看（每个Driver调度他自己的任务），还是从运行角度看（来自不同Application的Task运行在不同JVM中），当然这样意味着Spark Application不能跨应用程序共享数据，除非将数据写入外部存储系统Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了提交SparkContext的Client应该靠近Worker节点（运行Executor的节点），最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换Task采用了数据本地性和推测执行的优化机制释义：Application: Appliction都是指用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码Driver: Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用SparkContext代表DriverExecutor: 某个Application运行在worker节点上的一个进程， 该进程负责运行某些Task， 并且负责将数据存到内存或磁盘上，每个Application都有各自独立的一批Executor， 在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象， 负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task， 这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型Standalone : spark原生的资源管理，由Master负责资源的分配Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架Hadoop Yarn: 主要是指Yarn中的ResourceManagerWorker: 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NodeManager节点Task: 被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责Job: 包含多个Task组成的并行计算，往往由Spark Action触发生成， 一个Application中往往会产生多个JobStage: 每个Job会被拆分成多组Task， 作为一个TaskSet， 其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方DAGScheduler: 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图)，并提交Stage给TASkScheduler。 其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法TASKSedulter: 将TaskSET提交给worker运行，每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet，当Executor向Driver发生心跳时，TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行标签，重试失败的Task。下图展示了DAGScheduler， TaskScheduler的作用Job=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency1234在不同运行模式中任务调度器具体为： a. Spark on Standalone模式为TaskScheduler b. YARN-Client模式为YarnClientClusterScheduler c. YARN-Cluster模式为YarnClusterScheduler将这些术语串起来的运行层次图如下：运行模式Spark的运行模式多种多样，灵活多变，部署在单机上时，既可以用本地模式运行，也可以用伪分布模式运行，而当以分布式集群的方式部署时，也有众多的运行模式可供选择，这取决于集群的实际情况，底层的资源调度即可以依赖外部资源调度框架，也可以使用Spark内建的Standalone模式。对于外部资源调度框架的支持，目前的实现包括相对稳定的Mesos模式，以及hadoop YARN模式本地模式：常用于本地开发测试，本地还分别 local 和 local clusterstandalone: 独立集群运行模式**Standalone模式使用Spark自带的资源调度框架，采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA。框架结构图如下:该模式主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的Yarn模式运行：Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-ClusterYarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://xxxx:4040访问，而YARN通过http:// xxxx:8088访问YARN-client的工作流程步骤为：YARN-cluster的工作流程步骤Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己Spark Client 和 Spark Cluster的区别理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"Spark 初识","slug":"Spark-初识","date":"2017-03-09T11:27:07.000Z","updated":"2020-04-10T03:22:37.781Z","comments":true,"path":"2017/03/09/Spark-初识/","link":"","permalink":"cpeixin.cn/2017/03/09/Spark-%E5%88%9D%E8%AF%86/","excerpt":"","text":"这篇分享从介绍Spark开始，算是入门前的入门，不涉及原理，不涉及技术，但是却很重要，重要的是理解Spark是什么？用来做什么？为什么要选择Spark?这将对以后工程师们在选择和使用Spark的时候，更加能知道，应该怎样的去用Spark，在什么样的场景下去使用什么样的Spark组件，来发挥它的价值。What - SparkSpark是一个通用数据处理引擎。适用于各种环境 ，这里介绍一下，主要应用于两种最常见的场景离线场景：可以是时间为维度，几年的数据集，或者是业务为维度，某个领域的大数据集，这种数据可以我们一般叫做离线数据，或者冷数据。实时场景：网站埋点，实时从前端页面传输过来的数据，或者业务系统，物理硬件实时传输过来的数据，硬件信号或者图像数据，我们需要实时的去计算处理并且返回结果。应用程序开发人员和数据科学家将Spark纳入其应用程序，以便快速查询，分析和转换数据。所以与Spark关联最频繁的任务包括跨大型数据集的交互式查询，处理来自传感器或金融系统的流数据以及机器学习任务。Spark于2009年开始在加利福尼亚大学伯克利分校的AMPLab项目中生活。更具体地说，它是由于需要证明Meso的概念而诞生的，这也是在AMPLab中创建的。（科普：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核。）从一开始，Spark就被优化成在内存中运行。它比Hadoop的MapReduce等替代方法更快速地处理数据，这往往会在每个处理阶段之间向计算机硬盘驱动器写入数据。Spark的支持者声称，Spark在内存中的运行速度比Hadoop MapReduce快100倍，而且在处理基于Hadoop MapReduce本身的磁盘数据时速度也快了10倍。这种比较并不完全公平，这不仅仅是因为对于Spark的典型用例，原始速度往往比批处理更重要，在这种情况下类似MapReduce的解决方案仍然非常出色。基于Hadoop基于YARN的体系结构为Spark和其他应用程序共享通用集群和数据集提供了基础，同时确保一致的服务和响应级别。下图是构成Spark的生态系统，强大的生态系统：其中 Spark Core是Spark的核心API，并且支持Scala,Python,Java编程语言，R,SQL分析语言,在以Spark Core为基础之上，有着Spark SQL，Spark Streaming，Spark Mlib,Spark Graphx四个亲儿子。这四个组件，我将在之后的文章详细的介绍，并且会从应用和代码示例来讲解。Spark do WhatSpark能够一次处理数PB的数据，分布在数千个协作的物理或虚拟服务器集群中。（科普：什么是分布式计算？所谓分布式计算是一门计算机科学，它研究如何把一个需要非常巨大的计算能力才能解决的问题分成许多小的部分，然后把这些部分分配给许多计算机进行处理，最后把这些计算结果综合起来得到最终的结果。分布式网络存储技术是将数据分散的存储于多台独立的机器设备上。分布式网络存储系统采用 可扩展的系统结构，利用多台存储服务器分担存储负荷，利用位置服务器定位存储信息，不但解决了传统集中式存储系统中单存储服务器的瓶颈问题，还提高了系统的可靠性、可用性和扩展性。）它有一套广泛的开发者库和API，并且支持Java，Python，R和Scala等语言; (现在在写spark应用程序时，最长使用的是Scala语言，因为spark的源码就是用scala来编写的，再其次就是python语言，python的第三方库很多，节省了程序员很多的时间要去自己实现某些功能，这两个语言的语法都很简洁，上手简单，支持函数式编程)它的灵活性使其非常适合于各种用例。Spark通常与Hadoop的数据存储模块HDFS一起使用，但它也可以与HBase，Cassandra，MapR-DB，MongoDB和Amazon S3 等其他流行的数据存储子系统集成，并且可以和Kafka，Flume等数据传输队列和数据采集工具一起搭配使用。Who Use SparkSpark是为数据科学设计的，其抽象使数据科学变得更加简单。数据科学家通常使用机器学习 - 一套可以从数据中学习的技术和算法。这些算法通常是迭代的，Spark将数据集缓存在内存中的能力大大加快了迭代数据处理速度，使得Spark成为实现这种算法的理想处理引擎。Spark是为大数据工程师设计的，在强大的计算能力和优秀的架构设计面前，可以让数据工程师在不管是离线情景下还是实时的业务需求下，都可以放心的选择使用Spark,一次读取，并行化处理，对数据集支持容错，操作灵活性，第三方社区的积极支持，虽然Spark还面对着缺点，但我相信Spark的明天会更好。Why Use Spark在技术不断高速更迭的程序圈，一个新工具的出现与流行，必然是因为它满足了很大一部分人长期未被满足的需求，或是解决了一个长期让很多人难受的痛点。这里就不能不提MapReduce了，既然已经有了看似很成熟的 Hadoop 和 MapReduce，为什么我们还需要 Spark？MapReduce 被硅谷一线公司淘汰的两大主要原因：高昂的维护成本、时间性能“达不到”用户的期待。除此之外，MapReduce 模型的抽象层次低，大量的底层逻辑都需要开发者手工完成。只提供 Map 和 Reduce 两个操作。在 Hadoop 中，每一个 Job 的计算结果都会存储在 HDFS 文件存储系统中，所以每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟。由于这一原因，MapReduce 对于迭代算法的处理性能很差，而且很耗资源。因为迭代的每一步都要对 HDFS 进行读写，所以每一步都需要差不多的等待时间。第四，只支持批数据处理，欠缺对流数据处理的支持。因此，在 Hadoop 推出后，有很多人想办法对 Hadoop 进行优化，其中发展到现在最成熟的就是 Spark。选择Spark有很多原因，但三个关键：简单性：Spark的功能可以通过一组丰富的API来访问，所有这些都是专门为大规模数据快速轻松地交互而设计的。这些API都有详细的文档和结构，使数据科学家和应用程序开发人员能够快速地将Spark工作。速度：Spark是为速度而设计的，可以在内存和磁盘上运行。来自Databricks的团队使用Spark在2014年Daytona Grey Sort 100TB Benchmark挑战赛中与加利福尼亚大学圣地亚哥分校的一队队员并列第一名。挑战包括处理静态数据集; Databricks团队能够在23分钟内处理存储在固态硬盘上的100TB的数据，而之前的获胜者通过使用Hadoop和不同的集群配置需要72分钟的时间。在支持存储在内存中的数据的交互式查询时，Spark可以执行得更好。在这种情况下，有人声称Spark可以比Hadoop的MapReduce快100倍。关于速度来好好讲解一下，Spark以速度为出名，所以要把Spark为什么这么快来聊一聊由于 Spark 可以把迭代过程中每一步的计算结果都缓存在内存中，所以非常适用于各类迭代算法。Spark 第一次启动时需要把数据载入到内存，之后的迭代可以直接在内存里利用中间结果做不落地的运算。所以，后期的迭代速度快到可以忽略不计。在当今机器学习和人工智能大热的环境下，Spark 无疑是更好的数据处理引擎。下图是在 Spark 和 Hadoop 上运行逻辑回归算法的运行时间对比。在任务（task）级别上，Spark 的并行机制是多线程模型，而 MapReduce 是多进程模型。多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间。而 Spark 同一节点上的任务以多线程的方式运行在一个 JVM 进程中，可以带来更快的启动速度、更高的 CPU 利用率，以及更好的内存共享。从前文中你可以看出，Spark 作为新的分布式数据处理引擎，对 MapReduce 进行了很多改进，使得性能大大提升，并且更加适用于新时代的数据处理场景。支持：Spark支持一系列编程语言，包括Java，Python，R和Scala。尽管通常与HDFS密切相关，但Spark还包括对Hadoop生态系统及其以后的许多领先存储解决方案的紧密集成的本地支持。此外，Apache Spark社区是大型的，活跃的和国际性的。包括Databricks，IBM以及所有主要Hadoop供应商在内的不断增长的商业提供商为Spark解决方案提供全面的支持。Spark Use Case随着 Apache Spark的发展势头继续增长，几乎所有一站式大数据平台都早已集成Spark,国外最为著名的CDH,HDP，国内的TDH等，所有行业用于实际应用。正在使用Spark来改善他们的业务，通过检测模式和提供可操作的洞察力来推动组织变革，并开始改变生活的某些方面。下面提供了一些从保险到互联网公司如何使用Spark的例子：保险行业： 通过使用Spark的机器学习功能来处理和分析所有索赔，优化索赔报销流程。医疗保健： 使用Spark Core，Streaming和SQL构建病人护理系统。零售业 ： 使用Spark分析销售点数据和优惠券使用情况。互联网 ： 使用Spark的ML功能来识别虚假的配置文件，并增强他们向客户展示的产品匹配。银行业 ： 使用机器学习模型来预测某些金融产品的零售银行客户的资料。政府 ： 分析地理，时间和财政支出。科学研究 ： 通过时间，深度，地理分析地震事件来预测未来的事件。投资银行 ： 分析日内股价以预测未来的价格走势。地理空间分析： 按时间和地理分析Uber旅行，以预测未来的需求和定价。Twitter情绪分析： 分析大量的推文，以确定特定组织和产品的积极，消极或中立的情绪。航空公司 ： 建立预测航空旅行延误的模型。设备 ： 预测建筑物超过临界温度的可能性。上面所举的应用实例是想让大家更直接的去理解，Spark到底在实际的生产环境中能带来什么样的作用和发挥什么样的价值，对以后的学习，能更好的指导方向！最后来纠正一个不正确的观点，貌似很多技术论坛和网站上都有一些标题党在说“Spark是Hadoop的代替者”，“Hadoop被Spark终结”等类似标题的文章，内行的人一看就是脑残一样的标题😄，Spark 并不是一个完全替代 Hadoop 的全新工具。因为 Hadoop 还包含了很多组件：数据存储层：分布式文件存储系统 HDFS，分布式数据库存储的 HBase；数据处理层：进行数据处理的 MapReduce，负责集群和资源管理的 YARN；数据访问层：Hive、Pig、Mahout……从狭义上来看，Spark 只是 MapReduce 的替代方案，大部分应用场景中，它还要依赖于 HDFS 和 HBase 来存储数据，依赖于 YARN 来管理集群和资源。当然，Spark 并不是一定要依附于 Hadoop 才能生存，它还可以运行在 Apache Mesos、Kubernetes、standalone 等其他云平台上。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"}]},{"title":"【转载】数据结构和算法的思维框架","slug":"【转载】数据结构和算法的思维框架","date":"2017-02-20T03:34:45.000Z","updated":"2020-05-20T03:39:02.516Z","comments":true,"path":"2017/02/20/【转载】数据结构和算法的思维框架/","link":"","permalink":"cpeixin.cn/2017/02/20/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%9D%E7%BB%B4%E6%A1%86%E6%9E%B6/","excerpt":"","text":"这是好久之前的一篇文章「学习数据结构和算法的框架思维」的修订版。之前那篇文章收到广泛好评，没看过也没关系，这篇文章会涵盖之前的所有内容，并且会举很多代码的实例，教你如何使用框架思维。首先，这里讲的都是普通的数据结构，咱不是搞算法竞赛的，野路子出生，我只会解决常规的问题。另外，以下是我个人的经验的总结，没有哪本算法书会写这些东西，所以请读者试着理解我的角度，别纠结于细节问题，因为这篇文章就是希望对数据结构和算法建立一个框架性的认识。从整体到细节，自顶向下，从抽象到具体的框架思维是通用的，不只是学习数据结构和算法，学习其他任何知识都是高效的。一、数据结构的存储方式数据结构的存储方式只有两种：数组（顺序存储）和链表（链式存储）。这句话怎么理解，不是还有散列表、栈、队列、堆、树、图等等各种数据结构吗？我们分析问题，一定要有递归的思想，自顶向下，从抽象到具体。你上来就列出这么多，那些都属于「上层建筑」，而数组和链表才是「结构基础」。因为那些多样化的数据结构，究其源头，都是在链表或者数组上的特殊操作，API 不同而已。比如说「队列」、「栈」这两种数据结构既可以使用链表也可以使用数组实现。用数组实现，就要处理扩容缩容的问题；用链表实现，没有这个问题，但需要更多的内存空间存储节点指针。「图」的两种表示方法，邻接表就是链表，邻接矩阵就是二维数组。邻接矩阵判断连通性迅速，并可以进行矩阵运算解决一些问题，但是如果图比较稀疏的话很耗费空间。邻接表比较节省空间，但是很多操作的效率上肯定比不过邻接矩阵。「散列表」就是通过散列函数把键映射到一个大数组里。而且对于解决散列冲突的方法，拉链法需要链表特性，操作简单，但需要额外的空间存储指针；线性探查法就需要数组特性，以便连续寻址，不需要指针的存储空间，但操作稍微复杂些。「树」，用数组实现就是「堆」，因为「堆」是一个完全二叉树，用数组存储不需要节点指针，操作也比较简单；用链表实现就是很常见的那种「树」，因为不一定是完全二叉树，所以不适合用数组存储。为此，在这种链表「树」结构之上，又衍生出各种巧妙的设计，比如二叉搜索树、AVL 树、红黑树、区间树、B 树等等，以应对不同的问题。了解 Redis 数据库的朋友可能也知道，Redis 提供列表、字符串、集合等等几种常用数据结构，但是对于每种数据结构，底层的存储方式都至少有两种，以便于根据存储数据的实际情况使用合适的存储方式。综上，数据结构种类很多，甚至你也可以发明自己的数据结构，但是底层存储无非数组或者链表，二者的优缺点如下：数组由于是紧凑连续存储,可以随机访问，通过索引快速找到对应元素，而且相对节约存储空间。但正因为连续存储，内存空间必须一次性分配够，所以说数组如果要扩容，需要重新分配一块更大的空间，再把数据全部复制过去，时间复杂度 O(N)；而且你如果想在数组中间进行插入和删除，每次必须搬移后面的所有数据以保持连续，时间复杂度 O(N)。链表**因为元素不连续，而是靠指针指向下一个元素的位置，所以不存在数组的扩容问题；如果知道某一元素的前驱和后驱，操作指针即可删除该元素或者插入新元素，时间复杂度 O(1)。但是正因为存储空间不连续，你无法根据一个索引算出对应元素的地址，所以不能随机访问；而且由于每个元素必须存储指向前后元素位置的指针，会消耗相对更多的储存空间。二、数据结构的基本操作对于任何数据结构，其基本操作无非遍历 + 访问，再具体一点就是：增删查改。数据结构种类很多，但它们存在的目的都是在不同的应用场景，尽可能高效地增删查改**。话说这不就是数据结构的使命么？如何遍历 + 访问？我们仍然从最高层来看，各种数据结构的遍历 + 访问无非两种形式：线性的和非线性的。线性就是 for/while 迭代为代表，非线性就是递归为代表。再具体一步，无非以下几种框架：数组遍历框架，典型的线性迭代结构：12345void traverse(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; // 迭代访问 arr[i] &#125;&#125;链表遍历框架，兼具迭代和递归结构：1234567891011121314/* 基本的单链表节点 */class ListNode &#123; int val; ListNode next;&#125;void traverse(ListNode head) &#123; for (ListNode p = head; p != null; p = p.next) &#123; // 迭代访问 p.val &#125;&#125;void traverse(ListNode head) &#123; // 递归访问 head.val traverse(head.next)&#125;二叉树遍历框架，典型的非线性递归遍历结构：123456789/* 基本的二叉树节点 */class TreeNode &#123; int val; TreeNode left, right;&#125;void traverse(TreeNode root) &#123; traverse(root.left) traverse(root.right)&#125;你看二叉树的递归遍历方式和链表的递归遍历方式，相似不？再看看二叉树结构和单链表结构，相似不？如果再多几条叉，N 叉树你会不会遍历？二叉树框架可以扩展为 N 叉树的遍历框架：123456789/* 基本的 N 叉树节点 */class TreeNode &#123; int val; TreeNode[] children;&#125;void traverse(TreeNode root) &#123; for (TreeNode child : root.children) traverse(child)&#125;N 叉树的遍历又可以扩展为图的遍历，因为图就是好几 N 叉棵树的结合体。你说图是可能出现环的？这个很好办，用个布尔数组 visited 做标记就行了，这里就不写代码了。所谓框架，就是套路。不管增删查改，这些代码都是永远无法脱离的结构，你可以把这个结构作为大纲，根据具体问题在框架上添加代码就行了，下面会具体举例**。三、算法刷题指南首先要明确的是，数据结构是工具，算法是通过合适的工具解决特定问题的方法。也就是说，学习算法之前，最起码得了解那些常用的数据结构，了解它们的特性和缺陷。那么该如何在 LeetCode 刷题呢？之前的文章算法学习之路写过一些，什么按标签刷，坚持下去云云。现在距那篇文章已经过去将近一年了，我不说那些不痛不痒的话，直接说具体的建议：先刷二叉树，先刷二叉树，先刷二叉树！这是我这刷题一年的亲身体会，下图是去年十月份的提交截图：公众号文章的阅读数据显示，大部分人对数据结构相关的算法文章不感兴趣，而是更关心动规回溯分治等等技巧。为什么要先刷二叉树呢，因为二叉树是最容易培养框架思维的，而且大部分算法技巧，本质上都是树的遍历问题。刷二叉树看到题目没思路？根据很多读者的问题，其实大家不是没思路，只是没有理解我们说的「框架」是什么。不要小看这几行破代码，几乎所有二叉树的题目都是一套这个框架就出来了**。1234567void traverse(TreeNode root) &#123; // 前序遍历 traverse(root.left) // 中序遍历 traverse(root.right) // 后序遍历&#125;比如说我随便拿几道题的解法出来，不用管具体的代码逻辑，只要看看框架在其中是如何发挥作用的就行。LeetCode 124 题，难度 Hard，让你求二叉树中最大路径和，主要代码如下：12345678int ans = INT_MIN;int oneSideMax(TreeNode* root) &#123; if (root == nullptr) return 0; int left = max(0, oneSideMax(root-&gt;left)); int right = max(0, oneSideMax(root-&gt;right)); ans = max(ans, left + right + root-&gt;val); return max(left, right) + root-&gt;val;&#125;你看，这就是个后序遍历嘛。LeetCode 105 题，难度 Medium，让你根据前序遍历和中序遍历的结果还原一棵二叉树，很经典的问题吧，主要代码如下：123456789101112TreeNode buildTree(int[] preorder, int preStart, int preEnd, int[] inorder, int inStart, int inEnd, Map&lt;Integer, Integer&gt; inMap) &#123; if(preStart &gt; preEnd || inStart &gt; inEnd) return null; TreeNode root = new TreeNode(preorder[preStart]); int inRoot = inMap.get(root.val); int numsLeft = inRoot - inStart; root.left = buildTree(preorder, preStart + 1, preStart + numsLeft, inorder, inStart, inRoot - 1, inMap); root.right = buildTree(preorder, preStart + numsLeft + 1, preEnd, inorder, inRoot + 1, inEnd, inMap); return root;&#125;不要看这个函数的参数很多，只是为了控制数组索引而已，本质上该算法也就是一个前序遍历。LeetCode 99 题，难度 Hard，恢复一棵 BST，主要代码如下：12345678910void traverse(TreeNode* node) &#123; if (!node) return; traverse(node-&gt;left); if (node-&gt;val &lt; prev-&gt;val) &#123; s = (s == NULL) ? prev : s; t = node; &#125; prev = node; traverse(node-&gt;right);&#125;这不就是个中序遍历嘛，对于一棵 BST 中序遍历意味着什么，应该不需要解释了吧。你看，Hard 难度的题目不过如此，而且还这么有规律可循，只要把框架写出来，然后往相应的位置加东西就行了，这不就是思路吗。对于一个理解二叉树的人来说，刷一道二叉树的题目花不了多长时间。那么如果你对刷题无从下手或者有畏惧心理，不妨从二叉树下手，前 10 道也许有点难受；结合框架再做 20 道，也许你就有点自己的理解了；刷完整个专题，再去做什么回溯动规分治专题，你就会发现只要涉及递归的问题，都是树的问题。再举例吧，说几道我们之前文章写过的问题。动态规划详解说过凑零钱问题，暴力解法就是遍历一棵 N 叉树：12345678910111213def coinChange(coins: List[int], amount: int): def dp(n): if n == 0: return 0 if n &lt; 0: return -1 res = float('INF') for coin in coins: subproblem = dp(n - coin) # 子问题无解，跳过 if subproblem == -1: continue res = min(res, 1 + subproblem) return res if res != float('INF') else -1 return dp(amount)这么多代码看不懂咋办？直接提取出框架，就能看出核心思路了：1234# 不过是一个 N 叉树的遍历问题而已def dp(n): for coin in coins: dp(n - coin)其实很多动态规划问题就是在遍历一棵树，你如果对树的遍历操作烂熟于心，起码知道怎么把思路转化成代码，也知道如何提取别人解法的核心思路。再看看回溯算法，前文回溯算法详解干脆直接说了，回溯算法就是个 N 叉树的前后序遍历问题，没有例外。比如 N 皇后问题吧，主要代码如下：12345678910111213141516171819void backtrack(int[] nums, LinkedList&lt;Integer&gt; track) &#123; if (track.size() == nums.length) &#123; res.add(new LinkedList(track)); return; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; if (track.contains(nums[i])) continue; track.add(nums[i]); // 进入下一层决策树 backtrack(nums, track); track.removeLast(); &#125;/* 提取出 N 叉树遍历框架 */void backtrack(int[] nums, LinkedList&lt;Integer&gt; track) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; backtrack(nums, track);&#125;N 叉树的遍历框架，找出来了把～你说，树这种结构重不重要？综上，对于畏惧算法的朋友来说，可以先刷树的相关题目，试着从框架上看问题，而不要纠结于细节问题。纠结细节问题，就比如纠结 i 到底应该加到 n 还是加到 n - 1，这个数组的大小到底应该开 n 还是 n + 1 ？从框架上看问题，就是像我们这样基于框架进行抽取和扩展，既可以在看别人解法时快速理解核心逻辑，也有助于找到我们自己写解法时的思路方向。当然，如果细节出错，你得不到正确的答案，但是只要有框架，你再错也错不到哪去，因为你的方向是对的。但是，你要是心中没有框架，那么你根本无法解题，给了你答案，你也不会发现这就是个树的遍历问题。这种思维是很重要的，动态规划详解中总结的找状态转移方程的几步流程，有时候按照流程写出解法，说实话我自己都不知道为啥是对的，反正它就是对了。。。这就是框架的力量，能够保证你在快睡着的时候，依然能写出正确的程序；就算你啥都不会，都能比别人高一个级别。四、总结几句数据结构的基本存储方式就是链式和顺序两种，基本操作就是增删查改，遍历方式无非迭代和递归。刷算法题建议从「树」分类开始刷，结合框架思维，把这几十道题刷完，对于树结构的理解应该就到位了。这时候去看回溯、动规、分治等算法专题，对思路的理解可能会更加深刻一些。转载自：labuladong / fucking-algorithm","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Scala assert()断言函数","slug":"Scala-assert-断言函数","date":"2017-02-03T16:46:14.000Z","updated":"2020-05-18T16:47:47.992Z","comments":true,"path":"2017/02/04/Scala-assert-断言函数/","link":"","permalink":"cpeixin.cn/2017/02/04/Scala-assert-%E6%96%AD%E8%A8%80%E5%87%BD%E6%95%B0/","excerpt":"","text":"Scala assert()断言函数assert源码assert的作用是现计算表达式 assertion ，如果其值为假（即为0），则抛出异常，终止程序运行。常用在单元测试中进行值或者条件的判断调试，例如在编程中，我们在手动测试时，可以不用写大量的if else以下是使用断言的几个原则：（1）使用断言捕捉不应该发生的非法情况。不要混淆非法情况与错误情况之间的区别，后者是必然存在的并且是一定要作出处理的。（2）使用断言对函数的参数进行确认。（3）在编写函数时，要进行反复的考查，并且自问：“我打算做哪些假定？”一旦确定了的假定，就要使用断言对假定进行检查。（4）一般教科书都鼓励程序员们进行防错性的程序设计，但要记住这种编程风格会隐瞒错误。当进行防错性编程时，如果“不可能发生”的事情的确发生了，则要使用断言进行报警。ASSERT ()是一个调试程序时经常使用的宏，在程序运行时它计算括号内的表达式，如果表达式为FALSE (0), 程序将报告错误，并终止执行。如果表达式不为0，则继续执行后面的语句。这个宏通常原来判断程序中是否出现了明显非法的数据，如果出现了终止程序以免导致严重后果，同时也便于查找错误。ASSERT只有在Debug版本中才有效，如果编译为Release版本则被忽略。比较好的在程序中使用assert的地方：(1)空指针检查。例如，针对一个函数的参数进行空指针检查。当出现空指针时，你的程序就会退出，并很好的给出错误信息。(2)检查函数参数的值。例如，如果一个函数只能在它的一个参数啊为正值的时候被调用，你可以在函数开始时这样写:assert (a &gt; 0);，这将帮助你检测函数的错误使用，这也给源代码阅读者很清晰的印象，那就是在这里对函数的参数值有限制。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 方法和函数","slug":"Scala-方法和函数","date":"2017-01-04T09:23:59.000Z","updated":"2020-04-10T03:22:41.218Z","comments":true,"path":"2017/01/04/Scala-方法和函数/","link":"","permalink":"cpeixin.cn/2017/01/04/Scala-%E6%96%B9%E6%B3%95%E5%92%8C%E5%87%BD%E6%95%B0/","excerpt":"","text":"Scala 方法和函数方法与函数Scala 有方法与函数，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。Scala 中的方法跟 Java 的类似，方法是组成类的一部分。Scala 中的函数则是一个完整的对象，Scala 中的函数其实就是继承了 Trait 的类的对象。Scala 中使用 val 语句可以定义函数，def 语句定义方法。方法定义方法定义由一个 def 关键字开始，紧接着是可选的参数列表，一个冒号 : 和方法的返回类型，一个等于号 = ，最后是方法的主体。定义格式如下：1234def functionName ([参数列表]) : [return type] = &#123; function body return [expr]&#125;以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。以下方法的功能是将两个传入的参数相加并求和：1234567object add&#123; def addInt( a:Int, b:Int ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;函数Scala 也是一种函数式语言，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程，定义方式上，和scala方法是一样的。传值函数&amp;传名函数**Scala的解释器在解析函数参数(function arguments)时有两种方式：先计算参数表达式的值(reduce the arguments)，再应用到函数内部；或者是将未计算的参数表达式直接应用到函数内部。前者叫做传值调用（call-by-value），后者叫做传名调用（call-by-name）123456789101112131415object Test &#123; def main(args: Array[String]) &#123; delayed(time()); &#125; def time() = &#123; println(\"获取时间，单位为纳秒\") System.nanoTime &#125; def delayed( t: =&gt; Long ) = &#123; println(\"在 delayed 方法内\") println(\"参数： \" + t) 此时计算 time()函数 t &#125;&#125;注：scala函数体和方法体中的最后一行为返回值结果：1234在 delayed 方法内获取时间，单位为纳秒参数： 241550840475831获取时间，单位为纳秒=&gt; Unit 与 () =&gt;Unit的区别简单来说, =&gt; Unit是 传名函数, 只传入了一个表达式, 在调用时才会去执行, 使用 code调用() =&gt; 是传值函数, 传入的计算后的值，示例例如：1234567def function_1(t: () =&gt; Long): Unit = &#123; xxxx &#125; def function_2(t: =&gt; Long): Unit = &#123; xxxx &#125;指定函数参数名**不管在用什么语言在进行开发的过程中，对于函数的传参，我们几乎都是按照函数定义中，参数的顺序进行传参的，但是在Scala中会灵活一些，我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下：123456789object Test &#123; def main(args: Array[String]) &#123; printInt(b=5, a=7); &#125; def printInt( a:Int, b:Int ) = &#123; println(\"Value of a : \" + a ); println(\"Value of b : \" + b ); &#125;&#125;可变参数**Scala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。Scala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：123456789101112object Test &#123; def main(args: Array[String]) &#123; printStrings(\"Runoob\", \"Scala\", \"Python\"); &#125; def printStrings( args:String* ) = &#123; var i : Int = 0; for( arg &lt;- args )&#123; println(\"Arg value[\" + i + \"] = \" + arg ); i = i + 1; &#125; &#125;&#125;**递归函数**12345678910111213object Test &#123; def main(args: Array[String]) &#123; for (i &lt;- 1 to 10) println(i + \" 的阶乘为: = \" + factorial(i) ) &#125; def factorial(n: BigInt): BigInt = &#123; if (n &lt;= 1) 1 else n * factorial(n - 1) &#125;&#125;默认参数值Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：12345678910object Test &#123; def main(args: Array[String]) &#123; println( \"返回值 : \" + addInt() ); &#125; def addInt( a:Int=5, b:Int=7 ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;高阶函数高阶函数（Higher-Order Function）就是操作其他函数的函数。Scala 中允许使用高阶函数, 高阶函数可以使用其他函数作为参数，或者使用函数作为输出结果。以下实例中，apply() 函数使用了另外一个函数 f 和 值 v 作为参数，而函数 f 又调用了参数 v：**123456789101112object Test &#123; def main(args: Array[String]) &#123; println( apply( layout, 10) ) &#125; // 函数 f 和 值 v 作为参数，而函数 f 又调用了参数 v def apply(f: Int =&gt; String, v: Int) = f(v) def layout[A](x: A) = \"[\" + x.toString() + \"]\" &#125;匿名函数**匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。使用匿名函数后，我们的代码变得更简洁了。下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:1var inc = (x:Int) =&gt; x+1上述定义的匿名函数，其实是下面这种写法的简写：123def add2 = new Function1[Int,Int]&#123; def apply(x:Int):Int = x+1; &#125;以上实例的 inc 现在可作为一个函数，使用方式如下：1var x = inc(7)-1同样我们可以在匿名函数中定义多个参数：1var mul = (x: Int, y: Int) =&gt; x*ymul 现在可作为一个函数，使用方式如下：1println(mul(3, 4))我们也可以不给匿名函数设置参数，如下所示：1var userDir = () =&gt; &#123; System.getProperty(\"user.dir\") &#125;userDir 现在可作为一个函数，使用方式如下：1println( userDir() )偏应用函数偏应用函数也是一个蛮有意思的用法，**Scala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。1234567891011121314import java.util.Dateobject Test &#123; def main(args: Array[String]) &#123; val date = new Date log(date, \"message1\" ) Thread.sleep(1000) log(date, \"message2\" ) Thread.sleep(1000) log(date, \"message3\" ) &#125; def log(date: Date, message: String) = &#123; println(date + \"----\" + message) &#125;&#125;执行以上代码，输出结果为：12345$ scalac Test.scala$ scala TestMon Dec 02 12:52:41 CST 2018----message1Mon Dec 02 12:52:41 CST 2018----message2Mon Dec 02 12:52:41 CST 2018----message3实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。以上实例修改如下：123456789101112131415import java.util.Dateobject Test &#123; def main(args: Array[String]) &#123; val date = new Date val logWithDateBound = log(date, _ : String) logWithDateBound(\"message1\" ) Thread.sleep(1000) logWithDateBound(\"message2\" ) Thread.sleep(1000) logWithDateBound(\"message3\" ) &#125; def log(date: Date, message: String) = &#123; println(date + \"----\" + message) &#125;&#125;执行以上代码，输出结果为和上面的是一样的。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 基本语法","slug":"Scala-基本语法","date":"2017-01-03T09:21:58.000Z","updated":"2020-04-10T03:22:44.338Z","comments":true,"path":"2017/01/03/Scala-基本语法/","link":"","permalink":"cpeixin.cn/2017/01/03/Scala-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/","excerpt":"","text":"数据类型Scala 与 Java有着相同的数据类型，下表列出了 Scala 支持的数据类型：上表中列出的数据类型都是对象，也就是说scala没有java中的原生类型。在scala是可以对数字等基础类型调用方法的。这里就不多说了，其中需要注意的一点，就是字符与字符串的表示，因为平常使用python编程也是比较多的，在python中字符串是用单引号表示的，所以切换回scala，总会出现使用单引号定义字符串的情况在 Scala 字符变量使用单引号 ‘ 来定义，如下：‘a’而字符串字面量使用双引号 “ 来定义，如下：“Hello,World!”变量定义声明变量实例如下：1var myVar : String = \"Foo\"声明常量实例如下：1val myVal : String = \"Foo\"以上定义了常量 myVal，它是不能修改的。如果程序尝试修改常量 myVal 的值，程序将会在编译时报错。变量类型声明变量的类型在变量名之后的 ：声明。定义变量的类型的语法格式如下：12345var VariableName : DataType = Value或val VariableName : DataType = Value在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。上图则为省略变量类型，scala根据初始值进行判断。但是在项目开发中，建议大家不要省略变量类型。访问修饰符Scala 访问修饰符基本和Java的一样，分别有：private，protected，public。如果没有指定访问修饰符，默认情况下，Scala 对象的访问级别都是 public。Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。条件语句1234567891011121314151617181920if(布尔表达式)&#123; // 如果布尔表达式为 true 则执行该语句块&#125;if(布尔表达式)&#123; // 如果布尔表达式为 true 则执行该语句块&#125;else&#123; // 如果布尔表达式为 false 则执行该语句块&#125;if(布尔表达式 1)&#123; // 如果布尔表达式 1 为 true 则执行该语句块&#125;else if(布尔表达式 2)&#123; // 如果布尔表达式 2 为 true 则执行该语句块&#125;else if(布尔表达式 3)&#123; // 如果布尔表达式 3 为 true 则执行该语句块&#125;else &#123; // 如果以上条件都为 false 执行该语句块&#125;循环语句这里就不描述 while() do while() 语句了和其他语言基本一致，这里只描述和其他语言有差异的for循环语法如下：1234567891011for( var x &lt;- Range )&#123; statement(s);&#125;以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值。for( var x &lt;- List )&#123; statement(s);&#125;以上语法中， List 变量是一个集合，for 循环会迭代所有集合的元素。yield：你可以利用yield 将 for 循环中符合条件的值作为一个变量存储。语法格式如下：12345678910111213141516object Test &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6,7,8,9,10); // for 循环 var retVal = for&#123; a &lt;- numList if a != 3; if a &lt; 8 &#125;yield a // 输出返回值 for( a &lt;- retVal)&#123; println( \"Value of a: \" + a ); &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"Scala 创建程序文件","slug":"Scala-创建程序文件","date":"2017-01-02T09:20:44.000Z","updated":"2020-04-10T03:22:51.424Z","comments":true,"path":"2017/01/02/Scala-创建程序文件/","link":"","permalink":"cpeixin.cn/2017/01/02/Scala-%E5%88%9B%E5%BB%BA%E7%A8%8B%E5%BA%8F%E6%96%87%E4%BB%B6/","excerpt":"","text":"Scala 是一门类 Java 的编程语言，它结合了面向对象编程和函数式编程。Scala 是纯面向对象的，每个值都是一个对象，对象的类型和行为由类定义，不同的类可以通过混入(mixin)的方式组合在一起。Scala 的设计目的是要和两种主流面向对象编程语言Java 和 C#实现无缝互操作，这两种主流语言都非纯面向对象。Scala 也是一门函数式变成语言，每个函数都是一个值，原生支持嵌套函数定义和高阶函数。Scala 也支持一种通用形式的模式匹配，模式匹配用来操作代数式类型，在很多函数式语言中都有实现。Scala 被设计用来和 Java 无缝互操作（另一个修改的 Scala 实现可以工作在.NET上）。Scala 类可以调用 Java 方法，创建 Java 对象，继承 Java 类和实现 Java 接口。这些都不需要额外的接口定义或者胶合代码。前言我相信大多数接触scala的工程师，都是因为接触到了大数据技术栈，而被迫去学习scala的。scala这门语言完全可以理解成因为其开源项目的火爆，而将这门语言带到了大众的视野，例如计算框架Spark，消息队列Kafka， akka，使用Scala编写Spark程序真的是太便利了，简洁的函数编程让你欲罢不能，那么从另一方面来说，如果有一天，Spark被大数据技术栈淘汰了，那么Scala对于大数据工程师还是硬需求么？从现在来看，异军突起的Flink实时计算框架也是支持Scala的，同样相对于Java和python来说，Scala都有独特迷人的地方。创建文件对于有其他语言基础的工程师来讲，新学一门语言并不难，那么这里我就先写一些简明扼要的，工程师看完就可以直接创建文件，编写hello word就可以运行的。首先，我假定你已经配置好了scala的环境，编译器IDEA也已经配置完毕了，在选择好的目录中可以创建Scala文件了，如下图，我们该怎么选择呢？简单的来说：类class里无static类型，类里的属性和方法，必须通过new出来的对象来调用，所以有main主函数也没用。而object的特点是：可以拥有属性和方法，且默认都是”static”类型，可以直接用object名直接调用属性和方法，不需要通过new出来的对象（也不支持）。object里的main函数式应用程序的入口。object和class有很多和class相同的地方，可以extends父类或Trait，但object不可以extends object，即object无法作为父类。Scala的Trait相当于Java里的Interface根据上面的红字，我们也可以知道，如果想创建文件，并且运行文件，我们需要选择Object来写main()函数main函数怎么写呢？1def main(args: Array[String]) - Scala程序从main()方法开始处理，这是每一个Scala程序的强制程序入口部分。打印 hello word12345object helloword &#123; def main(args: Array[String]): Unit = &#123; println(\"hello word\") &#125;&#125;假如现在你对scala其他语法一无所知的情况下，你懂的了怎么创建文件，怎么去定义main()函数，还有和其他语言差不太多的打印函数 println(‘’)，就完成了编程语言第一课，HelloWorld这里有几个需要记住的点scala语言，每行语句后，不用 ； 结尾，java中需要使用 ；号结尾，python和scala一样，不需要分号。但是如果你想如果一行里写多个语句那么分号是需要的。例如val s = “哈哈哈”; println(s)Scala 使用 import 关键字引用包","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"}]},{"title":"算法 - 回溯","slug":"算法-回溯","date":"2016-11-25T23:36:39.000Z","updated":"2020-06-25T23:40:14.275Z","comments":true,"path":"2016/11/26/算法-回溯/","link":"","permalink":"cpeixin.cn/2016/11/26/%E7%AE%97%E6%B3%95-%E5%9B%9E%E6%BA%AF/","excerpt":"","text":"深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用却非常广泛。它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1 背包、图的着色、旅行商问题、全排列等等。既然应用如此广泛，我们今天就来学习一下这个算法思想，看看它是如何指导我们解决问题的。如何理解“回溯算法”？在我们的一生中，会遇到很多重要的岔路口。在岔路口上，每个选择都会影响我们今后的人生。有的人在每个岔路口都能做出最正确的选择，最后生活、事业都达到了一个很高的高度；而有的人一路选错，最后碌碌无为。如果人生可以量化，那如何才能在岔路口做出最正确的选择，让自己的人生“最优”呢？我们可以借助前面学过的贪心算法，在每次面对岔路口的时候，都做出看起来最优的选择，期望这一组选择可以使得我们的人生达到“最优”。但是，我们前面也讲过，贪心算法并不一定能得到最优解。那有没有什么办法能得到最优解呢？2004 年上映了一部非常著名的电影《蝴蝶效应》，讲的就是主人公为了达到自己的目标，一直通过回溯的方法，回到童年，在关键的岔路口，重新做选择。当然，这只是科幻电影，我们的人生是无法倒退的，但是这其中蕴含的思想其实就是回溯算法。笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。理论的东西还是过于抽象，老规矩，我还是举例说明一下。我举一个经典的回溯例子，我想你可能已经猜到了，那就是八皇后问题。我们有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。我们把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。回溯算法非常适合用递归代码实现，所以，我把八皇后的算法翻译成代码。我在代码里添加了详细的注释，你可以对比着看下。如果你之前没有接触过八皇后问题，建议你自己用熟悉的编程语言实现一遍，这对你理解回溯思想非常有帮助。12345678910111213141516171819202122232425262728293031323334353637383940int[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列public void cal8queens(int row) &#123; // 调用方式：cal8queens(0); if (row == 8) &#123; // 8个棋子都放置好了，打印结果 printQueens(result); return; // 8行棋子都放好了，已经没法再往下递归了，所以就return &#125; for (int column = 0; column &lt; 8; ++column) &#123; // 每一行都有8中放法 if (isOk(row, column)) &#123; // 有些放法不满足要求 result[row] = column; // 第row行的棋子放到了column列 cal8queens(row+1); // 考察下一行 &#125; &#125;&#125;private boolean isOk(int row, int column) &#123;//判断row行column列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &gt;= 0; --i) &#123; // 逐行往上考察每一行 if (result[i] == column) return false; // 第i行的column列有棋子吗？ if (leftup &gt;= 0) &#123; // 考察左上对角线：第i行leftup列有棋子吗？ if (result[i] == leftup) return false; &#125; if (rightup &lt; 8) &#123; // 考察右上对角线：第i行rightup列有棋子吗？ if (result[i] == rightup) return false; &#125; --leftup; ++rightup; &#125; return true;&#125;private void printQueens(int[] result) &#123; // 打印出一个二维矩阵 for (int row = 0; row &lt; 8; ++row) &#123; for (int column = 0; column &lt; 8; ++column) &#123; if (result[row] == column) System.out.print(\"Q \"); else System.out.print(\"* \"); &#125; System.out.println(); &#125; System.out.println();&#125;两个回溯算法的经典应用回溯算法的理论知识很容易弄懂。不过，对于新手来说，比较难的是用递归来实现。所以，我们再通过两个例子，来练习一下回溯算法的应用和实现。1.0-1 背包0-1背包是非常经典的算法问题，很多场景都可以抽象成这个问题模型。这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是今天讲的回溯算法。动态规划的解法我下一节再讲，我们先来看下，如何用回溯法解决这个问题。0-1 背包问题有很多变体，我这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是 Wkg。现在我们有 n 个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？实际上，背包问题我们在贪心算法那一节，已经讲过一个了，不过那里讲的物品是可以分割的，我可以装某个物品的一部分到背包里面。今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫 0-1 背包问题。显然，这个问题已经无法通过贪心算法来解决了。我们现在来看看，用回溯算法如何来解决。对于每个物品来说，都有两种选择，装进背包或者不装进背包。对于 n 个物品来说，总的装法就有 2^n 种，去掉总重量超过 Wkg 的，从剩下的装法中选择总重量最接近 Wkg 的。不过，我们如何才能不重复地穷举出这 2^n 种装法呢？这里就可以用回溯的方法。我们可以把物品依次排列，整个问题就分解为了 n 个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。描述起来很费劲，我们直接看代码，反而会更加清晰一些。这里还稍微用到了一点搜索剪枝的技巧，就是当发现已经选择的物品的重量超过 Wkg 之后，我们就停止继续探测剩下的物品。你可以看我写的具体的代码。12345678910111213141516public int maxW = Integer.MIN_VALUE; //存储背包中物品总重量的最大值// cw表示当前已经装进去的物品的重量和；i表示考察到哪个物品了；// w背包重量；items表示每个物品的重量；n表示物品个数// 假设背包可承受重量100，物品个数10，物品重量存储在数组a中，那可以这样调用函数：// f(0, 0, a, 10, 100)public void f(int i, int cw, int[] items, int n, int w) &#123; if (cw == w || i == n) &#123; // cw==w表示装满了;i==n表示已经考察完所有的物品 if (cw &gt; maxW) maxW = cw; return; &#125; f(i+1, cw, items, n, w); if (cw + items[i] &lt;= w) &#123;// 已经超过可以背包承受的重量的时候，就不要再装了 f(i+1,cw + items[i], items, n, w); &#125;&#125;2. 正则表达式看懂了 0-1 背包问题，我们再来看另外一个例子，正则表达式匹配。对于一个开发工程师来说，正则表达式你应该不陌生吧？在平时的开发中，或多或少都应该用过。实际上，正则表达式里最重要的一种算法思想就是回溯。正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。为了方便讲解，我假设正则表达式中只包含“”和“?”这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，“”匹配任意多个（大于等于 0 个）任意字符，“?”匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？我们依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如“*”有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。有了前面的基础，是不是这个问题就好懂多了呢？我把这个过程翻译成了代码，你可以结合着一块看下，应该有助于你理解。1234567891011121314151617181920212223242526272829303132333435public class Pattern &#123; private boolean matched = false; private char[] pattern; // 正则表达式 private int plen; // 正则表达式长度 public Pattern(char[] pattern, int plen) &#123; this.pattern = pattern; this.plen = plen; &#125; public boolean match(char[] text, int tlen) &#123; // 文本串及长度 matched = false; rmatch(0, 0, text, tlen); return matched; &#125; private void rmatch(int ti, int pj, char[] text, int tlen) &#123; if (matched) return; // 如果已经匹配了，就不要继续递归了 if (pj == plen) &#123; // 正则表达式到结尾了 if (ti == tlen) matched = true; // 文本串也到结尾了 return; &#125; if (pattern[pj] == '*') &#123; // *匹配任意个字符 for (int k = 0; k &lt;= tlen-ti; ++k) &#123; rmatch(ti+k, pj+1, text, tlen); &#125; &#125; else if (pattern[pj] == '?') &#123; // ?匹配0个或者1个字符 rmatch(ti, pj+1, text, tlen); rmatch(ti+1, pj+1, text, tlen); &#125; else if (ti &lt; tlen &amp;&amp; pattern[pj] == text[ti]) &#123; // 纯字符匹配才行 rmatch(ti+1, pj+1, text, tlen); &#125; &#125;&#125;内容小结回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。如果感兴趣的话，你可以自己搜索研究一下，最好还能用代码实现一下。如果这几个问题都能实现的话，你基本就掌握了回溯算法。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"回溯","slug":"回溯","permalink":"cpeixin.cn/tags/%E5%9B%9E%E6%BA%AF/"}]},{"title":"数据结构 - 堆","slug":"数据结构-堆","date":"2016-11-16T18:17:37.000Z","updated":"2020-06-16T18:19:30.651Z","comments":true,"path":"2016/11/17/数据结构-堆/","link":"","permalink":"cpeixin.cn/2016/11/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A0%86/","excerpt":"","text":"我们今天讲另外一种特殊的树，“堆”（Heap）。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序了。堆排序是一种原地的、时间复杂度为 O(nlogn) 的排序算法。前面我们学过快速排序，平均情况下，它的时间复杂度为 O(nlogn)。尽管这两种排序算法的时间复杂度都是 O(nlogn)，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？如何理解“堆”？前面我们提到，堆是一种特殊的树。我们现在就来看看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。堆是一个完全二叉树；堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。我分别解释一下这两点。第一点，堆必须是一个完全二叉树。还记得我们之前讲的完全二叉树的定义吗？完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。定义解释清楚了，你来看看，下面这几个二叉树是不是堆？其中第 1 个和第 2 个是大顶堆，第 3 个是小顶堆，第 4 个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。如何实现一个堆？要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。我之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。我画了一个用数组存储堆的例子，你可以先看下。从图中我们可以看到，数组中下标为 i 的节点的左子节点，就是下标为 i∗2 的节点，右子节点就是下标为 i∗2+1 的节点，父节点就是下标为 2i 的节点。知道了如何存储一个堆，那我们再来看看，堆上的操作有哪些呢？我罗列了几个非常核心的操作，分别是往堆中插入一个元素和删除堆顶元素。（如果没有特殊说明，我下面都是拿大顶堆来讲解）。1. 往堆中插入一个元素往堆中插入一个元素后，我们需要继续满足堆的两个特性。如果我们把新插入的元素放到堆的最后，你可以看我画的这个图，是不是不符合堆的特性了？于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫作堆化（heapify）。堆化实际上有两种，从下往上和从上往下。这里我先讲从下往上的堆化方法。堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。我这里画了一张堆化的过程分解图。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。我将上面讲的往堆中插入数据的过程，翻译成了代码，你可以结合着一块看。1234567891011121314151617181920212223public class Heap &#123; private int[] a; // 数组，从下标1开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) &#123; a = new int[capacity + 1]; n = capacity; count = 0; &#125; public void insert(int data) &#123; if (count &gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &gt; 0 &amp;&amp; a[i] &gt; a[i/2]) &#123; // 自下往上堆化 swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素 i = i/2; &#125; &#125; &#125;2. 删除堆顶元素从堆的定义的第二条中，任何节点的值都大于等于（或小于等于）子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或者最小值。假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。这里我也画了一个分解图。不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性。实际上，我们稍微改变一下思路，就可以解决这个问题。你看我画的下面这幅图。我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。我们知道，一个包含 n 个节点的完全二叉树，树的高度不会超过 log2n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。如何基于堆实现排序？前面我们讲过好几种排序算法，我们再来回忆一下，有时间复杂度是 O(n2) 的冒泡排序、插入排序、选择排序，有时间复杂度是 O(nlogn) 的归并排序、快速排序，还有线性排序。这里我们借助于堆这种数据结构实现的排序算法，就叫作堆排序。这种排序方法的时间复杂度非常稳定，是 O(nlogn)，并且它还是原地排序算法。如此优秀，它是怎么做到的呢？我们可以把堆排序的过程大致分解成两个大的步骤，建堆和排序。1. 建堆我们首先将数组原地建成一个堆。所谓“原地”就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。我举了一个例子，并且画了一个第二种实现思路的建堆分解步骤图，你可以看下。因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从第一个非叶子节点开始，依次堆化就行了。)代码：1234567891011121314151617private static void buildHeap(int[] a, int n) &#123; for (int i = n/2; i &gt;= 1; --i) &#123; heapify(a, n, i); &#125;&#125;private static void heapify(int[] a, int n, int i) &#123; while (true) &#123; int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; &#125;&#125;你可能已经发现了，在这段代码中，我们对下标从 2n 开始到 1 的数据进行堆化，下标是 2n+1 到 n 的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从 2n+1 到 n 的节点都是叶子节点。现在，我们来看，建堆操作的时间复杂度是多少呢？每个节点堆化的时间复杂度是 O(logn)，那 2n+1 个节点堆化的总时间复杂度是不是就是 O(nlogn) 呢？这个答案虽然也没错，但是这个值还是不够精确。实际上，堆排序的建堆过程的时间复杂度是 O(n)。我带你推导一下。因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度 k 成正比。我把每一层的节点个数和对应的高度画了出来，你可以看看。我们只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。我们将每个非叶子节点的高度求和，就是下面这个公式：这个公式的求解稍微有点技巧，不过我们高中应该都学过：把公式左右都乘以 2，就得到另一个公式 S2。我们将 S2 错位对齐，并且用 S2 减去 S1，可以得到 S。S 的中间部分是一个等比数列，所以最后可以用等比数列的求和公式来计算，最终的结果就是下面图中画的这个样子。因为 h=log2n，代入公式 S，就能得到 S=O(n)，所以，建堆的时间复杂度就是 O(n)。2. 排序建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素移除之后，我们把下标为 n 的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。1234567891011// n表示数据的个数，数组a中的数据从下标1到n的位置。public static void sort(int[] a, int n) &#123; buildHeap(a, n); int k = n; while (k &gt; 1) &#123; swap(a, 1, k); --k; heapify(a, k, 1); &#125;&#125;现在，我们再来分析一下堆排序的时间复杂度、空间复杂度以及稳定性。整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn)。堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。今天的内容到此就讲完了。我这里要稍微解释一下，在前面的讲解以及代码中，我都假设，堆中的数据是从数组下标为 1 的位置开始存储。那如果从 0 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。如果节点的下标是 i，那左子节点的下标就是 2∗i+1，右子节点的下标就是 2∗i+2，父节点的下标就是 2i−1。解答开篇现在我们来看开篇的问题，在实际开发中，为什么快速排序要比堆排序性能好？我觉得主要有两方面的原因。第一点，堆排序数据访问的方式没有快速排序友好。对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样对 CPU 缓存是不友好的。第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。我们在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。内容小结今天我们讲了堆这种数据结构。堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于（或小于等于）其子树节点的值。因此，堆被分成了两类，大顶堆和小顶堆。堆中比较重要的两个操作是插入一个数据和删除堆顶元素。这两个操作都要用到堆化。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是 O(logn)。除此之外，我们还讲了堆的一个经典应用，堆排序。堆排序包含两个过程，建堆和排序。我们将下标从 2n 到 1 的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"堆","slug":"堆","permalink":"cpeixin.cn/tags/%E5%A0%86/"}]},{"title":"LeetCode - 从中序与后序遍历序列构造二叉树","slug":"LeetCode-从中序与后序遍历序列构造二叉树","date":"2016-11-12T06:53:21.000Z","updated":"2020-06-05T16:45:15.890Z","comments":true,"path":"2016/11/12/LeetCode-从中序与后序遍历序列构造二叉树/","link":"","permalink":"cpeixin.cn/2016/11/12/LeetCode-%E4%BB%8E%E4%B8%AD%E5%BA%8F%E4%B8%8E%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"根据一棵树的中序遍历与后序遍历构造二叉树。注意:**你可以假设树中没有重复的元素。例如，给出中序遍历 inorder = [9,3,15,20,7]后序遍历 postorder = [9,15,7,20,3]返回如下的二叉树：12345 3 &#x2F; \\9 20 &#x2F; \\ 15 7前提解决此问题的关键在于要很熟悉树的各种遍历次序代表的什么，最好能够将图画出来。本题解带你先进行中序遍历和后续遍历二叉树，然后再根据遍历结果将二叉树进行还原。首先，来一棵树然后再看树的遍历结果根据中序和后序遍历结果还原二叉树中序遍历和后续遍历的特性**首先来看题目给出的两个已知条件中序遍历序列和后序遍历序列 根据这两种遍历的特性我们可以得出两个结论在后序遍历序列中,最后一个元素为树的根节点在中序遍历序列中,根节点的左边为左子树，根节点的右边为右子树如下图所示树的还原过程描述根据中序遍历和后续遍历的特性我们进行树的还原过程分析首先在后序遍历序列中找到根节点(最后一个元素)根据根节点在中序遍历序列中找到根节点的位置根据根节点的位置将中序遍历序列分为左子树和右子树根据根节点的位置确定左子树和右子树在中序数组和后序数组中的左右边界位置递归构造左子树和右子树返回根节点结束树的还原过程变量定义需要定义几个变量帮助我们进行树的还原HashMap memo 需要一个哈希表来保存中序遍历序列中,元素和索引的位置关系.因为从后序序列中拿到根节点后，要在中序序列中查找对应的位置,从而将数组分为左子树和右子树int ri 根节点在中序遍历数组中的索引位置中序遍历数组的两个位置标记 [is, ie],is是起始位置，ie是结束位置后序遍历数组的两个位置标记 [ps, pe] ps是起始位置，pe是结束位置位置关系的计算在找到根节点位置以后，我们要确定下一轮中，左子树和右子树在中序数组和后续数组中的左右边界的位置。左子树-中序数组 is = is, ie = ri - 1左子树-后序数组 ps = ps, pe = ps + ri - is - 1 (pe计算过程解释，后续数组的起始位置加上左子树长度-1 就是后后序数组结束位置了，左子树的长度 = 根节点索引-左子树)右子树-中序数组 is = ri + 1, ie = ie右子树-后序数组 ps = ps + ri - is, pe - 1听不明白没关系，看图就对了,计算图示如下树的还原过程代码123456789101112131415161718class Solution: def __init__(self): self.memo=&#123;&#125; self.post=[] def buildTree(self, inorder: List[int], postorder: List[int]) -&gt; TreeNode: self.memo=&#123;val:idx for idx,val in enumerate(inorder)&#125; self.post=postorder root=self.helper(0,len(inorder)-1,0,len(self.post)-1) return root def helper(self,istart,iend,pstart,pend): if istart&gt;iend or pstart&gt;pend: return None root=self.post[pend] # 获取根结点的val ri=self.memo[root] # 获取根结点的索引 node=TreeNode(self.post[pend]) node.left=self.helper(istart,ri-1,pstart,pstart+ri-istart-1) node.right=self.helper(ri+1,iend,pstart+ri-istart,pend-1) return node最后总结首先新建一个hashmap，将inorder也就是中序遍历数组的值作为key，下标作为value放进去；然后用一个不同参数的buildTree函数帮助实现，其中有四个参数要注意：inL、inR、poL、poR，其中的含义顾名思义，分别代表着当前子树在后序遍历和中序遍历中的范围，从L到R的值都处于当前子树中；代码的难点可能在于node.left和node.right这两部分，做题时可以通过画图来确定四个参数的值，其中难点可能就是左子树右端点poL+(head-inL)-1和右子树左端点poL+(head-inL)这两个边界的确定，其实只要明白中序遍历头结点左边的即为左子树，head-inL长度即为左子树长度，然后在后序遍历的左端点加上这么一段，即为右子树的左端点，同理可得左子树的右端点","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"数据结构-跳表","slug":"数据结构-跳表","date":"2016-11-08T08:46:05.000Z","updated":"2020-06-08T09:38:41.120Z","comments":true,"path":"2016/11/08/数据结构-跳表/","link":"","permalink":"cpeixin.cn/2016/11/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E8%B7%B3%E8%A1%A8/","excerpt":"","text":"二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没法用二分查找算法了吗？实际上，我们只需要对链表稍加改造，就可以支持类似“二分”的查找算法。我们把改造之后的数据结构叫作跳表（Skip list），也就是今天要讲的内容。跳表这种数据结构对你来说，可能会比较陌生，因为一般的数据结构和算法书籍里都不怎么会讲，跳表这个数据结构出现的时间相对与其他数据结构出现的较晚，大概在1980年左右才出现，所以大多数的系统底层都是使用树等其他数据结构实现的。但是它确实是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。Redis 中的有序集合（Sorted Set）就是用跳表来实现的。如果你有一定基础，应该知道红黑树也可以实现快速的插入、删除和查找操作。那 Redis 为什么会选择用跳表来实现有序集合呢？如何理解“跳表”？对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 O(n)。那怎么来提高查找效率呢？如果像图中那样，对链表建立一级“索引”，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫作索引或索引层。你可以看我画的图。图中的 down 表示 down 指针，指向下一级结点。如果我们现在要查找某个结点，比如 16。我们可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，我们发现下一个结点是 17，那要查找的结点 16 肯定就在这两个结点之间。然后我们通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。从这个例子里，我们看出，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。那如果我们再加一级索引呢？效率会不会提升更多呢？跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找 16，只需要遍历 6 个结点了，需要遍历的结点数量又减少了。我举的例子数据量不大，所以即便加了两级索引，查找效率的提升也并不明显。为了让你能真切地感受索引提升查询效率。我画了一个包含 64 个结点的链表，按照前面讲的这种思路，建立了五级索引。从图中我们可以看出，原来没有索引的时候，查找 62 需要遍历 62 个结点，现在只需要遍历 11 个结点，速度是不是提高了很多？所以，当链表的长度 n 比较大时，比如 1000、10000 的时候，在构建索引之后，查找效率的提升就会非常明显。前面讲的这种链表加多级索引的结构，就是跳表。我通过例子给你展示了跳表是如何减少查询次数的，现在你应该比较清晰地知道，跳表确实是可以提高查询效率的。接下来，我会定量地分析一下，用跳表查询到底有多快。用跳表查询到底有多快？前面我讲过，算法的执行效率可以通过时间复杂度来度量，这里依旧可以用。我们知道，在一个单链表中查询某个数据的时间复杂度是 O(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？这个时间复杂度的分析方法比较难想到。我把问题分解一下，先来看这样一个问题，如果链表里有 n 个结点，会有多少级索引呢？按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2k)。假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn)。那这个 m 的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历 3 个结点，也就是说 m=3，为什么是 3 呢？我来解释一下。假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。通过上面的分析，我们得到 m=3，所以在跳表中查询任意数据的时间复杂度就是 O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找，是不是很神奇？不过，天下没有免费的午餐，这种查询效率的提升，前提是建立了很多级索引，也就是我们在第 6 节讲过的空间换时间的设计思路。跳表是不是很浪费内存？比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间呢？我们来分析一下跳表的空间复杂度。跳表的空间复杂度分析并不难，我在前面说了，假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是 O(n)。计算过程n/2, n/4, .., 2 这个数列中一共有log2(n/2)项等比数列求和公式S = a0(1-q^n)/(1-q)其中a0表示首项，n表示项数这里的a0=n/2, 项数=log2(n/2), q=1/2，其中由对数的恒等式2log2(X)=X可得s=n(1-2log2(2/n))S = n/2(1-2/n)/(1-1/2) = n-2也就是说，如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 n 个结点的存储空间。那我们有没有办法降低索引占用的内存空间呢？我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢？我画了一个每三个结点抽一个的示意图，你可以看下。![7.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1591608508060-0fa55a6d-3586-4c21-b7b2-3dd90771785a.jpeg#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&name=7.jpg&originHeight=378&originWidth=1142&size=38717&status=done&style=none&width=1142)从图中可以看出，第一级索引需要大约 n/3 个结点，第二级索引需要大约 n/9 个结点。每往上一级，索引结点个数都除以 3。为了方便计算，我们假设最高一级的索引结点个数是 1。我们把每级索引的结点个数都写下来，也是一个等比数列。![8.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1591608532950-bc75afc9-3833-4291-9cfb-09c8b0be8d0a.jpeg#align=left&display=inline&height=360&margin=%5Bobject%20Object%5D&name=8.jpg&originHeight=360&originWidth=1142&size=33537&status=done&style=none&width=1142)通过等比数列求和公式，总的索引结点大约就是 n/3+n/9+n/27+…+9+3+1=n/2。尽管空间复杂度还是 O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。高效的动态插入和删除跳表长什么样子我想你应该已经很清楚了，它的查找操作我们刚才也讲过了。实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。我们现在来看下， 如何在跳表中插入一个数据，以及它是如何做到 O(logn) 的时间复杂度的？我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。我画了一张图，你可以很清晰地看到插入的过程。好了，我们再来看删除操作。如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。跳表索引动态更新当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。如果你了解红黑树、AVL 树这样平衡二叉树，你就知道它们是通过左右旋的方式保持左右子树的大小平衡（如果不了解也没关系，我们后面会讲），而跳表是通过随机函数来维护前面提到的“平衡性”。当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。解答开篇今天的内容到此就讲完了。现在，我来讲解一下开篇的思考题：为什么 Redis 要用跳表来实现有序集合，而不是红黑树？Redis 中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。不过散列表我们后面才会讲到，所以我们现在暂且忽略这部分。如果你去查看 Redis 的开发手册，就会发现，Redis 中的有序集合支持的核心操作主要有下面这几个：插入一个数据；删除一个数据；查找一个数据；按照区间查找数据（比如查找值在[100, 356]之间的数据）；迭代输出有序序列。其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。对于按照区间查找数据这个操作，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。当然，Redis 之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的 Map 类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。内容小结今天我们讲了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速的插入、删除、查找操作，时间复杂度都是 O(logn)。跳表的空间复杂度是 O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"skipList","slug":"skipList","permalink":"cpeixin.cn/tags/skipList/"}]},{"title":"数据结构 - 红黑树（上）","slug":"数据结构-红黑树（上）","date":"2016-11-07T00:38:30.000Z","updated":"2020-06-07T16:59:53.547Z","comments":true,"path":"2016/11/07/数据结构-红黑树（上）/","link":"","permalink":"cpeixin.cn/2016/11/07/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BA%A2%E9%BB%91%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是 O(logn)。不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于 log2n 的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。要解决这个复杂度退化的问题，我们需要设计一种平衡二叉查找树，也就是今天要讲的这种数据结构。很多书籍里，但凡讲到平衡二叉查找树，就会拿红黑树作为例子。不仅如此，如果你有一定的开发经验，你会发现，在工程中，很多用到平衡二叉查找树的地方都会用红黑树。你有没有想过，为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？什么是“平衡二叉查找树”？平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于 1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。最先被发明的平衡二叉查找树是AVL 树，它严格符合我刚讲到的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过 1，是一种高度平衡的二叉查找树。但是很多平衡二叉查找树其实并没有严格符合上面的定义（树中任意一个节点的左右子树的高度相差不能大于 1），比如我们下面要讲的红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。我们学习数据结构和算法是为了应用到实际的开发中的，所以，我觉得没必去死抠定义。对于平衡二叉查找树这个概念，我觉得我们要从这个数据结构的由来，去理解“平衡”的意思。发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比 log2n 大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。如何定义一棵“红黑树”？平衡二叉查找树其实有很多，比如，Splay Tree（伸展树）、Treap（树堆）等，但是我们提到平衡二叉查找树，听到的基本都是红黑树。它的出镜率甚至要高于“平衡二叉查找树”这几个字，有时候，我们甚至默认平衡二叉查找树就是红黑树，那我们现在就来看看这个“明星树”。红黑树的英文是“Red-Black Tree”，简称 R-B Tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的。那红黑树究竟是怎么定义的呢？顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：根节点是黑色的；每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；这里的第二点要求“叶子节点都是黑色的空节点”，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，所以，在画图和讲解的时候，我将黑色的、空的叶子节点都省略掉了。为了让你更好地理解上面的定义，我画了两个红黑树的图例，你可以对照着看下。为什么说红黑树是“近似平衡”的？我们前面也讲到，平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化的太严重。我们在上一节讲过，二叉查找树很多操作的性能都跟树的高度成正比。一棵极其平衡的二叉树（满二叉树或完全二叉树）的高度大约是 log2n，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近 log2n 就好了。红黑树的高度不是很好分析，我带你一步一步来推导。首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。前面红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。完全二叉树的高度近似 log2n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过 log2n。我们现在知道只包含黑色节点的“黑树”的高度，那我们现在把红色节点加回去，高度会变成多少呢？从上面我画的红黑树的例子和定义看，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过 log2n，所以加入红色节点之后，最长路径不会超过 2log2n，也就是说，红黑树的高度近似 2log2n。所以，红黑树的高度只比高度平衡的 AVL 树的高度（log2n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。解答开篇我们刚刚提到了很多平衡二叉查找树，现在我们就来看下，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树？我们前面提到 Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树内容小结很多同学都觉得红黑树很难，的确，它算是最难掌握的一种数据结构。其实红黑树最难的地方是它的实现，我们今天还没有涉及，下一节我会专门来讲。不过呢，我认为，我们其实不应该把学习的侧重点，放到它的实现上。那你可能要问了，关于红黑树，我们究竟需要掌握哪些东西呢？还记得我多次说过的观点吗？我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。你如果能搞懂这几个问题，其实就已经足够了。红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似 log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"红黑树","slug":"红黑树","permalink":"cpeixin.cn/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/"}]},{"title":"数据结构 - 二叉树基础（下）","slug":"数据结构-二叉树基础（下）","date":"2016-11-02T10:11:04.000Z","updated":"2020-05-31T10:16:21.213Z","comments":true,"path":"2016/11/02/数据结构-二叉树基础（下）/","link":"","permalink":"cpeixin.cn/2016/11/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"二叉查找树（Binary Search Tree）二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。它是怎么做到这些的呢？这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 我画了几个二叉查找树的例子，你一看应该就清楚了。前面我们讲到，二叉查找树支持快速查找、插入、删除操作，现在我们就依次来看下，这三个操作是如何实现的。1. 二叉查找树的查找操作首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。这里我把查找的代码实现了一下，贴在下面了，结合代码，理解起来会更加容易。123456789101112131415161718192021222324252627282930313233343536class Node: def __init__(self,value, left_child_node=None, right_child_node=None): self.value = value self.left_child_node = left_child_node self.right_child_node = right_child_nodeclass BinarySearchTree: def __init__(self, value, left_child_node=None, right_child_node=None): self.root = Node(value, left_child_node, right_child_node) def find_elem(self, value, node, parent_node, node_type): if node is None: return False, node, parent_node, node_type elif node.value == value: return True, node, parent_node, node_type elif value &gt; node.value: return self.find_elem(value, node.right_child_node, node, \"right_child_node\") elif value &lt; node.value: return self.find_elem(value, node.left_child_node, node, \"left_child_node\") def insert_elem(self, value): flag, node, parent_node, node_type = self.find_elem(value, self.root, self.root, None) if node_type == \"left_child_node\": node.left_child_node = Node(value) else: node.right_child_node = Node(value)if __name__ == '__main__': tree = BinarySearchTree(33, Node(17, Node(13,right_child_node=Node(16)), Node(18,right_child_node=Node(25, Node(19), Node(27)))), Node(50, Node(34), Node(58, Node(51), Node(66)))) flag, *rest = tree.find_elem(19, tree.root, tree.root, None) print(flag, rest[0].value, rest[1].value, rest[2])2. 二叉查找树的插入操作二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。同样，插入的代码我也实现了一下，贴在下面，你可以看看。1234567891011121314151617181920212223242526272829303132333435class Node: def __init__(self,value, left_child_node=None, right_child_node=None): self.value = value self.left_child_node = left_child_node self.right_child_node = right_child_nodeclass BinarySearchTree: def __init__(self, value, left_child_node=None, right_child_node=None): self.root = Node(value, left_child_node, right_child_node) def find_elem(self, value, node, parent_node, node_type): if node is None: return False, node, parent_node, node_type elif node.value == value: return True, node, parent_node, node_type elif value &gt; node.value: return self.find_elem(value, node.right_child_node, node, \"right_child_node\") elif value &lt; node.value: return self.find_elem(value, node.left_child_node, node, \"left_child_node\") def insert_elem(self, value): flag, node, parent_node, node_type = self.find_elem(value, self.root, self.root, None) if node_type == \"left_child_node\": parent_node.left_child_node = Node(value) else: parent_node.right_child_node = Node(value)if __name__ == '__main__': tree = BinarySearchTree(33, Node(17, Node(13,right_child_node=Node(16)), Node(18,right_child_node=Node(25, Node(19), Node(27)))), Node(50, Node(34), Node(58, Node(51), Node(66)))) flag, *rest = tree.find_elem(19, tree.root, tree.root, None) print(flag, rest[0].value, rest[1].value, rest[2]) tree.insert_elem(100) flag, *rest = tree.find_elem(100, tree.root, tree.root, None) print(flag, rest[0].value, rest[1].value, rest[2])3. 二叉查找树的删除操作二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了 。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。删除代码（要借助查找方法）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Node: def __init__(self,value, left_child_node=None, right_child_node=None): self.value = value self.left_child_node = left_child_node self.right_child_node = right_child_nodeclass BinarySearchTree: def __init__(self, value, left_child_node=None, right_child_node=None): self.root = Node(value, left_child_node, right_child_node) def find_elem(self, value, node, parent_node, node_type): if node is None: return False, node, parent_node, node_type elif node.value == value: return True, node, parent_node, node_type elif value &gt; node.value: return self.find_elem(value, node.right_child_node, node, \"right_child_node\") elif value &lt; node.value: return self.find_elem(value, node.left_child_node, node, \"left_child_node\") def insert_elem(self, value): flag, node, parent_node, node_type = self.find_elem(value, self.root, self.root, None) if node_type == \"left_child_node\": parent_node.left_child_node = Node(value) else: parent_node.right_child_node = Node(value) def delete_elem(self, value): \"\"\"Delete 1). 无子节点 2). 一个子节点 3). 两个子节点 \"\"\" flag, node, parent_node, node_type = self.find_elem(value, self.root, self.root, None) if flag is False: return else: if node.left_child_node == None and node.right_child_node == None: if node_type == \"left_child_node\": parent_node.left_child_node = None else: parent_node.right_child_node = None elif node.left_child_node != None and node.right_child_node == None: parent_node.left_child_node = node.left_child_node elif node.left_child_node == None and node.right_child_node != None: parent_node.right_child_node = node.right_child_node else: min_node = self.find_min_node(node.right_child_node) self.delete_elem(min_node.value) if node_type == \"left_child_node\": parent_node.left_child_node = min_node else: parent_node.right_child_node = min_node min_node.left_child_node = node.left_child_node min_node.right_child_node = node.right_child_node def find_min_node(self, node): \"\"\"查找最小值\"\"\" if node.left_child_node == None: return node else: return self.find_min_node(node.left_child_node)if __name__ == '__main__': tree = BinarySearchTree(33, Node(16, Node(13,right_child_node=Node(15)), Node(18,Node(17),right_child_node=Node(25, Node(19), Node(27)))), Node(50, Node(34), Node(58, Node(51), Node(66)))) flag, *rest = tree.find_elem(19, tree.root, tree.root, None) print(flag, rest[0].value, rest[1].value, rest[2]) # tree.insert_elem(100) # flag, *rest = tree.find_elem(100, tree.root, tree.root, None) # print(flag, rest[0].value, rest[1].value, rest[2]) tree.delete_elem(18)实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。4. 二叉查找树的其他操作除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。二叉树的遍历：1234567891011121314151617181920212223242526272829303132def preTraverse(self, root): ''' 前序遍历 中左右 ''' if root == None: return print(root.value) self.preTraverse(root.left_child_node) self.preTraverse(root.right_child_node)def midTraverse(self, root): ''' 中序遍历 左中右 ''' if root == None: return self.midTraverse(root.left_child_node) print(root.value) self.midTraverse(root.right_child_node)def afterTraverse(self, root): ''' 后序遍历 左右中 ''' if root == None: return self.afterTraverse(root.left_child_node) self.afterTraverse(root.right_child_node) print(root.value)二叉树查找最大最小值：123456789101112def find_min_node(self, node): \"\"\"查找最小值\"\"\" if node.left_child_node == None: return node else: return self.find_min_node(node.left_child_node)def find_max(self, root): if root.right_child_node is None: return root else: return self.find_max(root.right_child_node)二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。支持重复数据的二叉查找树前面讲二叉查找树的时候，我们默认树中节点存储的都是数字。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值（key）来构建二叉查找树。我们把对象中的其他字段叫作卫星数据。前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？我这里有两种解决方法。第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。第二种方法比较不好理解，不过更加优雅。每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。二叉查找树的时间复杂度分析好了，对于二叉查找树常用操作的实现方式，你应该掌握得差不多了。现在，我们来分析一下，二叉查找树的插入、删除、查找操作的时间复杂度。实际上，二叉查找树的形态各式各样。比如这个图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都是不一样的。图中第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了 O(n)。我刚刚其实分析了一种最糟糕的情况，我们现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树（或满二叉树）。这个时候，插入、删除、查找的时间复杂度是多少呢？从我前面的例子、图，以及还有代码来看，不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。既然这样，现在问题就转变成另外一个了，也就是，如何求一棵包含 n 个节点的完全二叉树的高度？树的高度就等于最大层数减一，为了方便计算，我们转换成层来表示。从图中可以看出，包含 n 个节点的完全二叉树中，第一层包含 1 个节点，第二层包含 2 个节点，第三层包含 4 个节点，依次类推，下面一层节点个数是上一层的 2 倍，第 K 层包含的节点个数就是 2^(K-1)。不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了。它包含的节点个数在 1 个到 2^(L-1) 个之间（我们假设最大层数是 L）。如果我们把每一层的节点个数加起来就是总的节点个数 n。也就是说，如果节点的个数是 n，那么 n 满足这样一个关系：12n &gt;= 1+2+4+8+...+2^(L-2)+1n &lt;= 1+2+4+8+...+2^(L-2)+2^(L-1)借助等比数列的求和公式，我们可以计算出，L 的范围是[log2(n+1), log2n +1]。完全二叉树的层数小于等于 log2n +1，也就是说，完全二叉树的高度小于等于 log2n。显然，极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树，这就是我们下一节课要详细讲的，一种特殊的二叉查找树，平衡二叉查找树。平衡二叉查找树的高度接近 logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是 O(logn)。解答开篇我们在散列表那节中讲过，散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？我认为有下面几个原因：第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。内容小结今天我们学习了一种特殊的二叉树，二叉查找树。它支持快速地查找、插入、删除操作。二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。不过，这只是针对没有重复数据的情况。对于存在重复数据的二叉查找树，我介绍了两种构建方法，一种是让每个节点存储多个值相同的数据；另一种是，每个节点中存储一个数据。针对这种情况，我们只需要稍加改造原来的插入、删除、查找操作即可。在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是 O(n) 和 O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。为了避免时间复杂度的退化，针对二叉查找树，我们又设计了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的 O(logn)。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"数据结构 - 二叉树基础（上）","slug":"数据结构-二叉树基础（上）","date":"2016-11-01T10:11:04.000Z","updated":"2020-05-31T10:16:37.334Z","comments":true,"path":"2016/11/01/数据结构-二叉树基础（上）/","link":"","permalink":"cpeixin.cn/2016/11/01/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？带着这些问题，我们就来学习今天的内容，树！树（Tree）我们首先来看，什么是“树”？再完备的定义，都没有图直观。所以我在图中画了几棵“树”。你来看看，这些“树”都有什么特征？你有没有发现，“树”这种数据结构真的很像我们现实生活中的“树”，这里面每个元素我们叫作“节点”；用来连线相邻节点之间的关系，我们叫作“父子关系”。比如下面这幅图，A 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫作根节点，也就是图中的节点 E。我们把没有子节点的节点叫作叶子节点或者叶节点，比如图中的 G、H、I、J、K、L 都是叶子节点。除此之外，关于“树”，还有三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。它们的定义是这样的：这三个概念的定义比较容易混淆，描述起来也比较空洞。我举个例子说明一下，你一看应该就能明白。记这几个概念，我还有一个小窍门，就是类比“高度”“深度”“层”这几个名词在生活中的含义。在我们的生活中，“高度”这个概念，其实就是从下往上度量，比如我们要度量第 10 层楼的高度、第 13 层楼的高度，起点都是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是 0。“深度”这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的深度也是类似的，从根结点开始度量，并且计数起点也是 0。“层数”跟深度的计算类似，不过，计数起点是 1，也就是说根节点的位于第 1 层。二叉树（Binary Tree）树结构多种多样，不过我们最常用还是二叉树。二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。我画的这几个都是二叉树。以此类推，你可以想象一下四叉树、八叉树长什么样子。这个图里面，有两个比较特殊的二叉树，分别是编号 2 和编号 3 这两个。其中，编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。满二叉树很好理解，也很好识别，但是完全二叉树，有的人可能就分不清了。我画了几个完全二叉树和非完全二叉树的例子，你可以对比着看看。你可能会说，满二叉树的特征非常明显，我们把它单独拎出来讲，这个可以理解。但是完全二叉树的特征不怎么明显啊，单从长相上来看，完全二叉树并没有特别特殊的地方啊，更像是“芸芸众树”中的一种。那我们为什么还要特意把它拎出来讲呢？为什么偏偏把最后一层的叶子节点靠左排列的叫完全二叉树？如果靠右排列就不能叫完全二叉树了吗？这个定义的由来或者说目的在哪里？要理解完全二叉树定义的由来，我们需要先了解，如何表示（或者存储）一棵二叉树？想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。我们再来看，基于数组的顺序存储法。我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。我来总结一下，如果节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），这样就可以通过下标计算，把整棵树都串起来。不过，我刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为 0 的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。你可以看我举的下面这个例子。所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。**二叉树的遍历前面我讲了二叉树的基本定义和存储方法，现在我们来看二叉树中非常重要的操作，二叉树的遍历。这也是非常常见的面试题。如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是，如果要解决问题 A，就假设子问题 B、C 已经解决，然后再来看如何利用 B、C 来解决 A。所以，我们可以把前、中、后序遍历的递推公式都写出来。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Node: def __init__(self, value=None, left=None, right=None): self.value = value self.left = left # 左子树 self.right = right # 右子树def preTraverse(root): ''' 前序遍历 ''' if root == None: return print(root.value) preTraverse(root.left) preTraverse(root.right)def midTraverse(root): ''' 中序遍历 ''' if root == None: return midTraverse(root.left) print(root.value) midTraverse(root.right)def afterTraverse(root): ''' 后序遍历 ''' if root == None: return afterTraverse(root.left) afterTraverse(root.right) print(root.value)if __name__=='__main__': root=Node('D',Node('B',Node('A'),Node('C')),Node('E',right=Node('G',Node('F')))) print('前序遍历：') preTraverse(root) print('\\n') print('中序遍历：') midTraverse(root) print('\\n') print('后序遍历：') afterTraverse(root) print('\\n')","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"Hive 基本操作","slug":"Hive-基本操作","date":"2016-10-20T14:14:13.000Z","updated":"2020-06-14T14:31:01.292Z","comments":true,"path":"2016/10/20/Hive-基本操作/","link":"","permalink":"cpeixin.cn/2016/10/20/Hive-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Hive 交互命令刚刚安装好hive后，先来看一看基本的交互12345678910111213141516(base) [cpeixin@CpeixindeMBP:] ~ $ hive -helpHive Session ID &#x3D; 626af458-fe89-406e-9a63-5967e2962486usage: hive -d,--define &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. -d A&#x3D;B or --define A&#x3D;B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property&#x3D;value&gt; Use value for given property --hivevar &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. --hivevar A&#x3D;B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console)在不进入客户端的情况下，我们可以上面这些交互命令来操作hive-e 在linux命令行窗口执行sql语句[cpeixin@CpeixindeMBP]$ hive -e “hive -e “select * from test.car””注： 记得加库名-f 执行脚本中sql语句，并将结果写到指定文件[cpeixin@CpeixindeMBP]$ hive -f /opt/sql/test.sql &gt; /opt/data/test_result.txt查看hive的执行历史[cpeixin@CpeixindeMBP:] ~ $ cat .hivehistory在进入hive客户端中，我们还可以使用命令查看本地和HDFS文件系统的文件，这个还是蛮实用的Hive 数据类型基本的交互了解后呢，我们来看一下Hive中的数据类型基本数据类型：数据类型字节范围示例TINYINT1byte-128 ~ 127100YSMALLINT2byte-32,768 ~ 32,767100SINT/INTEGER4byte-2,147,483,648 ~ 2,147,483,647100BIGINT8byte-9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807100LFLOAT4byte单精度浮点数0.2DOUBLE8byte双精度浮点数0.2DECIMAL高精度浮点数DECIMAL(9,8)BOOLEANTRUE/FALSEtrueBINARY二进制类型TIMESTAMP时间戳DATE日期2016-08-08STRINGVARCHAR长度 1～65535CHAR最大长度255关于整型：默认情况下，整型数据默认为INT，除非数字超出INT的范围，在这种情况下它被表示为 BIGINT，或者直接指定100Y，100S, 100L 才会对应转换成TINYINT、SMALLINT、BIGINT。关于浮点型：浮点数假定为 DOUBLE，Decimal 字为 DOUBLE 类型提供精确值和浮点数的更大范围。 Decimal 数据类型存储数值的精确表示，而 DOUBLE 数据类型存储非常接近数值的近似值。DECIMAL不指定精度时默认为DECIMAL(10,0)；DOUBLE 的(非常接近)近似值不足以满足要求是，需要使用Decimal 类型，例如财务应用程序，相等和不等式检查以及舍入操作。对于处理 DOUBLE 范围(大约-10308 到 10308)或非常接近零(-10-308 到 10-308)之外的数的用例，也需要它们。另外，Decimal为专门为财务相关问题设计的数据类型。关于字符型：Strings 字符串数据可以用单引号(‘)或 双引号(“)表示.Hive 在 strings 中使用 C-style 转义。Varchar 使用长度说明符(介于 1 和 65535 之间)创建 Varchar 类型，该长度说明符定义字符 string 中允许的最大字符数。如果varchar value超过了长度说明符，则会以静默方式截断 string。字符长度由字符 串中包含的字符数决定。与 string 一样，尾随空格在 varchar 中很重要，会影响比较结果。Char 类型与 Varchar 类似，但它们是固定长度意味着短于指定长度 value 的值用空格填充，但尾部空格在比较期间不影响比较。最大长度固定为 255。关于时间类型：Timestampstimestamp表示UTC时间，可以是以秒为单位的整数；带精度的浮点数，最大精确到小数点后9位，纳秒级；java.sql.Timestamp格式的字符串 YYYY-MM-DD hh:mm:ss.fffffffffDateHive中的Date只支持YYYY-MM-DD格式的日期，其余写法都是错误的，如需带上时分秒，请使用timestamp复杂数据类型：数据类型释义ARRAYARRAY类型是由一系列相同数据类型的元素组成，这些元素可以通过下标来访问。比如有一个ARRAY类型的变量fruits，它是由[‘apple’,’orange’,’mango’]组成，那么我们可以通过fruits[1]来访问元素orange，因为ARRAY类型的下标是从0开始的；MAPMAP包含key-&gt;value键值对，可以通过key来访问元素。比如”userlist”是一个map类型，其中username是key，password是value；那么我们可以通过userlist[‘username’]来得到这个用户对应的password；STRUCTSTRUCT可以包含不同数据类型的元素。这些元素可以通过”点语法”的方式来得到所需要的元素，比如user是一个STRUCT类型，那么可以通过user.address得到这个用户的地址。UNIONUNIONTYPE&lt;data_type, data_type, …&gt;Hive DDL操作关于库：创建库CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name[COMMENT database_comment] //关于数据块的描述[LOCATION hdfs_path] //指定数据库在HDFS上的存储位置[WITH DBPROPERTIES (property_name=property_value, …)]; //指定数据块属性eg: create database t1;** create database if not exists t1;** create database if not exists t1 comment ‘comment dor t1’;**create database if not exists t3 with dbproperties(‘creator’=’cpeixin’,’date’=’2016-04-05’);查看库show databases;** desc database extended t1;**show create database t1;删除库** **drop database dbname;切换库use dbname;关于表：创建表下面为官网给出的建表参数：1234567891011121314CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [&lt;font&gt;&lt;&#x2F;font&gt; [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path]内部表和外部表默认情况下，Hive创建内部表，其中文件，元数据和统计信息由内部Hive进程管理这里我们需要知道最根本的区别就是，在删除内部表的时候，数据也会被删除，而外部表不会。STORED as 存储格式是指定文件的类型，保存在hive中的文件的类型有多种，一般简单就保存为文本格式，即TEXTFILE，但是企业中一般不使用这种格式来保存数据，主要是因为文本格式占的空间比较大，不利于大数据分析。企业中一般使用ORC和PARQUET，AVRO三种文件类型来保存，具体的会在后面讲解。ROW FORMAT DELIMITED 行分隔符ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘ ‘这句的意思是以空格来分隔行数据，那么这一行中的数据只要遇到一个空格就可以划分为一个数据。这里的分隔符可以是其他字符，比如”,”,”#”,”|”等，一般只要用数据文件中可以区分每一行中的不同数据即可。列与列直接的分隔符通常是以换行符来区分，可以用如下的语句来指定：`COLLECTION ITEMS TERMINATED BY ‘\\n’， 通常列与列直接的分隔符是不需要写的。LOCATION hdfs_path **可以在创建表的时候指定该表映射到到hdfs的文件路径，默认是映射到/user/hive/warehouse目录下。PARTITIONED BY 分区为了对表进行合理的管理以及提高查询效率，Hive可以将表组织成“分区”。一个分区实际上就是表下的一个目录，一个表可以在多个维度上进行分区，分区之间的关系就是目录树的关系。通过PARTITIONED BY子句指定，分区的顺序决定了谁是父目录，谁是子目录。在这里分区又分为 静态分区和动态分区。简单的来说，静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断下面进行建表，建表的示例也会尽可量的使用各种数据类型进行解释说明：建表实例：1.创建普通表，不添加任何参数**123456789101112create table t_user_details(user_name string,age tinyint,phone_number string,birth_date string,deposit_amount float,promotion_amount double,register_date date,last_login_time timestamp,user_level int,vip_flag boolean);**12345678910111213141516171819202122232425hive (test)&gt; show create table t_user_details;OKcreatetab_stmtCREATE TABLE &#96;t_user_details&#96;( &#96;user_name&#96; string, &#96;age&#96; tinyint, &#96;phone_number&#96; string, &#96;birth_date&#96; string, &#96;deposit_amount&#96; float, &#96;promotion_amount&#96; double, &#96;register_date&#96; date, &#96;last_login_time&#96; timestamp, &#96;user_level&#96; int, &#96;vip_flag&#96; boolean)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39; STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;LOCATION &#39;hdfs:&#x2F;&#x2F;localhost:8020&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;t_user_details&#39;TBLPROPERTIES ( &#39;bucketing_version&#39;&#x3D;&#39;2&#39;, &#39;transient_lastDdlTime&#39;&#x3D;&#39;1586087852&#39;)1insert into t_user_details values (&#39;brent&#39;,27,&#39;13611111111&#39;,&#39;1993-03-14&#39;,100.91,4000.56,2016-04-05,&#39;2016-04-07 14:20:36.345&#39;,6,True),(&#39;haylee&#39;,25,&#39;13211111111&#39;,&#39;1994-03-14&#39;,130.91,40000000.56,&#39;2016-04-06&#39;,&#39;2016-04-08 14:20:36.345&#39;,6,False);观察以上内容，是针对没有输入任何建表参数的情况下，所生成的结果，在show create table xx的结果中，体现出了完整的建表默认参数。关于ROW FORMAT SERDE，用于指定序列化和反序列化的规则，默认值：LazySimpleSerDe简单来说，就是它希望对于Deserialization，反序列化，可以lazy一点。对于Serialization，序列化，可以simple一点在没有指定字段之间的分隔符时，默认是用\\001 不可见字符进行分割的，我们也可以在建表的时候使用FIELDS TERMINATED BY ‘,’ 参数来指定字段之间使用逗号分割。除此之外，在使用hive的时候，存储格式的选择非常重要，不同存储格式直接在最底层影响着你的执行效率，所以这部分在之后用单独的一篇文章来说。这里大家先知道企业里面常用的ORC，Parquet，Avro等格式就可以了2.创建分区表**分区表在显示工作中非常常用，例如针对网站数据的存储，网站每天产生数据量过大的话，我们不能始终在表末尾进行数据的追加，而是应该利用动态分区或者静态分区，按月，按天的粒度进行分区存储123456789101112131415161718192021222324CREATE TABLE &#96;t_user_detail_partition&#96;( user_name string, age tinyint, phone_number string, birth_date string, deposit_amount float, promotion_amount double, register_date date, last_login_time timestamp, user_level int, vip_flag boolean)PARTITIONED BY ( &#96;snapshot_date&#96; string)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39; WITH SERDEPROPERTIES ( &#39;field.delim&#39;&#x3D;&#39;\\u0001&#39;, &#39;line.delim&#39;&#x3D;&#39;\\n&#39;, &#39;serialization.format&#39;&#x3D;&#39;\\u0001&#39;) STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;;1insert into t_user_detail_partition values (&#39;brent&#39;,27,&#39;13611111111&#39;,&#39;1993-03-14&#39;,100.91,4000.56,2016-04-05,&#39;2016-04-07 14:20:36.345&#39;,6,True,&#39;2016-04-04&#39;),(&#39;haylee&#39;,25,&#39;13211111111&#39;,&#39;1994-03-14&#39;,130.91,40000000.56,&#39;2016-04-06&#39;,&#39;2016-04-08 14:20:36.345&#39;,6,False,&#39;2016-04-05&#39;);上面的建表语句使用 PARTITIONED BY 指定了 snapshot_date 为分区字段，当然你还可以再添加分区字段，hive支持多分区，理论上最多支持8级分区在插入数据的语句中，将分区字段的值顺序的写在表中字段值的后面，则可以按照分区进行插入数据。在hdfs中，t_user_detail_partition表则按照分区字段进行划分，将数据存储到不同的分区目录下。Hive中的分区是使用的表外字段，MySQL使用的是表内字段静态分区和动态分区在创建表时，语句是一样的。只是在赋值的时候有区别动态分区和静态分区的区别加载数据的方式：静态分区可以通过load命令，向不同的分区加载数据，加载数据时要指定分区的值；静态分区只能通过select加载数据，并且不需要指定分区的名字，而是根据伪列的值，动态的确定分区值确定分区值的方式：两者在创建表的时候命令完全一致，只是在确定分区值的时候不同，静态分区需要手动指定分区值，而动态分区会自动识别伪列的属性，动态生成分区值_动态分区插入_（或多分区插入）旨在通过动态确定在扫描输入表时应创建和填充哪些分区来解决此问题。这是新增功能，仅从0.6.0版本开始可用。在动态分区插入中，将评估输入列的值，以确定应将此行插入哪个分区。如果尚未创建该分区，它将自动创建该分区。使用此功能，您只需一个插入语句即可创建并填充所有必要的分区。另外，由于只有一个insert语句，因此只有一个对应的MapReduce作业。与多次插入的情况相比，这显着提高了性能并减少了Hadoop集群的工作量。以下是使用一个插入语句将数据加载到所有国家/地区分区的示例：123FROM page_view_stg pvsINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.country创建Parquet格式，snappy压缩Hive表12345678CREATE TABLE t_user_behavior ( user_id bigint, good_id bigint, good_category_id bigint, behavior_type string, timestamps timestamp)partitioned by (snapshot_date string)STORED AS PARQUET TBLPROPERTIES('parquet.compression'='SNAPPY');不要忘记加参数1234567891011121314151617val spark: SparkSession = SparkSession .builder() .master(\"local\") .appName(\"login_data_2_hive\") .config(\"spark.sql.session.timeZone\", \"UTC\") .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\") .config(\"hive.exec.dynamici.partition\",true) .config(\"hive.exec.dynamic.partition.mode\",\"nonstrict\") .enableHiveSupport() .getOrCreate()user_behavior_dataframe .write .format(\"Hive\") .mode(SaveMode.Overwrite) .partitionBy(\"snapshot_date\") .saveAsTable(\"test.t_user_behavior\") #### 浏览表和分区123456789101112131415161718SHOW TABLES;列出仓库中的现有表；其中有很多，可能比您想要浏览的更多。SHOW TABLES 'page.*';列出前缀为“ page”的表。该模式遵循Java正则表达式语法（因此句点是通配符）。SHOW PARTITIONS page_view;列出表分区。如果该表不是分区表，则会引发错误。DESCRIBE page_view;列出表的列和列类型。DESCRIBE EXTENDED page_view;列出表的列和所有其他属性。这会打印很多信息，但也不会以漂亮的格式显示。通常用于调试。DESCRIBE EXTENDED page_view PARTITION (ds='2008-08-08');列出分区的列和所有其他属性。这还会打印很多通常用于调试的信息。修改表1234567891011将现有表重命名为新名称。如果具有新名称的表已经存在，则返回错误：ALTER TABLE old_table_name RENAME TO new_table_name;重命名现有表的列。确保使用相同的列类型，并为每个现有列包括一个条目：ALTER TABLE old_table_name REPLACE COLUMNS (col1 TYPE, ...);要将列添加到现有表：ALTER TABLE tab1 ADD COLUMNS (c1 INT COMMENT 'a new int column', c2 STRING DEFAULT 'def val');请注意，架构更改（例如添加列）会保留表的旧分区的架构，以防它是分区表。所有访问这些列并在旧分区上运行的查询都隐式返回这些列的空值或指定的默认值。在以后的版本中，如果在特定分区中未找到该列，则可以使某些行为具有假设值，而不是抛出错误。删除表和分区1234567删除表相当简单。将该表放下将隐式删除该表上已建立的所有索引（这是将来的功能）。关联的命令是：DROP TABLE pv_users;删除分区。更改表以删除分区。ALTER TABLE pv_users DROP PARTITION (ds='2008-08-08') 请注意，此表或分区的任何数据都将被删除并且可能无法恢复。 ### Hive DML操作加载文件数据到表123LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)]复制代码LOCAL 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：从本地文件系统加载文件时， filepath 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；从 HDFS 加载文件时候，filepath 为文件完整的 URL 地址：如 hdfs://namenode:port/user/hive/project/ data1filepath 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；加载文件的格式必须与建表时使用 STORED AS 指定的存储格式相同。查询结果插入到表12345INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)] select_statement1 FROM from_statement;复制代码Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 immutable 属性的影响）;可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；从 Hive 1.1.0 开始，TABLE 关键字是可选的；从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列；可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：12345FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...;**动态插入分区****12345INSERT OVERWRITE TABLE tablename PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) select_statement FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) select_statement FROM from_statement;复制代码在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。使用SQL语句插入值123INSERT INTO TABLE tablename [PARTITION (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...)] VALUES ( value [, value ...] )复制代码使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；不支持支持复杂类型 (array, map, struct, union) 的插入。更新和删除数据更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。1234-- 更新UPDATE tablename SET column &#x3D; value [, column &#x3D; value ...] [WHERE expression]--删除DELETE FROM tablename [WHERE expression]查询结果写出到文件系统1234INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] SELECT ... FROM ...复制代码OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：1234567-- 定义列分隔符为&#39;\\t&#39; insert overwrite local directory &#39;.&#x2F;test-04&#39; row format delimited FIELDS TERMINATED BY &#39;\\t&#39;COLLECTION ITEMS TERMINATED BY &#39;,&#39;MAP KEYS TERMINATED BY &#39;:&#39;select * from src;","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"Hive 初识","slug":"Hive-初识","date":"2016-10-15T14:11:24.000Z","updated":"2020-04-05T14:32:53.687Z","comments":true,"path":"2016/10/15/Hive-初识/","link":"","permalink":"cpeixin.cn/2016/10/15/Hive-%E5%88%9D%E8%AF%86/","excerpt":"","text":"What - Hivehive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。Why - HiveHive最初是Facebook为了满足对海量社交网络数据的管理和机器学习的需求而产生和发展的。大数据是现在互联网的趋势，而hadoop就是大数据时代里的核心技术，但是hadoop的mapreduce操作专业性太强，所以facebook在这些基础上开发了hive框架，业务人员在不学习编程语言的情况下，只要学会基本的SQL语句，就可以对大数据平台的数据进行分析。How - Hive在hadoop集群中，安装配置好hive，就可以在命令行中直接输入hive，进入hive客户端接下来的操作就和操作数据库的SQL几乎一样。具体操作将在后面的文章里进行介绍。Hive - 优缺点优点：简单容易上手：提供了类SQL查询语言HQL可扩展：为超大数据集设计了计算/扩展能力（MR作为计算引擎，HDFS作为存储系统，Yarn作为资源调度）提供统一的元数据管理延展性：Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数容错：良好的容错性，节点出现问题SQL仍可完成执行支持用户自定义函数缺点：hive的HQL表达能力有限hive的效率比较低（后面可用spark计算框架代替Hive分析）hive调优比较困难，粒度较粗Hive - 架构","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"}]},{"title":"HDFS-三思","slug":"HDFS-三思","date":"2016-10-06T07:30:21.000Z","updated":"2020-04-04T17:25:03.805Z","comments":true,"path":"2016/10/06/HDFS-三思/","link":"","permalink":"cpeixin.cn/2016/10/06/HDFS-%E4%B8%89%E6%80%9D/","excerpt":"","text":"读写流程写流程具体过程如下：Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象；通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入；调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。**读流程**![p2.gif](https://cdn.nlark.com/yuque/0/2020/gif/1072113/1586020955556-6aa55757-4f0d-4d46-91bb-90e9963517fd.gif#align=left&display=inline&height=541&name=p2.gif&originHeight=541&originWidth=960&size=1319443&status=done&style=none&width=960)具体过程：Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；NameNode 返回存储的每个块的 DataNode 列表；Client 将连接到列表中最近的 DataNode；Client 开始从 DataNode 并行读取数据；一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。写流程中备份三，其中一个写失败了怎么办？只要成功写入的节点数量达到dfs.replication.min(默认为1)，那么就任务是写成功的。然后NameNode会通过异步的方式将block复制到其他节点，使数据副本达到dfs.replication参数配置的个数HDFS HA 启动流程①开启zookeeper服务1zkServer.sh start②开启`journalNode`守护进程（在`journal`协议指定的节点上执行）[ˈdʒɜːnl]1hadoop-daemon.sh start journalnode③开启namenode守护进程（在nn1和nn2执行）1hadoop-daemon.sh start namenode④开启datanode守护进程123hadoop-daemons.sh start datanode（在namenode节点上执行开启全部datanode）⑤开启zkfc守护进程1hadoop-daemon.sh --script $HADOOP_PREFIX&#x2F;bin&#x2F;hdfs start zkfcHDFS 存储类型HDFS支持如下4种存储类型：DISK：表示普通磁盘(机械磁盘)SSD：表示固态硬盘RAM_DISK：表示内存硬盘，参考虚拟内存盘，说白了就是内存ARCHIVE：这个并不是特指某种存储介质，而是为了满足高密度存储而定义的一种存储类型，一般对于归档的、访问不怎么频繁的数据可以以 ARCHIVE 的形式存储。以上四种的存储类型的存取的速度大小为：RAM_DISK-&gt;SSD-&gt;DISK-&gt;ARCHIVE。但是单bit存储成本由高到低那么我们在配置DataNode的存储路径的时候，我们可以分别为上面四种存储类型配置存储位置，如下图：12345&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;[RAM_DISK]file:///ram_disk,[SSD]file:///ssd1/dn,[DISK]file:///disk1/dn,[ARCHIVE]file:///archive1/dn&lt;/value&gt; &lt;description&gt;DataNode存放数据的地方&lt;/description&gt;&lt;/property&gt;上面配置的DataNode的多个存储位置由逗号隔开，每一个存储位置由存储类型和存储物理路径组成。HDFS通过该配置感知底层存储的位置和类型HDFS是否有异步访问模式？在现有HDFS的RPC调用方式上,采用的基本是blocking call的形式,也就是阻塞式的调用方式.阻塞方式的一个明显的缺点是它的请求过程是同步的,也就是说,客户端必须等待当前请求结果的返回,才能接着发送下一次请求.如果此客户端打算在一个线程中发送大量请求的话,阻塞式的RPC调用将会非常耗时.但是如果为了每一次请求调用而专门单独开一个线程的话,系统资源将会被大幅度的使用,显然这也不是一个好的解决的办法.那么有没有什么好的办法呢,在HDFS中是否存在有异步模式的RPC请求接口呢本文我们就来聊聊HDFS的异步访问模式.HDFS异步访问模式老实说,在目前Hadoop的发布版本中,确实还不存在HDFS异步访问的模式,但是这并不代表社区没有在关注这方面的问题.在许多特殊的场景下,HDFS的异步访问模式还是有它独到的用处的.社区在JIRA HDFS-9924([umbrella] Nonblocking HDFS Access)上对此功能特性进行了实现.在本文中,我们姑且取名”HDFS异步访问模式”为AsyncDistributedFileSystem,与DistributedFileSystem相对应.HDFS异步访问模式原理在HDFS异步访问模式的设计文档中,给出了新的异步的RPC调用模式,采用了Future-Get的异步调用模式,以FileSystem的rename方法客户端异步请求的控制在前面HDFS异步访问模式的过程中,有一点必须格外引起注意:客户端异步请求的控制.客户端应有异步请求数的限制,以此防止客户端利用大量的异步请求冲垮服务端.如果超过了此限制阈值,客户端的请求将会处于阻塞状态.这点必须要引起足够重视,否则客户端随随便便发起的请求将会摧毁NameNode.HDFS异步访问模式的优化点第一, 保证异步请求的有序性.在某些场景下,我们需要保证异步请求能够按照请求发起的时间,顺序执行.第二, 客户端对HDFS读写异步请求的支持.总结HDFS Async调用模式的出现将会带给用户更灵活的RPC请求方式的选择,但是可能考虑到此种方式对比之前的方式而言,改动较大,目前这些异步调用相关的方法许多是打上了@Unstable标记的.HDFS异步调用的方式同样可以很好的运用在单元测试上.鉴于此特性是还暂未发布,大家可以根据自己的需要,进行部分的合入.调整数据块大小会有什么影响？Hadoop 1.x 中， 默认的数据块大小是 64MHadoop 2.x 中， 默认的数据块大小是 128M在HDFS中，数据块不宜设置的过大，也不适宜设置的过小。主要是从 减少寻址时间和MR任务并行方面去考虑为什么HDFS中块（block）不能设置太大，也不能设置太小？如果块设置过大，一方面，从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢；另一方面，mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。2. 如果块设置过小，一方面存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；另一方面文件块过小，寻址时间增大，导致程序一直在找block的开始位置。因而，块适当设置大一些，减少寻址时间，那么传输一个由多个块组成的文件的时间主要取决于磁盘的传输速率。HDFS中块（block）的大小为什么设置为128M？HDFS中平均寻址时间大概为10ms；2. 经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态；所以最佳传输时间为10ms/0.01=1000ms=1s3. 目前磁盘的传输速率普遍为100MB/s；计算出最佳block大小：100MB/s x 1s = 100MB所以我们设定block大小为128MB。实际在工业生产中，磁盘传输速率为200MB/s时，一般设定block大小为256MB，磁盘传输速率为400MB/s时，一般设定block大小为512MB以后随着新一代磁盘驱动器传输速率的提升，块的大小将被设置得更大","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"Hadoop 2.x - HDFS","slug":"Hadoop-2-x-HDFS","date":"2016-10-06T03:33:21.000Z","updated":"2020-04-04T17:26:26.578Z","comments":true,"path":"2016/10/06/Hadoop-2-x-HDFS/","link":"","permalink":"cpeixin.cn/2016/10/06/Hadoop-2-x-HDFS/","excerpt":"","text":"在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。NameNode 高可用整体架构概述在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。HDFS NameNode 和 YARN ResourceManger 的高可用 (High Availability，HA) 方案基本类似，两者也复用了部分代码，但是由于 HDFS NameNode 对于数据存储和数据一致性的要求比 YARN ResourceManger 高得多，所以 HDFS NameNode 的高可用实现更为复杂一些，本文从内部实现的角度对 HDFS NameNode 的高可用机制进行详细的分析。HDFS NameNode 的高可用整体架构如图 1 所示图 1.HDFS NameNode 高可用整体架构从上图中，我们可以看出 NameNode 的高可用架构主要分为下面几个部分：Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。Zookeeper 集群：为主备切换控制器提供主备选举支持。共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。下面开始分别介绍 NameNode 的主备切换实现和共享存储系统的实现，在文章的最后会结合笔者的实践介绍一下在 NameNode 的高可用运维中的一些注意事项。NameNode 的主备切换实现NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现：ZKFailoverController 作为 NameNode 机器上一个独立的进程启动 (在 hdfs 启动脚本之中的进程名为 zkfc)，启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，也会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调方法。HealthMonitor 主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举。ActiveStandbyElector 主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。NameNode 实现主备切换的流程如图 2 所示，有以下几步：HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。图 2.NameNode 的主备切换流程下面分别对 HealthMonitor、ActiveStandbyElector 和 ZKFailoverController 的实现细节进行分析：HealthMonitor 实现分析ZKFailoverController 在初始化的时候会创建 HealthMonitor，HealthMonitor 在内部会启动一个线程来循环调用 NameNode 的 HAServiceProtocol RPC 接口的方法来检测 NameNode 的状态，并将状态的变化通过回调的方式来通知 ZKFailoverController。HealthMonitor 主要检测 NameNode 的两类状态，分别是 HealthMonitor.State 和 HAServiceStatus。HealthMonitor.State 是通过 HAServiceProtocol RPC 接口的 monitorHealth 方法来获取的，反映了 NameNode 节点的健康状况，主要是磁盘存储资源是否充足。HealthMonitor.State 包括下面几种状态：INITIALIZING：HealthMonitor 在初始化过程中，还没有开始进行健康状况检测；SERVICE_HEALTHY：NameNode 状态正常；SERVICE_NOT_RESPONDING：调用 NameNode 的 monitorHealth 方法调用无响应或响应超时；SERVICE_UNHEALTHY：NameNode 还在运行，但是 monitorHealth 方法返回状态不正常，磁盘存储资源不足；HEALTH_MONITOR_FAILED：HealthMonitor 自己在运行过程中发生了异常，不能继续检测 NameNode 的健康状况，会导致 ZKFailoverController 进程退出；HealthMonitor.State 在状态检测之中起主要的作用，在 HealthMonitor.State 发生变化的时候，HealthMonitor 会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。而 HAServiceStatus 则是通过 HAServiceProtocol RPC 接口的 getServiceStatus 方法来获取的，主要反映的是 NameNode 的 HA 状态，包括：INITIALIZING：NameNode 在初始化过程中；ACTIVE：当前 NameNode 为主 NameNode；STANDBY：当前 NameNode 为备 NameNode；STOPPING：当前 NameNode 已停止；HAServiceStatus 在状态检测之中只是起辅助的作用，在 HAServiceStatus 发生变化时，HealthMonitor 也会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。ActiveStandbyElector 实现分析Namenode(包括 YARN ResourceManager) 的主备选举是通过 ActiveStandbyElector 来完成的，ActiveStandbyElector 主要是利用了 Zookeeper 的写一致性和临时节点机制，具体的主备选举实现如下：创建锁节点如果 HealthMonitor 检测到对应的 NameNode 的状态正常，那么表示这个 NameNode 有资格参加 Zookeeper 的主备选举。如果目前还没有进行过主备选举的话，那么相应的 ActiveStandbyElector 就会发起一次主备选举，尝试在 Zookeeper 上创建一个路径为/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 的临时节点 (${dfs.nameservices} 为 Hadoop 的配置参数 dfs.nameservices 的值，下同)，Zookeeper 的写一致性会保证最终只会有一个 ActiveStandbyElector 创建成功，那么创建成功的 ActiveStandbyElector 对应的 NameNode 就会成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Active 状态。而创建失败的 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Standby 状态。注册 Watcher 监听不管创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点是否成功，ActiveStandbyElector 随后都会向 Zookeeper 注册一个 Watcher 来监听这个节点的状态变化事件，ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件。自动触发主备选举如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。防止脑裂Zookeeper 在工程实践的过程中经常会发生的一个现象就是 Zookeeper 客户端“假死”，所谓的“假死”是指如果 Zookeeper 客户端机器负载过高或者正在进行 JVM Full GC，那么可能会导致 Zookeeper 客户端到 Zookeeper 服务端的心跳不能正常发出，一旦这个时间持续较长，超过了配置的 Zookeeper Session Timeout 参数的话，Zookeeper 服务端就会认为客户端的 session 已经过期从而将客户端的 Session 关闭。“假死”有可能引起分布式系统常说的双主或脑裂 (brain-split) 现象。具体到本文所述的 NameNode，假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，即使随后 NameNode1 对应的 ZKFailoverController 因为负载下降或者 Full GC 结束而恢复了正常，感知到自己和 Zookeeper 的 Session 已经关闭，但是由于网络的延迟以及 CPU 线程调度的不确定性，仍然有可能会在接下来的一段时间窗口内 NameNode1 认为自己还是处于 Active 状态。这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况对于 NameNode 这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。Zookeeper 社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息。Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候 (注意由于/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 是临时节点，也会随之删除)，会一起删除节点/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb。但是如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来。后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing，具体处理见后文 ZKFailoverController 部分所述。ZKFailoverController 实现分析ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调函数，ZKFailoverController 的处理逻辑主要靠 HealthMonitor 和 ActiveStandbyElector 的回调函数来驱动。对 HealthMonitor 状态变化的处理如前所述，HealthMonitor 会检测 NameNode 的两类状态，HealthMonitor.State 在状态检测之中起主要的作用，ZKFailoverController 注册到 HealthMonitor 上的处理 HealthMonitor.State 状态变化的回调函数主要关注 SERVICE_HEALTHY、SERVICE_NOT_RESPONDING 和 SERVICE_UNHEALTHY 这 3 种状态：如果检测到状态为 SERVICE_HEALTHY，表示当前的 NameNode 有资格参加 Zookeeper 的主备选举，如果目前还没有进行过主备选举的话，ZKFailoverController 会调用 ActiveStandbyElector 的 joinElection 方法发起一次主备选举。如果检测到状态为 SERVICE_NOT_RESPONDING 或者是 SERVICE_UNHEALTHY，就表示当前的 NameNode 出现问题了，ZKFailoverController 会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举，这样其它的 NameNode 就有机会成为主 NameNode。而 HAServiceStatus 在状态检测之中仅起辅助的作用，在 HAServiceStatus 发生变化时，ZKFailoverController 注册到 HealthMonitor 上的处理 HAServiceStatus 状态变化的回调函数会判断 NameNode 返回的 HAServiceStatus 和 ZKFailoverController 所期望的是否一致，如果不一致的话，ZKFailoverController 也会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举。对 ActiveStandbyElector 主备选举状态变化的处理在 ActiveStandbyElector 的主备选举状态发生变化时，会回调 ZKFailoverController 注册的回调函数来进行相应的处理：如果 ActiveStandbyElector 选主成功，那么 ActiveStandbyElector 对应的 NameNode 成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeActive 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToActive 方法，将 NameNode 转换为 Active 状态。如果 ActiveStandbyElector 选主失败，那么 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeStandby 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，将 NameNode 转换为 Standby 状态。如果 ActiveStandbyElector 选主成功之后，发现了上一个 Active NameNode 遗留下来的/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 节点 (见“ActiveStandbyElector 实现分析”一节“防止脑裂”部分所述)，那么 ActiveStandbyElector 会首先回调 ZKFailoverController 注册的 fenceOldActive 方法，尝试对旧的 Active NameNode 进行 fencing，在进行 fencing 的时候，会执行以下的操作：首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离；只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。NameNode 的共享存储实现过去几年中 Hadoop 社区涌现过很多的 NameNode 共享存储方案，比如 shared NAS+NFS、BookKeeper、BackupNode 和 QJM(Quorum Journal Manager) 等等。目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。为了理解 QJM 的设计和实现，首先要对 NameNode 的元数据存储结构有所了解。NameNode 的元数据存储概述一个典型的 NameNode 的元数据存储目录结构如图 3 所示 (图片来源于参考文献 [4])，这里主要关注其中的 EditLog 文件和 FSImage 文件：图 3 .NameNode 的元数据存储目录结构NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog 仅仅只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment。正在写入的 EditLog Segment 处于 in-progress 状态，其文件名形如 edits_inprogress_${start_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，例如上图中的 edits_inprogress_0000000000000000020。而已经写入完成的 EditLog Segment 处于 finalized 状态，其文件名形如 edits_${start_txid}-${end_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，${end_txid} 表示这个 segment 的结束事务 id，例如上图中的 edits_0000000000000000001-0000000000000000019。NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，FSImage 文件的文件名形如 fsimage_${end_txid}，其中${end_txid} 表示这个 fsimage 文件的结束事务 id，例如上图中的 fsimage_0000000000000000020。在 NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。基于 QJM 的共享存储系统的总体架构基于 QJM 的共享存储系统主要用于保存 EditLog，并不保存 FSImage 文件。FSImage 文件还是在 NameNode 的本地磁盘上。QJM 共享存储的基本思想来自于 Paxos 算法 (参见参考文献 [3])，采用多个称为 JournalNode 的节点组成的 JournalNode 集群来存储 EditLog。每个 JournalNode 保存同样的 EditLog 副本。每次 NameNode 写 EditLog 的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向 JournalNode 集群之中的每一个 JournalNode 发送写请求，只要大多数 (majority) 的 JournalNode 节点返回成功就认为向 JournalNode 集群写入 EditLog 成功。如果有 2N+1 台 JournalNode，那么根据大多数的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。基于 QJM 的共享存储系统的内部实现架构图如图 4 所示，主要包含下面几个主要的组件：图 4 . 基于 QJM 的共享存储系统的内部实现架构图FSEditLog：这个类封装了对 EditLog 的所有操作，是 NameNode 对 EditLog 的所有操作的入口。JournalSet： 这个类封装了对本地磁盘和 JournalNode 集群上的 EditLog 的操作，内部包含了两类 JournalManager，一类为 FileJournalManager，用于实现对本地磁盘上 EditLog 的操作。一类为 QuorumJournalManager，用于实现对 JournalNode 集群上共享目录的 EditLog 的操作。FSEditLog 只会调用 JournalSet 的相关方法，而不会直接使用 FileJournalManager 和 QuorumJournalManager。FileJournalManager：封装了对本地磁盘上的 EditLog 文件的操作，不仅 NameNode 在向本地磁盘上写入 EditLog 的时候使用 FileJournalManager，JournalNode 在向本地磁盘写入 EditLog 的时候也复用了 FileJournalManager 的代码和逻辑。QuorumJournalManager：封装了对 JournalNode 集群上的 EditLog 的操作，它会根据 JournalNode 集群的 URI 创建负责与 JournalNode 集群通信的类 AsyncLoggerSet， QuorumJournalManager 通过 AsyncLoggerSet 来实现对 JournalNode 集群上的 EditLog 的写操作，对于读操作，QuorumJournalManager 则是通过 Http 接口从 JournalNode 上的 JournalNodeHttpServer 读取 EditLog 的数据。AsyncLoggerSet：内部包含了与 JournalNode 集群进行通信的 AsyncLogger 列表，每一个 AsyncLogger 对应于一个 JournalNode 节点，另外 AsyncLoggerSet 也包含了用于等待大多数 JournalNode 返回结果的工具类方法给 QuorumJournalManager 使用。AsyncLogger：具体的实现类是 IPCLoggerChannel，IPCLoggerChannel 在执行方法调用的时候，会把调用提交到一个单线程的线程池之中，由线程池线程来负责向对应的 JournalNode 的 JournalNodeRpcServer 发送 RPC 请求。JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。下面对基于 QJM 的共享存储系统的两个关键性问题同步数据和恢复数据进行详细分析。基于 QJM 的共享存储系统的数据同步机制分析Active NameNode 和 StandbyNameNode 使用 JouranlNode 集群来进行数据同步的过程如图 5 所示，Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog：图 5 . 基于 QJM 的共享存储的数据同步机制Active NameNode 提交 EditLog 到 JournalNode 集群当处于 Active 状态的 NameNode 调用 FSEditLog 类的 logSync 方法来提交 EditLog 的时候，会通过 JouranlSet 同时向本地磁盘目录和 JournalNode 集群上的共享存储目录写入 EditLog。写入 JournalNode 集群是通过并行调用每一个 JournalNode 的 QJournalProtocol RPC 接口的 journal 方法实现的，如果对大多数 JournalNode 的 journal 方法调用成功，那么就认为提交 EditLog 成功，否则 NameNode 就会认为这次提交 EditLog 失败。提交 EditLog 失败会导致 Active NameNode 关闭 JournalSet 之后退出进程，留待处于 Standby 状态的 NameNode 接管之后进行数据恢复。从上面的叙述可以看出，Active NameNode 提交 EditLog 到 JournalNode 集群的过程实际上是同步阻塞的，但是并不需要所有的 JournalNode 都调用成功，只要大多数 JournalNode 调用成功就可以了。如果无法形成大多数，那么就认为提交 EditLog 失败，NameNode 停止服务退出进程。如果对应到分布式系统的 CAP 理论的话，虽然采用了 Paxos 的“大多数”思想对 C(consistency，一致性) 和 A(availability，可用性) 进行了折衷，但还是可以认为 NameNode 选择了 C 而放弃了 A，这也符合 NameNode 对数据一致性的要求。Standby NameNode 从 JournalNode 集群同步 EditLog当 NameNode 进入 Standby 状态之后，会启动一个 EditLogTailer 线程。这个线程会定期调用 EditLogTailer 类的 doTailEdits 方法从 JournalNode 集群上同步 EditLog，然后把同步的 EditLog 回放到内存之中的文件系统镜像上 (并不会同时把 EditLog 写入到本地磁盘上)。这里需要关注的是：从 JournalNode 集群上同步的 EditLog 都是处于 finalized 状态的 EditLog Segment。“NameNode 的元数据存储概述”一节说过 EditLog Segment 实际上有两种状态，处于 in-progress 状态的 Edit Log 当前正在被写入，被认为是处于不稳定的中间态，有可能会在后续的过程之中发生修改，比如被截断。Active NameNode 在完成一个 EditLog Segment 的写入之后，就会向 JournalNode 集群发送 finalizeLogSegment RPC 请求，将完成写入的 EditLog Segment finalized，然后开始下一个新的 EditLog Segment。一旦 finalizeLogSegment 方法在大多数的 JournalNode 上调用成功，表明这个 EditLog Segment 已经在大多数的 JournalNode 上达成一致。一个 EditLog Segment 处于 finalized 状态之后，可以保证它再也不会变化。从上面描述的过程可以看出，虽然 Active NameNode 向 JournalNode 集群提交 EditLog 是同步的，但 Standby NameNode 采用的是定时从 JournalNode 集群上同步 EditLog 的方式，那么 Standby NameNode 内存中文件系统镜像有很大的可能是落后于 Active NameNode 的，所以 Standby NameNode 在转换为 Active NameNode 的时候需要把落后的 EditLog 补上来。基于 QJM 的共享存储系统的数据恢复机制分析处于 Standby 状态的 NameNode 转换为 Active 状态的时候，有可能上一个 Active NameNode 发生了异常退出，那么 JournalNode 集群中各个 JournalNode 上的 EditLog 就可能会处于不一致的状态，所以首先要做的事情就是让 JournalNode 集群中各个节点上的 EditLog 恢复为一致。另外如前所述，当前处于 Standby 状态的 NameNode 的内存中的文件系统镜像有很大的可能是落后于旧的 Active NameNode 的，所以在 JournalNode 集群中各个节点上的 EditLog 达成一致之后，接下来要做的事情就是从 JournalNode 集群上补齐落后的 EditLog。只有在这两步完成之后，当前新的 Active NameNode 才能安全地对外提供服务。补齐落后的 EditLog 的过程复用了前面描述的 Standby NameNode 从 JournalNode 集群同步 EditLog 的逻辑和代码，最终调用 EditLogTailer 类的 doTailEdits 方法来完成 EditLog 的补齐。使 JournalNode 集群上的 EditLog 达成一致的过程是一致性算法 Paxos 的典型应用场景，QJM 对这部分的处理可以看做是 Single Instance Paxos(参见参考文献 [3]) 算法的一个实现，在达成一致的过程中，Active NameNode 和 JournalNode 集群之间的交互流程如图 6 所示，具体描述如下：图 6.Active NameNode 和 JournalNode 集群的交互流程图生成一个新的 EpochEpoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制，为什么需要 fencing 已经在前面“ActiveStandbyElector 实现分析”一节的“防止脑裂”部分进行了说明。产生新 Epoch 的流程与 Zookeeper 的 ZAB(Zookeeper Atomic Broadcast) 协议在进行数据恢复之前产生新 Epoch 的过程完全类似：Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功。在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。选择需要数据恢复的 EditLog Segment 的 id需要恢复的 Edit Log 只可能是各个 JournalNode 上的最后一个 Edit Log Segment，如前所述，JournalNode 在处理完 newEpoch RPC 请求之后，会向 NameNode 返回它自己的本地磁盘上最新的一个 EditLog Segment 的起始事务 id，这个起始事务 id 实际上也作为这个 EditLog Segment 的 id。NameNode 会在所有这些 id 之中选择一个最大的 id 作为要进行数据恢复的 EditLog Segment 的 id。向 JournalNode 集群发送 prepareRecovery RPC 请求NameNode 接下来向 JournalNode 集群发送 prepareRecovery RPC 请求，请求的参数就是选出的 EditLog Segment 的 id。JournalNode 收到请求后返回本地磁盘上这个 Segment 的起始事务 id、结束事务 id 和状态 (in-progress 或 finalized)。这一步对应于 Paxos 算法的 Phase 1a 和 Phase 1b(参见参考文献 [3]) 两步。Paxos 算法的 Phase1 是 prepare 阶段，这也与方法名 prepareRecovery 相对应。并且这里以前面产生的新的 Epoch 作为 Paxos 算法中的提案编号 (proposal number)。只要大多数的 JournalNode 的 prepareRecovery RPC 调用成功返回，NameNode 就认为成功。选择进行同步的基准数据源，向 JournalNode 集群发送 acceptRecovery RPC 请求 NameNode 根据 prepareRecovery 的返回结果，选择一个 JournalNode 上的 EditLog Segment 作为同步的基准数据源。选择基准数据源的原则大致是：在 in-progress 状态和 finalized 状态的 Segment 之间优先选择 finalized 状态的 Segment。如果都是 in-progress 状态的话，那么优先选择 Epoch 比较高的 Segment(也就是优先选择更新的)，如果 Epoch 也一样，那么优先选择包含的事务数更多的 Segment。在选定了同步的基准数据源之后，NameNode 向 JournalNode 集群发送 acceptRecovery RPC 请求，将选定的基准数据源作为参数。JournalNode 接收到 acceptRecovery RPC 请求之后，从基准数据源 JournalNode 的 JournalNodeHttpServer 上下载 EditLog Segment，将本地的 EditLog Segment 替换为下载的 EditLog Segment。这一步对应于 Paxos 算法的 Phase 2a 和 Phase 2b(参见参考文献 [3]) 两步。Paxos 算法的 Phase2 是 accept 阶段，这也与方法名 acceptRecovery 相对应。只要大多数 JournalNode 的 acceptRecovery RPC 调用成功返回，NameNode 就认为成功。向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成上一步执行完成之后，NameNode 确认大多数 JournalNode 上的 EditLog Segment 已经从基准数据源进行了同步。接下来，NameNode 向 JournalNode 集群发送 finalizeLogSegment RPC 请求，JournalNode 接收到请求之后，将对应的 EditLog Segment 从 in-progress 状态转换为 finalized 状态，实际上就是将文件名从 edits_inprogress_${startTxid} 重命名为 edits_${startTxid}-${endTxid}，见“NameNode 的元数据存储概述”一节的描述。只要大多数 JournalNode 的 finalizeLogSegment RPC 调用成功返回，NameNode 就认为成功。此时可以保证 JournalNode 集群的大多数节点上的 EditLog 已经处于一致的状态，这样 NameNode 才能安全地从 JournalNode 集群上补齐落后的 EditLog 数据。需要注意的是，尽管基于 QJM 的共享存储方案看起来理论完备，设计精巧，但是仍然无法保证数据的绝对强一致，下面选取参考文献 [2] 中的一个例子来说明：假设有 3 个 JournalNode：JN1、JN2 和 JN3，Active NameNode 发送了事务 id 为 151、152 和 153 的 3 个事务到 JournalNode 集群，这 3 个事务成功地写入了 JN2，但是在还没能写入 JN1 和 JN3 之前，Active NameNode 就宕机了。同时，JN3 在整个写入的过程中延迟较大，落后于 JN1 和 JN2。最终成功写入 JN1 的事务 id 为 150，成功写入 JN2 的事务 id 为 153，而写入到 JN3 的事务 id 仅为 125，如图 7 所示 (图片来源于参考文献 [2])。按照前面描述的只有成功地写入了大多数的 JournalNode 才认为写入成功的原则，显然事务 id 为 151、152 和 153 的这 3 个事务只能算作写入失败。在进行数据恢复的过程中，会发生下面两种情况：图 7.JournalNode 集群写入的事务 id 情况如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段收到了 JN2 的回复，那么肯定会以 JN2 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 153。从恢复的结果来看，实际上可以认为前面宕机的 Active NameNode 对事务 id 为 151、152 和 153 的这 3 个事务的写入成功了。但是如果从 NameNode 自身的角度来看，这显然就发生了数据不一致的情况。如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段没有收到 JN2 的回复，那么肯定会以 JN1 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 150。在这种情况下，如果从 NameNode 自身的角度来看的话，数据就是一致的了。事实上不光本文描述的基于 QJM 的共享存储方案无法保证数据的绝对一致，大家通常认为的一致性程度非常高的 Zookeeper 也会发生类似的情况，这也从侧面说明了要实现一个数据绝对一致的分布式存储系统的确非常困难。NameNode 在进行状态转换时对共享存储的处理下面对 NameNode 在进行状态转换的过程中对共享存储的处理进行描述，使得大家对基于 QJM 的共享存储方案有一个完整的了解，同时也作为本部分的总结。NameNode 初始化启动，进入 Standby 状态在 NameNode 以 HA 模式启动的时候，NameNode 会认为自己处于 Standby 模式，在 NameNode 的构造函数中会加载 FSImage 文件和 EditLog Segment 文件来恢复自己的内存文件系统镜像。在加载 EditLog Segment 的时候，调用 FSEditLog 类的 initSharedJournalsForRead 方法来创建只包含了在 JournalNode 集群上的共享目录的 JournalSet，也就是说，这个时候只会从 JournalNode 集群之中加载 EditLog，而不会加载本地磁盘上的 EditLog。另外值得注意的是，加载的 EditLog Segment 只是处于 finalized 状态的 EditLog Segment，而处于 in-progress 状态的 Segment 需要后续在切换为 Active 状态的时候，进行一次数据恢复过程，将 in-progress 状态的 Segment 转换为 finalized 状态的 Segment 之后再进行读取。加载完 FSImage 文件和共享目录上的 EditLog Segment 文件之后，NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式。如前所述，EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog。而 StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。NameNode 从 Standby 状态切换为 Active 状态当 NameNode 从 Standby 状态切换为 Active 状态的时候，首先需要做的就是停止它在 Standby 状态的时候启动的线程和相关的服务，包括上面提到的 EditLogTailer 线程和 StandbyCheckpointer 线程，然后关闭用于读取 JournalNode 集群的共享目录上的 EditLog 的 JournalSet，接下来会调用 FSEditLog 的 initJournalSetForWrite 方法重新打开 JournalSet。不同的是，这个 JournalSet 内部同时包含了本地磁盘目录和 JournalNode 集群上的共享目录。这些工作完成之后，就开始执行“基于 QJM 的共享存储系统的数据恢复机制分析”一节所描述的流程，调用 FSEditLog 类的 recoverUnclosedStreams 方法让 JournalNode 集群中各个节点上的 EditLog 达成一致。然后调用 EditLogTailer 类的 catchupDuringFailover 方法从 JournalNode 集群上补齐落后的 EditLog。最后打开一个新的 EditLog Segment 用于新写入数据，同时启动 Active NameNode 所需要的线程和服务。NameNode 从 Active 状态切换为 Standby 状态当 NameNode 从 Active 状态切换为 Standby 状态的时候，首先需要做的就是停止它在 Active 状态的时候启动的线程和服务，然后关闭用于读取本地磁盘目录和 JournalNode 集群上的共享目录的 EditLog 的 JournalSet。接下来会调用 FSEditLog 的 initSharedJournalsForRead 方法重新打开用于读取 JournalNode 集群上的共享目录的 JournalSet。这些工作完成之后，就会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，EditLogTailer 线程会定时从 JournalNode 集群上同步 Edit Log。NameNode 高可用运维中的注意事项本节结合笔者的实践，从初始化部署和日常运维两个方面介绍一些在 NameNode 高可用运维中的注意事项。初始化部署如果在开始部署 Hadoop 集群的时候就启用 NameNode 的高可用的话，那么相对会比较容易。但是如果在采用传统的单 NameNode 的架构运行了一段时间之后，升级为 NameNode 的高可用架构的话，就要特别注意在升级的时候需要按照以下的步骤进行操作：对 Zookeeper 进行初始化，创建 Zookeeper 上的/hadoop-ha/${dfs.nameservices} 节点。创建节点是为随后通过 Zookeeper 进行主备选举做好准备，在进行主备选举的时候会在这个节点下面创建子节点 (具体可参照“ActiveStandbyElector 实现分析”一节的叙述)。这一步通过在原有的 NameNode 上执行命令 hdfs zkfc -formatZK 来完成。启动所有的 JournalNode，这通过脚本命令 hadoop-daemon.sh start journalnode 来完成。对 JouranlNode 集群的共享存储目录进行格式化，并且将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件 (具体可参照“NameNode 的元数据存储概述”一节的叙述) 之后的 EditLog 拷贝到 JournalNode 集群上的共享目录之中，这通过在原有的 NameNode 上执行命令 hdfs namenode -initializeSharedEdits 来完成。启动原有的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。对新增的 NameNode 节点进行初始化，将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件拷贝到这个新增的 NameNode 的本地磁盘上，同时需要验证 JournalNode 集群的共享存储目录上已经具有了这个 FSImage 文件之后的 EditLog(已经在第 3 步完成了)。这一步通过在新增的 NameNode 上执行命令 hdfs namenode -bootstrapStandby 来完成。启动新增的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。在这两个 NameNode 上启动 zkfc(ZKFailoverController) 进程，谁通过 Zookeeper 选主成功，谁就是主 NameNode，另一个为备 NameNode。这通过脚本命令 hadoop-daemon.sh start zkfc 完成。日常维护笔者在日常的维护之中主要遇到过下面两种问题：Zookeeper 过于敏感：Hadoop 的配置项中 Zookeeper 的 session timeout 的配置参数 ha.zookeeper.session-timeout.ms 的默认值为 5000，也就是 5s，这个值比较小，会导致 Zookeeper 比较敏感，可以把这个值尽量设置得大一些，避免因为网络抖动等原因引起 NameNode 进行无谓的主备切换。单台 JouranlNode 故障时会导致主备无法切换：在理论上，如果有 3 台或者更多的 JournalNode，那么挂掉一台 JouranlNode 应该仍然可以进行正常的主备切换。但是笔者在某次 NameNode 重启的时候，正好赶上一台 JournalNode 挂掉宕机了，这个时候虽然某一台 NameNode 通过 Zookeeper 选主成功，但是这台被选为主的 NameNode 无法成功地从 Standby 状态切换为 Active 状态。事后追查原因发现，被选为主的 NameNode 卡在退出 Standby 状态的最后一步，这个时候它需要等待到 JournalNode 的请求全部完成之后才能退出。但是由于有一台 JouranlNode 宕机，到这台 JournalNode 的请求都积压在一起并且在不断地进行重试，同时在 Hadoop 的配置项中重试次数的默认值非常大，所以就会导致被选为主的 NameNode 无法及时退出 Standby 状态。这个问题主要是 Hadoop 内部的 RPC 通信框架的设计缺陷引起的，Hadoop HA 的源代码 IPCLoggerChannel 类中有关于这个问题的 TODO，但是截止到社区发布的 2.7.1 版本这个问题仍然存在。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"算法 - 排序总结","slug":"算法-排序总结","date":"2016-10-05T07:56:14.000Z","updated":"2020-05-22T08:00:46.964Z","comments":true,"path":"2016/10/05/算法-排序总结/","link":"","permalink":"cpeixin.cn/2016/10/05/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E6%80%BB%E7%BB%93/","excerpt":"","text":"如何选择合适的排序算法？如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法？我们先回顾一下前面讲过的几种排序算法。我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。如果对小规模数据进行排序，可以选择时间复杂度是 O(n2) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。时间复杂度是 O(nlogn) 的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。不知道你有没有发现，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是 O(n2)，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是 O(nlogn)，从这点上看起来很诱人，那为什么它还是没能得到“宠信”呢？还记得我们上一节讲的归并排序的空间复杂度吗？归并排序并不是原地排序算法，空间复杂度是 O(n)。所以，粗略点、夸张点讲，如果要排序 100MB 的数据，除了数据本身占用的内存之外，排序算法还要额外再占用 100MB 的内存空间，空间耗费就翻倍了。前面我们讲到，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序在最坏情况下的时间复杂度是 O(n2)，如何来解决这个“复杂度恶化”的问题呢？如何优化快速排序？我们先来看下，为什么最坏情况下快速排序的时间复杂度是 O(n2) 呢？我们前面讲过，如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 O(n2)。实际上，这种 O(n2) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理。那什么样的分区点是好的分区点呢？或者说如何来选择分区点呢？最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是 O(n2)。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。我这里介绍两个比较常用、比较简单的分区算法，你可以直观地感受一下。1. 三数取中法我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。2. 随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n2) 的情况，出现的可能性不大。快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。Python中的排序python的 sort 内部实现机制为：Timesort最坏时间复杂度为：O（n log n）空间复杂度为：O（n）Timsort是结合了归并排序（merge sort）和插入排序（insertion sort）而得出的排序算法，它在现实中有很好的效率。Tim Peters在2002年设计了该算法并在Python中使用（TimSort 是 Python 中 list.sort 的默认实现）。该算法找到数据中已经排好序的块-分区，每一个分区叫一个run，然后按规则合并这些run。Pyhton自从2.3版以来一直采用Timsort算法排序，现在Java SE7和Android也采用Timsort算法对数组排序。Timsort是稳定的算法，当待排序的数组中已经有排序好的数，它的时间复杂度会小于n logn。与其他合并排序一样，Timesrot是稳定的排序算法，最坏时间复杂度是O（n log n）。在最坏情况下，Timsort算法需要的临时空间是n/2，在最好情况下，它只需要一个很小的临时存储空间Java 语言排序函数堆排序实现C 语言排序函数快速排序实现","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"Hadoop 2.x - Yarn","slug":"Hadoop-2-x-Yarn","date":"2016-10-04T05:30:26.000Z","updated":"2020-04-04T17:25:59.731Z","comments":true,"path":"2016/10/04/Hadoop-2-x-Yarn/","link":"","permalink":"cpeixin.cn/2016/10/04/Hadoop-2-x-Yarn/","excerpt":"","text":"What - YarnYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。Why - Yarn随着互联网的高速发展，新的计算框架不断出现，如内存计算框架、流式计算框架、迭代计算资源框架、这几种框架通常都会被用到考虑到资源的利用率运维和数据共享等因素，企业通常希望将所有的计算框架部署到一个 公共集群中，让他们共享集群的计算资源，并对资源进行同意使用，同时又能采用简单的资源隔离方案，这样便催生了轻量弹性计算平台需求Yarn的设计是一个弹性计算平台，不仅仅支持Mapreduce计算框架而是朝着对多种计算框架进行统一管理方向发展。优点:资源利利用率高，按照框架角度进行资源划分，往往存在应用程序数据和计算资源需求的不均衡性，使得某段时间内计算资源紧张，而另外一种计算方式的资源空闲，共享集群模式则通过框架共享全部的计算资源，使得集群中的资源更加充分合理的利用。运维成本低，如果使用：”一个框架一个集群“的模式，运维人员需要独立管理多个框架，进而增加运维的难度，共享模式通常只需要少数管理员可以完成多个框架的管理。数据共享，随着数据量的增加，跨集群之间的数据不仅增加了硬件成本，而且耗费时间，共享集群模式可以共享框架和硬件资源，大大降低了数据移动带来的成本。How - Yarnyarn是内置在Hadoop平台中的，所以在已经搭建好的Hadoop集群中就可以直接使用。对于其他计算框架，在配置文档配置后即可使用，例如spark的任务提交方式在生产环境中，一定是要用yarn方式来提交，例如：spark-submit –master yarn –class com.xx.xx.classname spark_job_name.jarYarn - 组成YARN总体上仍然是master/slave结构，在整个资源管理框架中，resourcemanager为master，nodemanager是slave。Resourcemanager负责对各个nademanger上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。YARN的基本组成结构，YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。ResourceManager是Master上一个独立运行的进程，负责集群统一的资源管理、调度、分配等等；NodeManager是Slave上一个独立运行的进程，负责上报节点的状态；App Master和Container是运行在Slave上的组件，Container是yarn中分配资源的一个单位，包涵内存、CPU等等资源，yarn以Container为单位分配资源。Client向ResourceManager提交的每一个应用程序都必须有一个Application Master，它经过ResourceManager分配资源后，运行于某一个Slave节点的Container中，具体做事情的Task，同样也运行与某一个Slave节点的Container中。RM，NM，AM乃至普通的Container之间的通信，都是用RPC机制。YARN的架构设计使其越来越像是一个云操作系统，数据处理操作系统。Yarn - 架构ResourcemanagerRM是一个全局的资源管理器，集群只有一个，负责整个系统的资源管理和分配，包括处理客户端请求、启动/监控APP master、监控nodemanager、资源的分配与调度。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）。调度器（Scheduler）调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。应用程序管理器（Applications Manager，ASM）应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。ApplicationMaster（AM）** **管理YARN内运行的应用程序的每个实例。功能：数据切分为应用程序申请资源并进一步分配给内部任务。任务监控与容错负责协调来自resourcemanager的资源，并通过nodemanager监视容易的执行和资源使用情况。NodeManager（NM）Nodemanager整个集群有多个，负责每个节点上的资源和使用。功能：单个节点上的资源管理和任务。处理来自于resourcemanager的命令。处理来自域app master的命令。Nodemanager管理着抽象容器，这些抽象容器代表着一些特定程序使用针对每个节点的资源。Nodemanager定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态（cpu和内存等资源）ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。需要注意的是，Container不同于MRv1中的slot，它是一个动态资源划分单位，是根据应用程序的需求动态生成的。目前为止，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。功能：对task环境的抽象描述一系列信息任务运行资源的集合（cpu、内存、io等）任务运行环境Yarn - 资源管理资源调度和隔离是yarn作为一个资源管理系统，最重要且最基础的两个功能。资源调度由resourcemanager完成，而资源隔离由各个nodemanager实现。Resourcemanager将某个nodemanager上资源分配给任务（这就是所谓的“资源调度”）后，nodemanager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础和保证，这就是所谓的资源隔离。当谈及到资源时，我们通常指内存、cpu、io三种资源。Hadoop yarn目前为止仅支持cpu和内存两种资源管理和调度。内存资源多少决定任务的生死，如果内存不够，任务可能运行失败；相比之下，cpu资源则不同，它只会决定任务的快慢，不会对任务的生死产生影响。 #### Yarn - 队列调度策略Yarn的队列调度策略主要分三种：FIFO、Capacity调度、Fair调度。FIFO调度策略：为先进去的任务分配资源，后入的任务等待前面任务完成才能获得资源。(大任务可能导致后续任务饿死)Capacity调度策略：将集群资源分为一条条队列，每个队列包含一定百分比资源。每个队列中任务采取FIFO的调度策略。在某些队列资源宽裕的情况下，允许跨队列申请资源，同时允许抢占机制。当其他队列任务使用了当前队列任务资源时，当前队列任务在等待一定时间后，允许抢占该队列资源(将改队列内不属于其他队列任务的Container杀死)。Fair调度策略：n个任务情况下，每个任务占据1/n份额的资源。在某任务结束后，该任务资源会被其余资源瓜分，每个任务占据1/(n-1)份资源。与Capacity一样也将集群分为队列且允许抢占机制。不同的是队列内部的资源调度同时允许FIFO和Fair调度。Yarn - 内存管理yarn允许用户配置每个节点上可用的物理内存资源，注意，这里是“可用的”，因为一个节点上内存会被若干个服务共享，比如一部分给了yarn，一部分给了hdfs，一部分给了hbase等，yarn配置的只是自己可用的，配置参数如下：yarn.nodemanager.resource.memory-mb表示该节点上yarn可以使用的物理内存总量，默认是8192m，注意，如果你的节点内存资源不够8g，则需要调减这个值，yarn不会智能的探测节点物理内存总量。yarn.nodemanager.vmem-pmem-ratio任务使用1m物理内存最多可以使用虚拟内存量，默认是2.1yarn.nodemanager.pmem-check-enabled是否启用一个线程检查每个任务证使用的物理内存量，如果任务超出了分配值，则直接将其kill，默认是true。yarn.nodemanager.vmem-check-enabled是否启用一个线程检查每个任务证使用的虚拟内存量，如果任务超出了分配值，则直接将其kill，默认是true。yarn.scheduler.minimum-allocation-mb单个任务可以使用最小物理内存量，默认1024m，如果一个任务申请物理内存量少于该值，则该对应值改为这个数。yarn.scheduler.maximum-allocation-mb单个任务可以申请的最多的内存量，默认8192mYarn - cpu管理目前cpu被划分为虚拟cpu，这里的虚拟cpu是yarn自己引入的概念，初衷是考虑到不同节点cpu性能可能不同，每个cpu具有计算能力也是不一样的，比如，某个物理cpu计算能力可能是另外一个物理cpu的2倍，这时候，你可以通过为第一个物理cpu多配置几个虚拟cpu弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟cpu个数。在yarn中，cpu相关配置参数如下：yarn.nodemanager.resource.cpu-vcores表示该节点上yarn可使用的虚拟cpu个数，默认是8个，注意，目前推荐将该值为与物理cpu核数相同。如果你的节点cpu合数不够8个，则需要调减小这个值，而yarn不会智能的探测节点物理cpu总数。yarn.scheduler.minimum-allocation-vcores单个任务可申请最小cpu个数，默认1，如果一个任务申请的cpu个数少于该数，则该对应值被修改为这个数yarn.scheduler.maximum-allocation-vcores单个任务可以申请最多虚拟cpu个数，默认是32.","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"cpeixin.cn/tags/yarn/"}]},{"title":"Hadoop 2.x - MapReduce","slug":"Hadoop-2-x-MapReduce","date":"2016-10-02T06:30:21.000Z","updated":"2020-04-04T17:26:12.269Z","comments":true,"path":"2016/10/02/Hadoop-2-x-MapReduce/","link":"","permalink":"cpeixin.cn/2016/10/02/Hadoop-2-x-MapReduce/","excerpt":"","text":"MapReduce 工作原理What - MapReduceHadoop主要解决了两个问题，海量数据的存储和海量数据的计算。MapReduce就是Hadoop大数据框架下的分布式计算框架，是基于Hadoop数据分析中的核心计算框架。同时，Mapreduce是一种编程模型，是一种编程方法，抽象理论Why - MapReduce用简洁的方式，就能实现 TB，PB级别数据在百台，千台，万台服务器上的并行运算，程序人员并不需要关心如何处理并行计算、如何分发数据、如何处理错误，这些问题都已经由mapreduce框架来处理。How - MapReduce我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。MapReduce 优缺点优点Mapreduce易于编程它简单的实现一些接口，就可以完成一个分布式程序，这个程序可以分布到大量的廉价的pc机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特性使的Mapreduce编程变得非常流行。良好的扩展性项目当你的计算资源得不到满足的时候，你可以通过简单的通过增加机器来扩展它的计算能力高容错性Mapreduce的设计初衷就是使程序能够部署在廉价的pc机器上，这就要求它具有很高的容错性。比如一个机器挂了，它可以把上面的计算任务转移到另一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由hadoop内部完成的。适合PB级以上海量数据的离线处理**缺点不擅长实时计算Mapreduce无法做到像Mysql那样做到毫秒或者秒级的返回结果不擅长流式计算流式计算的输入数据是动态的，而Mapreduce的输入数据集是静态的，不能流态变化。这是Mapreduce自身的设计特点决定了数据源必须是静态的。不擅长DAG(有向图)计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个应用程序的输出，在这种情况下，Mapreduce并不是不能做，而是使用后每个Mapreduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常低下。MapReduce 工作流程MapReduce整体流程图input file的切分成小文件，默认情况下是按照hdfs block块大小一致为128M对文件进行切分。Map的数量：input file文件按照128Msplit后，有多少个数据块，就对应着有多少个Map。Reduce的数量： 与Map阶段定义的Partition数量一致。MapReduce shuffle阶段：下图先从整体来看，other maps、 other reduces的指向，可以看出，一共有4个Map 和 3个 reduces。那么现在再细分来看，下图表示的是一个 map task 和 reduce task 以及 map 和 reduce之间的shuffle执行流程map 阶段开始后，会经过用户自定义逻辑对数据进行处理。完成map端处理完的数据，会被写入到环形缓冲区（buffer in memory）。这里的环形缓冲区需要说明一下，环形缓冲区的底层实现为环形队列， 默认大小为100MB数据在写入环形缓冲区后，首先进行分区（partition）, 随后对每个partition内的数据进行sort操作，这里的排序方式是按照字典排序，采用快速排序算法进行排序。当map阶段数据不断的向环形缓冲区写的过程中，环形缓冲区有一个阈值，默认为80%，当数据量达到80%后，缓冲区的数据会溢写到磁盘上（merge on disk）溢写到磁盘上后，可以想像成磁盘上存放了很多内部已经排序好，并且带有分区信息的小文件，这时候要对属于同一分区的小文件进行merge，并同时进行排序，这里的排序算法选择的是归并排序此时，每个Map端，都已经准备好了每个partition的文件。上图中，绿色箭头的指向，各个Map端，带有partition信息的文件，fetch到对应的reduce端，可以看到，红色虚线箭头 other reduces对应的磁盘文件则指向其他的两个reduce端。fetch到同一reduce端的partition数据，则属于同一分区数据，这时还要进行一次merge合并排序操作，排序算法选择的是归并排序，来合并成大文件最后，reduce端的shuffle阶段完成，合并好的大文件由reduce函数进行处理，最后到output输出结果。超超超超超超超超详细的工作流程图","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"cpeixin.cn/tags/mapreduce/"}]},{"title":"Hadoop 1.x - HDFS","slug":"Hadoop-1-x-HDFS","date":"2016-10-01T07:36:21.000Z","updated":"2020-04-04T17:26:36.675Z","comments":true,"path":"2016/10/01/Hadoop-1-x-HDFS/","link":"","permalink":"cpeixin.cn/2016/10/01/Hadoop-1-x-HDFS/","excerpt":"","text":"介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。假设与目标硬件故障硬件故障是正常现象，而非例外。HDFS实例可能包含数百或数千个服务器计算机，每个服务器计算机都存储文件系统数据的一部分。存在大量组件并且每个组件的故障概率都很低的事实意味着HDFS的某些组件始终无法运行。因此，检测故障并快速，自动地从故障中恢复是HDFS的核心目标。流数据访问在HDFS上运行的应用程序需要对其数据集进行流式访问。它们不是通常在通用文件系统上运行的通用应用程序。HDFS设计用于批处理，而不是用户交互使用。重点在于数据访问的高吞吐量，而不是数据访问的低延迟。POSIX提出了许多针对HDFS的应用程序不需要的硬性要求。在一些关键领域中，POSIX语义已经被交易以提高数据吞吐率。大数据集在HDFS上运行的应用程序具有大量数据集。HDFS中的典型文件大小为GB到TB。因此，HDFS已调整为支持大文件。它应提供较高的聚合数据带宽，并可以扩展到单个群集中的数百个节点。它应该在单个实例中支持数千万个文件。简单一致性模型HDFS应用程序需要文件一次写入多次读取访问模型。一旦创建，写入和关闭文件，除了追加和截断外，无需更改。支持将内容追加到文件末尾，但不能在任意点更新。该假设简化了数据一致性问题并实现了高吞吐量数据访问。MapReduce应用程序或Web爬网程序应用程序非常适合此模型。移动计算比移动数据便宜如果应用程序所请求的计算在其所操作的数据附近执行，则效率会更高。当数据集的大小巨大时，尤其如此。这样可以最大程度地减少网络拥塞，并提高系统的整体吞吐量。假设通常是将计算迁移到更靠近数据的位置，而不是将数据移动到应用程序正在运行的位置。HDFS为应用程序提供了接口，使它们自己更靠近数据所在的位置。跨异构硬件和软件平台的可移植性HDFS的设计目的是可以轻松地从一个平台移植到另一个平台。这有助于将HDFS广泛用作大量应用程序的首选平台。HDFS 设计原理2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。架构的稳定性心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。HDFS 的特点高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。附：图解HDFS存储原理说明：以下图片引用自博客：翻译经典 HDFS 原理讲解漫画HDFS写数据原理HDFS读数据原理HDFS故障类型和其检测方法检测故障并快速，自动地从故障中恢复是HDFS的核心目标。第二部分：读写故障的处理第三部分：DataNode 故障处理副本布局策略：HDFS shell-ls查看目录hdfs dfs -ls /-mkdir创建目录hdfs dfs -mkdir -p /aaa/bbb/cc/dd-rm删除文件或文件夹hdfs dfs -rm -r /aaa/bbb/cc/dd-rmdir删除空目录hdfs dfs -rmdir /aaa/bbb/cc/dd-puthdfs dfs -put /opt/jdk-8u181-linux-x64.tar.gz /opt/-gethdfs dfs -get /aaa/jdk.tar.gz-cp从hdfs的一个路径拷贝hdfs的另一个路径hdfs dfs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2-count统计一个指定目录下的文件节点数量hdfs dfs -count /-df统计文件系统的可用空间信息hdfs dfs -df -h /-setrep设置hdfs中文件的副本数量hdfs dfs -setrep 3 /aaa/jdk.tar.gzdifference between hdfs dfs and hadoop fshadoop fs是一种更“通用”的命令，它使您可以与包括Hadoop在内的多个文件系统进行交互，而hdfs dfs该命令专用于HDFS。请注意，如果使用的文件系统是HDFS ，则hdfs dfs and hadoop fs命令成为同义词。实际上，如果您发出命令，它将告诉您该命令已被弃用，您应该改用hdfs dfs但是，当您调用这些命令时，实际文件将在hadoop安装目录的bin目录中执行。如果运行hdfs dfs命令，它将调用hadoop安装目录中bin目录中的hdfs文件，后跟第一个参数dfs 告诉hdfs命令我们要使用分布式文件系统（hdfs）而不是本地文件系统文件系统，这是默认选项。第二个命令hadoop fs。它将在hadoop安装目录的bin目录中调用hadoop文件。它将通过发送fs作为第一个参数来跟进该命令/文件。它会告诉“ HADOOP”，我们想要做的任何操作都应该在该特定群集（即HDFS）上由HADOOP安装管理的文件系统上完成。","categories":[{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"}]},{"title":"算法 - 基数排序","slug":"算法-基数排序","date":"2016-09-26T05:56:15.000Z","updated":"2020-05-22T06:12:46.151Z","comments":true,"path":"2016/09/26/算法-基数排序/","link":"","permalink":"cpeixin.cn/2016/09/26/%E7%AE%97%E6%B3%95-%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/","excerpt":"","text":"基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。我们再来看这样一个排序问题。假设我们有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，你有什么比较快速的排序方法呢？我们之前讲的快排，时间复杂度可以做到 O(nlogn)，还有更高效的排序算法吗？桶排序、计数排序能派上用场吗？手机号码有 11 位，范围太大，显然不适合用这两种排序算法。针对这个排序问题，有没有时间复杂度是 O(n) 的算法呢？现在我就来介绍一种新的排序算法，基数排序。刚刚这个问题里有这样的规律：假设要比较两个手机号码 a，b 的大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了。借助稳定排序算法，这里有一个巧妙的实现思路，先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。手机号码稍微有点长，画图比较不容易看清楚，我用字符串排序的例子，画了一张基数排序的过程分解图，你可以看下。注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 O(n)。实际上，有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的 20 万个英文单词，最短的只有 1 个字母，最长的我特意去查了下，有 45 个字母，中文翻译是尘肺病。对于这种不等长的数据，基数排序还适用吗？实际上，我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII 值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。我来总结一下，基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。基数排序 vs 计数排序 vs 桶排序基数排序有两种方法：这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异：基数排序：根据键值的每位数字来分配桶；计数排序：每个桶只存储单一键值；桶排序：每个桶存储一定范围的数值；我们学习了 3 种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 递归","slug":"算法-递归","date":"2016-09-15T13:01:56.000Z","updated":"2020-05-21T15:13:08.403Z","comments":true,"path":"2016/09/15/算法-递归/","link":"","permalink":"cpeixin.cn/2016/09/15/%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92/","excerpt":"","text":"递归对我来说真的是晦涩难懂啊，看似简单，但是当动手写代码想逻辑的时候，无从下手的感觉。一本书上说，爱递归的人爱的要死，不爱递归的人恨的要死，还有一种不爱递归但是随后又爱的要死。leigh caldwell 在 stack overflow上说，如果使用循环，程序的效率可能更高，但是使用递归，程序更容易理解。存在即合理，接下来我们看一下递归推荐注册返佣金的这个功能我想你应该不陌生吧？现在很多 App 都有这个功能。这个功能中，用户 A 推荐用户 B 来注册，用户 B 又推荐了用户 C 来注册。我们可以说，用户 C 的“最终推荐人”为用户 A，用户 B 的“最终推荐人”也为用户 A，而用户 A 没有“最终推荐人”。一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中 actor_id 表示用户 id，referrer_id 表示推荐人 id。基于这个背景，我的问题是，给定一个用户 ID，如何查找这个用户的“最终推荐人”？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！如何理解“递归”？从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。不过，别看我说了这么多，递归本身可是一点儿都不“高冷”，咱们生活中就有很多用到递归的例子。周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？别忘了你是程序员，这个可难不倒你，递归就开始排上用场了。于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：1f(n)=f(n-1)+1 其中，f(1)=1f(n) 表示你想知道自己在哪一排，f(n-1) 表示前面一排所在的排数，f(1)=1 表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松地将它改为递归代码，如下：1234int f(int n) &#123; if (n == 1) return 1; return f(n-1) + 1;&#125;如果上面找座位的例子你没有明白递归是怎么一回事，那么请读一读下面的问题：如何给一堆数字排序？ 答：分成两半，先排左半边再排右半边，最后合并就行了，至于怎么排左边和右边，请重新阅读这句话。孙悟空身上有多少根毛？ 答：一根毛加剩下的毛。你今年几岁？ 答：去年的岁数加一岁,1999 年我出生。递归需要满足的三个条件刚刚这个例子是非常典型的递归，那究竟什么样的问题可以用递归来解决呢？我总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。1. 一个问题的解可以分解为几个子问题的解何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。3. 存在递归终止条件把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是 f(1)=1，这就是递归的终止条件。如何编写递归代码？刚刚铺垫了这么多，现在我们来看，如何来写递归代码？我个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。你先记住这个理论。我举一个例子，带你一步一步实现一个递归代码，帮你理解。假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：12f(n) = f(n-1)+f(n-2)有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以 f(1)=1。这个递归终止条件足够吗？我们可以用 n=2，n=3 这样比较小的数试验一下。n=2 时，f(2)=f(1)+f(0)。如果递归终止条件只有一个 f(1)=1，那 f(2) 就无法求解了。所以除了 f(1)=1 这一个递归终止条件外，还要有 f(0)=1，表示走 0 个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。所以，我们可以把 f(2)=2 作为一种终止条件，表示走 2 个台阶，有两种走法，一步走完或者分两步来走。所以，递归终止条件就是 f(1)=1，f(2)=2。这个时候，你可以再拿 n=3，n=4 来验证一下，这个终止条件是否足够并且正确。我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：123f(1) = 1;f(2) = 2;f(n) = f(n-1)+f(n-2)有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：12345int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2);&#125;我总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。计算机擅长做重复的事情，所以递归正和它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。递归代码要警惕堆栈溢出在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果会非常严重。为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？我在“栈”那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。比如前面的讲到的电影院的例子，如果我们将系统栈或者 JVM 堆栈大小设置为 1KB，在求解 f(19999) 时便会出现如下堆栈报错：12Exception in thread \"main\" java.lang.StackOverflowError递归代码要警惕重复计算除此之外，使用递归时还会出现重复计算的问题。刚才我讲的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的：从图中，我们可以直观地看到，想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。怎么将递归代码改写为非递归代码？我们刚说了，递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。那我们是否可以把递归代码改写为非递归代码呢笼统地讲，是的。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。但是这种思路实际上是将递归改为了“手动”递归，本质并没有变，而且也并没有解决前面讲到的某些问题，徒增了实现的复杂度。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"递归","slug":"递归","permalink":"cpeixin.cn/tags/%E9%80%92%E5%BD%92/"}]},{"title":"算法 - 计数排序","slug":"算法-计数排序","date":"2016-09-15T05:26:20.000Z","updated":"2020-05-22T06:12:37.856Z","comments":true,"path":"2016/09/15/算法-计数排序/","link":"","permalink":"cpeixin.cn/2016/09/15/%E7%AE%97%E6%B3%95-%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/","excerpt":"","text":"我个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。我们都经历过高考，高考查分数系统你还记得吗？我们查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次呢？考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？想弄明白这个问题，我们就要来看计数排序算法的实现方法。我还拿考生那个例子来解释。为了方便说明，我对数据规模做了简化。假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8]中，它们分别是：2，5，3，0，2，3，0，3。考生的成绩从 0 到 5 分，我们使用大小为 6 的数组 C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到 C[6]的值。从图中可以看出，分数为 3 分的考生有 3 个，小于 3 分的考生有 4 个，所以，成绩为 3 分的考生在排序之后的有序数组 R[8]中，会保存下标 4，5，6 的位置。那我们如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？这个处理方法非常巧妙，很不容易想到。思路是这样的：我们对 C[6]数组顺序求和，C[6]存储的数据就变成了下面这样子。C[k]里存储小于等于分数 k 的考生个数。有了前面的数据准备之后，现在我就要讲计数排序中最复杂、最难理解的一部分了，请集中精力跟着我的思路！我们从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3]要减 1，变成 6。以此类推，当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 R 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数组 A 后，数组 R 内的数据就是按照分数从小到大有序排列的了。这种利用另外一个数组来计数的实现方式是不是很巧妙呢？这也是为什么这种排序算法叫计数排序的原因。不过，你千万不要死记硬背上面的排序过程，重要的是理解和会用。我总结一下，计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。比如，还是拿考生这个例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。**当输入的元素是 n 个 0 到 k 之间的整数时，它的运行时间是 Θ(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。例如：计数排序是用来排序0到100之间的数字的最好的算法，但是它不适合按字母顺序排序人名。但是，计数排序可以用在基数排序中的算法来排序数据范围很大的数组。通俗地理解，例如有 10 个年龄不同的人，统计出有 8 个人的年龄比 A 小，那 A 的年龄就排在第 9 位,用这个方法可以得到其他每个人的位置,也就排好了序。当然，年龄有重复时需要特殊处理（保证稳定性），这就是为什么最后要反向填充目标数组，以及将每个数字的统计减去 1 的原因。算法的步骤如下：（1）找出待排序的数组中最大和最小的元素（2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项（3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）（4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1动图代码**123456789101112131415def countingSort(arr, maxValue): bucketLen = maxValue+1 bucket = [0]*bucketLen sortedIndex =0 arrLen = len(arr) for i in range(arrLen): if not bucket[arr[i]]: bucket[arr[i]]=0 bucket[arr[i]]+=1 for j in range(bucketLen): while bucket[j]&gt;0: arr[sortedIndex] = j sortedIndex+=1 bucket[j]-=1 return arr","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 桶排序","slug":"算法-桶排序","date":"2016-09-12T05:01:27.000Z","updated":"2020-05-22T06:12:41.814Z","comments":true,"path":"2016/09/12/算法-桶排序/","link":"","permalink":"cpeixin.cn/2016/09/12/%E7%AE%97%E6%B3%95-%E6%A1%B6%E6%8E%92%E5%BA%8F/","excerpt":"","text":"前面分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。接下来我会讲三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天学习重点的是掌握这些排序算法的适用场景。按照惯例，我先给你出一道思考题：如何根据年龄给 100 万用户排序？ 你可能会说，我用上一节课讲的归并、快排就可以搞定啊！是的，它们也可以完成功能，但是时间复杂度最低也是 O(nlogn)。有没有更快的排序方法呢？让我们一起进入今天的内容！桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。（有木有点像归并排序）桶排序的时间复杂度为什么是 O(n) 呢？我们一块儿来分析一下。如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(nlog(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？答案当然是否定的。为了让你轻松理解桶排序的核心思想，我刚才做了很多假设。实际上，桶排序对要排序数据的要求是非常苛刻的。*首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。桶排序比较适合用在外部排序**中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？现在我来讲一下，如何借助桶排序的处理思想来解决这个问题。我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02…99）。理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。不过，你可能也发现了，订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元…901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：在额外空间充足的情况下，尽量增大桶的数量使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。1. 什么时候最快当输入的数据可以均匀的分配到每一个桶中。2. 什么时候最慢当输入的数据被分配到了同一个桶中。元素分布在桶中然后，元素在每个桶中排序：代码排序一个数组[5,3,6,1,2,7,5,10]， 值都在1-10之间，建立10个桶：[0 0 0 0 0 0 0 0 0 0] 桶，十个空桶[1 2 3 4 5 6 7 8 9 10] 桶代表的值遍历数组，第一个数字5，第五个桶加11[0 0 0 0 1 0 0 0 0 0]第二个数字3，第三个桶加11[0 0 1 0 1 0 0 0 0 0]遍历后1[1 1 1 0 2 1 1 0 0 1]输出1[1 2 3 5 5 6 7 10]代码：**1234567891011121314151617181920def bucket_sort(lst): \"\"\"创建桶\"\"\" buckets = [0] * ((max(lst) - min(lst))+1) for i in range(len(lst)): \"\"\"对应下标添加标识位\"\"\" buckets[lst[i]-min(lst)] += 1 res=[] for i in range(len(buckets)): if buckets[i] != 0: res += [i+min(lst)]*buckets[i] print(res)def main(): array = [5,3,6,2,7,5,10,11,20] bucket_sort(array)if __name__ == '__main__': main()","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 快速排序","slug":"算法-快速排序","date":"2016-09-10T15:05:47.000Z","updated":"2020-05-21T15:07:21.682Z","comments":true,"path":"2016/09/10/算法-快速排序/","link":"","permalink":"cpeixin.cn/2016/09/10/%E7%AE%97%E6%B3%95-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。快速排序的名字起的是简单粗暴，因为一听到这个名字你就知道它存在的意义，就是快，而且效率高！它是处理大数据最快的排序算法之一了。虽然 Worst Case 的时间复杂度达到了 O(n²)，但是人家就是优秀，在大多数情况下都比平均时间复杂度为 O(n logn) 的排序算法表现要更好，可是这是为什么呢，我也不知道。好在我的强迫症又犯了，查了 N 多资料终于在《算法艺术与信息学竞赛》上找到了满意的答案：快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。根据分治、递归的处理思想，我们可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。如果我们用递推公式来将上面的过程写出来的话，就是这样：123456递推公式：quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)终止条件：p &gt;= r我将递推公式转化成递归代码。跟归并排序一样，我还是用伪代码来实现，你可以翻译成你熟悉的任何语言。12345678910111213// 快速排序，A是数组，n表示数组的大小quick_sort(A, n) &#123; quick_sort_c(A, 0, n-1)&#125;// 快速排序递归函数，p,r为下标quick_sort_c(A, p, r) &#123; if p &gt;= r then return q = partition(A, p, r) // 获取分区点 quick_sort_c(A, p, q-1) quick_sort_c(A, q+1, r)&#125;归并排序中有一个 merge() 合并函数，我们这里有一个 partition() 分区函数。partition() 分区函数实际上我们前面已经讲过了，就是随机选择一个元素作为 pivot（一般情况下，可以选择 p 到 r 区间的最后一个元素），然后对 A[p…r]分区，函数返回 pivot 的下标。如果我们不考虑空间消耗的话，partition() 分区函数可以写得非常简单。我们申请两个临时数组 X 和 Y，遍历 A[p…r]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。但是，如果按照这种思路实现的话，partition() 函数就需要很多额外的内存空间，所以快排就不是原地排序算法了。如果我们希望快排是原地排序算法，那它的空间复杂度得是 O(1)，那 partition() 分区函数就不能占用太多额外的内存空间，我们就需要在 A[p…r]的原地完成分区操作。原地分区函数的实现思路非常巧妙，我写成了伪代码，我们一起来看一下。1234567891011partition(A, p, r) &#123; pivot := A[r] i := p for j := p to r-1 do &#123; if A[j] &lt; pivot &#123; swap A[i] with A[j] i := i+1 &#125; &#125; swap A[i] with A[r] return i这里的处理有点类似选择排序。我们通过游标 i 把 A[p…r-1]分成两部分。A[p…i-1]的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i…r-1]是“未处理区间”。我们每次都从未处理的区间 A[i…r-1]中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i]的位置。数组的插入操作还记得吗？在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也讲了一种处理技巧，就是交换，在 O(1) 的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将 A[i]与 A[j]交换，就可以在 O(1) 时间复杂度内将 A[j]放到下标为 i 的位置。文字不如图直观，所以我画了一张图来展示分区的整个过程。因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。到此，快速排序的原理你应该也掌握了。现在，我再来看另外一个问题：快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢？可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。算法步骤从数列中挑出一个元素，称为 “基准”（pivot）;重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；动图代码1234567891011121314151617181920212223def quickSort(arr, left=None, right=None): left = 0 if not isinstance(left,(int, float)) else left right = len(arr)-1 if not isinstance(right,(int, float)) else right if left &lt; right: partitionIndex = partition(arr, left, right) quickSort(arr, left, partitionIndex-1) quickSort(arr, partitionIndex+1, right) return arrdef partition(arr, left, right): pivot = left index = pivot+1 i = index while i &lt;= right: if arr[i] &lt; arr[pivot]: swap(arr, i, index) index+=1 i+=1 swap(arr,pivot,index-1) return index-1def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i]快速排序的性能分析现在，我们来分析一下快速排序的性能。我在讲解快排的实现原理的时候，已经分析了稳定性和空间复杂度。快排是一种原地、不稳定的排序算法。现在，我们集中精力来看快排的时间复杂度。快排也是用递归来实现的。对于递归代码的时间复杂度，我前面总结的公式，这里也还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn)。123T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = 2*T(n/2) + n； n&gt;1但是，公式成立的前提是每次分区操作，我们选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的。我举一个比较极端的例子。如果数组中的数据原来已经是有序的了，比如 1，3，5，6，8。如果我们每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n2)。我们刚刚讲了两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。它们分别对应快排的最好情况时间复杂度和最坏情况时间复杂度。那快排的平均情况时间复杂度是多少呢？我们假设每次分区操作都将区间分成大小为 9:1 的两个小区间。我们继续套用递归时间复杂度的递推公式，就会变成这样：1234T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = T(n/10) + T(9*n/10) + n； n&gt;1这个公式的递推求解的过程非常复杂，虽然可以求解，但我不推荐用这种方法。实际上，递归的时间复杂度的求解方法除了递推公式之外，还有递归树，在树那一节我再讲，这里暂时不说。我这里直接给你结论：T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 二分查找_重复数据情况","slug":"算法-二分查找-重复数据情况","date":"2016-09-02T16:37:00.000Z","updated":"2020-05-15T14:58:27.795Z","comments":true,"path":"2016/09/03/算法-二分查找-重复数据情况/","link":"","permalink":"cpeixin.cn/2016/09/03/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE-%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%E6%83%85%E5%86%B5/","excerpt":"","text":"上一篇讲解了二分查找的原理，并且介绍了最简单的一种二分查找的代码实现。今天我们来讲几种二分查找的变形问题。不知道你有没有听过这样一个说法：“十个二分九个错”。二分查找虽然原理极其简单，但是想要写出没有 Bug 的二分查找并不容易。唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第 3 卷《排序和查找》中说到：“尽管第一个二分查找算法于 1946 年出现，然而第一个完全正确的二分查找算法实现直到 1962 年才出现。”你可能会说，我们上一节学的二分查找的代码实现并不难写啊。那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。二分查找的变形问题很多，我只选择几个典型的来讲解，其他的你可以借助我今天讲的思路自己来分析变体一：查找第一个值等于给定值的元素上一节中的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据1234567891011121314151617def binarySearch_first(array, array_size, search_value): \"\"\"查找第一个值等于给定值的元素\"\"\" low = 0 high = array_size - 1 while low &lt;= high: mid = low + ((high - low) &gt;&gt; 1) if array[mid] == search_value: while array[mid-1] == search_value: if mid - 1 == 0: return 0 mid -= 1 return mid elif array[mid] &lt; search_value: low = mid + 1 else: high = mid - 1这里还有一种更好的写法：123456789101112131415def binarySearch_first_better(array, array_size, search_value): \"\"\"查找第一个值等于给定值的元素\"\"\" low = 0 high = array_size - 1 while low &lt;= high: mid = low + ((high - low) &gt;&gt; 1) if array[mid] &lt; search_value: low = mid + 1 elif array[mid] &gt; search_value: high = mid - 1 else: if mid == 0 or array[mid - 1] != search_value: return mid high = mid - 1变体二：查找最后一个值等于给定值的元素前面的问题是查找第一个值等于给定值的元素，我现在把问题稍微改一下，查找最后一个值等于给定值的元素，又该如何做呢？123456789101112131415def binarySearch_last(array, array_size, search_value): \"\"\"查找最后一个值等于给定值的元素\"\"\" low = 0 high = array_size - 1 while low &lt;= high: mid = low + ((high - low) &gt;&gt; 1) if array[mid] &lt; search_value: low = mid + 1 elif array[mid] &gt; search_value: high = mid - 1 else: if mid == array_size-1 or array[mid + 1] != search_value: return mid low = mid + 1变体三：查找第一个大于等于给定值的元素现在我们再来看另外一类变形问题。在有序数组中，查找第一个大于等于给定值的元素。比如，数组中存储的这样一个序列：3，4，6，7，10。如果查找第一个大于等于 5 的元素，那就是 6。123456789101112131415public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt;= value) &#123; if ((mid == 0) || (a[mid - 1] &lt; value)) return mid; else high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; return -1;&#125;变体四：查找最后一个小于等于给定值的元素123456789101112131415public int bsearch7(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) &#123; high = mid - 1; &#125; else &#123; if ((mid == n - 1) || (a[mid + 1] &gt; value)) return mid; else low = mid + 1; &#125; &#125; return -1;&#125;实际上，很多人都觉得变形的二分查找很难写，主要原因是太追求第一种那样完美、简洁的写法。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"二分查找","slug":"二分查找","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"}]},{"title":"算法 - 归并排序","slug":"算法-归并排序","date":"2016-09-01T14:16:42.000Z","updated":"2020-05-22T06:12:39.910Z","comments":true,"path":"2016/09/01/算法-归并排序/","link":"","permalink":"cpeixin.cn/2016/09/01/%E7%AE%97%E6%B3%95-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/","excerpt":"","text":"归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法：自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第 2 种方法）；自下而上的迭代；在《数据结构与算法 JavaScript 描述》中，作者给出了自下而上的迭代方法。但是对于递归法，作者却认为：However, it is not possible to do so in JavaScript, as the recursion goes too deep for the language to handle.然而，在 JavaScript 中这种方式不太可行，因为这个算法的递归深度对它来讲太深了。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是 O(nlogn) 的时间复杂度。代价是需要额外的内存空间。归并排序（Merge Sort）的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。从我刚才的描述，你有没有感觉到，分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。前面我通过举例让你对归并有了一个感性的认识，又告诉你，归并排序用的是分治思想，可以用递归来实现。我们现在就来看看如何用递归代码来实现归并排序。写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。123456递推公式：merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))终止条件：p &gt;= r 不用再继续分解我来解释一下这个递推公式。merge_sort(p…r) 表示，给下标从 p 到 r 之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q) 和 merge_sort(q+1…r)，其中下标 q 等于 p 和 r 的中间位置，也就是 (p+r)/2。当下标从 p 到 q 和从 q+1 到 r 这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起merge，这样下标从 p 到 r 之间的数据就也排好序了。有了递推公式，转化成代码就简单多了。为了阅读方便，我这里只给出伪代码，你可以翻译成你熟悉的编程语言。算法步骤申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；设定两个指针，最初位置分别为两个已经排序序列的起始位置；比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；重复步骤 3 直到某一指针达到序列尾；将另一序列剩下的所有元素直接复制到合并序列尾。动图根据直方图的颜色来对应相应划分的组更具体的分解步骤可以看到这种结构很像一棵完全二叉树，本文的归并排序我们采用递归去实现（也可采用迭代的方式去实现）。分阶段可以理解为就是递归拆分子序列的过程，递归深度为logn。合并相邻有序子序列再来看看治**阶段，我们需要将两个已经有序的子序列合并成一个有序序列，比如上图中的最后一次合并，要将[4,5,7,8]和[1,2,3,6]两个已经有序的子序列，合并为最终序列[1,2,3,4,5,6,7,8]，来看下实现步骤。代码123456789101112131415161718192021def mergeSort(arr): import math if(len(arr)&lt;2): return arr middle = math.floor(len(arr)/2) left, right = arr[0:middle], arr[middle:] return merge(mergeSort(left), mergeSort(right))def merge(left,right): result = [] while left and right: if left[0] &lt;= right[0]: result.append(left.pop(0)) else: result.append(right.pop(0)); //针对一方为空的情况 while left: result.append(left.pop(0)) while right: result.append(right.pop(0)); return result归并排序的性能分析这样跟着我一步一步分析，归并排序是不是没那么难啦？我们来看归并排序的三个问题。第一，归并排序是稳定的排序算法吗？结合我前面画的那张图和归并排序的伪代码，你应该能发现，归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。在合并的过程中，如果 A[p…q]和 A[q+1…r]之间有值相同的元素，那我们可以像伪代码中那样，先把 A[p…q]中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。第二，归并排序的时间复杂度是多少？归并排序涉及递归，时间复杂度的分析稍微有点复杂。我们正好借此机会来学习一下，如何分析递归代码的时间复杂度。在递归那一节我们讲过，递归的适用场景是，一个问题 a 可以分解为多个子问题 b、c，那求解问题 a 就可以分解为求解问题 b、c。问题 b、c 解决之后，我们再把 b、c 的结果合并成 a 的结果。如果我们定义求解问题 a 的时间是 T(a)，求解问题 b、c 的时间分别是 T(b) 和 T( c)，那我们就可以得到这样的递推关系式：12T(a) = T(b) + T(c) + K其中 K 等于将两个子问题 b、c 的结果合并成问题 a 的结果所消耗的时间。从刚刚的分析，我们可以得到一个重要的结论：不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。套用这个公式，我们来分析一下归并排序的时间复杂度。我们假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。我们知道，merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是：123T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = 2*T(n/2) + n； n&gt;1通过这个公式，如何来求解 T(n) 呢？还不够直观？那我们再进一步分解一下计算过程。12345678T(n) = 2*T(n/2) + n = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n ...... = 2^k * T(n/2^k) + k * n ......通过这样一步一步分解推导，我们可以得到 T(n) = 2^kT(n/2^k)+kn。当 T(n/2^k)=T(1) 时，也就是 n/2^k=1，我们得到 k=log2n 。我们将 k 值代入上面的公式，得到 T(n)=Cn+nlog2n 。如果我们用大 O 标记法来表示的话，T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。第三，归并排序的空间复杂度是多少？归并排序的时间复杂度任何情况下都是 O(nlogn)，看起来非常优秀。（待会儿你会发现，即便是快速排序，最坏情况下，时间复杂度也是 O(n2)。）但是，归并排序并没有像快排那样，应用广泛，这是为什么呢？因为它有一个致命的“弱点”，那就是归并排序不是原地排序算法。这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。这一点你应该很容易理解。那我现在问你，归并排序的空间复杂度到底是多少呢？是 O(n)，还是 O(nlogn)，应该如何分析呢？如果我们继续按照分析递归时间复杂度的方法，通过递推公式来求解，那整个归并过程需要的空间复杂度就是 O(nlogn)。不过，类似分析时间复杂度那样来分析空间复杂度，这个思路对吗？实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法 - 二分查找","slug":"算法-二分查找","date":"2016-08-31T16:37:00.000Z","updated":"2020-05-13T16:40:10.208Z","comments":true,"path":"2016/09/01/算法-二分查找/","link":"","permalink":"cpeixin.cn/2016/09/01/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","excerpt":"","text":"针对有序数据集合的查找算法：二分查找（Binary Search）算法，也叫折半查找算法。二分查找的思想非常简单，很多非计算机专业的同学很容易就能理解，但是看似越简单的东西往往越难掌握好，想要灵活应用就更加困难。假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB，你会怎么做呢？无处不在的二分思想二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。比如说，我们现在来做一个猜字游戏。我随机写一个 0 到 99 之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？假设我写的数字是 23，你可以按照下面的步骤来试一试。（如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。）7 次就猜出来了，是不是很快？这个例子用的就是二分思想，按照这个思想，即便我让你猜的是 0 到 999 的数字，最多也只要 10 次就能猜中。不信的话，你可以试一试。这是一个生活中的例子，我们现在回到实际的开发场景中。假设有 1000 条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于 19 元的订单。如果存在，则返回订单数据，如果不存在则返回 null。最简单的办法当然是从第一个订单开始，一个一个遍历这 1000 个订单，直到找到金额等于 19 元的订单为止。但这样查找会比较慢，最坏情况下，可能要遍历完这 1000 条记录才能找到。那用二分查找能不能更快速地解决呢？为了方便讲解，我们假设只有 10 个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。还是利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，我画了一张查找过程的图。其中，low 和 high 表示待查找区间的下标，mid 表示待查找区间的中间元素下标。看懂这两个例子，你现在对二分的思想应该掌握得妥妥的了。我这里稍微总结升华一下，二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。O(logn) 惊人的查找速度二分查找是一种非常高效的查找算法，高效到什么程度呢？我们来分析一下它的时间复杂度。我们假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。可以看出来，这是一个等比数列。其中 n/2k=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过 n/2k=1，我们可以求得 k=log2n，所以时间复杂度就是 O(logn)。二分查找是我们目前为止遇到的第一个时间复杂度为 O(logn) 的算法。后面章节我们还会讲堆、二叉树的操作等等，它们的时间复杂度也是 O(logn)。我这里就再深入地讲讲 O(logn) 这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1) 的算法还要高效。为什么这么说呢？因为 logn 是一个非常“恐怖”的数量级，即便 n 非常非常大，对应的 logn 也很小。比如 n 等于 2 的 32 次方，这个数很大了吧？大约是 42 亿。也就是说，如果我们在 42 亿个数据中用二分查找一个数据，最多需要比较 32 次。我们前面讲过，用大 O 标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1) 有可能表示的是一个非常大的常量值，比如 O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高。反过来，对数对应的就是指数。有一个非常著名的“阿基米德与国王下棋的故事”，你可以自行搜索一下，感受一下指数的“恐怖”。这也是为什么我们说，指数时间复杂度的算法在大规模数据面前是无效的。二分查找的递归与非递归实现实际上，简单的二分查找并不难写，注意我这里的“简单”二字。下一节，我们会讲到二分查找的变体问题，那才是真正烧脑的。今天，我们来看如何来写最简单的二分查找。最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。我用 python 代码实现了一个最简单的二分查找算法。12345678910111213141516171819202122232425def binarySearch(array, array_size, search_value): low = 0 high = array_size - 1 while low &lt;= high: mid = low+((high-low)&gt;&gt;1) if array[mid] == search_value: return mid elif mid &lt; search_value: low = mid+1 else: high = mid-1def main(): array = [] for i in range(100): array.append(i) mid_num = binarySearch(array,len(array),57) print(mid_num)if __name__ == '__main__': main()实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)&gt;&gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。二分查找应用场景的局限性前面我们分析过，二分查找的时间复杂度是 O(logn)，查找数据的效率非常高。不过，并不是什么情况下都可以用二分查找，它的应用场景是有很大局限性的。那什么情况下适合用二分查找，什么情况下不适合呢？首先，二分查找依赖的是顺序表结构，简单点说就是数组。那二分查找能否依赖其他数据结构呢？比如链表。答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元素。我们在数组和链表那两节讲过，数组按照下标随机访问数据的时间复杂度是 O(1)，而链表随机访问的时间复杂度是 O(n)。所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。其次，二分查找针对的是有序数据。二分查找对这一点的要求比较苛刻，数据必须是有序的。如果数据没有序，我们需要先排序。前面章节里我们讲到，排序的时间复杂度最低是 O(nlogn)。所以，如果我们针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。但是，如果我们的数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都是很高的。所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。那针对动态数据集合，我们可以选择二叉树。再次，数据量太小不适合二分查找。如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了。比如我们在一个大小为 10 的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。不过，这里有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过 300 的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。最后，数据量太大也不适合二分查找。二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间。注意这里的“连续”二字，也就是说，即便有 2GB 的内存空间剩余，但是如果这剩余的 2GB 内存空间都是零散的，没有连续的 1GB 大小的内存空间，那照样无法申请一个 1GB 大小的数组。而我们的二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。解答开篇二分查找的理论知识你应该已经掌握了。我们来看下开篇的思考题：如何在 1000 万个整数中快速查找某个整数？这个问题并不难。我们的内存限制是 100MB，每个数据大小是 8 字节，最简单的办法就是将数据存储在数组中，内存占用差不多是 80MB，符合内存的限制。借助今天讲的内容，我们可以先对这 1000 万数据从小到大排序，然后再利用二分查找算法，就可以快速地查找想要的数据了。看起来这个问题并不难，很轻松就能解决。实际上，它暗藏了“玄机”。如果你对数据结构和算法有一定了解，知道散列表、二叉树这些支持快速查找的动态数据结构。你可能会觉得，用散列表和二叉树也可以解决这个问题。实际上是不行的。虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但是，我们后面会讲，不管是散列表还是二叉树，都会需要比较多的额外的内存空间。如果用散列表或者二叉树来存储这 1000 万的数据，用 100MB 的内存肯定是存不下的。而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题。总结二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"二分查找","slug":"二分查找","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"}]},{"title":"算法 - 选择排序","slug":"算法-选择排序","date":"2016-08-22T15:22:43.000Z","updated":"2020-05-21T09:13:39.426Z","comments":true,"path":"2016/08/22/算法-选择排序/","link":"","permalink":"cpeixin.cn/2016/08/22/%E7%AE%97%E6%B3%95-%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/","excerpt":"","text":"选择排序是一种简单直观的排序算法，无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。动态示例图算法步骤首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。重复第二步，直到所有元素均排序完毕。代码12345678def selection_sort(array): for i in range(0, len(array)-1): min_index = i for j in range(i+1, len(array)): if array[j] &lt; array[min_index]: min_index = j if i != min_index: array[i], array[min_index] = array[min_index], array[i]照例，也有三个问题需要你思考首先，选择排序空间复杂度为 O(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)。你可以自己来分析看看。那选择排序是稳定的排序算法吗？这个问题我着重来说一下。答案是否定的，选择排序是一种不稳定的排序算法。从我前面画的那张图中，你可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。比如 5，8，5，2，9 这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素 2，与第一个 5 交换位置，那第一个 5 和中间的 5 顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"数据结构 - 散列表原理","slug":"数据结构-散列表原理","date":"2016-08-22T01:31:55.000Z","updated":"2020-06-15T05:48:21.751Z","comments":true,"path":"2016/08/22/数据结构-散列表原理/","link":"","permalink":"cpeixin.cn/2016/08/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%95%A3%E5%88%97%E8%A1%A8%E5%8E%9F%E7%90%86/","excerpt":"","text":"Word 这种文本编辑器你平时应该经常用吧，那你有没有留意过它的拼写检查功能呢？一旦我们在 Word 里输入一个错误的英文单词，它就会用标红的方式提示“拼写错误”。Word 的这个单词拼写检查功能，虽然很小但却非常实用。你有没有想过，这个功能是如何实现的呢？其实啊，一点儿都不难。只要你学会散列表（Hash Table）。你就能像微软 Office 的工程师一样，轻松实现这个功能。散列思想散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash 表”，你一定也经常听过它。散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。我用一个例子来解释一下。假如我们有 89 名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这 89 名选手的编号依次是 1 到 89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。你会怎么做呢？我们可以把这 89 名选手的信息放在数组里。编号为 1 的选手，我们放到数组中下标为 1 的位置；编号为 2 的选手，我们放到数组中下标为 2 的位置。以此类推，编号为 k 的选手放到数组中下标为 k 的位置。因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为 x 的选手的时候，我们只需要将下标为 x 的数组元素取出来就可以了，时间复杂度就是 O(1)。这样按照编号查找选手信息，效率是不是很高？实际上，这个例子已经用到了散列的思想。在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是 O(1) 这一特性，就可以实现快速查找编号对应的选手信息。你可能要说了，这个例子中蕴含的散列思想还不够明显，那我来改造一下这个例子。假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用 6 位数字来表示。比如 051167，其中，前两位 05 表示年级，中间两位 11 表示班级，最后两位还是原来的编号 1 到 89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。这就是典型的散列思想。其中，参赛选手的编号我们叫作键（key）或者关键字。我们用它来标识一个选手。我们把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash 函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash 值”“哈希值”）。通过这个例子，我们可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。散列函数从上面的例子我们可以看到，散列函数在散列表中起着非常关键的作用。现在我们就来学习下散列函数。散列函数，顾名思义，它是一个函数。我们可以把它定义成 hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。那第一个例子中，编号就是数组下标，所以 hash(key) 就等于 key。改造后的例子，写成散列函数稍微有点复杂。我用伪代码将它写成函数就是下面这样：12345678int hash(String key) &#123; // 获取后两位字符 string lastTwoChars = key.substr(length-2, length); // 将后两位字符转换为整数 int hashValue = convert lastTwoChas to int-type; return hashValue;&#125;刚刚举的学校运动会的例子，散列函数比较简单，也比较容易想到。但是，如果参赛选手的编号是随机生成的 6 位数字，又或者用的是 a 到 z 之间的字符串，该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：散列函数计算得到的散列值是一个非负整数；如果 key1 = key2，那 hash(key1) == hash(key2)；如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。我来解释一下这三点。其中，第一点理解起来应该没有任何问题。因为数组下标是从 0 开始的，所以散列函数生成的散列值也要是非负整数。第二点也很好理解。相同的 key，经过散列函数得到的散列值也应该是相同的。第三点理解起来可能会有问题，我着重说一下。这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。所以我们几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径来解决。散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，线性探测（Linear Probing）。当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。我说的可能比较抽象，我举一个例子具体给你说明一下。这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。从图中可以看出，散列表的大小为 10，在元素 x 插入散列表之前，已经 6 个元素插入到散列表中。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置 2，于是将其插入到这个位置。在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。散列表跟数组一样，不仅支持插入、查找操作，还支持删除操作。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。这是为什么呢？还记得我们刚讲的查找操作吗？在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？我们可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。你可能已经发现了，线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing）。所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+12，hash(key)+22……所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。装载因子的计算公式是：1散列表的装载因子=填入表中的元素个数/散列表的长度装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。链表法链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看这个图，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。那查找或删除操作的时间复杂度是多少呢？实际上，这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。解答开篇有了前面这些基本知识储备，我们来看一下开篇的思考题：Word 文档中单词拼写检查功能是如何实现的？常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。内容小结今天我讲了一些比较基础、比较偏理论的散列表知识，包括散列表的由来、散列函数、散列冲突的解决方法。散列表来源于数组，它借助散列函数对数组这种数据结构进行扩展，利用的是数组支持按照下标随机访问元素的特性。散列表两个核心问题是散列函数设计和散列冲突解决。散列冲突有两种常用的解决方法，开放寻址法和链表法。散列函数设计的好坏决定了散列冲突的概率，也就决定散列表的性能。针对散列函数和散列冲突。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"散列表","slug":"散列表","permalink":"cpeixin.cn/tags/%E6%95%A3%E5%88%97%E8%A1%A8/"}]},{"title":"算法-插入排序","slug":"算法-插入排序","date":"2016-08-21T15:22:43.000Z","updated":"2020-05-21T09:12:58.909Z","comments":true,"path":"2016/08/21/算法-插入排序/","link":"","permalink":"cpeixin.cn/2016/08/21/%E7%AE%97%E6%B3%95-%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/","excerpt":"","text":"插入排序的代码实现虽然没有冒泡排序和选择排序那么简单粗暴，但它的原理应该是最容易理解的了，因为只要打过扑克牌的人都应该能够秒懂。插入排序是一种最简单直观的排序算法，它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序和冒泡排序一样，也有一种优化算法，叫做拆半插入。我们先来看一个问题。一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。那插入排序具体是如何借助上面的思想来实现排序的呢？首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。为什么说移动次数就等于逆序度呢？我拿刚才的例子画了一个图表，你一看就明白了。满有序度是 n*(n-1)/2=15，初始序列的有序度是 5，所以逆序度是 10。插入排序中，数据移动的个数总和也等于 10=3+3+4。下面给出插入排序的动图代码如下：123456789def insert_sort_1(arr): for i in range(len(arr)): preIndex = i-1 current = arr[i] while preIndex &gt;= 0 and arr[preIndex] &gt; current: arr[preIndex+1] = arr[preIndex] preIndex-=1 arr[preIndex+1] = current return arr这里有一点需要注意，如上面的代码，current是我们每次进行比较的值，每次与前面的preIndex值比较，如果比preIndex值小，则preIndex值后移，current值这时候不要与preIndex的值进行交换，这样就写成了冒泡排序😄。现在，我们来看点稍微复杂的东西。我这里还是有三个问题要问你。第一，插入排序是原地排序算法吗？从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。第二，插入排序是稳定的排序算法吗？在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。第三，插入排序的时间复杂度是多少？如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n)。注意，这里是从尾到头遍历已经有序的数据。如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n2)。还记得我们在数组中插入一个数据的平均时间复杂度是多少吗？没错，是 O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n2)。冒泡排序和插入排序的时间复杂度都是 O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？我们前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。但是，从前面的代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个。我们把执行一个赋值语句的时间粗略地计为单位时间（unit_time），然后分别用冒泡排序和插入排序对同一个逆序度是 K 的数组进行排序。用冒泡排序，需要 K 次交换操作，每次需要 3 个赋值语句，所以交换操作总耗时就是 3*K 单位时间。而插入排序中数据移动操作只需要 K 个单位时间。这个只是我们非常理论的分析，对两种排序性能对比测试程序，随机生成 10000 个数组，每个数组中包含 200 个数据，然后在我的机器上分别用冒泡和插入排序算法来排序，冒泡排序算法大约 700ms 才能执行完成，而插入排序只需要 100ms 左右就能搞定！所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n2)，但是如果我们希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，我们只是讲了最基础的一种。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法-冒泡排序","slug":"算法-冒泡排序","date":"2016-08-20T15:22:43.000Z","updated":"2020-05-22T06:12:49.940Z","comments":true,"path":"2016/08/20/算法-冒泡排序/","link":"","permalink":"cpeixin.cn/2016/08/20/%E7%AE%97%E6%B3%95-%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法，可能就是排序。大部分编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序。排序非常重要，所以我会花多一点时间来详细讲一讲经典的排序算法。排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。更为精简的，在《算法图解》这本书中，排序模块只拿选择排序作为典型进行了讲解我按照时间复杂度把它们分成了三类，带着问题去学习，是最有效的学习方法。所以按照惯例，我还是先给你出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是 O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？你可以先思考一两分钟，带着这个问题，我们开始今天的内容！如何分析一个“排序算法”？学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：1. 最好情况、最坏情况、平均情况时间复杂度我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。2. 时间复杂度的系数、常数 、低阶我们知道，时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。3. 比较次数和交换（或移动）次数基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。排序算法的内存消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。插入排序、冒泡排序、选择排序都是原地排序算法。**排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。我通过一个例子来解释一下。比如我们有一组数据 2，9，3，4，8，3，按照大小排序之后就是 2，3，3，4，8，9。这组数据里有两个 3。经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。你可能要问了，两个 3 哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序。比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。冒泡排序（Bubble Sort）我们从冒泡排序开始，冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。下面就不讲具体的排序过程了，一张冒泡排序动图，带你看下冒泡排序的整个过程。冒泡排序算法的原理比较容易理解，具体的代码我贴到下面，你可以结合着代码来看我前面讲的原理。12345678910111213141516171819202122232425262728293031323334353637import time\"\"\"第一个(外层)for循环作用：控制排序的轮数第二个(内层)for循环作用：控制每一轮里的每一个比较步骤\"\"\"def bubble_sort(arr): for i in range(1, len(arr)): for j in range(0, len(arr)-i): if arr[j] &gt; arr[j+1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] # 可以配合打印i,j来更好的理解 # print(i,j) # time.sleep(2) return arrdef bubble_sort_v2(arr): \"\"\"改进版，避免右侧最大值重复比较\"\"\" for i in range(len(arr) - 1, 0, -1): # 反向遍历 for j in range(0, i): # 由于最右侧的值已经有序，不再比较，每次都减少遍历次数 if arr[j] &gt; arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] return arrdef main(): array = [3,4,1,2,5,6] result_array = bubble_sort(array) print(result_array)if __name__ == '__main__': main()现在，结合刚才我分析排序算法的三个方面，我有三个问题要问你。第一，冒泡排序是原地排序算法吗？冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。第二，冒泡排序是稳定的排序算法吗？在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。第三，冒泡排序的时间复杂度是多少？最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n2)。最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。对于包含 n 个数据的数组，这 n 个数据就有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：12有序元素对：a[i] &lt;= a[j], 如果i &lt; j。同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是 n(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作*满有序度**。逆序度的定义正好跟有序度相反（默认从小到大为有序），我想你应该已经想到了。关于逆序度，我就不举例子讲了。你可以对照我讲的有序度的例子自己看下。12逆序元素对：a[i] &gt; a[j], 如果i &lt; j。关于这三个概念，我们还可以得到一个公式：逆序度 = 满有序度 - 有序度。我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是 4，5，6，3，2，1 ，其中，有序元素对有 (4，5) (4，6)(5，6)，所以有序度是 3。n=6，所以排序完成之后终态的满有序度为 n(n-1)/2=15。冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n(n-1)/2–初始有序度。此例中就是 15–3=12，要进行 12 次交换操作。对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n(n-1)/2 次交换。最好情况下，初始状态的有序度是 n(n-1)/2，就不需要进行交换。我们可以取个中间值 n(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。换句话说，平均情况下，需要 n(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以平均情况下的时间复杂度就是 O(n2)。这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"数据结构-链表反转","slug":"数据结构-链表反转","date":"2016-08-18T14:16:51.000Z","updated":"2020-04-04T17:34:24.764Z","comments":true,"path":"2016/08/18/数据结构-链表反转/","link":"","permalink":"cpeixin.cn/2016/08/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/","excerpt":"","text":"题目：输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL这道题目可以用迭代，递归两种方法来实现迭代假设存在链表 1 → 2 → 3 → 4 → 5 → Ø，我们想要把它改成 Ø ← 1 ← 2 ← 3 ← 4 ← 5。在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！根据题目总结一下核心的操作逻辑：从头节点 1 开始，1 指向 2，更改指向顺序，需要将 2 的指针指向 1 (此时，虽然 2 的指针方向已经指向 1，但是 1 的指针方向没有改变，依然指向 2，则是 2 → 1 → 2 → 1….. ), 同时也要存储 2 的下一个节点 3，以便接下来迭代操作 2 和 3 节点之间的指针反转，每两个相邻节点进行相同的操作。迭代最后，尾节点 5 无后续节点，则跳出迭代。在迭代完相邻节点的指针反转后，还需要做的两步原头节点指向 None，变成尾节点将原尾节点设置成新的头节点操作完成～～！！12345678910111213141516171819202122232425262728293031323334def reversed_self(self): \"\"\"翻转链表自身.\"\"\" if self.head is None or self.head.next_node is None: return pre = self.head node = self.head.next_node while node is not None: pre,node = self.__reversed_with_two_node(pre,node) \"\"\"循环到最后一位 node is None退出\"\"\" \"\"\"将原链表的头节点指向为下一节点 改为 指向为None\"\"\" self.head.next_node = None \"\"\"将头节点设置为原链表的尾节点,链表反转则成功。链表元素位置不变，但是元素之间的指向改变\"\"\" self.head = predef __reversed_with_two_node(self, pre, node): \"\"\"翻转相邻两个节点. 参数: pre:前一个节点 node:当前节点 返回: *******(pre,node):下一个相邻节点的元组 \"\"\" tmp = node.next_node node.next_node = pre pre = node node = tmp return pre, node调试图：123while node is not None: pre,node = self.__reversed_with_two_node(pre,node)核心步骤迭代完，链表中的节点指向已经反转上图。此时，节点之间指针反转完毕，虽然指针反转了，但是头节点还是 1 ， 因为 1 指向仍然为 2 还没有处理。并且 5 还不是头节点，对应的还没有执行12self.head.next_node = Noneself.head = pre操作完下面两步后，上图中，1 的下一个节点指针变成None， 头节点赋值为 512self.head.next_node = Noneself.head = pre","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-如何写好链表代码","slug":"数据结构-如何写好链表代码","date":"2016-08-17T14:15:49.000Z","updated":"2020-04-04T11:59:24.090Z","comments":true,"path":"2016/08/17/数据结构-如何写好链表代码/","link":"","permalink":"cpeixin.cn/2016/08/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E5%86%99%E5%A5%BD%E9%93%BE%E8%A1%A8%E4%BB%A3%E7%A0%81/","excerpt":"","text":"如何写好链表代码？理解指针或引用的含义什么是指针？指针是一个变量将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。警惕指针丢失和内存泄漏在插入和删除结点时，要注意先持有后面的结点再操作，否者一旦后面结点的前继指针被断开，就无法再访问，导致内存泄漏。插入结点时，一定要注意操作的顺序删除链表结点时，也一定要记得手动释放内存空间利用哨兵简化难度链表的插入、删除操作，需要对插入第一个结点和删除最后一个节点做特殊处理。利用哨兵对象可以不用边界判断，链表的哨兵对象是只存指针不存数据的头结点。重点留意边界条件处理操作链表时要考虑如果链表为空时，代码是否能正常工作？如果链表只包含一个结点时，代码是否能正常工作？如果链表只包含两个结点时，代码是否能正常工作？代码逻辑在处理头结点和尾结点的时候，是否能正常工作？学习数据结构和算法主要是掌握一系列思想，能在其它的编码中也养成考虑边界的习惯。举例画图，辅助思考对于比较复杂的操作，可以用纸笔画一画，释放脑容量来做逻辑处理（时间换空间思想），也便于完成后的检查。多写多练，没有捷径孰能生巧，不管是什么算法，只有经过反复的练习，才能信手拈来。哨兵对象思想，在 iOS AutoreleasePool 中有用到，在 AutoreleasePoolPush 时添加一个哨兵对象，Pop 时将到哨兵对象之间的所有 Autorelease 对象发送 release 消息。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-如何实现LRU缓存淘汰算法","slug":"数据结构-如何实现LRU缓存淘汰算法","date":"2016-08-16T07:30:21.000Z","updated":"2020-04-04T11:59:17.945Z","comments":true,"path":"2016/08/16/数据结构-如何实现LRU缓存淘汰算法/","link":"","permalink":"cpeixin.cn/2016/08/16/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","excerpt":"","text":"今天我们来聊聊“链表（Linked list）”这个数据结构。学习链表有什么用呢？为了回答这个问题，我们先来讨论一个经典的链表应用场景，那就是 LRU 缓存淘汰算法。缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）最少使用策略 LFU（Least Frequently Used）最近最少使用策略 LRU（Least Recently Used）这些策略你不用死记，我打个比方你很容易就明白了。假如说，你买了很多本技术书，但有一天你发现，这些书太多了，太占书房空间了，你要做个大扫除，扔掉一些书籍。那这个时候，你会选择扔掉哪些书呢？对应一下，你的选择标准是不是和上面的三种策略神似呢？好了，回到正题，我们今天的开篇问题就是：如何用链表来实现 LRU 缓存淘汰策略呢？ 带着这个问题，我们开始今天的内容吧！五花八门的链表结构相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常将会放到一块儿来比较。所以我们先来看，这两者有什么区别。我们先从底层的存储结构上来看一看。为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表双向链表循环链表我们首先来看最简单、最常用的单链表。我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针 next。从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。与数组一样，链表也支持数据的查找、插入和删除操作。我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。但是，有利就有弊。链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第 k 位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要 O(n) 的时间复杂度。好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，循环链表和双向链表。循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。单链表和循环链表是不是都不难？接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适合解决哪种问题呢？从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是 O(1) 了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。我们先来看删除操作。在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：删除结点中“值等于某个给定值”的结点；删除给定指针指向的结点。对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉 Java 语言，你肯定用过 LinkedHashMap 这个容器。如果你深入研究 LinkedHashMap 的实现原理，就会发现其中就用到了双向链表这种数据结构。实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。**当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。所以我总结一下，对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不用我多讲，你应该知道双向循环链表长什么样子了吧？你可以自己试着在纸上画一画。链表 VS 数组性能大比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。你可能会说，我们 Java 中的 ArrayList 容器，也可以支持动态扩容啊？我们上一节课讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。我举一个稍微极端的例子。如果我们用 ArrayList 存储了了 1GB 大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList 会申请一个 1.5GB 大小的存储空间，并且把原来那 1GB 的数据拷贝到新申请的空间上。听起来是不是就很耗时？除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。解答开篇好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。如果此数据没有在缓存链表中，又可以分为两种情况：如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。这样我们就用链表实现了一个 LRU 缓存，是不是很简单？现在我们来看下 m 缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？我把这个问题留给你思考。LRU最后回到前题，来实现LRU缓存机制设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。O(1) 时间复杂度内完成这两种操作。设计lru算法的思路如何表示最近访问的数据和最早访问的数据如何查找是否缓存了数据有缓存，如何处理数据没有缓存，如何处理缓存未满缓存已满实现LRU数据结构选型有序字典题目要求实现 LRU 缓存机制，需要在 O(1)O(1) 时间内完成如下操作：获取键 / 检查键是否存在设置键删除最先插入的键前两个操作可以用标准的哈希表在 O(1)O(1) 时间内完成。有一种叫做有序字典的数据结构，综合了哈希表和链表，在 Python 中为 OrderedDict，在 Java 中为 LinkedHashMap。哈希表 + 双向链表这个问题可以用哈希表，辅以双向链表记录键值对的信息。所以可以在 O(1) 时间内完成 put 和 get 操作，同时也支持 O(1) 删除第一个添加的节点。使用双向链表的一个好处是不需要额外信息删除一个节点，同时可以在常数时间内从头部或尾部插入删除节点。一个需要注意的是，在双向链表实现中，这里使用一个伪头部和伪尾部标记界限，这样在更新的时候就不需要检查是否是 null 节点。实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125class DbListNode(object): def __init__(self, x, y): \"\"\" 节点为哈希表+双向链表 :param x: :param y: \"\"\" self.key = x self.value = y self.next = None self.prev = Noneclass LRUCache(object): def __init__(self, capacity): \"\"\" 初始化一个空双向链表 :type capacity: int \"\"\" self.cap = capacity self.catche = &#123;&#125; self.top = DbListNode(None, -1) self.tail = DbListNode(None, -1) self.top.next = self.tail self.tail.prev = self.top def get(self, key): \"\"\" :type key: int :rtype: int \"\"\" \"\"\"判断节点是否存在\"\"\" if key in self.catche.keys(): cur = self.catche[key] \"\"\"首先跳出原来的位置\"\"\" cur.prev.next = cur.next cur.next.prev = cur.prev \"\"\"top,tail为哨兵节点\"\"\" top_node = self.top.next cur.next = top_node top_node.prev = cur self.top.next = cur cur.prev = self.top return cur.value return -1 def put(self, key, value): \"\"\" :type key: int :type value: int :rtype: None \"\"\" if key in self.catche.keys(): \"\"\"如果插入节点存在，则将插入节点调换为位，插入到哨兵节点后的头节点。此时不存在增删节点，所以不用判断链表长度\"\"\" cur = self.catche[key] \"\"\"首先跳出原来的位置\"\"\" cur.prev.next = cur.next cur.next.prev = cur.prev \"\"\"top,tail为哨兵节点\"\"\" top_node = self.top.next cur.next = top_node top_node.prev = cur self.top.next = cur cur.prev = self.top else: # 增加新结点至首部 cur = DbListNode(key, value) self.catche[key] = cur # 最近用过的置于链表首部 top_node = self.top.next self.top.next = cur cur.prev = self.top cur.next = top_node top_node.prev = cur \"\"\"判断长度删除尾节点\"\"\" if len(self.catche.keys()) &gt; self.cap: self.catche.pop(self.tail.prev.key) # 去掉原尾结点 self.tail.prev.prev.next = self.tail self.tail.prev = self.tail.prev.prev def __repr__(self): vals = [] p = self.top.next while p.next: vals.append(str(p.value)) p = p.next return '-&gt;'.join(vals)if __name__ == '__main__': cache = LRUCache(2) cache.put(1, 1) cache.put(2, 2) print(cache) cache.get(1) # 返回 1 print(cache) cache.put(3, 3) # 该操作会使得密钥 2 作废 print(cache) cache.get(2) # 返回 -1 (未找到) print(cache) cache.put(4, 4) # 该操作会使得密钥 1 作废 print(cache) cache.get(1) # 返回 -1 (未找到) cache.get(3) # 返回 3 print(cache) cache.get(4) # 返回 4 print(cache)内容小结今天我们讲了一种跟数组“相反”的数据结构，链表。它跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。课后思考如何判断一个字符串是否是回文字符串的问题，我想你应该听过，我们今天的题目就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？欢迎留言和我分享，我会第一时间给你反馈。Q&amp;A关于CPU缓存机制CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块(这个大小我不太确定。。)并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。","categories":[{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LRU淘汰算法","slug":"LRU淘汰算法","permalink":"cpeixin.cn/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"}]},{"title":"数据结构-栈","slug":"数据结构-栈","date":"2016-08-14T09:27:21.000Z","updated":"2020-06-10T10:16:45.266Z","comments":true,"path":"2016/08/14/数据结构-栈/","link":"","permalink":"cpeixin.cn/2016/08/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88/","excerpt":"","text":"浏览器的前进、后退功能，我想你肯定很熟悉吧？当你依次访问完一串页面 a-b-c 之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面 b 和 a。当你后退到页面 a，点击前进按钮，就可以重新查看页面 b 和 c。但是，如果你后退到页面 b 后，点击了新的页面 d，那就无法再通过前进、后退功能查看页面 c 了。假设你是 Chrome 浏览器的开发工程师，你会如何实现这个功能呢？这就要用到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。如何理解“栈”？关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为我觉得，相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表不就好了吗？为什么还要用这个“操作受限”的“栈”呢？事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。如何实现一个“栈”？从刚才栈的定义里，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。我这里实现一个基于数组的顺序栈。基于数组12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class stack: def __init__(self, size): \"\"\" 栈结构 :param size: 栈大小 \"\"\" self.count = 0 self.size = size self.array = [] def push(self, value): \"\"\" 入栈 入栈判满 :param value: \"\"\" if self.count == self.size: return False self.array.append(value) self.count += 1 return True def pop(self): \"\"\" 出栈 出栈判空 :return: \"\"\" if self.count == 0: return False data = self.array[self.count - 1] self.count -= 1 return dataif __name__ == '__main__': st = stack(4) st.push(1) st.push(2) st.push(3) st.push(4) st.pop() st.pop() st.pop() data = st.pop() print(data) st.pop()基于链表实现的链式栈的代码基于链表12345678910111213141516171819202122232425262728293031323334353637383940414243from typing import Optionalclass Node: def __init__(self, data: int, next=None): self.data = data self.next = nextclass linkstack: def __init__(self): self.top: Node = None def push(self, value: int): new_node = Node(value) new_node.next = self.top self.top = new_node def pop(self)-&gt;Optional[int]: if self.top: data = self.top.data self.top = self.top.next return data def __repr__(self) -&gt; str: current = self._top nums = [] while current: nums.append(current._data) current = current._next return \" \".join(f\"&#123;num&#125;]\" for num in nums)if __name__ == \"__main__\": stack = linkstack() for i in range(9): stack.push(i) print(stack) for _ in range(3): stack.pop() print(stack)了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？不管是顺序栈还是链式栈，我们存储数据只需要一个大小为 n 的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。注意，这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。因为，这 n 个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。空间复杂度分析是不是很简单？时间复杂度也不难。不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 O(1)。支持动态扩容的顺序栈刚才那个基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储 next 指针，内存消耗相对较多。那我们如何基于数组实现一个可以支持动态扩容的栈呢？你还记得，我们在数组那一节，是如何来实现一个支持动态扩容的数组的吗？当数组空间不够时，我们就重新申请一块更大的内存，将原来数组中数据统统拷贝过去。这样就实现了一个支持动态扩容的数组。所以，如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。我画了一张图，你可以对照着理解一下。实际上，支持动态扩容的顺序栈，我们平时开发中并不常用到。我讲这一块的目的，主要还是希望带你练习一下前面讲的复杂度分析方法。所以这一小节的重点是复杂度分析。你不用死记硬背入栈、出栈的时间复杂度，你需要掌握的是分析方法。能够自己分析才算是真正掌握了。现在我就带你分析一下支持动态扩容的顺序栈的入栈、出栈操作的时间复杂度。对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。也就是说，对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。那平均情况下的时间复杂度又是多少呢？还记得我们在复杂度分析那一节中讲的摊还分析法吗？这个入栈操作的平均情况下的时间复杂度可以用摊还分析法来分析。我们也正好借此来实战一下摊还分析法。为了分析的方便，我们需要事先做一些假设和定义：栈空间不够时，我们重新申请一个是原来大小两倍的数组；为了简化分析，假设只有入栈操作没有出栈操作；定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。为了让你更加直观地理解这个过程，我画了一张图。你应该可以看出来，这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。通过这个例子的实战分析，也印证了前面讲到的，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。栈在函数调用中的应用前面我讲的都比较偏理论，我们现在来看下，栈在软件工程中的实际应用。栈作为一个比较基础的数据结构，应用场景还是蛮多的。其中，比较经典的一个应用场景就是函数调用栈。我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。为了让你更好地理解，我们一块来看下这段代码的执行过程。123456789101112131415int main() &#123; int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(\"%d\", res); reuturn 0;&#125;int add(int x, int y) &#123; int sum = 0; sum = x + y; return sum;&#125;从代码中我们可以看出，main() 函数调用了 add() 函数，获取计算结果，并且与临时变量 a 相加，最后打印 res 的值。为了让你清晰地看到这个过程对应的函数栈里出栈、入栈的操作，我画了一张图。图中显示的是，在执行到 add() 函数时，函数调用栈的情况。栈在表达式求值中的应用我们再来看栈的另一个常见的应用场景，编译器如何利用栈来实现表达式求值。为了方便解释，我将算术表达式简化为只包含加减乘除四则运算，比如：34+139+44-12/3。对于这个四则运算，我们人脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事儿。如果换作你，让你来实现这样一个表达式求值的功能，你会怎么做呢？实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。我将 3+58-6 这个表达式的计算过程画成了一张图，你可以结合图来理解我刚讲的计算过程。这样用两个栈来解决的思路是不是非常巧妙？你有没有想到呢？栈在括号匹配中的应用除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[] ()[{}]}或[{()}([])]等都为合法格式，而{[}()]或[({)]为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。解答开篇好了，我想现在你已经完全理解了栈的概念。我们再回来看看开篇的思考题，如何实现浏览器的前进、后退功能？其实，用两个栈就可以非常完美地解决这个问题。我们使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。比如你顺序查看了 a，b，c 三个页面，我们就依次把 a，b，c 压入栈，这个时候，两个栈的数据就是当你通过浏览器的后退按钮，从页面 c 后退到页面 a 之后，我们就依次把 c 和 b 从栈 X 中弹出，并且依次放入到栈 Y。这个时候，两个栈的数据就是这个样子：这个时候你又想看页面 b，于是你又点击前进按钮回到 b 页面，我们就把 b 再从栈 Y 中出栈，放入栈 X 中。此时两个栈的数据是这个样子：这个时候，你通过页面 b 又跳转到新的页面 d 了，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。此时两个栈的数据这个样子：内容小结我们来回顾一下今天讲的内容。栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。除此之外，我们还讲了一种支持动态扩容的顺序栈，你需要重点掌握它的均摊时间复杂度分析方法。课后思考为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。我们都知道，JVM 内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"栈","slug":"栈","permalink":"cpeixin.cn/tags/%E6%A0%88/"}]},{"title":"数据结构-队列","slug":"数据结构-队列","date":"2016-08-13T10:30:21.000Z","updated":"2020-06-10T10:44:50.676Z","comments":true,"path":"2016/08/13/数据结构-队列/","link":"","permalink":"cpeixin.cn/2016/08/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97/","excerpt":"","text":"我们知道，CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学的内容，队列（queue）。如何理解“队列”队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的 “队列”。我们知道，栈只支持两个基本操作： 入栈 push()和出栈 pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。所以，队列跟栈一样，也是一种操作受限的线性表数据结构。队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。顺序队列和链式队列我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在队头删除元素，那究竟该如何实现一个队列呢？跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。顺序队列数组实现**我们先来思考一下用数组实现队列，对于栈来说，我们只需要一个栈顶指针就可以了。但是队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾。你可以结合下面这幅图来理解。当 a、b、c、d 依次入队之后，队列中的 head 指针指向下标为 0 的位置，tail 指针指向下标为 4 的位置。当我们调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置。你肯定已经发现了，在固定长度的数组中，随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。再向后加，就会产生数组越界的错误。可实际上，我们的队列在下标为0和1的地方还是空闲的。此时又不应该扩充数组，我们把这种现象叫做“假溢出”。否则会造成数组越界而遭致程序出错。这个问题该如何解决呢？你是否还记得，在数组那一节，我们也遇到过类似的问题，就是数组的删除操作会导致数组中的数据不连续。你还记得我们当时是怎么解决的吗？对，用数据搬移！但是，每次进行出队操作都相当于删除数组下标为 0 的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的 O(1) 变为 O(n)。能不能优化一下呢？实际上，我们在出队时可以不用搬移数据。如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作。借助这个思想，出队函数 dequeue() 保持不变，我们稍加改造一下入队函数 enqueue() 的实现，就可以轻松解决刚才的问题了。下面是具体的代码：我们先来看下基于数组，需要进行数据迁移的实现方法。12345678910111213141516171819202122232425262728293031from typing import Optionalclass ArrayQueue: def __init__(self, capacity: int): self._items = [] self._capacity = capacity self._head = 0 self._tail = 0 def enqueue(self, item: str) -&gt; bool: if self._tail == self._capacity: if self._head == 0: return False else: for i in range(0, self._tail - self._head): self._items[i] = self._items[i + self._head] self._tail = self._tail - self._head self._head = 0 self._items.insert(self._tail, item) self._tail += 1 return True def dequeue(self) -&gt; Optional[str]: if self._head != self._tail: item = self._items[self._head] self._head += 1 return item else: return None从代码中我们看到，当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。链式队列链表实现接下来，我们再来看下基于链表的队列实现方法。基于链表的实现，我们同样需要两个指针：head 指针和 tail 指针。它们分别指向链表的第一个结点和最后一个结点。如图所示，入队时，tail-&gt;next= new_node, tail = tail-&gt;next；出队时，head = head-&gt;next。123456789101112131415161718192021222324252627282930313233343536373839404142class Node: def __init__(self, data, next_node=None): self.data = data self.next = next_nodeclass LinkedQueue: def __init__(self): self.head = None self.tail = None def enqueue(self, data): new_node = Node(data) if self.tail: self.tail.next = new_node else: self.head = new_node self.tail = new_node def dequeue(self): if self.head is None: return False self.head = self.head.next \"\"\"移除头节点后，如果头节点为空，则尾节点也\"\"\" if not self.head: self.tail = Noneif __name__ == '__main__': linked_queue = LinkedQueue() linked_queue.enqueue(1) linked_queue.enqueue(2) linked_queue.enqueue(3) linked_queue.dequeue() linked_queue.dequeue() linked_queue.dequeue() print(linked_queue.tail)循环队列循环队列动态效果如下图：我们刚才用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？我们来看看循环队列的解决思路。循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。现在我们把首尾相连，扳成了一个环, 可以想像像钟表一样，enqueue和dequeue节点时，tail, head指针都是逆时针运动。我们可以看到，图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子：通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有 bug 的循环队列的实现代码，我个人觉得，最关键的是，确定好队空和队满的判定条件。在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。那针对循环队列，如何判断队空和队满呢？队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。我画了一张队列满的图，你可以看一下，试着总结一下规律。就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。多画几张队满的图，你就会发现，当队满时，(tail+1)%n=head。**你有没有发现，当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。12345678910111213141516171819202122232425262728293031323334class circular_queue(): def __init__(self, size): self.array = [] self.head = 0 self.tail = 0 self.size = size def enqueue(self, value): \"\"\"入队判满\"\"\" if (self.tail + 1) % self.size == self.head: return False self.array.append(value) \"\"\"入队，尾节点下标变化\"\"\" self.tail = (self.tail + 1) % self.size return True def dequeue(self): \"\"\"出队判空\"\"\" if self.head != self.tail: item = self.array[self.head] \"\"\"出队，头节点下标变化\"\"\" self.head = (self.head + 1) % self.size return itemif __name__ == '__main__': q = circular_queue(5) for i in range(5): q.enqueue(i) q.dequeue() q.dequeue() q.enqueue(1) print(q)队列满的表达式这里讲一下，这个表达式是怎么来的。在一般情况下，我们可以看出来，当队列满时，tail+1=head。但是，有个特殊情况，就是tail=n-1，而head=0时，这时候，tail+1=n，而head=0，所以用(tail+1)%n == n%n == 0。而且，tail+1最大的情况就是 n ，不会大于 n，这样，tail+1 除了最大情况，不然怎么余 n 都是 tail+1 本身，也就是 head。这样，表达式就出现了。阻塞队列和并发队列前面讲的内容理论比较多，看起来很难跟实际的项目开发扯上关系。确实，队列这种数据结构很基础，平时的业务开发不大可能从零实现一个队列，甚至都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。你应该已经发现了，上述的定义就是一个“生产者 - 消费者模型”！是的，我们可以使用阻塞队列，轻松实现一个“生产者 - 消费者模型”！可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。而且不仅如此，基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。前面我们讲了阻塞队列，在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。队列的知识就讲完了，我们现在回过来看下开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢？基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。除了前面讲到队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。**内容小结今天我们讲了一种跟栈很相似的数据结构，队列。关于队列，你能掌握下面的内容，这节就没问题了。队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想解决数据搬移的问题，我们就需要像环一样的循环队列。循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。除此之外，我们还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全队列应用在现实生活中Queue的应用也很广泛，最广泛的就是排队了，”先来后到” First come first service ，以及Queue这个单词就有排队的意思。还有，比如我们的播放器上的播放列表，我们的数据流对象，异步的数据传输结构(文件IO，管道通讯，套接字等)还有一些解决对共享资源的冲突访问，比如打印机的打印队列等。消息队列等。交通状况模拟，呼叫中心用户等待的时间的模拟等等。最后，关于队列，入队要判满，出队要判空**","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"队列","slug":"队列","permalink":"cpeixin.cn/tags/%E9%98%9F%E5%88%97/"}]},{"title":"数据结构-线性表","slug":"数据结构-线性表","date":"2016-08-12T02:39:21.000Z","updated":"2020-06-08T08:45:14.641Z","comments":true,"path":"2016/08/12/数据结构-线性表/","link":"","permalink":"cpeixin.cn/2016/08/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BA%BF%E6%80%A7%E8%A1%A8/","excerpt":"","text":"线性表及其逻辑结构线性表是最简单也是最常用的一种数据结构。英文字母表（A、B、…、Z）是一个线性表，表中每个英文字母是一个数据元素；成绩单是一个线性表，表中每一行是一个数据元素，每个数据元素又由学号、姓名、成绩等数据项组成。线性表的定义线性表是具有相同特性的数据元素的一个有限序列。线性表一般表示为：1 L = (a1, a2, …, ai,ai+1 ,…, an)线性表中元素在位置上是有序的，这种位置上有序性就是一种线性关系，用二元组表示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110L = (D, R) D = &#123;ai| 1≤i≤n, n≥0&#125; R = &#123;r&#125; r = &#123;&lt;ai, ai+1&gt; | 1≤i≤n-1&#125;``` ### 线性表的抽象数据类型描述 将线性表数据结构抽象成为一种数据类型，这个数据类型中包含数据元素、元素之间的关系、操作元素的基本算法。 对于基本数据类型（int、float、boolean等等）java已经帮我们实现了用于操作他们的基本运算， 我们需要基于这些基本运算，为我们封装的自定义数据类型提供操作它们的算法。 比如数组就是一种被抽象出来的线性表数据类型，数组自带很多基本方法用于操作数据元素。 Java中的List我们经常会使用到，但是很少关注其内部实现，List是一个接口， 里面定义了一些抽象的方法，其目的就是对线性表的抽象，其中的方法就是线性表的一些常用基本运算。 而对于线性表的不同**存储结构**其实现方式就有所不同了， 比如**ArrayList**是对线性表顺序存储结构的实现， **LinkedList**是线性表链式存储结构的实现等。 存储结构没有确定我们就不知道数据怎么存储，但是对于线性表这种逻辑结构中数据的基本操作我们可以预知， 无非就是获取长度、获取指定位置的数据、插入数据、删除数据等等操作，可以参考List。 对于本系列文章，只是对数据结构和一些常用算法学习，接下来的代码将选择性实现并分析算法。 对线性表的抽象数据类型描述如下：```javapublic interface IList&lt;T&gt; &#123; /** * 判断线性表是否为空 * @return */ boolean isEmpty(); /** * 获取长度 * @return */ int length(); /** * 将结点添加到指定序列的位置 * @param index * @param data * @return */ boolean add(int index, T data); /** * 将指定的元素追加到列表的末尾 * @param data * @return */ boolean add(T data); /** * 根据index移除元素 * @param index * @return */ T remove(int index); /** * 移除值为data的第一个结点 * @param data * @return */ boolean remove(T data); /** * 移除所有值为data的结点 * @param data * @return */ boolean removeAll(T data); /** * 清空表 */ void clear(); /** * 设置指定序列元素的值 * @param index * @param data * @return */ T set(int index, T data); /** * 是否包含值为data的结点 * @param data * @return */ boolean contains(T data); /** * 根据值查询索引 * @param data * @return */ int indexOf(T data); /** * 根据data值查询最后一次出现在表中的索引 * @param data * @return */ int lastIndexOf(T data); /** * 获取指定序列的元素 * @param index * @return */ T get(int index); /** * 输出格式 * @return */ String toString();&#125;线性表的顺序存储结构顺序表把线性表中的所有元素按照其逻辑顺序依次存储在计算机存储器中指定存储位置开始的一块连续的存储空间中。在Java中创建一个数组对象就是分配了一块可供用户使用的连续的存储空间，该存储空间的起始位置就是由数组名表示的地址常量。线性表的顺序存储结构是利用数组来实现的。在Java中，我们通常利用下面的方式来使用数组：123int[] array = new int[]&#123;1,2,3&#125;; //创建一个数组Array.getInt(array, 0); //获取数组中序列为0的元素Array.set(array, 0, 1); //设置序列为0的元素值为1Array这种方式创建的数组是固定长度的，其容量无法修改，当array被创建出来的时候，系统只为其分配3个存储空间，所以我们无法对其进行添加和删除操作。Array这个类里面提供了很多方法用于操作数组，这些方法都是静态的，所以Array是一个用于操作数组的工具类，这个类提供的方法只有两种：get和set，所以只能获取和设置数组中的元素，然后对于这两种操作，我们通常使用array[i]、array[i] = 0的简化方式，所以Array这个类用的比较少。另外一种数组ArrayList，其内部维护了一个数组，所以本质上也是数组，其操作都是对数组的操作，与上述数组不同的是，ArrayList是一种可变长度的数组。既然数组创建时就已经分配了存储空间，为什么ArrayList是长度可变的呢？长度可变意味着可以从数组中添加、删除元素，向ArrayList中添加数据时，实际上是创建了一个新的数组，将原数组中元素一个个复制到新数组后，将新元素添加进来。如果ArrayList仅仅做了这么简单的操作，那他就不应该出现了。ArrayList中的数组长度是大于等于其元素个数的，当执行add()操作时首先会检查数组长度是否够用，只有当数组长度不够用时才会创建新的数组，由于创建新数组意味着老数据的搬迁，所以这个机制也算是利用空间换取时间上的效率。但是如果添加操作并不是尾部添加，而是头部或者中间位置插入，也避免不了元素位置移动。顺序表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229public class LinearArray&lt;T&gt; implements IList&lt;T&gt;&#123; private Object[] datas; /** * 通过给定的数组 建立顺序表 * @param objs * @return */ public static &lt;T&gt; LinearArray&lt;T&gt; createArray(T[] objs)&#123; LinearArray&lt;T&gt; array = new LinearArray(); array.datas = new Object[objs.length]; for(int i = 0; i&lt;objs.length; i++) array.datas[i] = objs[i]; return array; &#125; private LinearArray()&#123; &#125; @Override public boolean isEmpty() &#123; return datas.length == 0; &#125; @Override public int length() &#123; return datas.length; &#125; /** * 获取指定位置的元素 * 分析：时间复杂度O(1) * 从顺序表中检索值是简单高效的，因为顺序表内部采用数组作为容器，数组可直接通过索引值访问元素 */ @Override public T get(int index) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); return (T) datas[index]; &#125; /** * 为指定索引的结点设置值 * 分析：时间复杂度O(1) */ @Override public T set(int index, T data) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; datas[index] = data; return oldValue; &#125; /** * 判断是否包含某值只需要判断该值有没有出现过 * 分析：时间复杂度O(n) */ @Override public boolean contains(T data) &#123; return indexOf(data) &gt;= 0; &#125; /** * 获取某值第一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int indexOf(T data) &#123; if (data == null) &#123; for (int i = 0; i &lt; datas.length; i++) if (datas[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; datas.length; i++) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 获取某值最后一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int lastIndexOf(T data) &#123; if (data == null) &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (datas[i]==null) return i; &#125; else &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 指定位置插入元素 * 分析：时间复杂度O(n) * 在数组中插入元素时，需要创建一个比原数组容量大1的新数组， * 将原数组中(0,index-1)位置的元素拷贝到新数组，指定新数组index位置元素值为新值， * 继续将原数组(index, length-1)的元素拷贝到新数组 * @param index * @param data * @return */ @Override public boolean add(int index, T data) &#123; if (index &gt; datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); destination[index] = data; System.arraycopy(datas, index, destination, index + 1, datas.length - index); datas = destination; return true; &#125; /** * 在顺序表末尾处插入元素 * 分析：时间复杂度O(n) * 同上面一样，也需要创建新数组 * @param data * @return */ @Override public boolean add(T data) &#123; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, datas.length); destination[datas.length] = data; datas = destination; return true; &#125; /** * 有序表添加元素 * @param data * @return */ public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true; &#125; /** * 移除指定索引的元素 * 分析：时间复杂度O(n) * 此处由于数组元素数量-1，所以需要创建新数组。 * ArrayList由于是动态数组（list.size()≠data.length），所以只需要将删除的元素之后的前移一位 * @param index * @return */ @Override public T remove(int index) &#123; if (index &gt;= datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; fastRemove(index); return oldValue; &#125; /** * 删除指定值的第一个元素 * @param data * @return */ @Override public boolean remove(T data) &#123; if (data == null) &#123; for (int index = 0; index &lt; datas.length; index++) if (datas[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; datas.length; index++) if (data.equals(datas[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /** * 移除指定序列的元素 * @param index */ private void fastRemove(int index) &#123; Object destination[] = new Object[datas.length - 1]; System.arraycopy(datas, 0, destination, 0, index); System.arraycopy(datas, index+1, destination, index, datas.length - index-1); datas = destination; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; @Override public void clear() &#123; datas = new Object[]&#123;&#125;; &#125; @Override public String toString() &#123; if(isEmpty()) return \"\"; String str = \"[\"; for(int i = 0; i&lt;datas.length; i++)&#123; str += (datas[i]+\", \"); &#125; str = str.substring(0, str.lastIndexOf(\", \")); return str+\"]\"; &#125;&#125;算法分析：插入元素：删除元素：**线性表的链式存储结构顺序表必须占用一整块事先分配大小固定的存储空间，这样不便于存储空间的管理。为此提出了可以实现存储空间动态管理的链式存储方式–链表。链表在链式存储中，每个存储结点不仅包含元素本身的信息（数据域），还包含元素之间逻辑关系的信息，即一个结点中包含有直接后继结点的地址信息，这称为指针域。这样可以通过一个结点的指针域方便的找到后继结点的位置。由于顺序表中每个元素至多只有一个直接前驱元素和一个直接后继元素。当采用链式存储时，一种最简单也最常用的方法是：在每个结点中除包含数据域外，只设置一个指针域用以指向其直接后继结点，这种构成的链接表称为线性单向链接表，简称单链表。另一种方法是，在每个结点中除包含数值域外，设置两个指针域，分别用以指向直接前驱结点和直接后继结点，这样构成的链接表称为线性双向链接表，简称双链表。单链表当访问一个结点后，只能接着访问它的直接后继结点，而无法访问他的直接前驱结点。双链表则既可以依次向后访问每个结点，也可以依次向前访问每个结点。单链表结点元素类型定义：1234public class LNode &#123; protected LNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;双链表结点元素类型定义：12345public class DNode &#123; protected DNode prior; //指针域，指向直接前驱结点 protected DNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;在顺序表中，逻辑上相邻的元素，对应的存储位置也相邻，所以进行插入或删除操作时，通常需要平均移动半个表的元素，这是相当费时的操作。在链表中，每个结点存储位置可以任意安排，不必要求相邻，插入或删除操作只需要修改相关结点的指针域即可，方便省时。对于单链表，如果要在结点p之前插入一个新结点，由于通过p并不能找到其前驱结点，我们需要从链表表头遍历至p的前驱结点然后进行插入操作，这样时间复杂度就是O(n)，而顺序表插入删除结点时间复杂度也是O(n)，那为什么说链表插入删除操作更加高效呢？因为单链表插入删除操作所消耗的时间主要在于查找前驱结点，这个查找工作的时间复杂度为O(n)，而真正超如删除时间为O(1)还有顺序表需要移动结点，移动结点通常比单纯的查找更加费时，链表不需要连续的空间，不需要扩容创建新表，所以同样时间复杂度O(n)，链表更适合插入和删除操作。对于遍历查找前驱结点的问题，在双链表中就能很好的解决，双链表在已知某结点的插入和删除操作时间复杂度是O(1)。由于链表的每个结点带有指针域，从存储密度来讲，这是不经济的。所谓存储密度是指结点数据本身所占存储量和整改结点结构所占存储量之比。3.2 单链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534public class LinkList&lt;T&gt; implements IList&lt;T&gt;&#123; public LNode&lt;T&gt; head; //单链表开始结点 /** * 1.1 创建单链表（头插法：倒序） * 解：遍历数组，创建新结点，新结点的指针域指向头结点，让新结点作为头结点 * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListF(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; for (T obj : array) &#123; LNode&lt;T&gt; node = new LNode(); node.data = obj; node.next = llist.head; llist.head = node; &#125; &#125; return llist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 解： * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListR(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; llist.head = new LNode(); llist.head.data = array[0]; LNode&lt;T&gt; temp = llist.head; for(int i = 1; i &lt; array.length; i++)&#123; LNode node = new LNode(); node.data = array[i]; temp.next = node; temp = node; &#125; &#125; return llist; &#125; /** * 判断单链表是否为空表 * 时间复杂度O(1) * @return */ @Override public boolean isEmpty() &#123; return head==null; &#125; /** * 4 获取单链表长度 * 时间复杂度O(n) * @return */ @Override public int length() &#123; if(head==null) return 0; int l = 1; LNode node = head; while(node.next!=null) &#123; l++; node = node.next; &#125; return l; &#125; @Override public void clear() &#123; head = null; &#125; @Override public T set(int index, T data) &#123; return null; &#125; @Override public boolean contains(T data) &#123; return false; &#125; @Override public T get(int index) &#123; return getNode(index).data; &#125; /** * 6.1 获取指定索引的结点 * 时间复杂度O(n) * @param index * @return */ public LNode&lt;T&gt; getNode(int index)&#123; LNode node = head; int j = 0; while(j &lt; index &amp;&amp; node!=null)&#123; j++; node = node.next; &#125; return node; &#125; /** * 6.2 获取指定数据值结点的索引 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @return */ @Override public int indexOf(T data) &#123; if(head==null) return -1; //没有此结点 LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) return j; j++; node = node.next; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; if(head==null) return -1; int index = -1; LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) &#123; index = j; &#125; j++; node = node.next; &#125; return index; &#125; /** * 6.3 单链表中的倒数第k个结点（k &gt; 0） * 解：先找到顺数第k个结点，然后使用前后指针移动到结尾即可 * 时间复杂度O(n) 空间复杂度O(1) * @param k * @return */ public LNode&lt;T&gt; getReNode(int k)&#123; if(head==null) return null; int len = length(); if(k &gt; len) return null; LNode target = head; LNode next = head; for(int i=0;i &lt; k;i++) next = next.next; while(next!=null)&#123; target = target.next; next = next.next; &#125; return target; &#125; /** * 6.4 查找单链表的中间结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode getMiddleNode()&#123; if(head == null|| head.next == null) return head; LNode target = head; LNode temp = head; while(temp != null &amp;&amp; temp.next != null)&#123; target = target.next; temp = temp.next.next; &#125; return target; &#125; /** * 2.1 将单链表合并为一个单链表 * 解：遍历第一个表，用其尾结点指向第二个表头结点 * 时间复杂度O(n) * @return */ public static LNode mergeList(LNode head1, LNode head2)&#123; if(head1==null) return head2; if(head2==null) return head1; LNode loop = head1; while(loop.next!=null) //找到list1尾结点 loop = loop.next; loop.next = head2; //将list1尾结点指向list2头结点 return head1; &#125; /** * 2.1 通过递归，合并两个有序的单链表head1和head2 * * 解：两个指针分别指向两个头结点，比较两个结点大小， * 小的结点指向下一次比较结果（两者中较小），最终返回第一次递归的最小结点 * @param head1 * @param head2 * @return */ public static LNode mergeSortedListRec(LNode head1, LNode head2)&#123; if(head1==null)return head2; if(head2==null)return head1; if (((int)head1.data)&gt;((int)head2.data)) &#123; head2.next = mergeSortedListRec(head2.next, head1); return head2; &#125; else &#123; head1.next = mergeSortedListRec(head1.next, head2); return head1; &#125; &#125; /** * 3.1 循环的方式将单链表反转 * 时间复杂度O(n) 空间复杂度O(1) */ public void reverseListByLoop() &#123; if (head == null || head.next == null) return; LNode pre = null; LNode nex = null; while (head != null) &#123; nex = head.next; head.next = pre; pre = head; head = nex; &#125; head = pre; &#125; /** * 3.2 递归的方式将单链表反转,返回反转后的链表头结点 * 时间复杂度O(n) 空间复杂度O(n) */ public LNode reverseListByRec(LNode head) &#123; if(head==null||head.next==null) return head; LNode reHead = reverseListByRec(head.next); head.next.next = head; head.next = null; return reHead; &#125; /** * 5.1 获取单链表字符串表示 * 时间复杂度O(n) */ @Override public String toString() &#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; return buffer.toString(); &#125; public static String display(LNode head)&#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(\" -&gt; \"+node.data); node = node.next; &#125; return buffer.toString(); &#125; /** * 5.2 用栈的方式获取单链表从尾到头倒叙字符串表示 * 解：由于栈具有先进后出的特性，现将表中的元素放入栈中，然后取出就倒序了 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public String displayReverseStack()&#123; if(head == null) return \"\"; Stack &lt;LNode&gt; stack = new Stack &lt; &gt;(); //堆栈 先进先出 LNode head = this.head; while(head!=null)&#123; stack.push(head); head=head.next; &#125; StringBuffer buffer = new StringBuffer(); while(!stack.isEmpty())&#123; //pop()移除堆栈顶部的对象，并将该对象作为该函数的值返回。 buffer.append(\" -&gt; \"+stack.pop().data); &#125; return buffer.toString(); &#125; /** * 5.3 用递归的方式获取单链表从尾到头倒叙字符串表示 * @return */ public void displayReverseRec(StringBuffer buffer, LNode head)&#123; if(head==null) return; displayReverseRec(buffer, head.next); buffer.append(\" -&gt; \"); buffer.append(head.data); &#125; /** * 7.1 插入结点 * 解：先找到第i-1个结点，让创建的新结点的指针域指向第i-1结点指针域指向的结点， * 然后将i-1结点的指针域指向新结点 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @param index */ @Override public boolean add(int index, T data) &#123; if(index==0)&#123; //插入为头结点 LNode temp = new LNode(); temp.next = head; return true; &#125; int j = 0; LNode node = head; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return false; LNode temp = new LNode(); //创建新结点 temp.data = data; temp.next = node.next; //新结点插入到Index-1结点之后 node.next = temp; return true; &#125; @Override public boolean add(T data) &#123; LNode node = head; while(node!=null &amp;&amp; node.next!=null) //找到尾结点 node = node.next; LNode temp = new LNode(); //创建新结点 temp.data = data; node.next = temp; return false; &#125; @Override public T remove(int index) &#123; LNode&lt;T&gt; node = deleteNode(index); return node==null?null:node.data; &#125; /** * 7.2 删除结点 * 解：让被删除的结点前一个结点的指针域指向后一个结点指针域 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode deleteNode(int index)&#123; LNode node = head; if(index==0)&#123; //删除头结点 if(node==null) return null; head = node.next; return node; &#125; //非头结点 int j = 0; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return null; LNode delete = node.next; if(delete==null) return null; //不存在第index个结点 node.next = delete.next; return delete; &#125; @Override public boolean remove(T data) &#123; return false; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; /** * 7.3 给出一单链表头指针head和一节点指针delete，要求O(1)时间复杂度删除节点delete * 解：将delete节点value值与它下个节点的值互换的方法， * 但是如果delete是最后一个节点，需要特殊处理，但是总得复杂度还是O(1) * @return */ public static void deleteNode(LNode head, LNode delete)&#123; if(delete==null) return; //首先处理delete节点为最后一个节点的情况 if(delete.next==null)&#123; if(head==delete) //只有一个结点 head = null; else&#123; //删除尾结点 LNode temp = head; while(temp.next!=delete) temp = temp.next; temp.next=null; &#125; &#125; else&#123; delete.data = delete.next.data; delete.next = delete.next.next; &#125; return; &#125; /** * 8.1 判断一个单链表中是否有环 * 解：使用快慢指针方法，如果存在环，两个指针必定指向同一结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public static boolean hasCycle(LNode head)&#123; LNode p1 = head; LNode p2 = head; while(p1!=null &amp;&amp; p2!=null)&#123; p1 = p1.next; //一次跳一步 p2 = p2.next.next; //一次跳两步 if(p2 == p1) return true; &#125; return false; &#125; /** * 8.2、已知一个单链表中存在环，求进入环中的第一个节点 * 利用hashmap，不要用ArrayList，因为判断ArrayList是否包含某个元素的效率不高 * @param head * @return */ public static LNode getFirstNodeInCycleHashMap(LNode head)&#123; LNode target = null; HashMap&lt;LNode,Boolean &gt; map=new HashMap&lt; &gt;(); while(head != null)&#123; if(map.containsKey(head)) &#123; target = head; break; &#125; else &#123; map.put(head, true); head = head.next; &#125; &#125; return target; &#125; /** * 8.3、已知一个单链表中存在环，求进入环中的第一个节点,不用hashmap * 用快慢指针，与判断一个单链表中是否有环一样，找到快慢指针第一次相交的节点， * 此时这个节点距离环开始节点的长度和链表头距离环开始的节点的长度相等 * 参考 https://www.cnblogs.com/fankongkong/p/7007869.html * @param head * @return */ public static LNode getFirstNodeInCycle(LNode head)&#123; LNode fast = head; LNode slow = head; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; if(slow == fast) break; &#125; if(fast == null||fast.next == null) return null;//判断是否包含环 //相遇节点距离环开始节点的长度和链表投距离环开始的节点的长度相等 slow=head; while(slow!=fast)&#123; slow=slow.next; fast=fast.next; &#125;//同步走 return slow; &#125; /** * 9、判断两个单链表是否相交,如果相交返回第一个节点，否则返回null * ①、暴力遍历两个表，是否有相同的结点(时间复杂度O(n²)) * ②、第一个表的尾结点指向第二个表头结点，然后判断第二个表是否存在环，但不容易找出交点（时间复杂度O(n)） * ③、两个链表相交，必然会经过同一个结点，这个结点的后继结点也是相同的（链表结点只有一个指针域，后继结点只能有一个）， * 所以他们的尾结点必然相同。两个链表相交，只能是前面的结点不同，所以，砍掉较长链表的差值后同步遍历，判断结点是否相同，相同的就是交点了。 * 时间复杂度（时间复杂度O(n)） * @param list1 * @param list2 * @return 交点 */ public static LNode isIntersect(LinkList list1, LinkList list2)&#123; LNode head1 = list1.head; LNode head2 = list2.head; if(head1==null || head2==null)return null; int len1 = list1.length(); int len2 = list2.length(); //砍掉较长链表的差值 if(len1 &gt;= len2)&#123; for(int i=0;i &lt; len1-len2;i++)&#123; head1=head1.next; &#125; &#125;else&#123; for(int i=0;i &lt; len2-len1;i++)&#123; head2=head2.next; &#125; &#125; //同步遍历 while(head1 != null&amp;&amp;head2 != null)&#123; if(head1 == head2) return head1; head1=head1.next; head2=head2.next; &#125; return null; &#125;&#125;算法分析判断一个单链表中是否有环我们可以通过HashMap判断，遍历结点，将结点值放入HashMap，如果某一刻发现当前结点在map中已经存在，则存在环，并且此结点正是环的入口，此算法见8.2方法。但是有一种问法是不通过任何其他数据结构怎么判断单链表是否存在环。这样我们可利用的就只有单链表本身，一种解法是通过快慢指针，遍历链表，一个指针跳一步（慢指针步长为1），另一个指针跳两步（快指针步长为2），如果存在环，这两个指针必将在某一刻指向同一结点，假设此时慢指针跳了n步，则快指针跳的步数为n/2步：判断两个单链表是否相交由于单链表的特性（只有一个指针域），如果两个表相交，那必定是Y形相交，不会是X形相交，如图所示。两个单链表后面的结点相同，不同的部分只有前面，砍掉较长的链表的前面部分，然后两个链表同步遍历，必将指向同一个结点，这个结点就是交点：双链表双链表中每个结点有两个指针域，一个指向其直接后继结点，一个指向其直接前驱结点。建立双链表也有两种方法，头插法和尾插法，这与创建单链表过程相似。在双链表中，有些算法如求长度、取元素值、查找元素等算法与单链表中相应算法是相同的。但是在单链表中，进行结点插入和删除时涉及前后结点的一个指针域的变化，而双链表中结点的插入和删除操作涉及前后结点的两个指针域的变化。java中LinkedList正是对双链表的实现，算法可参考此类。双链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327public class DLinkList&lt;T&gt; implements IList&lt;T&gt;&#123; transient DNode&lt;T&gt; first; //双链表开始结点 transient DNode&lt;T&gt; last; //双链表末端结点 private int size; //结点数 /** * 创建单链表（头插法：倒序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListF(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; dlist.size = array.length; for (T obj : array) &#123; DNode&lt;T&gt; node = new DNode(); node.data = obj; node.next = dlist.first; if(dlist.first!=null) dlist.first.prior = node; //相比单链表多了此步 else dlist.last = node; dlist.first = node; &#125; &#125; return dlist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListR(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; dlist.size = array.length; dlist.first = new DNode&lt;T&gt;(); dlist.first.data = array[0]; dlist.last = dlist.first; for(int i = 1; i &lt; array.length; i++)&#123; DNode&lt;T&gt; node = new DNode(); node.data = array[i]; dlist.last.next = node; node.prior = dlist.last; //相比单链表多了此步 dlist.last = node; &#125; &#125; return dlist; &#125; @Override public boolean isEmpty() &#123; return size==0; &#125; @Override public int length() &#123; return size; &#125; /**2 添加结点*/ @Override public boolean add(int index, T data) &#123; if(index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; newNode = new DNode(); newNode.data = data; if (index == size) &#123; //在末尾添加结点不需要遍历 final DNode&lt;T&gt; l = last; if (l == null) //空表 first = newNode; else &#123; l.next = newNode; newNode.prior = l; &#125; last = newNode; size++; &#125; else &#123; //其他位置添加结点需要遍历找到index位置的结点 DNode&lt;T&gt; indexNode = getNode(index); DNode&lt;T&gt; pred = indexNode.prior; newNode.prior = pred; newNode.next = indexNode; indexNode.prior = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; &#125; return false; &#125; @Override public boolean add(T data) &#123; return add(size, data); &#125; /**3 删除结点*/ @Override public T remove(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return unlink(getNode(index)); &#125; @Override public boolean remove(T data) &#123; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); return true; &#125; &#125; &#125; return false; &#125; @Override public boolean removeAll(T data) &#123; boolean result = false; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); result = true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); result = true; &#125; &#125; &#125; return result; &#125; /** * 将指定的结点解除链接 * @param x * @return */ private T unlink(DNode&lt;T&gt; x) &#123; // assert x != null; final T element = x.data; final DNode&lt;T&gt; next = x.next; final DNode&lt;T&gt; prev = x.prior; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prior = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prior = prev; x.next = null; &#125; x.data = null; size--; return element; &#125; /** * 清空 */ @Override public void clear() &#123; for (DNode&lt;T&gt; x = first; x != null; ) &#123; DNode&lt;T&gt; next = x.next; x.data = null; x.next = null; x.prior = null; x = next; &#125; first = last = null; size = 0; &#125; /** * 设置结点值 * @param index * @param data * @return */ @Override public T set(int index, T data) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; x = getNode(index); T oldVal = x.data; x.data = data; return oldVal; &#125; /** * 判断是否存在结点值 * @param data * @return */ @Override public boolean contains(T data) &#123; return indexOf(data) != -1; &#125; /** * 检索结点值 * @param data * @return */ @Override public int indexOf(T data) &#123; int index = 0; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) return index; index++; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) return index; index++; &#125; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; int index = size; if (data == null) &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (x.data == null) return index; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (data.equals(x.data)) return index; &#125; &#125; return -1; &#125; @Override public T get(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return getNode(index).data; &#125; /** * 获取指定索引的结点 * 解：由于双链表能双向检索，判断index离开始结点近还是终端结点近，从近的一段开始遍历 * 时间复杂度O(n) * @param index * @return */ private DNode&lt;T&gt; getNode(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123; DNode&lt;T&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; DNode&lt;T&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prior; return x; &#125; &#125; /** * 倒序 * 遍历每个结点，让node.next = node.prior; node.prior = (node.next此值需要体现保存); */ public void reverse()&#123; last = first; //反转后终端结点=开始结点 DNode now = first; DNode next; while(now!=null)&#123; next = now.next; //保存当前结点的后继结点 now.next = now.prior; now.prior = next; first = now; now = next; &#125; &#125; @Override public String toString() &#123; if(size == 0) return \"\"; DNode node = first; StringBuffer buffer = new StringBuffer(); buffer.append(\" \"); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; buffer.append(\"next\\npre\"); node = last; int start = buffer.length(); LogUtil.i(getClass().getSimpleName(), \"buffer长度：\"+buffer.length()); while(node != null)&#123; buffer.insert(start ,\" &lt;- \"+node.data); node = node.prior; &#125; return buffer.toString(); &#125;&#125;算法分析双链表与单链表不同之处在于，双链表能从两端依次访问各个结点。单链表相对于顺序表优点是插入、删除数据更方便，但是访问结点需要遍历，时间复杂度为O(n)；双链表就是在单链表基础上做了一个优化，使得访问结点更加便捷（从两端），这样从近的一端出发时间复杂度变为O(n/2)，虽然不是指数阶的区别，但也算是优化。双链表在插入、删除结点时逻辑比单链表稍麻烦：循环链表循环链表是另一种形式的链式存储结构，它的特点是表中尾结点的指针域不再是空，而是指向头结点，整个链表形成一个环。由此，从表中任意一结点出发均可找到链表中其他结点。如图所示为带头结点的循环单链表和循环双链表：有序表所谓有序表，是指所有结点元素值以递增或递减方式排列的线性表，并规定有序表中不存在元素值相同的结点。有序表可以采用顺序表和链表进行存储，若以顺序表存储有序表，其算法除了add(T data)以外，其他均与前面说的顺序表对应的运算相同。有序表的add(T data)操作不是插入到末尾，而需要遍历比较大小后插入相应位置。123456789101112131415public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true;&#125;最后 想强调的是 由于顺序表结构的底层实现借助的就是数组，因此对于初学者来说，可以把顺序表完全等价为数组，但实则不是这样。数据结构是研究数据存储方式的一门学科，它囊括的都是各种存储结构，而数组只是各种编程语言中的基本数据类型，并不属于数据结构的范畴。数据结构 - 线性表线性表及其逻辑结构线性表是最简单也是最常用的一种数据结构。英文字母表（A、B、…、Z）是一个线性表，表中每个英文字母是一个数据元素；成绩单是一个线性表，表中每一行是一个数据元素，每个数据元素又由学号、姓名、成绩等数据项组成。线性表的定义线性表是具有相同特性的数据元素的一个有限序列。线性表一般表示为：1 L = (a1, a2, …, ai,ai+1 ,…, an)线性表中元素在位置上是有序的，这种位置上有序性就是一种线性关系，用二元组表示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110L = (D, R) D = &#123;ai| 1≤i≤n, n≥0&#125; R = &#123;r&#125; r = &#123;&lt;ai, ai+1&gt; | 1≤i≤n-1&#125;``` ### 线性表的抽象数据类型描述 将线性表数据结构抽象成为一种数据类型，这个数据类型中包含数据元素、元素之间的关系、操作元素的基本算法。 对于基本数据类型（int、float、boolean等等）java已经帮我们实现了用于操作他们的基本运算， 我们需要基于这些基本运算，为我们封装的自定义数据类型提供操作它们的算法。 比如数组就是一种被抽象出来的线性表数据类型，数组自带很多基本方法用于操作数据元素。 Java中的List我们经常会使用到，但是很少关注其内部实现，List是一个接口， 里面定义了一些抽象的方法，其目的就是对线性表的抽象，其中的方法就是线性表的一些常用基本运算。 而对于线性表的不同**存储结构**其实现方式就有所不同了， 比如**ArrayList**是对线性表顺序存储结构的实现， **LinkedList**是线性表链式存储结构的实现等。 存储结构没有确定我们就不知道数据怎么存储，但是对于线性表这种逻辑结构中数据的基本操作我们可以预知， 无非就是获取长度、获取指定位置的数据、插入数据、删除数据等等操作，可以参考List。 对于本系列文章，只是对数据结构和一些常用算法学习，接下来的代码将选择性实现并分析算法。 对线性表的抽象数据类型描述如下：```javapublic interface IList&lt;T&gt; &#123; /** * 判断线性表是否为空 * @return */ boolean isEmpty(); /** * 获取长度 * @return */ int length(); /** * 将结点添加到指定序列的位置 * @param index * @param data * @return */ boolean add(int index, T data); /** * 将指定的元素追加到列表的末尾 * @param data * @return */ boolean add(T data); /** * 根据index移除元素 * @param index * @return */ T remove(int index); /** * 移除值为data的第一个结点 * @param data * @return */ boolean remove(T data); /** * 移除所有值为data的结点 * @param data * @return */ boolean removeAll(T data); /** * 清空表 */ void clear(); /** * 设置指定序列元素的值 * @param index * @param data * @return */ T set(int index, T data); /** * 是否包含值为data的结点 * @param data * @return */ boolean contains(T data); /** * 根据值查询索引 * @param data * @return */ int indexOf(T data); /** * 根据data值查询最后一次出现在表中的索引 * @param data * @return */ int lastIndexOf(T data); /** * 获取指定序列的元素 * @param index * @return */ T get(int index); /** * 输出格式 * @return */ String toString();&#125;线性表的顺序存储结构顺序表把线性表中的所有元素按照其逻辑顺序依次存储在计算机存储器中指定存储位置开始的一块连续的存储空间中。在Java中创建一个数组对象就是分配了一块可供用户使用的连续的存储空间，该存储空间的起始位置就是由数组名表示的地址常量。线性表的顺序存储结构是利用数组来实现的。在Java中，我们通常利用下面的方式来使用数组：123int[] array = new int[]&#123;1,2,3&#125;; //创建一个数组Array.getInt(array, 0); //获取数组中序列为0的元素Array.set(array, 0, 1); //设置序列为0的元素值为1Array这种方式创建的数组是固定长度的，其容量无法修改，当array被创建出来的时候，系统只为其分配3个存储空间，所以我们无法对其进行添加和删除操作。Array这个类里面提供了很多方法用于操作数组，这些方法都是静态的，所以Array是一个用于操作数组的工具类，这个类提供的方法只有两种：get和set，所以只能获取和设置数组中的元素，然后对于这两种操作，我们通常使用array[i]、array[i] = 0的简化方式，所以Array这个类用的比较少。另外一种数组ArrayList，其内部维护了一个数组，所以本质上也是数组，其操作都是对数组的操作，与上述数组不同的是，ArrayList是一种可变长度的数组。既然数组创建时就已经分配了存储空间，为什么ArrayList是长度可变的呢？长度可变意味着可以从数组中添加、删除元素，向ArrayList中添加数据时，实际上是创建了一个新的数组，将原数组中元素一个个复制到新数组后，将新元素添加进来。如果ArrayList仅仅做了这么简单的操作，那他就不应该出现了。ArrayList中的数组长度是大于等于其元素个数的，当执行add()操作时首先会检查数组长度是否够用，只有当数组长度不够用时才会创建新的数组，由于创建新数组意味着老数据的搬迁，所以这个机制也算是利用空间换取时间上的效率。但是如果添加操作并不是尾部添加，而是头部或者中间位置插入，也避免不了元素位置移动。顺序表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229public class LinearArray&lt;T&gt; implements IList&lt;T&gt;&#123; private Object[] datas; /** * 通过给定的数组 建立顺序表 * @param objs * @return */ public static &lt;T&gt; LinearArray&lt;T&gt; createArray(T[] objs)&#123; LinearArray&lt;T&gt; array = new LinearArray(); array.datas = new Object[objs.length]; for(int i = 0; i&lt;objs.length; i++) array.datas[i] = objs[i]; return array; &#125; private LinearArray()&#123; &#125; @Override public boolean isEmpty() &#123; return datas.length == 0; &#125; @Override public int length() &#123; return datas.length; &#125; /** * 获取指定位置的元素 * 分析：时间复杂度O(1) * 从顺序表中检索值是简单高效的，因为顺序表内部采用数组作为容器，数组可直接通过索引值访问元素 */ @Override public T get(int index) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); return (T) datas[index]; &#125; /** * 为指定索引的结点设置值 * 分析：时间复杂度O(1) */ @Override public T set(int index, T data) &#123; if (index&lt;0 || index &gt;= datas.length) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; datas[index] = data; return oldValue; &#125; /** * 判断是否包含某值只需要判断该值有没有出现过 * 分析：时间复杂度O(n) */ @Override public boolean contains(T data) &#123; return indexOf(data) &gt;= 0; &#125; /** * 获取某值第一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int indexOf(T data) &#123; if (data == null) &#123; for (int i = 0; i &lt; datas.length; i++) if (datas[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; datas.length; i++) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 获取某值最后一次出现的索引 * 分析：时间复杂度O(n) */ @Override public int lastIndexOf(T data) &#123; if (data == null) &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (datas[i]==null) return i; &#125; else &#123; for (int i = datas.length-1; i &gt;= 0; i--) if (data.equals(datas[i])) return i; &#125; return -1; &#125; /** * 指定位置插入元素 * 分析：时间复杂度O(n) * 在数组中插入元素时，需要创建一个比原数组容量大1的新数组， * 将原数组中(0,index-1)位置的元素拷贝到新数组，指定新数组index位置元素值为新值， * 继续将原数组(index, length-1)的元素拷贝到新数组 * @param index * @param data * @return */ @Override public boolean add(int index, T data) &#123; if (index &gt; datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); destination[index] = data; System.arraycopy(datas, index, destination, index + 1, datas.length - index); datas = destination; return true; &#125; /** * 在顺序表末尾处插入元素 * 分析：时间复杂度O(n) * 同上面一样，也需要创建新数组 * @param data * @return */ @Override public boolean add(T data) &#123; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, datas.length); destination[datas.length] = data; datas = destination; return true; &#125; /** * 有序表添加元素 * @param data * @return */ public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true; &#125; /** * 移除指定索引的元素 * 分析：时间复杂度O(n) * 此处由于数组元素数量-1，所以需要创建新数组。 * ArrayList由于是动态数组（list.size()≠data.length），所以只需要将删除的元素之后的前移一位 * @param index * @return */ @Override public T remove(int index) &#123; if (index &gt;= datas.length || index &lt; 0) throw new IndexOutOfBoundsException(); T oldValue = (T) datas[index]; fastRemove(index); return oldValue; &#125; /** * 删除指定值的第一个元素 * @param data * @return */ @Override public boolean remove(T data) &#123; if (data == null) &#123; for (int index = 0; index &lt; datas.length; index++) if (datas[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; datas.length; index++) if (data.equals(datas[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /** * 移除指定序列的元素 * @param index */ private void fastRemove(int index) &#123; Object destination[] = new Object[datas.length - 1]; System.arraycopy(datas, 0, destination, 0, index); System.arraycopy(datas, index+1, destination, index, datas.length - index-1); datas = destination; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; @Override public void clear() &#123; datas = new Object[]&#123;&#125;; &#125; @Override public String toString() &#123; if(isEmpty()) return \"\"; String str = \"[\"; for(int i = 0; i&lt;datas.length; i++)&#123; str += (datas[i]+\", \"); &#125; str = str.substring(0, str.lastIndexOf(\", \")); return str+\"]\"; &#125;&#125;算法分析：插入元素：删除元素：**线性表的链式存储结构顺序表必须占用一整块事先分配大小固定的存储空间，这样不便于存储空间的管理。为此提出了可以实现存储空间动态管理的链式存储方式–链表。链表在链式存储中，每个存储结点不仅包含元素本身的信息（数据域），还包含元素之间逻辑关系的信息，即一个结点中包含有直接后继结点的地址信息，这称为指针域。这样可以通过一个结点的指针域方便的找到后继结点的位置。由于顺序表中每个元素至多只有一个直接前驱元素和一个直接后继元素。当采用链式存储时，一种最简单也最常用的方法是：在每个结点中除包含数据域外，只设置一个指针域用以指向其直接后继结点，这种构成的链接表称为线性单向链接表，简称单链表。另一种方法是，在每个结点中除包含数值域外，设置两个指针域，分别用以指向直接前驱结点和直接后继结点，这样构成的链接表称为线性双向链接表，简称双链表。单链表当访问一个结点后，只能接着访问它的直接后继结点，而无法访问他的直接前驱结点。双链表则既可以依次向后访问每个结点，也可以依次向前访问每个结点。单链表结点元素类型定义：1234public class LNode &#123; protected LNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;双链表结点元素类型定义：12345public class DNode &#123; protected DNode prior; //指针域，指向直接前驱结点 protected DNode next; //指针域，指向直接后继结点 protected Object data; //数据域&#125;在顺序表中，逻辑上相邻的元素，对应的存储位置也相邻，所以进行插入或删除操作时，通常需要平均移动半个表的元素，这是相当费时的操作。在链表中，每个结点存储位置可以任意安排，不必要求相邻，插入或删除操作只需要修改相关结点的指针域即可，方便省时。对于单链表，如果要在结点p之前插入一个新结点，由于通过p并不能找到其前驱结点，我们需要从链表表头遍历至p的前驱结点然后进行插入操作，这样时间复杂度就是O(n)，而顺序表插入删除结点时间复杂度也是O(n)，那为什么说链表插入删除操作更加高效呢？因为单链表插入删除操作所消耗的时间主要在于查找前驱结点，这个查找工作的时间复杂度为O(n)，而真正超如删除时间为O(1)还有顺序表需要移动结点，移动结点通常比单纯的查找更加费时，链表不需要连续的空间，不需要扩容创建新表，所以同样时间复杂度O(n)，链表更适合插入和删除操作。对于遍历查找前驱结点的问题，在双链表中就能很好的解决，双链表在已知某结点的插入和删除操作时间复杂度是O(1)。由于链表的每个结点带有指针域，从存储密度来讲，这是不经济的。所谓存储密度是指结点数据本身所占存储量和整改结点结构所占存储量之比。3.2 单链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534public class LinkList&lt;T&gt; implements IList&lt;T&gt;&#123; public LNode&lt;T&gt; head; //单链表开始结点 /** * 1.1 创建单链表（头插法：倒序） * 解：遍历数组，创建新结点，新结点的指针域指向头结点，让新结点作为头结点 * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListF(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; for (T obj : array) &#123; LNode&lt;T&gt; node = new LNode(); node.data = obj; node.next = llist.head; llist.head = node; &#125; &#125; return llist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 解： * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; LinkList&lt;T&gt; createListR(T[] array)&#123; LinkList llist = new LinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; llist.head = new LNode(); llist.head.data = array[0]; LNode&lt;T&gt; temp = llist.head; for(int i = 1; i &lt; array.length; i++)&#123; LNode node = new LNode(); node.data = array[i]; temp.next = node; temp = node; &#125; &#125; return llist; &#125; /** * 判断单链表是否为空表 * 时间复杂度O(1) * @return */ @Override public boolean isEmpty() &#123; return head==null; &#125; /** * 4 获取单链表长度 * 时间复杂度O(n) * @return */ @Override public int length() &#123; if(head==null) return 0; int l = 1; LNode node = head; while(node.next!=null) &#123; l++; node = node.next; &#125; return l; &#125; @Override public void clear() &#123; head = null; &#125; @Override public T set(int index, T data) &#123; return null; &#125; @Override public boolean contains(T data) &#123; return false; &#125; @Override public T get(int index) &#123; return getNode(index).data; &#125; /** * 6.1 获取指定索引的结点 * 时间复杂度O(n) * @param index * @return */ public LNode&lt;T&gt; getNode(int index)&#123; LNode node = head; int j = 0; while(j &lt; index &amp;&amp; node!=null)&#123; j++; node = node.next; &#125; return node; &#125; /** * 6.2 获取指定数据值结点的索引 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @return */ @Override public int indexOf(T data) &#123; if(head==null) return -1; //没有此结点 LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) return j; j++; node = node.next; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; if(head==null) return -1; int index = -1; LNode node = head; int j = 0; while(node!=null)&#123; if(node.data.equals(data)) &#123; index = j; &#125; j++; node = node.next; &#125; return index; &#125; /** * 6.3 单链表中的倒数第k个结点（k &gt; 0） * 解：先找到顺数第k个结点，然后使用前后指针移动到结尾即可 * 时间复杂度O(n) 空间复杂度O(1) * @param k * @return */ public LNode&lt;T&gt; getReNode(int k)&#123; if(head==null) return null; int len = length(); if(k &gt; len) return null; LNode target = head; LNode next = head; for(int i=0;i &lt; k;i++) next = next.next; while(next!=null)&#123; target = target.next; next = next.next; &#125; return target; &#125; /** * 6.4 查找单链表的中间结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode getMiddleNode()&#123; if(head == null|| head.next == null) return head; LNode target = head; LNode temp = head; while(temp != null &amp;&amp; temp.next != null)&#123; target = target.next; temp = temp.next.next; &#125; return target; &#125; /** * 2.1 将单链表合并为一个单链表 * 解：遍历第一个表，用其尾结点指向第二个表头结点 * 时间复杂度O(n) * @return */ public static LNode mergeList(LNode head1, LNode head2)&#123; if(head1==null) return head2; if(head2==null) return head1; LNode loop = head1; while(loop.next!=null) //找到list1尾结点 loop = loop.next; loop.next = head2; //将list1尾结点指向list2头结点 return head1; &#125; /** * 2.1 通过递归，合并两个有序的单链表head1和head2 * * 解：两个指针分别指向两个头结点，比较两个结点大小， * 小的结点指向下一次比较结果（两者中较小），最终返回第一次递归的最小结点 * @param head1 * @param head2 * @return */ public static LNode mergeSortedListRec(LNode head1, LNode head2)&#123; if(head1==null)return head2; if(head2==null)return head1; if (((int)head1.data)&gt;((int)head2.data)) &#123; head2.next = mergeSortedListRec(head2.next, head1); return head2; &#125; else &#123; head1.next = mergeSortedListRec(head1.next, head2); return head1; &#125; &#125; /** * 3.1 循环的方式将单链表反转 * 时间复杂度O(n) 空间复杂度O(1) */ public void reverseListByLoop() &#123; if (head == null || head.next == null) return; LNode pre = null; LNode nex = null; while (head != null) &#123; nex = head.next; head.next = pre; pre = head; head = nex; &#125; head = pre; &#125; /** * 3.2 递归的方式将单链表反转,返回反转后的链表头结点 * 时间复杂度O(n) 空间复杂度O(n) */ public LNode reverseListByRec(LNode head) &#123; if(head==null||head.next==null) return head; LNode reHead = reverseListByRec(head.next); head.next.next = head; head.next = null; return reHead; &#125; /** * 5.1 获取单链表字符串表示 * 时间复杂度O(n) */ @Override public String toString() &#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; return buffer.toString(); &#125; public static String display(LNode head)&#123; if(head == null) return \"\"; LNode node = head; StringBuffer buffer = new StringBuffer(); while(node != null)&#123; buffer.append(\" -&gt; \"+node.data); node = node.next; &#125; return buffer.toString(); &#125; /** * 5.2 用栈的方式获取单链表从尾到头倒叙字符串表示 * 解：由于栈具有先进后出的特性，现将表中的元素放入栈中，然后取出就倒序了 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public String displayReverseStack()&#123; if(head == null) return \"\"; Stack &lt;LNode&gt; stack = new Stack &lt; &gt;(); //堆栈 先进先出 LNode head = this.head; while(head!=null)&#123; stack.push(head); head=head.next; &#125; StringBuffer buffer = new StringBuffer(); while(!stack.isEmpty())&#123; //pop()移除堆栈顶部的对象，并将该对象作为该函数的值返回。 buffer.append(\" -&gt; \"+stack.pop().data); &#125; return buffer.toString(); &#125; /** * 5.3 用递归的方式获取单链表从尾到头倒叙字符串表示 * @return */ public void displayReverseRec(StringBuffer buffer, LNode head)&#123; if(head==null) return; displayReverseRec(buffer, head.next); buffer.append(\" -&gt; \"); buffer.append(head.data); &#125; /** * 7.1 插入结点 * 解：先找到第i-1个结点，让创建的新结点的指针域指向第i-1结点指针域指向的结点， * 然后将i-1结点的指针域指向新结点 * 时间复杂度O(n) 空间复杂度O(1) * @param data * @param index */ @Override public boolean add(int index, T data) &#123; if(index==0)&#123; //插入为头结点 LNode temp = new LNode(); temp.next = head; return true; &#125; int j = 0; LNode node = head; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return false; LNode temp = new LNode(); //创建新结点 temp.data = data; temp.next = node.next; //新结点插入到Index-1结点之后 node.next = temp; return true; &#125; @Override public boolean add(T data) &#123; LNode node = head; while(node!=null &amp;&amp; node.next!=null) //找到尾结点 node = node.next; LNode temp = new LNode(); //创建新结点 temp.data = data; node.next = temp; return false; &#125; @Override public T remove(int index) &#123; LNode&lt;T&gt; node = deleteNode(index); return node==null?null:node.data; &#125; /** * 7.2 删除结点 * 解：让被删除的结点前一个结点的指针域指向后一个结点指针域 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public LNode deleteNode(int index)&#123; LNode node = head; if(index==0)&#123; //删除头结点 if(node==null) return null; head = node.next; return node; &#125; //非头结点 int j = 0; while(j &lt; index-1 &amp;&amp; node!=null)&#123; //找到序列号为index-1的结点 j++; node = node.next; &#125; if(node==null) return null; LNode delete = node.next; if(delete==null) return null; //不存在第index个结点 node.next = delete.next; return delete; &#125; @Override public boolean remove(T data) &#123; return false; &#125; @Override public boolean removeAll(T data) &#123; return false; &#125; /** * 7.3 给出一单链表头指针head和一节点指针delete，要求O(1)时间复杂度删除节点delete * 解：将delete节点value值与它下个节点的值互换的方法， * 但是如果delete是最后一个节点，需要特殊处理，但是总得复杂度还是O(1) * @return */ public static void deleteNode(LNode head, LNode delete)&#123; if(delete==null) return; //首先处理delete节点为最后一个节点的情况 if(delete.next==null)&#123; if(head==delete) //只有一个结点 head = null; else&#123; //删除尾结点 LNode temp = head; while(temp.next!=delete) temp = temp.next; temp.next=null; &#125; &#125; else&#123; delete.data = delete.next.data; delete.next = delete.next.next; &#125; return; &#125; /** * 8.1 判断一个单链表中是否有环 * 解：使用快慢指针方法，如果存在环，两个指针必定指向同一结点 * 时间复杂度O(n) 空间复杂度O(1) * @return */ public static boolean hasCycle(LNode head)&#123; LNode p1 = head; LNode p2 = head; while(p1!=null &amp;&amp; p2!=null)&#123; p1 = p1.next; //一次跳一步 p2 = p2.next.next; //一次跳两步 if(p2 == p1) return true; &#125; return false; &#125; /** * 8.2、已知一个单链表中存在环，求进入环中的第一个节点 * 利用hashmap，不要用ArrayList，因为判断ArrayList是否包含某个元素的效率不高 * @param head * @return */ public static LNode getFirstNodeInCycleHashMap(LNode head)&#123; LNode target = null; HashMap&lt;LNode,Boolean &gt; map=new HashMap&lt; &gt;(); while(head != null)&#123; if(map.containsKey(head)) &#123; target = head; break; &#125; else &#123; map.put(head, true); head = head.next; &#125; &#125; return target; &#125; /** * 8.3、已知一个单链表中存在环，求进入环中的第一个节点,不用hashmap * 用快慢指针，与判断一个单链表中是否有环一样，找到快慢指针第一次相交的节点， * 此时这个节点距离环开始节点的长度和链表头距离环开始的节点的长度相等 * 参考 https://www.cnblogs.com/fankongkong/p/7007869.html * @param head * @return */ public static LNode getFirstNodeInCycle(LNode head)&#123; LNode fast = head; LNode slow = head; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; if(slow == fast) break; &#125; if(fast == null||fast.next == null) return null;//判断是否包含环 //相遇节点距离环开始节点的长度和链表投距离环开始的节点的长度相等 slow=head; while(slow!=fast)&#123; slow=slow.next; fast=fast.next; &#125;//同步走 return slow; &#125; /** * 9、判断两个单链表是否相交,如果相交返回第一个节点，否则返回null * ①、暴力遍历两个表，是否有相同的结点(时间复杂度O(n²)) * ②、第一个表的尾结点指向第二个表头结点，然后判断第二个表是否存在环，但不容易找出交点（时间复杂度O(n)） * ③、两个链表相交，必然会经过同一个结点，这个结点的后继结点也是相同的（链表结点只有一个指针域，后继结点只能有一个）， * 所以他们的尾结点必然相同。两个链表相交，只能是前面的结点不同，所以，砍掉较长链表的差值后同步遍历，判断结点是否相同，相同的就是交点了。 * 时间复杂度（时间复杂度O(n)） * @param list1 * @param list2 * @return 交点 */ public static LNode isIntersect(LinkList list1, LinkList list2)&#123; LNode head1 = list1.head; LNode head2 = list2.head; if(head1==null || head2==null)return null; int len1 = list1.length(); int len2 = list2.length(); //砍掉较长链表的差值 if(len1 &gt;= len2)&#123; for(int i=0;i &lt; len1-len2;i++)&#123; head1=head1.next; &#125; &#125;else&#123; for(int i=0;i &lt; len2-len1;i++)&#123; head2=head2.next; &#125; &#125; //同步遍历 while(head1 != null&amp;&amp;head2 != null)&#123; if(head1 == head2) return head1; head1=head1.next; head2=head2.next; &#125; return null; &#125;&#125;算法分析判断一个单链表中是否有环我们可以通过HashMap判断，遍历结点，将结点值放入HashMap，如果某一刻发现当前结点在map中已经存在，则存在环，并且此结点正是环的入口，此算法见8.2方法。但是有一种问法是不通过任何其他数据结构怎么判断单链表是否存在环。这样我们可利用的就只有单链表本身，一种解法是通过快慢指针，遍历链表，一个指针跳一步（慢指针步长为1），另一个指针跳两步（快指针步长为2），如果存在环，这两个指针必将在某一刻指向同一结点，假设此时慢指针跳了n步，则快指针跳的步数为n/2步：判断两个单链表是否相交由于单链表的特性（只有一个指针域），如果两个表相交，那必定是Y形相交，不会是X形相交，如图所示。两个单链表后面的结点相同，不同的部分只有前面，砍掉较长的链表的前面部分，然后两个链表同步遍历，必将指向同一个结点，这个结点就是交点：双链表双链表中每个结点有两个指针域，一个指向其直接后继结点，一个指向其直接前驱结点。建立双链表也有两种方法，头插法和尾插法，这与创建单链表过程相似。在双链表中，有些算法如求长度、取元素值、查找元素等算法与单链表中相应算法是相同的。但是在单链表中，进行结点插入和删除时涉及前后结点的一个指针域的变化，而双链表中结点的插入和删除操作涉及前后结点的两个指针域的变化。java中LinkedList正是对双链表的实现，算法可参考此类。双链表基本运算的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327public class DLinkList&lt;T&gt; implements IList&lt;T&gt;&#123; transient DNode&lt;T&gt; first; //双链表开始结点 transient DNode&lt;T&gt; last; //双链表末端结点 private int size; //结点数 /** * 创建单链表（头插法：倒序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListF(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0) &#123; dlist.size = array.length; for (T obj : array) &#123; DNode&lt;T&gt; node = new DNode(); node.data = obj; node.next = dlist.first; if(dlist.first!=null) dlist.first.prior = node; //相比单链表多了此步 else dlist.last = node; dlist.first = node; &#125; &#125; return dlist; &#125; /** * 1.2 创建单链表（尾插法：顺序） * 时间复杂度O(n) * @param array * @return */ public static &lt;T&gt; DLinkList&lt;T&gt; createListR(T[] array)&#123; DLinkList dlist = new DLinkList(); if(array!=null &amp;&amp; array.length&gt;0)&#123; dlist.size = array.length; dlist.first = new DNode&lt;T&gt;(); dlist.first.data = array[0]; dlist.last = dlist.first; for(int i = 1; i &lt; array.length; i++)&#123; DNode&lt;T&gt; node = new DNode(); node.data = array[i]; dlist.last.next = node; node.prior = dlist.last; //相比单链表多了此步 dlist.last = node; &#125; &#125; return dlist; &#125; @Override public boolean isEmpty() &#123; return size==0; &#125; @Override public int length() &#123; return size; &#125; /**2 添加结点*/ @Override public boolean add(int index, T data) &#123; if(index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; newNode = new DNode(); newNode.data = data; if (index == size) &#123; //在末尾添加结点不需要遍历 final DNode&lt;T&gt; l = last; if (l == null) //空表 first = newNode; else &#123; l.next = newNode; newNode.prior = l; &#125; last = newNode; size++; &#125; else &#123; //其他位置添加结点需要遍历找到index位置的结点 DNode&lt;T&gt; indexNode = getNode(index); DNode&lt;T&gt; pred = indexNode.prior; newNode.prior = pred; newNode.next = indexNode; indexNode.prior = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; &#125; return false; &#125; @Override public boolean add(T data) &#123; return add(size, data); &#125; /**3 删除结点*/ @Override public T remove(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return unlink(getNode(index)); &#125; @Override public boolean remove(T data) &#123; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); return true; &#125; &#125; &#125; return false; &#125; @Override public boolean removeAll(T data) &#123; boolean result = false; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) &#123; unlink(x); result = true; &#125; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) &#123; unlink(x); result = true; &#125; &#125; &#125; return result; &#125; /** * 将指定的结点解除链接 * @param x * @return */ private T unlink(DNode&lt;T&gt; x) &#123; // assert x != null; final T element = x.data; final DNode&lt;T&gt; next = x.next; final DNode&lt;T&gt; prev = x.prior; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prior = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prior = prev; x.next = null; &#125; x.data = null; size--; return element; &#125; /** * 清空 */ @Override public void clear() &#123; for (DNode&lt;T&gt; x = first; x != null; ) &#123; DNode&lt;T&gt; next = x.next; x.data = null; x.next = null; x.prior = null; x = next; &#125; first = last = null; size = 0; &#125; /** * 设置结点值 * @param index * @param data * @return */ @Override public T set(int index, T data) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); DNode&lt;T&gt; x = getNode(index); T oldVal = x.data; x.data = data; return oldVal; &#125; /** * 判断是否存在结点值 * @param data * @return */ @Override public boolean contains(T data) &#123; return indexOf(data) != -1; &#125; /** * 检索结点值 * @param data * @return */ @Override public int indexOf(T data) &#123; int index = 0; if (data == null) &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (x.data == null) return index; index++; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = first; x != null; x = x.next) &#123; if (data.equals(x.data)) return index; index++; &#125; &#125; return -1; &#125; @Override public int lastIndexOf(T data) &#123; int index = size; if (data == null) &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (x.data == null) return index; &#125; &#125; else &#123; for (DNode&lt;T&gt; x = last; x != null; x = x.prior) &#123; index--; if (data.equals(x.data)) return index; &#125; &#125; return -1; &#125; @Override public T get(int index) &#123; if(index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(); return getNode(index).data; &#125; /** * 获取指定索引的结点 * 解：由于双链表能双向检索，判断index离开始结点近还是终端结点近，从近的一段开始遍历 * 时间复杂度O(n) * @param index * @return */ private DNode&lt;T&gt; getNode(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123; DNode&lt;T&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; DNode&lt;T&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prior; return x; &#125; &#125; /** * 倒序 * 遍历每个结点，让node.next = node.prior; node.prior = (node.next此值需要体现保存); */ public void reverse()&#123; last = first; //反转后终端结点=开始结点 DNode now = first; DNode next; while(now!=null)&#123; next = now.next; //保存当前结点的后继结点 now.next = now.prior; now.prior = next; first = now; now = next; &#125; &#125; @Override public String toString() &#123; if(size == 0) return \"\"; DNode node = first; StringBuffer buffer = new StringBuffer(); buffer.append(\" \"); while(node != null)&#123; buffer.append(node.data+\" -&gt; \"); node = node.next; &#125; buffer.append(\"next\\npre\"); node = last; int start = buffer.length(); LogUtil.i(getClass().getSimpleName(), \"buffer长度：\"+buffer.length()); while(node != null)&#123; buffer.insert(start ,\" &lt;- \"+node.data); node = node.prior; &#125; return buffer.toString(); &#125;&#125;算法分析双链表与单链表不同之处在于，双链表能从两端依次访问各个结点。单链表相对于顺序表优点是插入、删除数据更方便，但是访问结点需要遍历，时间复杂度为O(n)；双链表就是在单链表基础上做了一个优化，使得访问结点更加便捷（从两端），这样从近的一端出发时间复杂度变为O(n/2)，虽然不是指数阶的区别，但也算是优化。双链表在插入、删除结点时逻辑比单链表稍麻烦：循环链表循环链表是另一种形式的链式存储结构，它的特点是表中尾结点的指针域不再是空，而是指向头结点，整个链表形成一个环。由此，从表中任意一结点出发均可找到链表中其他结点。如图所示为带头结点的循环单链表和循环双链表：有序表所谓有序表，是指所有结点元素值以递增或递减方式排列的线性表，并规定有序表中不存在元素值相同的结点。有序表可以采用顺序表和链表进行存储，若以顺序表存储有序表，其算法除了add(T data)以外，其他均与前面说的顺序表对应的运算相同。有序表的add(T data)操作不是插入到末尾，而需要遍历比较大小后插入相应位置。123456789101112131415public boolean addByOrder(int data) &#123; int index = 0; //找到顺序表中第一个大于等于data的元素 while(index&lt;datas.length &amp;&amp; (int)datas[index]&lt;data) index++; if((int)datas[index] == data) //不能有相同元素 return false; Object destination[] = new Object[datas.length + 1]; System.arraycopy(datas, 0, destination, 0, index); //将datas[index]及后面元素后移一位 System.arraycopy(datas, index, destination, index+1, datas.length-index); destination[index] = data; datas = destination; return true;&#125;最后 想强调的是 由于顺序表结构的底层实现借助的就是数组，因此对于初学者来说，可以把顺序表完全等价为数组，但实则不是这样。数据结构是研究数据存储方式的一门学科，它囊括的都是各种存储结构，而数组只是各种编程语言中的基本数据类型，并不属于数据结构的范畴。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"线性表","slug":"线性表","permalink":"cpeixin.cn/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/"}]},{"title":"数据结构-数组","slug":"数据结构-数组","date":"2016-08-11T07:30:21.000Z","updated":"2020-06-08T08:43:43.141Z","comments":true,"path":"2016/08/11/数据结构-数组/","link":"","permalink":"cpeixin.cn/2016/08/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%95%B0%E7%BB%84/","excerpt":"","text":"数据结构-数组提到数组，我想你肯定不陌生，甚至还会自信地说，它很简单啊。是的，在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。在大部分编程语言中，数组都是从 0 开始编号的，但你是否下意识地想过，为什么数组要从 0 开始编号，而不是从 1 开始呢？ 从 1 开始不是更符合人类的思维习惯吗？Tips: 在C，Java中都有明显的数组实现，但是在Python中有list、tuple、set、dict, 并没有显示的数组关键字“数组”实际上确实存在于python中。当人们谈论数组时，Python至少有三件事要谈论：在Python list有许多在其他语言，如C或Java中 数组的行为。但是，它不需要预先分配内存，分配数组长度，它带有许多便捷的方法。此外，它的基础数据结构通常是一个数组。请记住，语言只是语法-它可以通过不同的方式实现。CPython中Python列表的基础数据结构是C数组，在Jython中是ArrayList（来源：Python列表的基础数据结构是什么？）列表对象被实现为数组。它们针对快速的固定长度操作进行了优化，并且会为pop（0）和insert（0，v）操作产生O（n）内存移动成本，这些操作会同时更改基础数据表示的大小和位置。最后，还有NumPy 数组。这是一个非常快速，非常有用的数据结构，可让您快速进行许多数值计算。在此处了解有关NumPy的更多信息：NumPy-Numpy这个定义里有几个关键词，理解了这几个关键词，我想你就能彻底掌握数组的概念了。下面就从我的角度分别给你“点拨”一下。第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10]来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：1a[i]_address = base_address + i * data_type_size其中 data_type_size 表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节。这个公式非常简单，我就不多做解释了。这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。Tips: 针对上面提到的 数组存储的必须是相同的数据类型，在这里我自己的理解是，对于数组内的元素，进行访问的过程中，实际上是根据每个元素的内存地址来进行访问的。元素的内存地址不是随意分配的，而是通过一个公式计算而来的，每个元素的内存地址合成一段连续的内存地址区间段。如上图，不同类型的元素所占字节不同，如果数组中存储不同数据类型的数据，那么就不能统一的按照 内存地址计算公式来计算。低效的“插入”和“删除”前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。现在我们就来详细说一下，究竟为什么会导致低效？又有哪些改进方法呢？我们先来看插入操作。假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？你可以自己先试着分析一下。如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。为了更好地理解，我们举一个例子。假设数组 a[10]中存储了如下 5 个元素：a，b，c，d，e。我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2]赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到我们再来看删除操作。跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？我们继续来看例子。数组 a[10]中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。**如果你了解 JVM，你会发现，这不就是 JVM 标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。容器能否完全替代数组这个问题，我在刚学编程语言之处，是根本没有想过的，在我眼里，程序语言中集合就是数据结构中的数组，没有任何差别，实际情况中，针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL 中的 vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？这里我拿 Java 语言来举例。如果你是 Java 工程师，几乎天天都在用 ArrayList，对它应该非常熟悉。那它与数组相比，到底有哪些优势呢？我个人觉得，ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。如果使用 ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList 已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。**不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。比如我们要从数据库中取出 10000 条数据放入 ArrayList。我们看下面这几行代码，你会发现，相比之下，事先指定数据大小可以省掉很多次内存申请和数据搬移操作。作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适些，我总结了几点自己的经验。Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 Object[][] array；而用容器的话则需要这样定义：ArrayListarray。我总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。解答开篇现在我们来思考开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：a[k]_address = base_address + k * type_size但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：a[k]_address = base_address + (k-1)*type_size对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。不过我认为，上面解释得再多其实都算不上压倒性的证明，说数组起始编号非 0 开始不可。所以我觉得最主要的原因可能是历史原因。C 语言设计者用 0 开始计数数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。实际上，很多语言中数组也并不是从 0 开始计数的，比如 Matlab。甚至还有一些语言支持负数下标，比如 Python。内容小结我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数组","slug":"数组","permalink":"cpeixin.cn/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"数据结构与算法-复盘","slug":"数据结构与算法-复盘","date":"2016-08-10T07:30:21.000Z","updated":"2020-04-04T17:30:49.884Z","comments":true,"path":"2016/08/10/数据结构与算法-复盘/","link":"","permalink":"cpeixin.cn/2016/08/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E5%A4%8D%E7%9B%98/","excerpt":"","text":"对于软件工程专业的同学来说，数据结构与算法这门课程是必修课程，课程时间应该是大学二年级，在学习完了一门编程语言后，进行展开学习的。我是在学习完C语言后，进行数据结构与算法这门课程的。个人感觉这门课程 veryyyyyyyyy 枯燥的。老师在课堂上也没有做相关知识点的扩展，再加上那时候我们对于知识的渴望度也没有那么高 哈哈哈哈，单纯的就是为了考试而学习，也没有利用网络资源来对这门课程进行进一步的拓展。等到工作之后，才慢慢发现，这门课程就是在大学时候埋的雷啊 XD但是毕竟还是学过的，时间还不晚，知识学到了，就是自己的，定期对自己复盘，重视自己，查缺补漏，就是最棒的！学前三问：WHAT ？ HOW ？ WHY？在之后写博客的时候，我也会遵循着 WWH 法则作为提纲。简单明了，直入主题。WHAT - 数据结构 算法数据结构简单直白的理解 ： 数据结构就是指一组数据的存储结构。百度百科：数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率。数据结构往往同高效的检索算法和索引技术有关算法简单直白的理解 ： 算法就是操作数据的一组方法。百度百科：算法（Algorithm）是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制。也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。HOW - 数据结构 算法有的人喜欢看书稳扎稳打有的人习惯看视频生动一点的获取知识也有人倾向于直接进入主题，找教程，找博文攻破单个知识点我这里只是说一下我的方法首先就是这篇博文，先搞清楚WWH然后呢，对数据结构和算法整个知识体系列一个大纲或者思维导图接下来就进入学习的过程了，我个人是比较喜欢看视 频和读好的博客的，每一个点学完后啊，先找习题，完了打开IDEA ，写一些Demo去理解，然后呢，带着学过的算法和数据结构啊，进入实战场，看看之前自己写过的项目，可不可以重新装修一下。最后一点，找一些成熟项目，读源码，读源码，读源码！！！WHY - 数据结构 算法这一栏位的内容，我找了很久，也想了很久怎样的去写，后来在搜寻的时候，发现很多博主和学习论坛都引用了知乎上涛吴的观点（涛吴，知乎上最受欢迎程序员前十名👍🏻），写的确实好，好东西就要分享出来😂如果说 Java 是自动档轿车，C 就是手动档吉普。数据结构呢？是变速箱的工作原理。你完全可以不知道变速箱怎样工作，就把自动档的车子从 A 开到 B，而且未必就比懂得的人慢。写程序这件事，和开车一样，经验可以起到很大作用，但如果你不知道底层是怎么工作的，就永远只能开车，既不会修车，也不能造车。如果你对这两件事都不感兴趣也就罢了，数据结构懂得用就好。但若你此生在编程领域还有点更高的追求，数据结构是绕不开的课题。Java 替你做了太多事情，那么多动不动还支持范型的容器类，加上垃圾收集，会让你觉得编程很容易。但你有没有想过，那些容器类是怎么来的，以及它存在的意义是什么？最粗浅的，比如 ArrayList 这个类，你想过它的存在是多么大的福利吗——一个可以随机访问、自动增加容量的数组，这种东西 C 是没有的，要自己实现。但是，具体怎么实现呢？如果你对这种问题感兴趣，那数据结构是一定要看的。甚至，面向对象编程范式本身，就是个数据结构问题：怎么才能把数据和操作数据的方法封装到一起，来造出 class / prototype 这种东西？此外，很重要的一点是，数据结构也是通向各种实用算法的基石，所以学习数据结构都是提升内力的事情。原文链接学 Java 有必要看数据结构的书吗？如果是，那么哪本书比较好？ - 涛吴的回答 - 知乎学数据结构和算法可以应用在哪些场景？在大学的时候，学习数据结构与算法，学的时候云里雾里的，搞不清楚这些数据结构和算法应该用在哪里？能干些什么？还没有一个长长的SQL带来的成就感强烈。以为这门课程只是应试教育中的必须进行的一环，现在想起来真是单纯的孩子啊 哈哈😄下面我引用InfoQ上的一篇文章，看完之后，有种醍醐灌顶的感觉，我觉得我明天应该会去翻遍所有的源码去看，有点迫不及待了……实际项目中的常见算法其次，以下场景中使用了丰富的数据结构来实现，可以择一来深入研读Linux 内核SQL引擎网络拓扑结构集群节点各种协议学数据结构和算法的精髓是什么？想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。时间复杂度和空间复杂度这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招怎样去选择合适的数据结构首先，选择数据结构的时候，第一点就要知道，数据怎么去组织，和数据规模是有关的。不一样规模的数据，难度也就不一样。解决问题的效率，是与数据组织方式相关的。首先上来我先Po出一张图，来应对第一步大类结构的选择查询操作更多的程序中，你应该用顺序表修改操作更多的程序中，你要使用链表单向链 双向链表 循环链表。栈，涉及后入先出的问题，例如函数递归就是个栈模型、Android的屏幕跳转就用到栈，很多类似的东西，你就会第一时间想到：我会用这东西来去写算法实现这个功能。队列，先入先出要排队的问题，你就要用到队列，例如多个网络下载任务，我该怎么去调度它们去获得网络资源呢？再例如操作系统的进程（or线程）调度，我该怎么去分配资源（像 CPU）给多个任务呢？肯定不能全部一起拥有的，资源只有一个，那就要排队！那么怎么排队呢？用普通的队列？但是对于那些优先级高的线程怎么办？这时，你就会想到了优先队列，优 先队列怎么实现？用堆，然后你就有疑问了，堆是啥玩意？怎样评测算法的好坏时间复杂度空间复杂度怎样将数据结构和算法应用到实际之中？写一些程序，尤其是比较底层的程序。数据结构如红黑树,后缀树, 算法如快速傅里叶变换,网络流等… 平时工作本来就很难碰上这些东西(如果确实从事尖端研究除外)，个人经常碰到的也无非就是些搜索, 优化剪枝…留下一个问题去给大家思考：如果让你实现qq那种分组的好友列表，支持各种qq里边的好友操作，你会怎么做？Sooooooooooo 学习数据结构与算法是提升自身内力，内力提升了，学习其他技术和框架，就会简单容易一些🐨","categories":[{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[]},{"title":"小谈多线程","slug":"小谈多线程","date":"2016-08-09T07:30:21.000Z","updated":"2020-04-04T11:10:51.174Z","comments":true,"path":"2016/08/09/小谈多线程/","link":"","permalink":"cpeixin.cn/2016/08/09/%E5%B0%8F%E8%B0%88%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"多线程简介Java 给多线程编程提供了内置的支持。 一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。多线程是多任务的一种特别的形式，但多线程使用了更小的资源开销。这里定义和线程相关的另一个术语 - 进程：一个进程包括由操作系统分配的内存空间，包含一个或多个线程。一个线程不能独立的存在，它必须是进程的一部分。一个进程一直运行，直到所有的非守护线程都结束运行后才能结束。多线程能满足程序员编写高效率的程序来达到充分利用 CPU 的目的。多线程状态VM启动时会有一个由主方法Main所定义的主线程，在主线程中可以通过Thread创建其它线程。Thread对象的方法run()称为线程体。通过调用Thread类的start()方法来启动一个线程。通俗的说就是在run()方法中定义要做什么事情，start()方法用来发出命令可以开始做了，但这不代表JVM会立即运行 run()方法中的内容，而只是让他具备运行的资格，具体什么时侯开始真正运行run()方法，需要看JVM的调度。在Java当中，线程通常都有五种状态，新建、就绪、运行、阻塞和死亡。新建状态（New）：新创建了一个线程对象。就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：（一）、等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。（二）、同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。（三）、其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。在这里我要说明一下start()和run()之间的关系，因为在多线程编程的过程中并没有去调用run()方法，但是run()又是主要的执行体，首先Po上来start()，run()的源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Causes this thread to begin execution; the Java Virtual Machine * calls the &lt;code&gt;run&lt;/code&gt; method of this thread. * &lt;p&gt; * The result is that two threads are running concurrently: the * current thread (which returns from the call to the * &lt;code&gt;start&lt;/code&gt; method) and the other thread (which executes its * &lt;code&gt;run&lt;/code&gt; method). * &lt;p&gt; * It is never legal to start a thread more than once. * In particular, a thread may not be restarted once it has completed * execution. * * @exception IllegalThreadStateException if the thread was already * started. * @see #run() * @see #stop() */ public synchronized void start() &#123; /** * This method is not invoked for the main method thread or \"system\" * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * A zero status value corresponds to state \"NEW\". */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group's list of threads * and the group's unstarted count can be decremented. */ group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; &#125; private native void start0(); /** * If this thread was constructed using a separate * &lt;code&gt;Runnable&lt;/code&gt; run object, then that * &lt;code&gt;Runnable&lt;/code&gt; object's &lt;code&gt;run&lt;/code&gt; method is called; * otherwise, this method does nothing and returns. * &lt;p&gt; * Subclasses of &lt;code&gt;Thread&lt;/code&gt; should override this method. * * @see #start() * @see #stop() * @see #Thread(ThreadGroup, Runnable, String) */ @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125;根据Java API ： Causes this thread to begin execution; the Java Virtual Machine calls the run method of this thread.start()方法会使得该线程开始执行；java虚拟机会自动去调用该线程的run()方法。因此，t.start()会导致run()方法被调用，run()方法中的内容称为线程体，它就是这个线程需要执行的工作。在start方法里调用了一次start0方法，这个方法是一个只声明未定义的方法，并且使用了native关键字进行定义native指的是调用本机的原生系统函数。所以，调用start方法，会告诉JVM去分配本机系统的资源，才能实现多线程。而如果使用run()来启动线程，就不是异步执行了，而是同步执行，不会达到使用线程的意义。用start()来启动线程，实现了真正意义上的启动线程，此时会出现异步执行的效果，即在线程的创建和启动中所述的随机性。多线程创建Java 提供了三种创建线程的方法实现 Runnable 接口我们经常使用的构造方法，threadOb为创建的实例对象Thread(Runnable threadOb,String threadName);12345678910111213141516171819202122232425262728293031323334353637383940414243class RunnableDemo implements Runnable &#123; private Thread t; private String threadName; RunnableDemo( String name) &#123; threadName = name; System.out.println(\"Creating \" + threadName ); &#125; public void run() &#123; System.out.println(\"Running \" + threadName ); try &#123; for(int i = 4; i &gt; 0; i--) &#123; System.out.println(\"Thread: \" + threadName + \", \" + i); // 让线程睡眠一会 Thread.sleep(50); &#125; &#125;catch (InterruptedException e) &#123; System.out.println(\"Thread \" + threadName + \" interrupted.\"); &#125; System.out.println(\"Thread \" + threadName + \" exiting.\"); &#125; public void start () &#123; System.out.println(\"Starting \" + threadName ); if (t == null) &#123; t = new Thread (this, threadName); t.start (); &#125; &#125;&#125; public class TestThread &#123; public static void main(String args[]) &#123; RunnableDemo R1 = new RunnableDemo( \"Thread-1\"); R1.start(); RunnableDemo R2 = new RunnableDemo( \"Thread-2\"); R2.start(); &#125; &#125;继承 Thread 类本身创建一个线程的第二种方法是创建一个新的类，该类继承 Thread 类，然后创建一个该类的实例。继承类必须重写 run() 方法，该方法是新线程的入口点。它也必须调用 start() 方法才能执行。该方法尽管被列为一种多线程实现方式，但是本质上也是实现了 Runnable 接口的一个实例。123456789101112131415161718192021222324252627282930313233343536373839404142class ThreadDemo extends Thread &#123; private Thread t; private String threadName; ThreadDemo( String name) &#123; threadName = name; System.out.println(\"Creating \" + threadName ); &#125; public void run() &#123; System.out.println(\"Running \" + threadName ); try &#123; for(int i = 4; i &gt; 0; i--) &#123; System.out.println(\"Thread: \" + threadName + \", \" + i); // 让线程睡眠一会 Thread.sleep(50); &#125; &#125;catch (InterruptedException e) &#123; System.out.println(\"Thread \" + threadName + \" interrupted.\"); &#125; System.out.println(\"Thread \" + threadName + \" exiting.\"); &#125; public void start () &#123; System.out.println(\"Starting \" + threadName ); if (t == null) &#123; t = new Thread (this, threadName); t.start (); &#125; &#125;&#125; public class TestThread &#123; public static void main(String args[]) &#123; ThreadDemo T1 = new ThreadDemo( \"Thread-1\"); T1.start(); ThreadDemo T2 = new ThreadDemo( \"Thread-2\"); T2.start(); &#125; &#125;Callable和Future创建线程创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值。123456789101112131415161718192021222324252627282930313233343536public class CallableThreadTest implements Callable&lt;Integer&gt; &#123; public static void main(String[] args) &#123; CallableThreadTest ctt = new CallableThreadTest(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt); for(int i = 0;i &lt; 100;i++) &#123; System.out.println(Thread.currentThread().getName()+\" 的循环变量i的值\"+i); if(i==20) &#123; new Thread(ft,\"有返回值的线程\").start(); &#125; &#125; try &#123; System.out.println(\"子线程的返回值：\"+ft.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Integer call() throws Exception &#123; int i = 0; for(;i&lt;100;i++) &#123; System.out.println(Thread.currentThread().getName()+\" \"+i); &#125; return i; &#125; &#125;创建线程方法之间的区别实现Runnable和实现Callable接口的方式基本相同，不过是后者执行call()方法有返回值，后者线程执行体run()方法无返回值，因此可以把这两种方式归为一种这种方式与继承Thread类的方法之间的差别如下：1、线程只是实现Runnable或实现Callable接口，还可以继承其他类。2、这种方式下，多个线程可以共享一个target对象，非常适合多线程处理同一份资源的情形。3、但是编程稍微复杂，如果需要访问当前线程，必须调用Thread.currentThread()方法。4、继承Thread类的线程类不能再继承其他父类（Java单继承决定）。注：一般推荐采用实现接口的方式来创建多线程示例Account类123456789101112131415161718192021222324252627public class Acount &#123; private String UserName; private float Money; public Acount(String userName, float money) &#123; super(); UserName = userName; Money = money; &#125; public String getUserName() &#123; return UserName; &#125; public void setUserName(String userName) &#123; UserName = userName; &#125; public float getMoney() &#123; return Money; &#125; public void setMoney(float money) &#123; Money = money; &#125;&#125;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class BankOperator implements Runnable &#123; @Override public void run() &#123; //锁定对象 synchronized (a) &#123; withdraw(5); deposit(50); System.out.println(\"brent账户余额： \"+a.getMoney()); &#125; &#125; static Acount a; public void deposit(float money) &#123; a.setMoney(a.getMoney() + money); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \":\" + \"存款后 \" + a.getMoney()); &#125; public void withdraw(float money) &#123; a.setMoney(a.getMoney() - money); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \":\" + \"取款后 \" + a.getMoney()); &#125; BankOperator(Acount a) &#123; super(); this.a = a; &#125; public static void main(String args[]) &#123; Acount a = new Acount(\"brent\", 100); BankOperator bko = new BankOperator(a); Thread t1 = new Thread(bko, \"customer1\"); Thread t2 = new Thread(bko, \"customer2\"); Thread t3 = new Thread(bko, \"customer3\"); Thread t4 = new Thread(bko, \"customer4\"); t1.start(); t2.start(); t3.start(); t4.start(); &#125;&#125;","categories":[],"tags":[]},{"title":"进程和线程基本概念","slug":"进程和线程基本概念","date":"2016-08-07T07:30:21.000Z","updated":"2020-04-04T17:55:34.701Z","comments":true,"path":"2016/08/07/进程和线程基本概念/","link":"","permalink":"cpeixin.cn/2016/08/07/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"前言进程（process）和线程（thread）是操作系统的基本概念，也是平常编程的过程中，我们经常遇到和听到的名词由于上大学的时候，老师在讲解有关线程、进程课程时，我应该在玩《神庙逃亡》，所以这一方面一直不是很扎实就在最近，我读到了国外的一篇文章和阮一峰博士的文章，这两篇文章利用‘车间’和‘工人’的关系来类比，浅显易懂，分享给大家概念计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。进程假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工背后的含义就是，单个CPU一次只能运行一个任务进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。线程一个车间里，可以有很多工人。他们协同完成一个任务。线程就好比车间里的工人。背后的含义就是，一个进程可以包括多个线程线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。共享内存车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。背后的含义就是，这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存互斥锁可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。“互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域信号量还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。优先级如果卫生间目前已锁定并且有许多人正在等待使用它怎么办？显然，所有的人都坐在外面，等待浴室里的任何人出去。真正的问题是，“当门解锁时会发生什么？谁下次去？“你会认为允许等待时间最长的人接下来是“公平的”。或者，让最老的人走向下一步可能是“公平的”。或者最高。或者最重要的。有很多方法可以确定什么是“公平的”。我们通过两个因素来解决这个问题：优先级和等待时间。假设两个人同时出现在（锁定的）浴室门口。其中一个人有一个紧迫的截止日期（他们已经迟到了会议），而另一个则没有。让紧迫的截止日期的人下一步是不是有意义？嗯，当然会。唯一的问题是你如何决定谁更“重要”。 这可以通过分配优先级来完成（让我们使用像Neutrino这样的数字 - 一个是最低的可用优先级，255是此版本中最高的优先级）。房屋内有紧迫期限的人将获得更高的优先权，而那些没有最后期限的人将被赋予较低的优先权。线程也一样。线程从其父线程继承其调度算法，但可以调用 pthread_setschedparam（） 来更改其调度策略和优先级（如果它有权执行此操作）。如果有多个线程在等待，并且互斥锁被解锁，我们会将互斥锁提供给具有最高优先级的等待线程。但是，假设两个人都有同样的优先权。那你现在怎么办？那么，在这种情况下，允许等待时间最长的人下一步是“公平的”。这不仅是“公平的”，而且也是Neutrino内核的作用。在一堆线程等待的情况下，我们主要通过优先级，其次 是等待的长度。互斥锁肯定不是我们遇到的唯一同步对象。我们来看看其他一些。操作系统的设计，因此可以归结为三点：（1）以多进程形式，允许多个任务同时运行；（2）以多线程形式，允许单个任务分成不同的部分运行；（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。进程和线程简单而基本靠谱的定义如下：进程：程序的一次执行线程：CPU的基本调度单位","categories":[],"tags":[]},{"title":"Java基本数据类型所占字节数","slug":"Java基本数据类型所占字节数","date":"2015-09-13T14:53:30.000Z","updated":"2020-09-13T14:55:24.936Z","comments":true,"path":"2015/09/13/Java基本数据类型所占字节数/","link":"","permalink":"cpeixin.cn/2015/09/13/Java%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%89%80%E5%8D%A0%E5%AD%97%E8%8A%82%E6%95%B0/","excerpt":"","text":"JAVA是采用 Unicode 编码。每一个字节占8位 bit 。你电脑系统应该是 32位系统，这样每个int就是 4个字节 。其中，一个字节由8个二进制位组成 。1、整型类型存储空间bit数取值范围备注byte1字节1*8-2^7 ~ 2^7-1 （ 即：-128～127 )-short2字节2*8-2^15 - 2^15-1 （ 即：－32768～32767）-int4字节4*8-2^31 - 2^31-1 （ 即： ）-long8字节8*8-2^63 - 2^63-1 （ 即： ）-2、浮点型类型存储空间bit数取值范围备注float4字节4*8-2^31 - 2^31-1 （ 即： ）float类型的数值有一个后缀F (例如：3.14F)double8字节8*8-2^63 - 2^63-1 （ 即： ）没有后缀F的浮点数值(如3.14)默认为double类型3、char类型类型存储空间bit数取值范围备注char2字节2*8-2^15 - 2^15-1 （ 即：－32768～32767 ）-4、boolean类型类型存储空间bit数取值范围备注boolean1字节1*8false、true-","categories":[],"tags":[{"name":"java","slug":"java","permalink":"cpeixin.cn/tags/java/"}]}],"categories":[{"name":"categories","slug":"categories","permalink":"cpeixin.cn/categories/categories/"},{"name":"大数据","slug":"大数据","permalink":"cpeixin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"分布式","slug":"分布式","permalink":"cpeixin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"DataBases","slug":"DataBases","permalink":"cpeixin.cn/categories/DataBases/"},{"name":"Tools","slug":"Tools","permalink":"cpeixin.cn/categories/Tools/"},{"name":"python","slug":"python","permalink":"cpeixin.cn/categories/python/"},{"name":"开发工具","slug":"开发工具","permalink":"cpeixin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"NLP","slug":"NLP","permalink":"cpeixin.cn/categories/NLP/"},{"name":"架构","slug":"架构","permalink":"cpeixin.cn/categories/%E6%9E%B6%E6%9E%84/"},{"name":"设计模式","slug":"设计模式","permalink":"cpeixin.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Linux","slug":"Linux","permalink":"cpeixin.cn/categories/Linux/"},{"name":"算法","slug":"算法","permalink":"cpeixin.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"cpeixin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"工具","slug":"工具","permalink":"cpeixin.cn/categories/%E5%B7%A5%E5%85%B7/"},{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"cpeixin.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"DataBase","slug":"DataBase","permalink":"cpeixin.cn/categories/DataBase/"},{"name":"Docker","slug":"Docker","permalink":"cpeixin.cn/categories/Docker/"},{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"cpeixin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"hive","slug":"hive","permalink":"cpeixin.cn/tags/hive/"},{"name":"DMP","slug":"DMP","permalink":"cpeixin.cn/tags/DMP/"},{"name":"HBase","slug":"HBase","permalink":"cpeixin.cn/tags/HBase/"},{"name":"kafka","slug":"kafka","permalink":"cpeixin.cn/tags/kafka/"},{"name":"spark","slug":"spark","permalink":"cpeixin.cn/tags/spark/"},{"name":"redis","slug":"redis","permalink":"cpeixin.cn/tags/redis/"},{"name":"paxos","slug":"paxos","permalink":"cpeixin.cn/tags/paxos/"},{"name":"Flink","slug":"Flink","permalink":"cpeixin.cn/tags/Flink/"},{"name":"数据仓库","slug":"数据仓库","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"cpeixin.cn/tags/mysql/"},{"name":"flask","slug":"flask","permalink":"cpeixin.cn/tags/flask/"},{"name":"IDEA","slug":"IDEA","permalink":"cpeixin.cn/tags/IDEA/"},{"name":"hdfs","slug":"hdfs","permalink":"cpeixin.cn/tags/hdfs/"},{"name":"GPT-2","slug":"GPT-2","permalink":"cpeixin.cn/tags/GPT-2/"},{"name":"单例模式","slug":"单例模式","permalink":"cpeixin.cn/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"kali","slug":"kali","permalink":"cpeixin.cn/tags/kali/"},{"name":"服务器安全","slug":"服务器安全","permalink":"cpeixin.cn/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/"},{"name":"动态规划","slug":"动态规划","permalink":"cpeixin.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"flink","slug":"flink","permalink":"cpeixin.cn/tags/flink/"},{"name":"爬虫","slug":"爬虫","permalink":"cpeixin.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"布隆过滤器","slug":"布隆过滤器","permalink":"cpeixin.cn/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"shadowsock","slug":"shadowsock","permalink":"cpeixin.cn/tags/shadowsock/"},{"name":"Parquet","slug":"Parquet","permalink":"cpeixin.cn/tags/Parquet/"},{"name":"深度学习","slug":"深度学习","permalink":"cpeixin.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"ETL","slug":"ETL","permalink":"cpeixin.cn/tags/ETL/"},{"name":"时间序列","slug":"时间序列","permalink":"cpeixin.cn/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"cpeixin.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"cpeixin.cn/tags/Random-Forest/"},{"name":"PageRank","slug":"PageRank","permalink":"cpeixin.cn/tags/PageRank/"},{"name":"Apriori","slug":"Apriori","permalink":"cpeixin.cn/tags/Apriori/"},{"name":"EM","slug":"EM","permalink":"cpeixin.cn/tags/EM/"},{"name":"K-Means","slug":"K-Means","permalink":"cpeixin.cn/tags/K-Means/"},{"name":"KNN","slug":"KNN","permalink":"cpeixin.cn/tags/KNN/"},{"name":"SVM","slug":"SVM","permalink":"cpeixin.cn/tags/SVM/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"cpeixin.cn/tags/Naive-Bayes/"},{"name":"Decision Tree","slug":"Decision-Tree","permalink":"cpeixin.cn/tags/Decision-Tree/"},{"name":"sklearn","slug":"sklearn","permalink":"cpeixin.cn/tags/sklearn/"},{"name":"特征工程","slug":"特征工程","permalink":"cpeixin.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"数据清洗","slug":"数据清洗","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"},{"name":"数据采集","slug":"数据采集","permalink":"cpeixin.cn/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"},{"name":"用户画像","slug":"用户画像","permalink":"cpeixin.cn/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"},{"name":"词向量","slug":"词向量","permalink":"cpeixin.cn/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"},{"name":"python","slug":"python","permalink":"cpeixin.cn/tags/python/"},{"name":"OLAP","slug":"OLAP","permalink":"cpeixin.cn/tags/OLAP/"},{"name":"docker","slug":"docker","permalink":"cpeixin.cn/tags/docker/"},{"name":"HashMap","slug":"HashMap","permalink":"cpeixin.cn/tags/HashMap/"},{"name":"CPU","slug":"CPU","permalink":"cpeixin.cn/tags/CPU/"},{"name":"scala","slug":"scala","permalink":"cpeixin.cn/tags/scala/"},{"name":"回溯","slug":"回溯","permalink":"cpeixin.cn/tags/%E5%9B%9E%E6%BA%AF/"},{"name":"堆","slug":"堆","permalink":"cpeixin.cn/tags/%E5%A0%86/"},{"name":"二叉树","slug":"二叉树","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"skipList","slug":"skipList","permalink":"cpeixin.cn/tags/skipList/"},{"name":"红黑树","slug":"红黑树","permalink":"cpeixin.cn/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"name":"排序","slug":"排序","permalink":"cpeixin.cn/tags/%E6%8E%92%E5%BA%8F/"},{"name":"yarn","slug":"yarn","permalink":"cpeixin.cn/tags/yarn/"},{"name":"mapreduce","slug":"mapreduce","permalink":"cpeixin.cn/tags/mapreduce/"},{"name":"递归","slug":"递归","permalink":"cpeixin.cn/tags/%E9%80%92%E5%BD%92/"},{"name":"二分查找","slug":"二分查找","permalink":"cpeixin.cn/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"name":"散列表","slug":"散列表","permalink":"cpeixin.cn/tags/%E6%95%A3%E5%88%97%E8%A1%A8/"},{"name":"链表","slug":"链表","permalink":"cpeixin.cn/tags/%E9%93%BE%E8%A1%A8/"},{"name":"LRU淘汰算法","slug":"LRU淘汰算法","permalink":"cpeixin.cn/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"},{"name":"栈","slug":"栈","permalink":"cpeixin.cn/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"cpeixin.cn/tags/%E9%98%9F%E5%88%97/"},{"name":"线性表","slug":"线性表","permalink":"cpeixin.cn/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/"},{"name":"数组","slug":"数组","permalink":"cpeixin.cn/tags/%E6%95%B0%E7%BB%84/"},{"name":"java","slug":"java","permalink":"cpeixin.cn/tags/java/"}]}