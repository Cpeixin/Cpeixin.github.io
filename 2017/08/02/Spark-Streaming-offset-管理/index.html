<!-- build time:Mon Jun 01 2020 10:39:01 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta name="referrer" content="no-referrer"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Spark Streaming offset 管理 | 布兰特 | 不忘初心</title><meta name="description" content="越来越多的实时项目需求，感觉好像各个业务线，产品都想让自己的数据动起来，并且配合上数据可视化展示出来。那么在使用Spark Streaming过程中，肯定不能避免一个问题，那就是，你全天24小时运行的实时程序，如果在某一时刻因为各种原因，停掉。当你发现实时程序已经因为故障停止运行了1个小时，或者产品运营中数据的使用者打电话给你，通知你最近一个小时的数据没有显示（尴尬😅），或者实时程序要临时升级，"><meta property="og:type" content="article"><meta property="og:title" content="Spark Streaming offset 管理"><meta property="og:url" content="cpeixin.cn/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/index.html"><meta property="og:site_name" content="布兰特 | 不忘初心"><meta property="og:description" content="越来越多的实时项目需求，感觉好像各个业务线，产品都想让自己的数据动起来，并且配合上数据可视化展示出来。那么在使用Spark Streaming过程中，肯定不能避免一个问题，那就是，你全天24小时运行的实时程序，如果在某一时刻因为各种原因，停掉。当你发现实时程序已经因为故障停止运行了1个小时，或者产品运营中数据的使用者打电话给你，通知你最近一个小时的数据没有显示（尴尬😅），或者实时程序要临时升级，"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1588788798279-e9188290-1341-4868-aa77-56f8f38a5004.jpeg#align=left&display=inline&height=359&margin=%5Bobject%20Object%5D&name=image.jpeg&originHeight=359&originWidth=638&size=34666&status=done&style=none&width=638"><meta property="article:published_time" content="2017-08-02T08:03:44.000Z"><meta property="article:modified_time" content="2020-05-07T08:13:24.491Z"><meta property="article:author" content="Brent"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1588788798279-e9188290-1341-4868-aa77-56f8f38a5004.jpeg#align=left&display=inline&height=359&margin=%5Bobject%20Object%5D&name=image.jpeg&originHeight=359&originWidth=638&size=34666&status=done&style=none&width=638"><link rel="canonical" href="cpeixin.cn/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/index.html"><link rel="alternate" href="/atom.xml" title="布兰特 | 不忘初心" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.0"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Brent</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">大数据工程师 &amp; 机器学习</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Malaysia</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">51</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">31</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%B6%E6%9E%84/">架构</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">14</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Apriori/" rel="tag">Apriori</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/" rel="tag">EM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ETL/" rel="tag">ETL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/" rel="tag">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/" rel="tag">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-Means/" rel="tag">K-Means</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/" rel="tag">KNN</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" rel="tag">LRU淘汰算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OLAP/" rel="tag">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/" rel="tag">PageRank</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Random-Forest/" rel="tag">Random Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flask/" rel="tag">flask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kali/" rel="tag">kali</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/" rel="tag">mapreduce</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shadowsock/" rel="tag">shadowsock</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a><span class="tag-list-count">16</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/" rel="tag">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" rel="tag">数据仓库</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" rel="tag">数据清洗</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" rel="tag">数据采集</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" rel="tag">时间序列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" rel="tag">服务器安全</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%88/" rel="tag">栈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" rel="tag">用户画像</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" rel="tag">线性表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag">词向量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8/" rel="tag">链表</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%9F%E5%88%97/" rel="tag">队列</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/Apriori/" style="font-size:13.14px">Apriori</a> <a href="/tags/Decision-Tree/" style="font-size:13.43px">Decision Tree</a> <a href="/tags/EM/" style="font-size:13.14px">EM</a> <a href="/tags/ETL/" style="font-size:13px">ETL</a> <a href="/tags/Flink/" style="font-size:13.86px">Flink</a> <a href="/tags/GPT-2/" style="font-size:13px">GPT-2</a> <a href="/tags/IDEA/" style="font-size:13px">IDEA</a> <a href="/tags/K-Means/" style="font-size:13px">K-Means</a> <a href="/tags/KNN/" style="font-size:13.14px">KNN</a> <a href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" style="font-size:13px">LRU淘汰算法</a> <a href="/tags/Naive-Bayes/" style="font-size:13.14px">Naive Bayes</a> <a href="/tags/OLAP/" style="font-size:13px">OLAP</a> <a href="/tags/PageRank/" style="font-size:13.14px">PageRank</a> <a href="/tags/Random-Forest/" style="font-size:13px">Random Forest</a> <a href="/tags/SVM/" style="font-size:13.14px">SVM</a> <a href="/tags/docker/" style="font-size:13.14px">docker</a> <a href="/tags/flask/" style="font-size:13.14px">flask</a> <a href="/tags/flink/" style="font-size:13px">flink</a> <a href="/tags/hdfs/" style="font-size:13.29px">hdfs</a> <a href="/tags/hive/" style="font-size:13.14px">hive</a> <a href="/tags/kali/" style="font-size:13px">kali</a> <a href="/tags/mapreduce/" style="font-size:13px">mapreduce</a> <a href="/tags/mysql/" style="font-size:13px">mysql</a> <a href="/tags/python/" style="font-size:13.57px">python</a> <a href="/tags/redis/" style="font-size:13px">redis</a> <a href="/tags/scala/" style="font-size:13.29px">scala</a> <a href="/tags/shadowsock/" style="font-size:13px">shadowsock</a> <a href="/tags/sklearn/" style="font-size:13px">sklearn</a> <a href="/tags/spark/" style="font-size:14px">spark</a> <a href="/tags/yarn/" style="font-size:13px">yarn</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size:13.14px">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size:13.14px">二叉树</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size:13.71px">排序</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" style="font-size:13.57px">数据仓库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" style="font-size:13px">数据清洗</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" style="font-size:13px">数据采集</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size:13px">数组</a> <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" style="font-size:13px">时间序列</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" style="font-size:13.14px">服务器安全</a> <a href="/tags/%E6%A0%88/" style="font-size:13px">栈</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:13px">深度学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:13px">爬虫</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size:13.29px">特征工程</a> <a href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" style="font-size:13.14px">用户画像</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" style="font-size:13px">线性表</a> <a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" style="font-size:13px">词向量</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size:13px">递归</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size:13px">逻辑回归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size:13.14px">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size:13px">队列</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">二月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">13</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Tools/">Tools</a></p><p class="item-title"><a href="/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/" class="title">Linux直接下载Google Drive文件</a></p><p class="item-date"><time datetime="2020-05-27T17:03:54.000Z" itemprop="datePublished">2020-05-28</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/2020/04/06/def-neverGrowUp/" class="title">def neverGrowUp()</a></p><p class="item-date"><time datetime="2020-04-05T16:00:00.000Z" itemprop="datePublished">2020-04-06</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/" class="title">抗疫英雄</a></p><p class="item-date"><time datetime="2020-04-04T14:45:15.000Z" itemprop="datePublished">2020-04-04</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/python/">python</a></p><p class="item-title"><a href="/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/" class="title">python Flask &amp; Ajax 数据传输</a></p><p class="item-date"><time datetime="2020-03-11T14:43:01.000Z" itemprop="datePublished">2020-03-11</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/python/">python</a></p><p class="item-title"><a href="/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/" class="title">Python Flask接口设计-示例</a></p><p class="item-date"><time datetime="2020-03-10T15:08:35.000Z" itemprop="datePublished">2020-03-10</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-Spark-Streaming-offset-管理" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Spark Streaming offset 管理</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/" class="article-date"><time datetime="2017-08-02T08:03:44.000Z" itemprop="datePublished">2017-08-02</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/" class="leancloud_visitors" data-flag-title="Spark Streaming offset 管理">0</span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 4k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 19(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p><br>越来越多的实时项目需求，感觉好像各个业务线，产品都想让自己的数据动起来，并且配合上数据可视化展示出来。那么在使用Spark Streaming过程中，肯定不能避免一个问题，那就是，你全天24小时运行的实时程序，如果在某一时刻因为各种原因，停掉。当你发现实时程序已经因为故障停止运行了1个小时，或者产品运营中数据的使用者打电话给你，通知你最近一个小时的数据没有显示（尴尬😅），或者实时程序要临时升级，需要添加新的业务逻辑重新部署。<br><br><br>那么此时，在你还没有做offset管理的时候，你准备怎么办呢？是”auto.offset.reset” -&gt; “latest” 从最新的offset位移数据读起，放弃那1个小时未读取到的数据，还是”auto.offset.reset” -&gt; “earliest”, 从最早的位移数据读取，重新处理一遍所有topic数据。思考一下，这两种情况都不是一个好的办法，所以，我们要在有这个需求的程序中，来进行offset的管理，避免数据的丢失以及重复计算的问题。<br></p><ul><li>将offset存储在外部数据存储中<ul><li>Checkpoints</li><li>HBase</li><li>ZooKeeper</li><li>Kafka</li><li>redis</li></ul></li><li>不管理offset</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1588788798279-e9188290-1341-4868-aa77-56f8f38a5004.jpeg#align=left&display=inline&height=359&margin=%5Bobject%20Object%5D&name=image.jpeg&originHeight=359&originWidth=638&size=34666&status=done&style=none&width=638" alt="image.jpeg"></p><p>上图描述了在Spark Streaming应用程序中管理offset的一般流程。offset可以通过几种方式进行管理，但通常遵循以下通用步骤。</p><ol><li>在Direct DStream初始化后，可以指定每个主题分区的offset映射，以了解Direct DStream应该从哪个分区开始读取。<ol><li>指定的偏移量与下面的第4步写入的位置相同。</li></ol></li><li>然后可以读取和处理这批消息。</li><li>处理后，结果和offset都可以存储。<ol><li>_存储结果<em>和</em>提交偏移量_动作周围的虚线只是突出显示了一系列步骤，如果需要特殊的交付语义更严格的情况，用户可能需要进一步检查。这可能包括检查幂等运算或将结果及其偏移量存储在原子运算中。</li></ol></li><li>最后，任何外部持久数据存储（例如HBase，Kafka，HDFS和ZooKeeper）都可以用来跟踪已处理的消息。</li></ol><p>根据业务需求，可以将不同的方案合并到上述步骤中。Spark的编程灵活性允许用户进行细粒度的控制，以在处理的周期性阶段之前或之后存储offset。考虑发生以下情况的应用程序：Spark Streaming应用程序正在从Kafka读取消息，针对HBase数据执行转换操作，然后将操作后的消息发布到另一个topic中或单独的系统（例如，其他消息传递系统，到HBase，Solr，DBMS等）。在这种情况下，只有将消息成功发布到辅助系统后，我们才将其视为已处理。</p><p><a name="KBWe3"></a></p><h3 id="外部存储offset"><a href="#外部存储offset" class="headerlink" title="外部存储offset"></a>外部存储offset</h3><p><br>在本节中，我们探索用于在持久数据存储区中将offset持久保存在外部的不同选项。对于本节中提到的方法，如果使用spark-streaming-kafka-0-10_2.**库，建议用户将enable.auto.commit 设置为false。此配置仅适用于此版本，将enable.auto.commit 设置为true意味着offset将以config auto.commit.interval.ms控制的频率自动提交。在Spark Streaming中，将此值设置为true会在从Kafka读取消息时自动向Kafka提交偏移量，这不一定意味着Spark已完成对这些消息的处理。要启用精确的偏移量控制，请将Kafka参数enable.auto.commit 设置为 false。<br><a name="LZmp5"></a></p><h4><a href="#" class="headerlink"></a></h4><p><a name="zzdS8"></a></p><h4 id="Spark-Streaming-checkpoints"><a href="#Spark-Streaming-checkpoints" class="headerlink" title="Spark Streaming checkpoints"></a>Spark Streaming checkpoints</h4><p>启用Spark Streaming的是存储checkpoints的最简单方法，因为它在Spark的框架中很容易获得。checkpoint是专门为保存应用程序的状态（一般情况下保存在HDFS）而设计的，以便可以在出现故障时将其恢复。</p><p>对Kafka流进行检查点将导致偏移范围存储在检查点中。如果出现故障，Spark Streaming应用程序可以开始从检查点偏移范围读取消息。但是，Spark Streaming检查点无法在Spark应用程序升级之后恢复，因此不是很可靠，尤其是当您将这种机制用于关键的生产应用程序时。我们不建议通过Spark检查点管理偏移量。</p><p><a name="vpiys"></a></p><h4 id="在HBase中存储offset"><a href="#在HBase中存储offset" class="headerlink" title="在HBase中存储offset"></a>在HBase中存储offset</h4><p><br>HBase可用作外部数据存储，以可靠的方式保留偏移范围。通过在外部存储偏移量范围，它使Spark Streaming应用程序能够从任意时间点重新启动和重播消息，只要消息在Kafka中仍然有效。</p><p>借助HBase的通用设计，该应用程序能够利用rowkey和column family 来处理跨同一表中的多个Spark Streaming应用程序和Kafka topic 存储偏移范围。在此示例中，是使用包含topic，group id和Spark Streaming 的 batchTime.milliSeconds 组合作为行键来区分写入表的每个条目。新记录将累积在我们在以下设计中配置的表格中，以在30天后自动过期。下面是HBase表的DDL和结构。<br><br><br>ddl</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#39;stream_kafka_offsets&#39;, &#123;NAME&#x3D;&gt;&#39;offsets&#39;, TTL&#x3D;&gt;2592000&#125;</span><br></pre></td></tr></table></figure><p><br>RowKey 设计</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">row:              &lt;TOPIC_NAME&gt;:&lt;GROUP_ID&gt;:&lt;EPOCH_BATCHTIME_MS&gt;</span><br><span class="line">column family:    offsets</span><br><span class="line">qualifier:        &lt;PARTITION_ID&gt;</span><br><span class="line">value:            &lt;OFFSET_ID&gt;</span><br></pre></td></tr></table></figure><p><br>下面直接给出在HBase中，offset的管理设计流程代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.utils.<span class="type">ZkUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.<span class="type">PrefixFilter</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">TableName</span>, <span class="type">HBaseConfiguration</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.&#123;<span class="type">Scan</span>, <span class="type">Put</span>, <span class="type">ConnectionFactory</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.<span class="type">TopicPartition</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">OffsetRange</span>, <span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by gmedasani on 6/10/17.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaOffsetsBlogStreamingDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">6</span>) &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: KafkaDirectStreamTest &lt;batch-duration-in-seconds&gt; &lt;kafka-bootstrap-servers&gt; "</span> +</span><br><span class="line">        <span class="string">"&lt;kafka-topics&gt; &lt;kafka-consumer-group-id&gt; &lt;hbase-table-name&gt; &lt;kafka-zookeeper-quorum&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> batchDuration = args(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> bootstrapServers = args(<span class="number">1</span>).toString</span><br><span class="line">    <span class="keyword">val</span> topicsSet = args(<span class="number">2</span>).toString.split(<span class="string">","</span>).toSet</span><br><span class="line">    <span class="keyword">val</span> consumerGroupID = args(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> hbaseTableName = args(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">val</span> zkQuorum = args(<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> zkKafkaRootDir = <span class="string">"kafka"</span></span><br><span class="line">    <span class="keyword">val</span> zkSessionTimeOut = <span class="number">10000</span></span><br><span class="line">    <span class="keyword">val</span> zkConnectionTimeOut = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Kafka-Offset-Management-Blog"</span>)</span><br><span class="line">                                  .setMaster(<span class="string">"local[4]"</span>)<span class="comment">//Uncomment this line to test while developing on a workstation</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(batchDuration.toLong))</span><br><span class="line">    <span class="keyword">val</span> topics = topicsSet.toArray</span><br><span class="line">    <span class="keyword">val</span> topic = topics(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; bootstrapServers,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; consumerGroupID,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>,</span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    Create a dummy process that simply returns the message as is.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">processMessage</span></span>(message:<span class="type">ConsumerRecord</span>[<span class="type">String</span>,<span class="type">String</span>]):<span class="type">ConsumerRecord</span>[<span class="type">String</span>,<span class="type">String</span>]=&#123;</span><br><span class="line">      message</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    Save Offsets into HBase</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">saveOffsets</span></span>(<span class="type">TOPIC_NAME</span>:<span class="type">String</span>,<span class="type">GROUP_ID</span>:<span class="type">String</span>,offsetRanges:<span class="type">Array</span>[<span class="type">OffsetRange</span>],hbaseTableName:<span class="type">String</span>,</span><br><span class="line">                    batchTime: org.apache.spark.streaming.<span class="type">Time</span>) =&#123;</span><br><span class="line">      <span class="keyword">val</span> hbaseConf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      hbaseConf.addResource(<span class="string">"src/main/resources/hbase-site.xml"</span>)</span><br><span class="line">      <span class="keyword">val</span> conn = <span class="type">ConnectionFactory</span>.createConnection(hbaseConf)</span><br><span class="line">      <span class="keyword">val</span> table = conn.getTable(<span class="type">TableName</span>.valueOf(hbaseTableName))</span><br><span class="line">      <span class="keyword">val</span> rowKey = <span class="type">TOPIC_NAME</span> + <span class="string">":"</span> + <span class="type">GROUP_ID</span> + <span class="string">":"</span> + <span class="type">String</span>.valueOf(batchTime.milliseconds)</span><br><span class="line">      <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(rowKey.getBytes)</span><br><span class="line">      <span class="keyword">for</span>(offset &lt;- offsetRanges)&#123;</span><br><span class="line">        put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="string">"offsets"</span>),<span class="type">Bytes</span>.toBytes(offset.partition.toString),</span><br><span class="line">          <span class="type">Bytes</span>.toBytes(offset.untilOffset.toString))</span><br><span class="line">      &#125;</span><br><span class="line">      table.put(put)</span><br><span class="line">      conn.close()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    Returns last committed offsets for all the partitions of a given topic from HBase in following cases.</span></span><br><span class="line"><span class="comment">      - CASE 1: SparkStreaming job is started for the first time. This function gets the number of topic partitions from</span></span><br><span class="line"><span class="comment">        Zookeeper and for each partition returns the last committed offset as 0</span></span><br><span class="line"><span class="comment">      - CASE 2: SparkStreaming is restarted and there are no changes to the number of partitions in a topic. Last</span></span><br><span class="line"><span class="comment">        committed offsets for each topic-partition is returned as is from HBase.</span></span><br><span class="line"><span class="comment">      - CASE 3: SparkStreaming is restarted and the number of partitions in a topic increased. For old partitions, last</span></span><br><span class="line"><span class="comment">        committed offsets for each topic-partition is returned as is from HBase as is. For newly added partitions,</span></span><br><span class="line"><span class="comment">        function returns last committed offsets as 0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getLastCommittedOffsets</span></span>(<span class="type">TOPIC_NAME</span>:<span class="type">String</span>,<span class="type">GROUP_ID</span>:<span class="type">String</span>,hbaseTableName:<span class="type">String</span>,zkQuorum:<span class="type">String</span>,</span><br><span class="line">                                zkRootDir:<span class="type">String</span>, sessionTimeout:<span class="type">Int</span>,connectionTimeOut:<span class="type">Int</span>):<span class="type">Map</span>[<span class="type">TopicPartition</span>,<span class="type">Long</span>] =&#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hbaseConf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      hbaseConf.addResource(<span class="string">"src/main/resources/hbase-site.xml"</span>)</span><br><span class="line">      <span class="keyword">val</span> zkUrl = zkQuorum+<span class="string">"/"</span>+zkRootDir</span><br><span class="line">      <span class="keyword">val</span> zkClientAndConnection = <span class="type">ZkUtils</span>.createZkClientAndConnection(zkUrl,sessionTimeout,connectionTimeOut)</span><br><span class="line">      <span class="keyword">val</span> zkUtils = <span class="keyword">new</span> <span class="type">ZkUtils</span>(zkClientAndConnection._1, zkClientAndConnection._2,<span class="literal">false</span>)</span><br><span class="line">      <span class="keyword">val</span> zKNumberOfPartitionsForTopic = zkUtils.getPartitionsForTopics(<span class="type">Seq</span>(<span class="type">TOPIC_NAME</span>)).get(<span class="type">TOPIC_NAME</span>).toList.head.size</span><br><span class="line"></span><br><span class="line">      <span class="comment">//Connect to HBase to retrieve last committed offsets</span></span><br><span class="line">      <span class="keyword">val</span> conn = <span class="type">ConnectionFactory</span>.createConnection(hbaseConf)</span><br><span class="line">      <span class="keyword">val</span> table = conn.getTable(<span class="type">TableName</span>.valueOf(hbaseTableName))</span><br><span class="line">      <span class="keyword">val</span> startRow = <span class="type">TOPIC_NAME</span> + <span class="string">":"</span> + <span class="type">GROUP_ID</span> + <span class="string">":"</span> + <span class="type">String</span>.valueOf(<span class="type">System</span>.currentTimeMillis())</span><br><span class="line">      <span class="keyword">val</span> stopRow = <span class="type">TOPIC_NAME</span> + <span class="string">":"</span> + <span class="type">GROUP_ID</span> + <span class="string">":"</span> + <span class="number">0</span></span><br><span class="line">      <span class="keyword">val</span> scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">      <span class="keyword">val</span> scanner = table.getScanner(scan.setStartRow(startRow.getBytes).setStopRow(stopRow.getBytes).setReversed(<span class="literal">true</span>))</span><br><span class="line">      <span class="keyword">val</span> result = scanner.next()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> hbaseNumberOfPartitionsForTopic = <span class="number">0</span> <span class="comment">//Set the number of partitions discovered for a topic in HBase to 0</span></span><br><span class="line">      <span class="keyword">if</span> (result != <span class="literal">null</span>)&#123;</span><br><span class="line">        <span class="comment">//If the result from hbase scanner is not null, set number of partitions from hbase to the number of cells</span></span><br><span class="line">        hbaseNumberOfPartitionsForTopic = result.listCells().size()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> fromOffsets = collection.mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>,<span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(hbaseNumberOfPartitionsForTopic == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="comment">// initialize fromOffsets to beginning</span></span><br><span class="line">          <span class="keyword">for</span> (partition &lt;- <span class="number">0</span> to zKNumberOfPartitionsForTopic<span class="number">-1</span>)&#123;</span><br><span class="line">            fromOffsets += (<span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">TOPIC_NAME</span>,partition) -&gt; <span class="number">0</span>)&#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(zKNumberOfPartitionsForTopic &gt; hbaseNumberOfPartitionsForTopic)&#123;</span><br><span class="line">        <span class="comment">// handle scenario where new partitions have been added to existing kafka topic</span></span><br><span class="line">          <span class="keyword">for</span> (partition &lt;- <span class="number">0</span> to hbaseNumberOfPartitionsForTopic<span class="number">-1</span>)&#123;</span><br><span class="line">            <span class="keyword">val</span> fromOffset = <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(<span class="string">"offsets"</span>),<span class="type">Bytes</span>.toBytes(partition.toString)))</span><br><span class="line">            fromOffsets += (<span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">TOPIC_NAME</span>,partition) -&gt; fromOffset.toLong)&#125;</span><br><span class="line">          <span class="keyword">for</span> (partition &lt;- hbaseNumberOfPartitionsForTopic to zKNumberOfPartitionsForTopic<span class="number">-1</span>)&#123;</span><br><span class="line">            fromOffsets += (<span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">TOPIC_NAME</span>,partition) -&gt; <span class="number">0</span>)&#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//initialize fromOffsets from last run</span></span><br><span class="line">          <span class="keyword">for</span> (partition &lt;- <span class="number">0</span> to hbaseNumberOfPartitionsForTopic<span class="number">-1</span> )&#123;</span><br><span class="line">            <span class="keyword">val</span> fromOffset = <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(<span class="string">"offsets"</span>),<span class="type">Bytes</span>.toBytes(partition.toString)))</span><br><span class="line">            fromOffsets += (<span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">TOPIC_NAME</span>,partition) -&gt; fromOffset.toLong)&#125;</span><br><span class="line">      &#125;</span><br><span class="line">      scanner.close()</span><br><span class="line">      conn.close()</span><br><span class="line">      fromOffsets.toMap</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fromOffsets= getLastCommittedOffsets(topic,consumerGroupID,hbaseTableName,zkQuorum,zkKafkaRootDir,</span><br><span class="line">      zkSessionTimeOut,zkConnectionTimeOut)</span><br><span class="line">    <span class="keyword">val</span> inputDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,<span class="type">PreferConsistent</span>,<span class="type">Assign</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      fromOffsets.keys,kafkaParams,fromOffsets))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      For each RDD in a DStream apply a map transformation that processes the message.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    inputDStream.foreachRDD((rdd,batchTime) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">      offsetRanges.foreach(offset =&gt; println(offset.topic, offset.partition, offset.fromOffset,offset.untilOffset))</span><br><span class="line">      <span class="keyword">val</span> newRDD = rdd.map(message =&gt; processMessage(message))</span><br><span class="line">      newRDD.count()</span><br><span class="line">      saveOffsets(topic,consumerGroupID,offsetRanges,hbaseTableName,batchTime) <span class="comment">//save the offsets to HBase</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"Number of messages processed "</span> + inputDStream.count())</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a name="HqsIW"></a></p><h4 id="在ZooKeeper中存储offset"><a href="#在ZooKeeper中存储offset" class="headerlink" title="在ZooKeeper中存储offset"></a>在ZooKeeper中存储offset</h4><p><br>用户可以将偏移范围存储在ZooKeeper中，这可以类似地提供一种可靠的方法，以在最后停止的Kafka流上开始流处理。</p><p>在这种情况下，启动时，Spark Streaming作业将从ZooKeeper中检索每个主题分区的最新处理过的偏移量。如果找到了一个以前在ZooKeeper中未管理过的新分区，则默认将其最新处理的偏移量从头开始。处理完每批后，用户可以存储第一个或最后一个处理过的偏移量。此外，在ZooKeeper中存储偏移量的znode位置使用与旧Kafka使用者API相同的格式。因此，用于跟踪或监视存储在ZooKeeper中的Kafka偏移量的任何工具仍然可以使用。</p><p>初始化ZooKeeper连接，以获取和存储到ZooKeeper的偏移量：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> zkClientAndConnection = <span class="type">ZkUtils</span>.createZkClientAndConnection(zkUrl, sessionTimeout, connectionTimeout)</span><br><span class="line"><span class="keyword">val</span> zkUtils = <span class="keyword">new</span> <span class="type">ZkUtils</span>(zkClientAndConnection._1, zkClientAndConnection._2, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>检索存储在使用者组和主题列表的ZooKeeper中的最后偏移量的方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readOffsets</span></span>(topics: <span class="type">Seq</span>[<span class="type">String</span>], groupId:<span class="type">String</span>):</span><br><span class="line"> <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">val</span> topicPartOffsetMap = collection.mutable.<span class="type">HashMap</span>.empty[<span class="type">TopicPartition</span>, <span class="type">Long</span>]</span><br><span class="line"> <span class="keyword">val</span> partitionMap = zkUtils.getPartitionsForTopics(topics)</span><br><span class="line"> </span><br><span class="line"> <span class="comment">// /consumers/&lt;groupId&gt;/offsets/&lt;topic&gt;/</span></span><br><span class="line"> partitionMap.foreach(topicPartitions =&gt; &#123;</span><br><span class="line">   <span class="keyword">val</span> zkGroupTopicDirs = <span class="keyword">new</span> <span class="type">ZKGroupTopicDirs</span>(groupId, topicPartitions._1)</span><br><span class="line">   topicPartitions._2.foreach(partition =&gt; &#123;</span><br><span class="line">     <span class="keyword">val</span> offsetPath = zkGroupTopicDirs.consumerOffsetDir + <span class="string">"/"</span> + partition</span><br><span class="line"> </span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="keyword">val</span> offsetStatTuple = zkUtils.readData(offsetPath)</span><br><span class="line">       <span class="keyword">if</span> (offsetStatTuple != <span class="literal">null</span>) &#123;</span><br><span class="line">         <span class="type">LOGGER</span>.info(<span class="string">"retrieving offset details - topic: &#123;&#125;, partition: &#123;&#125;, offset: &#123;&#125;, node path: &#123;&#125;"</span>, <span class="type">Seq</span>[<span class="type">AnyRef</span>](topicPartitions._1, partition.toString, offsetStatTuple._1, offsetPath): _*)</span><br><span class="line"> </span><br><span class="line">         topicPartOffsetMap.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(topicPartitions._1, <span class="type">Integer</span>.valueOf(partition)),</span><br><span class="line">           offsetStatTuple._1.toLong)</span><br><span class="line">       &#125;</span><br><span class="line"> </span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">         <span class="type">LOGGER</span>.warn(<span class="string">"retrieving offset details - no previous node exists:"</span> + <span class="string">" &#123;&#125;, topic: &#123;&#125;, partition: &#123;&#125;, node path: &#123;&#125;"</span>, <span class="type">Seq</span>[<span class="type">AnyRef</span>](e.getMessage, topicPartitions._1, partition.toString, offsetPath): _*)</span><br><span class="line"> </span><br><span class="line">         topicPartOffsetMap.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(topicPartitions._1, <span class="type">Integer</span>.valueOf(partition)), <span class="number">0</span>L)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;)</span><br><span class="line"> &#125;)</span><br><span class="line"> </span><br><span class="line"> topicPartOffsetMap.toMap</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用特定的偏移量初始化Kafka Direct Dstream以开始处理：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputDStream = <span class="type">KafkaUtils</span>.createDirectStream(ssc, <span class="type">PreferConsistent</span>, <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>,<span class="type">String</span>](topics, kafkaParams, fromOffsets))</span><br></pre></td></tr></table></figure><p>将一组可恢复的偏移量持久保存到ZooKeeper的方法。<br>注意：_offsetPath_是一个ZooKeeper位置，表示为/ consumers / [groupId] / offsets / topic / [partitionId]，用于存储偏移值</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persistOffsets</span></span>(offsets: <span class="type">Seq</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>, storeEndOffset: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"> offsets.foreach(or =&gt; &#123;</span><br><span class="line">   <span class="keyword">val</span> zkGroupTopicDirs = <span class="keyword">new</span> <span class="type">ZKGroupTopicDirs</span>(groupId, or.topic);</span><br><span class="line"> </span><br><span class="line">   <span class="keyword">val</span> acls = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">ACL</span>]()</span><br><span class="line">   <span class="keyword">val</span> acl = <span class="keyword">new</span> <span class="type">ACL</span></span><br><span class="line">   acl.setId(<span class="type">ANYONE_ID_UNSAFE</span>)</span><br><span class="line">   acl.setPerms(<span class="type">PERMISSIONS_ALL</span>)</span><br><span class="line">   acls += acl</span><br><span class="line"> </span><br><span class="line">   <span class="keyword">val</span> offsetPath = zkGroupTopicDirs.consumerOffsetDir + <span class="string">"/"</span> + or.partition;</span><br><span class="line">   <span class="keyword">val</span> offsetVal = <span class="keyword">if</span> (storeEndOffset) or.untilOffset <span class="keyword">else</span> or.fromOffset</span><br><span class="line">   zkUtils.updatePersistentPath(zkGroupTopicDirs.consumerOffsetDir + <span class="string">"/"</span></span><br><span class="line">     + or.partition, offsetVal + <span class="string">""</span>, <span class="type">JavaConversions</span>.bufferAsJavaList(acls))</span><br><span class="line"> </span><br><span class="line">   <span class="type">LOGGER</span>.debug(<span class="string">"persisting offset details - topic: &#123;&#125;, partition: &#123;&#125;, offset: &#123;&#125;, node path: &#123;&#125;"</span>, <span class="type">Seq</span>[<span class="type">AnyRef</span>](or.topic, or.partition.toString, offsetVal.toString, offsetPath): _*)</span><br><span class="line"> &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a name="rGAFt"></a></p><h4 id="在kafka中管理offset"><a href="#在kafka中管理offset" class="headerlink" title="在kafka中管理offset"></a>在kafka中管理offset</h4><p><br>在Apache Spark 2.1.x的Cloudera发行版中，spark-streaming-kafka-0-10使用了新的Consumer api，它公开了commitAsync API。使用commitAsync API，使用方可以在知道输出已存储后将偏移量提交给Kafka。新的使用者api根据使用者的_group.id_唯一地将偏移提交回Kafka 。</p><p>Persist Offsets in Kafka</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKafkaRDD</span></span>(ssc: <span class="type">StreamingContext</span>, config: <span class="type">Source</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> <span class="type">SparkDStream</span>: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">SparkDStream</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">          <span class="string">"bootstrap.servers"</span> -&gt; config.servers,</span><br><span class="line">          <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">          <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">          <span class="string">"group.id"</span> -&gt; config.group,</span><br><span class="line">          <span class="string">"auto.offset.reset"</span> -&gt; config.offset</span><br><span class="line">        )</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">          "enable.auto.commit" -&gt; config.getString("kafkaSource.enable.auto.commit"))*/</span></span><br><span class="line">        <span class="comment">// val subscribeTopics = config.getStringList("kafkaSource.topics").toIterable</span></span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">        <span class="keyword">val</span> kafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">          ssc,</span><br><span class="line">          <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">          <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](config.topic.toList, kafkaParams)</span><br><span class="line">        )</span><br><span class="line">        kafkaStream</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"Couldn't init Spark stream processing"</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">SparkDStream</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> inputDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = createKafkaRDD（）</span><br><span class="line">inputDStream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">            <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">            <span class="comment">// 更新 Offset 值</span></span><br><span class="line">            inputDStream.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>有关详细信息，请访问– <a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#kafka-itself" target="_blank" rel="external nofollow noopener noreferrer">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#kafka-itself</a><br>注意：commitAsync（）是Spark Streaming和Kafka Integration的kafka-0-10版本的一部分。如Spark文档所述，此集成仍处于试验阶段，API可能会发生变化。</p><p><a name="i24rb"></a></p><h4 id="在Redis中存储offset"><a href="#在Redis中存储offset" class="headerlink" title="在Redis中存储offset"></a>在Redis中存储offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.<span class="type">TopicPartition</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Try</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaOffsetsBlogStreamingDriver</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 根据groupId保存offset</span></span><br><span class="line"><span class="comment">    * @param ranges</span></span><br><span class="line"><span class="comment">    * @param groupId</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">storeOffset</span></span>(ranges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (o &lt;- ranges) &#123;</span><br><span class="line">      <span class="keyword">val</span> key = <span class="string">s"bi_kafka_offset_<span class="subst">$&#123;groupId&#125;</span>_<span class="subst">$&#123;o.topic&#125;</span>_<span class="subst">$&#123;o.partition&#125;</span>"</span></span><br><span class="line">      <span class="keyword">val</span> value = o.untilOffset</span><br><span class="line">      <span class="type">JedisUtil</span>.set(key, value.toString)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 根据topic，groupid获取offset</span></span><br><span class="line"><span class="comment">    * @param topics</span></span><br><span class="line"><span class="comment">    * @param groupId</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOffset</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): (<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>], <span class="type">Int</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> fromOffSets = scala.collection.mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">    topics.foreach(topic =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> keys = <span class="type">JedisUtil</span>.getKeys(<span class="string">s"bi_kafka_offset_<span class="subst">$&#123;groupId&#125;</span>_<span class="subst">$&#123;topic&#125;</span>*"</span>)</span><br><span class="line">      <span class="keyword">if</span> (!keys.isEmpty) &#123;</span><br><span class="line">        keys.asScala.foreach(key =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> offset = <span class="type">JedisUtil</span>.get(key)</span><br><span class="line">          <span class="keyword">val</span> partition = <span class="type">Try</span>(key.split(<span class="string">s"bi_kafka_offset_<span class="subst">$&#123;groupId&#125;</span>_<span class="subst">$&#123;topic&#125;</span>_"</span>).apply(<span class="number">1</span>)).getOrElse(<span class="string">"0"</span>)</span><br><span class="line">          fromOffSets.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition.toInt), offset.toLong)</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">if</span> (fromOffSets.isEmpty) &#123;</span><br><span class="line">      (fromOffSets.toMap, <span class="number">0</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      (fromOffSets.toMap, <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 创建InputDStream，如果auto.offset.reset为latest则从redis读取</span></span><br><span class="line"><span class="comment">    * @param ssc</span></span><br><span class="line"><span class="comment">    * @param topic</span></span><br><span class="line"><span class="comment">    * @param kafkaParams</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createStreamingContextRedis</span></span>(ssc: <span class="type">StreamingContext</span>, topic: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">                                  kafkaParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>]): <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">var</span> kafkaStreams: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> groupId = kafkaParams.get(<span class="string">"group.id"</span>).get</span><br><span class="line">    <span class="keyword">val</span> (fromOffSet, flag) = getOffset(topic, groupId.toString)</span><br><span class="line">    <span class="keyword">val</span> offsetReset = kafkaParams.get(<span class="string">"auto.offset.reset"</span>).get</span><br><span class="line">    <span class="keyword">if</span> (flag == <span class="number">1</span> &amp;&amp; offsetReset.equals(<span class="string">"latest"</span>)) &#123;</span><br><span class="line">      kafkaStreams = <span class="type">KafkaUtils</span>.createDirectStream(ssc, <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>(topic, kafkaParams, fromOffSet))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      kafkaStreams = <span class="type">KafkaUtils</span>.createDirectStream(ssc, <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>(topic, kafkaParams))</span><br><span class="line">    &#125;</span><br><span class="line">    kafkaStreams</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"offSet Redis"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">60</span>))</span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"localhost:9092"</span>,</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"binlog.test.rpt_test_1min"</span>,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>),</span><br><span class="line">      <span class="string">"session.timeout.ms"</span> -&gt; <span class="string">"20000"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>]</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> topic = <span class="type">Array</span>(<span class="string">"weibo_keyword"</span>)</span><br><span class="line">    <span class="keyword">val</span> groupId = <span class="string">"test"</span></span><br><span class="line">    <span class="keyword">val</span> lines = createStreamingContextRedis(ssc, topic, kafkaParams)</span><br><span class="line">    lines.foreachRDD(rdds =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (!rdds.isEmpty()) &#123;</span><br><span class="line">        println(<span class="string">"##################:"</span> + rdds.count())</span><br><span class="line">      &#125;</span><br><span class="line">      storeOffset(rdds.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges, groupId)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.typesafe.config.<span class="type">ConfigFactory</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.&#123;<span class="type">HostAndPort</span>, <span class="type">JedisCluster</span>, <span class="type">JedisPool</span>, <span class="type">JedisPoolConfig</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JedisUtil</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> config = <span class="type">ConfigFactory</span>.load(<span class="string">"realtime-etl.conf"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> redisHosts: <span class="type">String</span> = config.getString(<span class="string">"redis.server"</span>)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> port: <span class="type">Int</span> = config.getInt(<span class="string">"redis.port"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> hostAndPortsSet: java.util.<span class="type">Set</span>[<span class="type">HostAndPort</span>] = <span class="keyword">new</span> util.<span class="type">HashSet</span>[<span class="type">HostAndPort</span>]()</span><br><span class="line">  redisHosts.split(<span class="string">","</span>).foreach(host =&gt; &#123;</span><br><span class="line">    hostAndPortsSet.add(<span class="keyword">new</span> <span class="type">HostAndPort</span>(host, port))</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> jedisConf: <span class="type">JedisPoolConfig</span> = <span class="keyword">new</span> <span class="type">JedisPoolConfig</span>()</span><br><span class="line">  jedisConf.setMaxTotal(<span class="number">5000</span>)</span><br><span class="line">  jedisConf.setMaxWaitMillis(<span class="number">50000</span>)</span><br><span class="line">  jedisConf.setMaxIdle(<span class="number">300</span>)</span><br><span class="line">  jedisConf.setTestOnBorrow(<span class="literal">true</span>)</span><br><span class="line">  jedisConf.setTestOnReturn(<span class="literal">true</span>)</span><br><span class="line">  jedisConf.setTestWhileIdle(<span class="literal">true</span>)</span><br><span class="line">  jedisConf.setMinEvictableIdleTimeMillis(<span class="number">60000</span>l)</span><br><span class="line">  jedisConf.setTimeBetweenEvictionRunsMillis(<span class="number">3000</span>l)</span><br><span class="line">  jedisConf.setNumTestsPerEvictionRun(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> redis = <span class="keyword">new</span> <span class="type">JedisCluster</span>(hostAndPortsSet, jedisConf)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(key: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      redis.get(key)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">set</span></span>(key: <span class="type">String</span>, value: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      redis.set(key, value)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</span><br><span class="line">        e.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hmset</span></span>(key: <span class="type">String</span>, map: java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//    val redis=pool.getResource</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      redis.hmset(key, map)</span><br><span class="line">    &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hset</span></span>(key: <span class="type">String</span>, field: <span class="type">String</span>, value: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//    val redis=pool.getResource</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      redis.hset(key, field, value)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</span><br><span class="line">        e.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hget</span></span>(key: <span class="type">String</span>, field: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      redis.hget(key, field)</span><br><span class="line">    &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hgetAll</span></span>(key: <span class="type">String</span>): java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      redis.hgetAll(key)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><a name="25juk"></a> #### 其他方法 值得一提的是，您还可以将偏移量存储在HDFS之类的存储系统中。与上述选项相比，在HDFS中存储偏移量不太受欢迎，因为与其他系统（如ZooKeeper和HBase）相比，HDFS具有更高的延迟。此外，如果管理不当，则在HDFS中为每个批次编写offsetRanges可能会导致文件较小的问题。<p><a name="Tfo7r"></a></p><h3 id="不管理offset"><a href="#不管理offset" class="headerlink" title="不管理offset"></a>不管理offset</h3><p><br>当然，Spark Streaming应用程序并不是必须的去管理offset。对当前业务考虑好是否需要对offset进行保存。</p><p>本文参考如下：<br><a href="https://blog.cloudera.com/offset-management-for-apache-kafka-with-apache-spark-streaming/" target="_blank" rel="external nofollow noopener noreferrer">Offset Management For Apache Kafka With Apache Spark Streaming</a></p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="cpeixin.cn/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/" title="Spark Streaming offset 管理" target="_blank" rel="external">cpeixin.cn/2017/08/02/Spark-Streaming-offset-%E7%AE%A1%E7%90%86/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external nofollow noopener noreferrer">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/cpeixin" target="_blank" class="img-burn thumb-sm visible-lg" rel="external nofollow noopener noreferrer"><img src="/images/avatar.jpg" class="img-rounded w-full" alt></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><span class="text-dark">Brent</span><small class="ml-1x">大数据工程师 &amp; 机器学习</small></a></h3><div>一心九用的工程师</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2017/11/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0/" title="数据仓库 - 简述"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2017/05/31/python-%E5%8D%8F%E7%A8%8B/" title="python - 协程"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank" rel="external nofollow noopener noreferrer">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank" rel="external nofollow noopener noreferrer">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"SsxmBzBQ3R2S2zWTv0FrONel-gzGzoHsz",appKey:"w0K528Ye7NhOr07RHrzVzHbW",placeholder:"说点什么呢？",avatar:"mm",meta:meta,pageSize:"10",visitor:!0})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html><script type="text/javascript" src="//cdn.jsdelivr.net/gh/ygbhf/clicklove/clicklove.js"></script><!-- rebuild by neat -->