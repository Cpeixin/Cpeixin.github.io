<!-- build time:Sat Jul 04 2020 01:08:22 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta name="referrer" content="no-referrer"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Spark SQL 入门 | 布兰特 | 不忘初心</title><meta name="description" content="程序起点在Spark2.0之前， Spark程序必须做的第一件事是创建一个SparkContext对象，该对象告诉Spark如何访问集群。要创建一个SparkContext您首先需要构建一个SparkConf对象，其中包含有关您的应用程序的信息。每个JVM只能激活一个SparkContext。12345val sparkConf: SparkConf &#x3D; new SparkConf()   .se"><meta property="og:type" content="article"><meta property="og:title" content="Spark SQL 入门"><meta property="og:url" content="cpeixin.cn/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/index.html"><meta property="og:site_name" content="布兰特 | 不忘初心"><meta property="og:description" content="程序起点在Spark2.0之前， Spark程序必须做的第一件事是创建一个SparkContext对象，该对象告诉Spark如何访问集群。要创建一个SparkContext您首先需要构建一个SparkConf对象，其中包含有关您的应用程序的信息。每个JVM只能激活一个SparkContext。12345val sparkConf: SparkConf &#x3D; new SparkConf()   .se"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587040688045-34884f37-d0b7-442e-a8ae-f86d3a21509c.png#align=left&display=inline&height=471&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-16%20%E4%B8%8B%E5%8D%888.29.55.png&originHeight=972&originWidth=1538&size=208620&status=done&style=none&width=746"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587309130540-582c05e4-b95c-4d5c-9063-fc5ebfdd182b.png#align=left&display=inline&height=1748&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-19%20%E4%B8%8B%E5%8D%8811.05.09.png&originHeight=1748&originWidth=3270&size=714172&status=done&style=none&width=3270"><meta property="article:published_time" content="2017-03-25T15:48:43.000Z"><meta property="article:modified_time" content="2020-04-22T04:19:49.614Z"><meta property="article:author" content="Brent"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587040688045-34884f37-d0b7-442e-a8ae-f86d3a21509c.png#align=left&display=inline&height=471&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-16%20%E4%B8%8B%E5%8D%888.29.55.png&originHeight=972&originWidth=1538&size=208620&status=done&style=none&width=746"><link rel="canonical" href="cpeixin.cn/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/index.html"><link rel="alternate" href="/atom.xml" title="布兰特 | 不忘初心" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.0"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Brent</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">大数据工程师 &amp; 机器学习</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Malaysia</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">31</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%B6%E6%9E%84/">架构</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">15</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Apriori/" rel="tag">Apriori</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/" rel="tag">EM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ETL/" rel="tag">ETL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="tag-list-count">21</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/" rel="tag">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/" rel="tag">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-Means/" rel="tag">K-Means</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/" rel="tag">KNN</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" rel="tag">LRU淘汰算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OLAP/" rel="tag">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/" rel="tag">PageRank</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Random-Forest/" rel="tag">Random Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flask/" rel="tag">flask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kali/" rel="tag">kali</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/" rel="tag">mapreduce</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shadowsock/" rel="tag">shadowsock</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skipList/" rel="tag">skipList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a><span class="tag-list-count">20</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/" rel="tag">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A0%86/" rel="tag">堆</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%A3%E5%88%97%E8%A1%A8/" rel="tag">散列表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" rel="tag">数据仓库</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" rel="tag">数据清洗</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" rel="tag">数据采集</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" rel="tag">时间序列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" rel="tag">服务器安全</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%88/" rel="tag">栈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" rel="tag">用户画像</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" rel="tag">红黑树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" rel="tag">线性表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag">词向量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8/" rel="tag">链表</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%9F%E5%88%97/" rel="tag">队列</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/Apriori/" style="font-size:13.11px">Apriori</a> <a href="/tags/Decision-Tree/" style="font-size:13.44px">Decision Tree</a> <a href="/tags/EM/" style="font-size:13.11px">EM</a> <a href="/tags/ETL/" style="font-size:13px">ETL</a> <a href="/tags/Flink/" style="font-size:14px">Flink</a> <a href="/tags/GPT-2/" style="font-size:13px">GPT-2</a> <a href="/tags/IDEA/" style="font-size:13px">IDEA</a> <a href="/tags/K-Means/" style="font-size:13px">K-Means</a> <a href="/tags/KNN/" style="font-size:13.11px">KNN</a> <a href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" style="font-size:13px">LRU淘汰算法</a> <a href="/tags/Naive-Bayes/" style="font-size:13.11px">Naive Bayes</a> <a href="/tags/OLAP/" style="font-size:13px">OLAP</a> <a href="/tags/PageRank/" style="font-size:13.11px">PageRank</a> <a href="/tags/Random-Forest/" style="font-size:13px">Random Forest</a> <a href="/tags/SVM/" style="font-size:13.11px">SVM</a> <a href="/tags/docker/" style="font-size:13.11px">docker</a> <a href="/tags/flask/" style="font-size:13.11px">flask</a> <a href="/tags/flink/" style="font-size:13.22px">flink</a> <a href="/tags/hdfs/" style="font-size:13.33px">hdfs</a> <a href="/tags/hive/" style="font-size:13.22px">hive</a> <a href="/tags/kali/" style="font-size:13px">kali</a> <a href="/tags/mapreduce/" style="font-size:13px">mapreduce</a> <a href="/tags/mysql/" style="font-size:13px">mysql</a> <a href="/tags/python/" style="font-size:13.56px">python</a> <a href="/tags/redis/" style="font-size:13px">redis</a> <a href="/tags/scala/" style="font-size:13.33px">scala</a> <a href="/tags/shadowsock/" style="font-size:13px">shadowsock</a> <a href="/tags/skipList/" style="font-size:13px">skipList</a> <a href="/tags/sklearn/" style="font-size:13px">sklearn</a> <a href="/tags/spark/" style="font-size:13.89px">spark</a> <a href="/tags/yarn/" style="font-size:13px">yarn</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size:13.11px">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size:13.22px">二叉树</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size:13px">回溯</a> <a href="/tags/%E5%A0%86/" style="font-size:13px">堆</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size:13.78px">排序</a> <a href="/tags/%E6%95%A3%E5%88%97%E8%A1%A8/" style="font-size:13px">散列表</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" style="font-size:13.67px">数据仓库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" style="font-size:13px">数据清洗</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" style="font-size:13px">数据采集</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size:13px">数组</a> <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" style="font-size:13px">时间序列</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" style="font-size:13.11px">服务器安全</a> <a href="/tags/%E6%A0%88/" style="font-size:13px">栈</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:13px">深度学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:13px">爬虫</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size:13.33px">特征工程</a> <a href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" style="font-size:13.11px">用户画像</a> <a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" style="font-size:13px">红黑树</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" style="font-size:13px">线性表</a> <a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" style="font-size:13px">词向量</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size:13px">递归</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size:13px">逻辑回归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size:13.11px">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size:13px">队列</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">二月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">14</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Tools/">Tools</a></p><p class="item-title"><a href="/2020/05/28/Linux%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BDGoogle-Drive%E6%96%87%E4%BB%B6/" class="title">Linux直接下载Google Drive文件</a></p><p class="item-date"><time datetime="2020-05-27T17:03:54.000Z" itemprop="datePublished">2020-05-28</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/2020/04/06/def-neverGrowUp/" class="title">def neverGrowUp()</a></p><p class="item-date"><time datetime="2020-04-05T16:00:00.000Z" itemprop="datePublished">2020-04-06</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/" class="title">抗疫英雄</a></p><p class="item-date"><time datetime="2020-04-04T14:45:15.000Z" itemprop="datePublished">2020-04-04</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/python/">python</a></p><p class="item-title"><a href="/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/" class="title">python Flask &amp; Ajax 数据传输</a></p><p class="item-date"><time datetime="2020-03-11T14:43:01.000Z" itemprop="datePublished">2020-03-11</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/python/">python</a></p><p class="item-title"><a href="/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/" class="title">Python Flask接口设计-示例</a></p><p class="item-date"><time datetime="2020-03-10T15:08:35.000Z" itemprop="datePublished">2020-03-10</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-Spark-SQL-入门" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Spark SQL 入门</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/" class="article-date"><time datetime="2017-03-25T15:48:43.000Z" itemprop="datePublished">2017-03-25</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/" class="leancloud_visitors" data-flag-title="Spark SQL 入门">0</span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 3.8k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 21(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p><a name="1kzm8"></a></p><h3><a href="#" class="headerlink"></a></h3><p><a name="A95aP"></a></p><h3 id="程序起点"><a href="#程序起点" class="headerlink" title="程序起点"></a>程序起点</h3><p><br>在Spark2.0之前， Spark程序必须做的第一件事是创建一个<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext" target="_blank" rel="external nofollow noopener noreferrer">SparkContext</a>对象，该对象告诉Spark如何访问集群。要创建一个SparkContext您首先需要构建一个<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf" target="_blank" rel="external nofollow noopener noreferrer">SparkConf</a>对象，其中包含有关您的应用程序的信息。<br>每个JVM只能激活一个SparkContext。<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">   .setAppName(<span class="string">"transformation_func"</span>)</span><br><span class="line">   .setMaster(<span class="string">"local"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br></pre></td></tr></table></figure><p><br>在Spark2.0之后， <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession" target="_blank" rel="external nofollow noopener noreferrer">SparkSession</a>类是Spark中所有功能的入口点。为了引入dataframe和dataset的API，要创建一个基本的SparkSession，只需使用SparkSession.builder()。SparkConf、SparkContext和SQLContext都已经被封装在SparkSession当中，不需要显示的创建。并且提供了对Hive功能的内置支持，下图是SparkSession的源码定义：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587040688045-34884f37-d0b7-442e-a8ae-f86d3a21509c.png#align=left&display=inline&height=471&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-16%20%E4%B8%8B%E5%8D%888.29.55.png&originHeight=972&originWidth=1538&size=208620&status=done&style=none&width=746" alt="屏幕快照 2020-04-16 下午8.29.55.png"><br><strong><br></strong>SparkSession创建**</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p><a name="TwIcF"></a></p><h3 id="-1"><a href="#-1" class="headerlink"></a></h3><p><a name="3sJ3V"></a></p><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><p><br>使用SparkSession，应用程序可以从<a href="https://spark.apache.org/docs/latest/sql-getting-started.html#interoperating-with-rdds" target="_blank" rel="external nofollow noopener noreferrer">现有的RDD</a>，Hive表的或<a href="https://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="external nofollow noopener noreferrer">Spark数据源</a>创建DataFrame 。<br><br><br><strong>基于RDD转化DataFrame：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> read</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparksql_rdd</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">	<span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">user_name: <span class="type">String</span>, sex: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">  </span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"function_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> rdd_ss: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">"brent"</span>,<span class="string">"male"</span>),(<span class="string">"haylee"</span>,<span class="string">"female"</span>),(<span class="string">"vicky"</span>,<span class="string">"male"</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = rdd_ss.map((x: (<span class="type">String</span>, <span class="type">String</span>)) =&gt;&#123;<span class="type">Person</span>(x._1,x._2)&#125;).toDF()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong><br></strong><br><strong>基于JSON文件的内容创建一个DataFrame：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> spark_sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparksql_1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"sql_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df_json: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"hdfs://localhost:8020/data/user_data.json"</span>)</span><br><span class="line">    </span><br><span class="line">    df_json.show(<span class="number">5</span>) </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br><strong>基于Hive表内容创建一个DataFrame：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> spark_sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparksql_hive</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> warehouseLocation=<span class="string">"hdfs://localhost:8020/user/hive/warehouse"</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"sql_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .config(<span class="string">"spark.sql.warehouse.dir"</span>,warehouseLocation)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df_databases: <span class="type">DataFrame</span> = spark.sql(<span class="string">"show databases"</span>)</span><br><span class="line">    df_databases.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意： Spark读取Hive是需要两三个步骤的</p><ol><li>如果你是在集群上运行，需要注意，要将hive-site.xml复制一份到spark目录下的conf文件夹中，如果你是在本地连接集群中的Hive，那么请将hive-site.xml复制一份到你IDEA中，resources目录下。</li></ol><ol start="2"><li>如果你是在集群上运行，需要注意，要将mysql-connector-java-8.0.19.jar复制一份到spark目录下的jars文件夹中，如果你是在本地连接集群中的Hive，那么请在pom文件中不要忘记引入mysql-connector-java</li></ol><ol start="3"><li>在本地运行，可能会遇到/tmp/hive的权限问题，请用chmod修改/tmp权限为777</li></ol><br><br>**基于HBase内容创建一个DataFrame：**<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> spark_sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.execution.datasources.hbase.<span class="type">HBaseTableCatalog</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.immutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparksql_hbase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">col0: <span class="type">Int</span>, col1: <span class="type">Int</span>, col2: <span class="type">Boolean</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">spark</span></span>: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark HBase Example"</span>)</span><br><span class="line">      .master(<span class="string">"local[4]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">catalog</span></span>: <span class="type">String</span> =</span><br><span class="line">    	<span class="comment">// 这里，我们在读取数据的过程中，无论什么类型的数据，type字段统一指定成 string 即可。否则读取报错</span></span><br><span class="line">      <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">         |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span>t_<span class="string">user"&#125;,</span></span><br><span class="line"><span class="string">         |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">         |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">         |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>user_<span class="string">name", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>customer_<span class="string">id", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span><span class="string">age", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span><span class="string">birthday", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>deposit_<span class="string">amount", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>last_login_<span class="string">time", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span><span class="string">flag", "</span><span class="string">type":"</span><span class="string">string"&#125;</span></span><br><span class="line"><span class="string">         |&#125;</span></span><br><span class="line"><span class="string">         |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// read</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .option(<span class="type">HBaseTableCatalog</span>.tableCatalog, catalog)</span><br><span class="line">      .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br>注意：如果我们对于读取和写入HBase的场景很频繁的话，就需要考虑性能的问题，内置的读取数据源是使用了 TableInputFormat 来读取 HBase 中的数据。这个 TableInputFormat 有一些缺点：</p><ul><li>一个 Task 里面只能启动一个 Scan 去 HBase 中读取数据；</li><li>TableInputFormat 中不支持 BulkGet；</li><li>不能享受到 Spark SQL 内置的 catalyst 引擎的优化。</li></ul><p><br>基于这些问题，来自 Hortonworks 的工程师们为我们带来了全新的 Apache Spark—Apache HBase Connector，下面简称 SHC。通过这个类库，我们可以直接使用 Spark SQL 将 DataFrame 中的数据写入到 HBase 中；而且我们也可以使用 Spark SQL 去查询 HBase 中的数据，在查询 HBase 的时候充分利用了 catalyst 引擎做了许多优化，比如分区修剪（partition pruning），列修剪（column pruning），谓词下推（predicate pushdown）和数据本地性（data locality）等等。因为有了这些优化，通过 Spark 查询 HBase 的速度有了很大的提升。<br><br><br>但是对于使用SHC，目前还是有些麻烦的，网上的maven依赖可能是因为版本的原因，程序引入找不到org.apache.spark.sql.execution.datasources.hbase.HBaseTableCatalog类，这里推荐自己下载源码，进行编译成jar文件或者编译后上传到自己的maven库中进行使用<br></p><ol><li>下载源码 <a href="https://github.com/hortonworks-spark/shc" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/hortonworks-spark/shc</a>，选择相应低于或者等于spark，hbase的版本</li></ol><ol start="2"><li>本地中打开，点击程序根目录下的pom文件，注释掉distributionManagement，直接点击install，将jar包生成到你本地的maven库中，当然你也可以上传到你远程的私有Maven 库中。</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1587309130540-582c05e4-b95c-4d5c-9063-fc5ebfdd182b.png#align=left&display=inline&height=1748&margin=%5Bobject%20Object%5D&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-19%20%E4%B8%8B%E5%8D%8811.05.09.png&originHeight=1748&originWidth=3270&size=714172&status=done&style=none&width=3270" alt="屏幕快照 2020-04-19 下午11.05.09.png"></p><ol start="3"><li>pom文件中，引入下面依赖，就可以使用了（注意 version 版本号）<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.hortonworks&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;shc-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">1.1</span><span class="number">.2</span><span class="number">-2.2</span>-s_2<span class="number">.11</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li></ol><p><a name="Vu44d"></a></p><h3 id="DataFrame操作"><a href="#DataFrame操作" class="headerlink" title="DataFrame操作"></a>DataFrame操作</h3><p>上篇文章中，我们讲到DataFrame每一列并<strong>不存储类型信息</strong>，所以在编译时并不能发现类型错误，所以在这里我们也可以叫做** 无类型的数据集操作。<strong><br></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparksql_function</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"function_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .config(<span class="string">"spark.sql.crossJoin.enabled"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 样例数据</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * &#123;"user_name":"brent","customer_id":12031602,"age": 22,"birthday":"1993-04-05","deposit_amount":3000,"last_login_time":"2017-03-10 14:55:22"&#125;</span></span><br><span class="line"><span class="comment">        &#123;"user_name":"haylee","customer_id":12031603,"age":23,"birthday":"1992-08-10","deposit_amount":4000.56,"last_login_time":"2017-03-11 10:55:00"&#125;</span></span><br><span class="line"><span class="comment">        &#123;"user_name":"vicky","customer_id":12031604,"age":30,"birthday":"2000-03-02","deposit_amount":200.4,"last_login_time":"2017-03-10 09:10:00"&#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"hdfs://localhost:8020/data/user_data.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd_row: <span class="type">RDD</span>[<span class="type">Row</span>] = spark.sparkContext</span><br><span class="line">      .makeRDD(<span class="type">List</span>((<span class="string">"brent"</span>, <span class="string">"male"</span>), (<span class="string">"haylee"</span>, <span class="string">"female"</span>), (<span class="string">"vicky"</span>, <span class="string">"male"</span>)))</span><br><span class="line">      .map((x: (<span class="type">String</span>, <span class="type">String</span>)) =&gt; <span class="type">Row</span>(x._1, x._2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// The schema is encoded in a string</span></span><br><span class="line">    <span class="keyword">val</span> schemaString = <span class="string">"user_name sex"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">    <span class="keyword">val</span> fields: <span class="type">Array</span>[<span class="type">StructField</span>] = schemaString.split(<span class="string">" "</span>)</span><br><span class="line">      .map((fieldName: <span class="type">String</span>) =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = spark.createDataFrame(rdd_row, schema)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    show_get_data(spark, df)</span><br><span class="line">    map_data(spark, df)</span><br><span class="line">    filter_data(spark, df)</span><br><span class="line">    sort_data(spark, df)</span><br><span class="line">    groupBy_data(spark, df)</span><br><span class="line">    join_data(spark, df, df2)</span><br><span class="line">    intersect_data(spark, df, df2)</span><br><span class="line">    withColumn_rename_dataframe(spark, df)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show_get_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    <span class="comment">//    root</span></span><br><span class="line">    <span class="comment">//    |-- age: long (nullable = true)</span></span><br><span class="line">    <span class="comment">//    |-- birthday: string (nullable = true)</span></span><br><span class="line">    <span class="comment">//    |-- customer_id: long (nullable = true)</span></span><br><span class="line">    <span class="comment">//    |-- deposit_amount: double (nullable = true)</span></span><br><span class="line">    <span class="comment">//    |-- last_login_time: string (nullable = true)</span></span><br><span class="line">    <span class="comment">//    |-- user_name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line">    df.show(<span class="number">5</span>)</span><br><span class="line">    <span class="comment">//默认打印前20条结果</span></span><br><span class="line">    <span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">    <span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line">    <span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">    <span class="comment">//    | 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|    brent|</span></span><br><span class="line">    <span class="comment">//    | 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|   haylee|</span></span><br><span class="line">    <span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line">    <span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Select only the "name" column</span></span><br><span class="line">    <span class="comment">// 这个表达式不能进行计算操作</span></span><br><span class="line">    df.select(<span class="string">"user_name"</span>, <span class="string">"age"</span>).show()</span><br><span class="line">    <span class="comment">//    +---------+</span></span><br><span class="line">    <span class="comment">//    |user_name|</span></span><br><span class="line">    <span class="comment">//    +---------+</span></span><br><span class="line">    <span class="comment">//    |    brent|</span></span><br><span class="line">    <span class="comment">//    |   haylee|</span></span><br><span class="line">    <span class="comment">//    |    vicky|</span></span><br><span class="line">    <span class="comment">//    +---------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">    <span class="comment">// This import is needed to use the $-notation</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    df.select($<span class="string">"user_name"</span>, $<span class="string">"age"</span> + <span class="number">1</span> as <span class="string">"new_age"</span>).show()</span><br><span class="line">    <span class="comment">//    +---------+-------+</span></span><br><span class="line">    <span class="comment">//    |user_name|new_age|</span></span><br><span class="line">    <span class="comment">//    +---------+-------+</span></span><br><span class="line">    <span class="comment">//    |    brent|     23|</span></span><br><span class="line">    <span class="comment">//    |   haylee|     24|</span></span><br><span class="line">    <span class="comment">//    |    vicky|     31|</span></span><br><span class="line">    <span class="comment">//    +---------+-------+</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    df.select(col(<span class="string">"customer_id"</span>), col(<span class="string">"deposit_amount"</span>)).show()</span><br><span class="line"></span><br><span class="line">    df.limit(<span class="number">5</span>).show()</span><br><span class="line"></span><br><span class="line">    df.describe()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">map_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 注意 这里是Row类型</span></span><br><span class="line">    df.map((x: <span class="type">Row</span>) =&gt; &#123;<span class="string">"name: "</span>+x.getAs[<span class="type">String</span>](<span class="string">"user_name"</span>)&#125;).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 取等于时必须用===</span></span><br><span class="line">    df.filter($<span class="string">"user_name"</span> === <span class="string">"brent"</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|    brent|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">25</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">    df.filter(<span class="string">"deposit_amount = 3000.0"</span>).show()</span><br><span class="line">    df.filter($<span class="string">"deposit_amount"</span> &gt; <span class="number">200</span> and $<span class="string">"age"</span> &lt; <span class="number">25</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|    brent|</span></span><br><span class="line"><span class="comment">//    | 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|   haylee|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"></span><br><span class="line">    df.filter(<span class="string">"substring(user_name,0,1) = 'h'"</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|   haylee|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//  在源码中可以看到，where算子，底层是filter实现的。</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    df.where(col(<span class="string">"age"</span>) &gt; <span class="number">23</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"></span><br><span class="line">    df.where(<span class="string">"age&gt; 23"</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sort_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    df.sort($<span class="string">"age"</span>.desc).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line"><span class="comment">//    | 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|   haylee|</span></span><br><span class="line"><span class="comment">//    | 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|    brent|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">    df.sort($<span class="string">"age"</span>.asc).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|    brent|</span></span><br><span class="line"><span class="comment">//    | 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|   haylee|</span></span><br><span class="line"><span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 只能对数字类型和日期类型生效</span></span><br><span class="line">    df.orderBy($<span class="string">"age"</span>)</span><br><span class="line"></span><br><span class="line">    df.orderBy(- df(<span class="string">"age"</span>))</span><br><span class="line"></span><br><span class="line">    df.orderBy(df(<span class="string">"age"</span>).desc)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">groupBy_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">//    +---+-----+</span></span><br><span class="line"><span class="comment">//    |age|count|</span></span><br><span class="line"><span class="comment">//    +---+-----+</span></span><br><span class="line"><span class="comment">//    | 22|    1|</span></span><br><span class="line"><span class="comment">//    | 30|    1|</span></span><br><span class="line"><span class="comment">//    | 23|    1|</span></span><br><span class="line"><span class="comment">//    +---+-----+</span></span><br><span class="line">    <span class="comment">// 只能作用于数值字段</span></span><br><span class="line">    df.groupBy(<span class="string">"user_name"</span>).max(<span class="string">"deposit_amount"</span>).show()</span><br><span class="line">    df.groupBy(<span class="string">"user_name"</span>).min(<span class="string">"deposit_amount"</span>).show()</span><br><span class="line">    df.groupBy(<span class="string">"user_name"</span>).mean(<span class="string">"deposit_amount"</span>).as(<span class="string">"mean_deposit_amount"</span>).show()</span><br><span class="line">    df.groupBy(<span class="string">"user_name"</span>).sum(<span class="string">"deposit_amount"</span>).toDF(<span class="string">"user_name"</span>, <span class="string">"sum_deposit_amount"</span>).show()</span><br><span class="line"><span class="comment">//    +---------+------------------+</span></span><br><span class="line"><span class="comment">//    |user_name|sum_deposit_amount|</span></span><br><span class="line"><span class="comment">//    +---------+------------------+</span></span><br><span class="line"><span class="comment">//    |    vicky|             200.4|</span></span><br><span class="line"><span class="comment">//    |   haylee|           4000.56|</span></span><br><span class="line"><span class="comment">//    |    brent|            3000.0|</span></span><br><span class="line"><span class="comment">//    +---------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    df.groupBy(<span class="string">"user_name"</span>, <span class="string">"age"</span>)</span><br><span class="line">      .agg(min(<span class="string">"deposit_amount"</span>).as(<span class="string">"min_deposit_amount"</span>))</span><br><span class="line">      .show()</span><br><span class="line"><span class="comment">//    +---------+---+------------------+</span></span><br><span class="line"><span class="comment">//    |user_name|age|min_deposit_amount|</span></span><br><span class="line"><span class="comment">//    +---------+---+------------------+</span></span><br><span class="line"><span class="comment">//    |    vicky| 30|             200.4|</span></span><br><span class="line"><span class="comment">//    |   haylee| 23|           4000.56|</span></span><br><span class="line"><span class="comment">//    |    brent| 22|            3000.0|</span></span><br><span class="line"><span class="comment">//    +---------+---+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//单独使用 agg</span></span><br><span class="line">    df.agg(<span class="string">"age"</span> -&gt; <span class="string">"max"</span>).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">distinct_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// distinct 底层实现实则为 dropDuplicates（）</span></span><br><span class="line">    df.distinct()</span><br><span class="line">    df.dropDuplicates()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>, df2: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//笛卡尔积, spark2中默认不开启笛卡尔积，需添加"spark.sql.crossJoin.enabled", "true"配置</span></span><br><span class="line">    df.join(df2).show()</span><br><span class="line"></span><br><span class="line">    df.join(df2, <span class="string">"user_name"</span>).show()</span><br><span class="line"></span><br><span class="line">    df.join(df2, <span class="type">Seq</span>(<span class="string">"user_name"</span>), <span class="string">"left"</span>).show()</span><br><span class="line"><span class="comment">//    +---------+---+----------+-----------+--------------+-------------------+------+</span></span><br><span class="line"><span class="comment">//    |user_name|age|  birthday|customer_id|deposit_amount|    last_login_time|   sex|</span></span><br><span class="line"><span class="comment">//    +---------+---+----------+-----------+--------------+-------------------+------+</span></span><br><span class="line"><span class="comment">//    |    vicky| 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|  male|</span></span><br><span class="line"><span class="comment">//    |   haylee| 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|female|</span></span><br><span class="line"><span class="comment">//    |    brent| 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|  male|</span></span><br><span class="line"><span class="comment">//    +---------+---+----------+-----------+--------------+-------------------+------+</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">intersect_data</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>, df2: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取两个DataFrame中共有的记录</span></span><br><span class="line">    df.intersect(df2).show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">withColumn_rename_dataframe</span></span>(spark: <span class="type">SparkSession</span>, df: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 字段重命名</span></span><br><span class="line">    df.withColumnRenamed(<span class="string">"deposit_amount"</span>,<span class="string">"withdraw_amount"</span>).show()</span><br><span class="line">    <span class="comment">// 添加新列</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    df.withColumn(<span class="string">"next_year_age"</span>, $<span class="string">"age"</span>+<span class="number">1</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a name="Nd59I"></a></p><h3 id="-2"><a href="#-2" class="headerlink"></a></h3><p><a name="Qrd7K"></a></p><h3 id="以编程方式运行SQL查询"><a href="#以编程方式运行SQL查询" class="headerlink" title="以编程方式运行SQL查询"></a>以编程方式运行SQL查询</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">spark_use_sql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"sql_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 样例数据</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * &#123;"user_name":"brent","customer_id":12031602,"age": 22,"birthday":"1993-04-05","deposit_amount":3000,"last_login_time":"2017-03-10 14:55:22"&#125;</span></span><br><span class="line"><span class="comment">      * &#123;"user_name":"haylee","customer_id":12031603,"age":23,"birthday":"1992-08-10","deposit_amount":4000.56,"last_login_time":"2017-03-11 10:55:00"&#125;</span></span><br><span class="line"><span class="comment">      * &#123;"user_name":"vicky","customer_id":12031604,"age":30,"birthday":"2000-03-02","deposit_amount":200.4,"last_login_time":"2017-03-10 09:10:00"&#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"hdfs://localhost:8020/data/user_data.json"</span>)</span><br><span class="line"></span><br><span class="line">    df.createTempView(<span class="string">"t_user"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"select * from t_user"</span>).show()</span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    |age|  birthday|customer_id|deposit_amount|    last_login_time|user_name|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line"><span class="comment">//    | 22|1993-04-05|   12031602|        3000.0|2017-03-10 14:55:22|    brent|</span></span><br><span class="line"><span class="comment">//    | 23|1992-08-10|   12031603|       4000.56|2017-03-11 10:55:00|   haylee|</span></span><br><span class="line"><span class="comment">//    | 30|2000-03-02|   12031604|         200.4|2017-03-10 09:10:00|    vicky|</span></span><br><span class="line"><span class="comment">//    +---+----------+-----------+--------------+-------------------+---------+</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    spark.sql(<span class="string">"select * from t_user"</span>).groupBy(<span class="string">"user_name"</span>).agg(<span class="string">"deposit_amount"</span>-&gt;<span class="string">"sum"</span>).show()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><a name="y48DV"></a></p><h3 id="DataSet创建"><a href="#DataSet创建" class="headerlink" title="DataSet创建"></a>DataSet创建</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">dataset</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"dataset_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// $example on:create_ds$</span></span><br><span class="line">    <span class="comment">// Encoders are created for case classes</span></span><br><span class="line">    <span class="keyword">val</span> caseClassDS: <span class="type">Dataset</span>[<span class="type">Person</span>] = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">    caseClassDS.show()</span><br><span class="line">    <span class="comment">// +----+---+</span></span><br><span class="line">    <span class="comment">// |name|age|</span></span><br><span class="line">    <span class="comment">// +----+---+</span></span><br><span class="line">    <span class="comment">// |Andy| 32|</span></span><br><span class="line">    <span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line">    <span class="keyword">val</span> primitiveDS: <span class="type">Dataset</span>[<span class="type">Int</span>] = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">    primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span></span><br><span class="line">    <span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line">    <span class="keyword">val</span> peopleDS: <span class="type">Dataset</span>[<span class="type">Person</span>] = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">    peopleDS.show()</span><br><span class="line">    <span class="comment">// +----+-------+</span></span><br><span class="line">    <span class="comment">// | age|   name|</span></span><br><span class="line">    <span class="comment">// +----+-------+</span></span><br><span class="line">    <span class="comment">// |null|Michael|</span></span><br><span class="line">    <span class="comment">// |  30|   Andy|</span></span><br><span class="line">    <span class="comment">// |  19| Justin|</span></span><br><span class="line">    <span class="comment">// +----+-------+</span></span><br><span class="line">    <span class="comment">// $example off:create_ds$</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a name="mHND0"></a></p><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><p><br><strong>文件</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// $example on:generic_load_save_functions$</span></span><br><span class="line">    <span class="keyword">val</span> usersDF = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">    usersDF.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write.save(<span class="string">"namesAndFavColors.parquet"</span>)</span><br><span class="line">    <span class="comment">// $example off:generic_load_save_functions$</span></span><br><span class="line">    <span class="comment">// $example on:manual_load_options$</span></span><br><span class="line">    <span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">    peopleDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.format(<span class="string">"parquet"</span>).save(<span class="string">"namesAndAges.parquet"</span>)</span><br><span class="line">    <span class="comment">// $example off:manual_load_options$</span></span><br><span class="line">    <span class="comment">// $example on:manual_load_options_csv$</span></span><br><span class="line">    <span class="keyword">val</span> peopleDFCsv = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .load(<span class="string">"examples/src/main/resources/people.csv"</span>)</span><br><span class="line">    <span class="comment">// $example off:manual_load_options_csv$</span></span><br><span class="line">    <span class="comment">// $example on:manual_save_options_orc$</span></span><br><span class="line">    usersDF.write.format(<span class="string">"orc"</span>)</span><br><span class="line">      .option(<span class="string">"orc.bloom.filter.columns"</span>, <span class="string">"favorite_color"</span>)</span><br><span class="line">      .option(<span class="string">"orc.dictionary.key.threshold"</span>, <span class="string">"1.0"</span>)</span><br><span class="line">      .option(<span class="string">"orc.column.encoding.direct"</span>, <span class="string">"name"</span>)</span><br><span class="line">      .save(<span class="string">"users_with_options.orc"</span>)</span><br><span class="line">    <span class="comment">// $example off:manual_save_options_orc$</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// $example on:direct_sql$</span></span><br><span class="line">    <span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>)</span><br><span class="line">    <span class="comment">// $example off:direct_sql$</span></span><br><span class="line">    <span class="comment">// $example on:write_sorting_and_bucketing$</span></span><br><span class="line">    peopleDF.write.bucketBy(<span class="number">42</span>, <span class="string">"name"</span>).sortBy(<span class="string">"age"</span>).saveAsTable(<span class="string">"people_bucketed"</span>)</span><br><span class="line">    <span class="comment">// $example off:write_sorting_and_bucketing$</span></span><br><span class="line">    <span class="comment">// $example on:write_partitioning$</span></span><br><span class="line">    usersDF.write.partitionBy(<span class="string">"favorite_color"</span>).format(<span class="string">"parquet"</span>).save(<span class="string">"namesPartByColor.parquet"</span>)</span><br><span class="line">    <span class="comment">// $example off:write_partitioning$</span></span><br><span class="line">    <span class="comment">// $example on:write_partition_and_bucket$</span></span><br><span class="line">    usersDF</span><br><span class="line">      .write</span><br><span class="line">      .partitionBy(<span class="string">"favorite_color"</span>)</span><br><span class="line">      .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</span><br><span class="line">      .saveAsTable(<span class="string">"users_partitioned_bucketed"</span>)</span><br><span class="line">    <span class="comment">// $example off:write_partition_and_bucket$</span></span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"DROP TABLE IF EXISTS people_bucketed"</span>)</span><br><span class="line">    spark.sql(<span class="string">"DROP TABLE IF EXISTS users_partitioned_bucketed"</span>)</span><br></pre></td></tr></table></figure><p><br><strong>jdbc</strong><br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> write</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">write_data</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"write_case"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// read</span></span><br><span class="line">    <span class="keyword">val</span> jdbcDF: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// write</span></span><br><span class="line">    <span class="comment">// Saving data to a JDBC source</span></span><br><span class="line">    jdbcDF.write</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</span><br><span class="line">      .mode(<span class="string">"append"</span>)</span><br><span class="line">      .save()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// or</span></span><br><span class="line">    <span class="keyword">val</span> properties=<span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>,<span class="string">"secret_password"</span>)</span><br><span class="line">    jdbcDF.write</span><br><span class="line">      .mode(<span class="string">"append"</span>)</span><br><span class="line">      .jdbc(<span class="string">"jdbc:mysql://your_ip:3306/my_test?useUnicode=true&amp;characterEncoding=UTF-8"</span>,<span class="string">"t_result"</span>,properties)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br><strong>hive</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">warehouseLocation</span> <span class="title">points</span> <span class="title">to</span> <span class="title">the</span> <span class="title">default</span> <span class="title">location</span> <span class="title">for</span> <span class="title">managed</span> <span class="title">databases</span> <span class="title">and</span> <span class="title">tables</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"spark-warehouse"</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">  .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>)</span><br><span class="line">sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line"><span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span></span><br><span class="line"><span class="comment">// `USING hive`</span></span><br><span class="line">sql(<span class="string">"CREATE TABLE hive_records(key int, value string) STORED AS PARQUET"</span>)</span><br><span class="line"><span class="comment">// Save DataFrame to the Hive managed table</span></span><br><span class="line"><span class="keyword">val</span> df = spark.table(<span class="string">"src"</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"hive_records"</span>)</span><br><span class="line"><span class="comment">// After insertion, the Hive managed table has data now</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM hive_records"</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Prepare a Parquet data directory</span></span><br><span class="line"><span class="keyword">val</span> dataDir = <span class="string">"/tmp/parquet_data"</span></span><br><span class="line">spark.range(<span class="number">10</span>).write.parquet(dataDir)</span><br><span class="line"><span class="comment">// Create a Hive external Parquet table</span></span><br><span class="line">sql(<span class="string">s"CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION '<span class="subst">$dataDir</span>'"</span>)</span><br><span class="line"><span class="comment">// The Hive external table should already have data</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM hive_bigints"</span>).show()</span><br><span class="line"><span class="comment">// +---+</span></span><br><span class="line"><span class="comment">// | id|</span></span><br><span class="line"><span class="comment">// +---+</span></span><br><span class="line"><span class="comment">// |  0|</span></span><br><span class="line"><span class="comment">// |  1|</span></span><br><span class="line"><span class="comment">// |  2|</span></span><br><span class="line"><span class="comment">// ... Order may vary, as spark processes the partitions in parallel.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Turn on flag for Hive Dynamic Partitioning</span></span><br><span class="line">spark.sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition"</span>, <span class="string">"true"</span>)</span><br><span class="line">spark.sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition.mode"</span>, <span class="string">"nonstrict"</span>)</span><br><span class="line"><span class="comment">// Create a Hive partitioned table using DataFrame API</span></span><br><span class="line">df.write.partitionBy(<span class="string">"key"</span>).format(<span class="string">"hive"</span>).saveAsTable(<span class="string">"hive_part_tbl"</span>)</span><br><span class="line"><span class="comment">// Partitioned column `key` will be moved to the end of the schema.</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM hive_part_tbl"</span>).show()</span><br><span class="line"><span class="comment">// +-------+---+</span></span><br><span class="line"><span class="comment">// |  value|key|</span></span><br><span class="line"><span class="comment">// +-------+---+</span></span><br><span class="line"><span class="comment">// |val_238|238|</span></span><br><span class="line"><span class="comment">// | val_86| 86|</span></span><br><span class="line"><span class="comment">// |val_311|311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><p><br><strong>hbase</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    	.builder()</span><br><span class="line">    	.master(<span class="string">"local"</span>)</span><br><span class="line">    	.appName(<span class="string">"normal"</span>)</span><br><span class="line">    	.getOrCreate()</span><br><span class="line">	  spark.sparkContext.setLogLevel(<span class="string">"warn"</span>)</span><br><span class="line">		<span class="keyword">val</span> data = (<span class="number">0</span> to <span class="number">255</span>).map &#123; i =&gt;  <span class="type">HBaseRecord</span>(i, <span class="string">"extra"</span>)&#125;</span><br><span class="line"></span><br><span class="line">	  <span class="keyword">val</span> df:<span class="type">DataFrame</span> = spark.createDataFrame(data)</span><br><span class="line">	  </span><br><span class="line">    df.write</span><br><span class="line">	      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">	      .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; catalog))</span><br><span class="line">	      .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">	      .save()</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">catalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">                   |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">rec", "</span><span class="string">name":"</span>user_<span class="string">rec"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">                   |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">                   |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">                   |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">t", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">                   |&#125;</span></span><br><span class="line"><span class="string">                   |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseRecord</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">                  col0: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col1: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col2: <span class="type">Double</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col3: <span class="type">Float</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col4: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col5: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col6: <span class="type">Short</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col7: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  col8: <span class="type">Byte</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">HBaseRecord</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(i: <span class="type">Int</span>, t: <span class="type">String</span>): <span class="type">HBaseRecord</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> s = <span class="string">s""</span><span class="string">"row$&#123;"</span>%<span class="number">03</span><span class="string">d".format(i)&#125;"</span><span class="string">""</span></span><br><span class="line">    <span class="type">HBaseRecord</span>(s,</span><br><span class="line">      i % <span class="number">2</span> == <span class="number">0</span>,</span><br><span class="line">      i.toDouble,</span><br><span class="line">      i.toFloat,</span><br><span class="line">      i,</span><br><span class="line">      i.toLong,</span><br><span class="line">      i.toShort,</span><br><span class="line">      <span class="string">s"String<span class="subst">$i</span>: <span class="subst">$t</span>"</span>,</span><br><span class="line">      i.toByte)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="cpeixin.cn/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/" title="Spark SQL 入门" target="_blank" rel="external">cpeixin.cn/2017/03/25/Spark-SQL-%E5%85%A5%E9%97%A8/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external nofollow noopener noreferrer">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/cpeixin" target="_blank" class="img-burn thumb-sm visible-lg" rel="external nofollow noopener noreferrer"><img src="/images/avatar.jpg" class="img-rounded w-full" alt></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><span class="text-dark">Brent</span><small class="ml-1x">大数据工程师 &amp; 机器学习</small></a></h3><div>一心九用的工程师</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2017/04/01/Spark-SQL-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/" title="Spark SQL 源码解析"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2017/03/17/Spark-SQL-%E8%AE%B2%E8%A7%A3/" title="Spark SQL 讲解"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank" rel="external nofollow noopener noreferrer">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank" rel="external nofollow noopener noreferrer">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"SsxmBzBQ3R2S2zWTv0FrONel-gzGzoHsz",appKey:"w0K528Ye7NhOr07RHrzVzHbW",placeholder:"说点什么呢？",avatar:"mm",meta:meta,pageSize:"10",visitor:!0})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html><script type="text/javascript" src="//cdn.jsdelivr.net/gh/ygbhf/clicklove/clicklove.js"></script><!-- rebuild by neat -->