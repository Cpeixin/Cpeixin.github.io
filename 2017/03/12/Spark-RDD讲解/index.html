<!-- build time:Wed Apr 22 2020 12:20:12 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta name="referrer" content="no-referrer"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Spark - RDD讲解 | 布兰特 | 不忘初心</title><meta name="description" content="What is RDD在讲RDD之前，先和大家说一下在Spark中，我们分析数据的过程，主要会碰到RDD，DataFrame，DataSet概念，这三种都是我们计算过程中的数据单元。在Spark 2.0之前，主要的数据操作都是操作RDD，而在Spark 2.0以后，官方开始呼吁大家迁移到基于DataSet，即使你是从Spark 2.0以后开始接触的spark，但是我也很负责人的告诉你，RDD你必须"><meta property="og:type" content="article"><meta property="og:title" content="Spark - RDD讲解"><meta property="og:url" content="cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/index.html"><meta property="og:site_name" content="布兰特 | 不忘初心"><meta property="og:description" content="What is RDD在讲RDD之前，先和大家说一下在Spark中，我们分析数据的过程，主要会碰到RDD，DataFrame，DataSet概念，这三种都是我们计算过程中的数据单元。在Spark 2.0之前，主要的数据操作都是操作RDD，而在Spark 2.0以后，官方开始呼吁大家迁移到基于DataSet，即使你是从Spark 2.0以后开始接触的spark，但是我也很负责人的告诉你，RDD你必须"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586532563251-f0130d61-3860-45ce-9f05-6d1bc50120ba.png#align=left&display=inline&height=595&name=rdd-1024x595.png&originHeight=595&originWidth=1024&size=139751&status=done&style=none&width=1024"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586532448747-18dcac15-77c0-4b3a-a3ba-c0051f88621a.png#align=left&display=inline&height=408&name=1_gDz_AuuB-q0ux9Pl9CrvHA.png&originHeight=408&originWidth=648&size=26208&status=done&style=none&width=648"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586533275342-812154ce-5c32-4b4b-8d84-f8ce3214d419.png#align=left&display=inline&height=958&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-10%20%E4%B8%8B%E5%8D%8811.36.45.png&originHeight=958&originWidth=1502&size=184038&status=done&style=none&width=1502"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586535998022-c05a9f99-ec99-4918-8c80-518dc452f94c.png#align=left&display=inline&height=2263&name=image.png&originHeight=2263&originWidth=1776&size=821123&status=done&style=none&width=1776"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586536275036-3197bdda-1278-474a-a00a-d054cf9f5b75.jpeg#align=left&display=inline&height=1346&name=image.jpeg&originHeight=1346&originWidth=2554&size=358946&status=done&style=none&width=2554"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586536297023-1fcf7cd3-99bf-4509-933d-4c22c7b5a37d.jpeg#align=left&display=inline&height=1608&name=image.jpeg&originHeight=1608&originWidth=3313&size=608667&status=done&style=none&width=3313"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586537393419-89e9f68c-cd15-4fe4-9523-ed88c26d7f8a.jpeg#align=left&display=inline&height=359&name=map-reduce-vs-spark-16-638.jpg&originHeight=359&originWidth=638&size=26721&status=done&style=none&width=638"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586536929586-31e35a09-fff8-46fa-93e4-48ae08c98429.jpeg#align=left&display=inline&height=308&name=image.jpeg&originHeight=308&originWidth=441&size=61707&status=done&style=none&width=441"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586537506644-95706e68-11e9-46a9-b365-37c5586ef8ad.png#align=left&display=inline&height=879&name=Screen-Shot-2015-06-19-at-2.04.05-PM-1024x879.png&originHeight=879&originWidth=1024&size=175040&status=done&style=none&width=1024"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598425017-b170b3f9-8c83-413c-af4e-26910a40520a.png#align=left&display=inline&height=188&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.42.55.png&originHeight=188&originWidth=1032&size=74739&status=done&style=none&width=1032"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598440371-17cc06c4-0c80-435c-840a-f7fe4a9a9eb1.png#align=left&display=inline&height=120&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.43.10.png&originHeight=120&originWidth=1082&size=52437&status=done&style=none&width=1082"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598754505-bf688f34-4e86-4839-a469-2217c41f41c4.png#align=left&display=inline&height=898&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.47.53.png&originHeight=898&originWidth=1566&size=212047&status=done&style=none&width=1566"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598769350-0c9e738d-8582-4306-ab4d-1a1c59360b8f.png#align=left&display=inline&height=634&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.48.07.png&originHeight=634&originWidth=1706&size=177034&status=done&style=none&width=1706"><meta property="article:published_time" content="2017-03-11T22:41:50.000Z"><meta property="article:modified_time" content="2020-04-14T17:27:39.435Z"><meta property="article:author" content="Brent"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586532563251-f0130d61-3860-45ce-9f05-6d1bc50120ba.png#align=left&display=inline&height=595&name=rdd-1024x595.png&originHeight=595&originWidth=1024&size=139751&status=done&style=none&width=1024"><link rel="canonical" href="cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/index.html"><link rel="alternate" href="/atom.xml" title="布兰特 | 不忘初心" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.0"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Brent</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">大数据工程师 &amp; 机器学习</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Malaysia</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>人处在一种默默奋斗的状态，精神就会从琐碎生活中得到升华</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">30</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%B6%E6%9E%84/">架构</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">4</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Apriori/" rel="tag">Apriori</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/" rel="tag">EM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/" rel="tag">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/" rel="tag">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-Means/" rel="tag">K-Means</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/" rel="tag">KNN</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" rel="tag">LRU淘汰算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/" rel="tag">PageRank</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Random-Forest/" rel="tag">Random Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flask/" rel="tag">flask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kali/" rel="tag">kali</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/" rel="tag">mapreduce</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shadowsock/" rel="tag">shadowsock</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/" rel="tag">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" rel="tag">数据仓库</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" rel="tag">数据清洗</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" rel="tag">数据采集</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" rel="tag">服务器安全</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%88/" rel="tag">栈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" rel="tag">用户画像</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" rel="tag">线性表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag">词向量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8/" rel="tag">链表</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%9F%E5%88%97/" rel="tag">队列</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/Apriori/" style="font-size:13.2px">Apriori</a> <a href="/tags/Decision-Tree/" style="font-size:13.8px">Decision Tree</a> <a href="/tags/EM/" style="font-size:13.2px">EM</a> <a href="/tags/GPT-2/" style="font-size:13px">GPT-2</a> <a href="/tags/IDEA/" style="font-size:13px">IDEA</a> <a href="/tags/K-Means/" style="font-size:13px">K-Means</a> <a href="/tags/KNN/" style="font-size:13.2px">KNN</a> <a href="/tags/LRU%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/" style="font-size:13px">LRU淘汰算法</a> <a href="/tags/Naive-Bayes/" style="font-size:13.2px">Naive Bayes</a> <a href="/tags/PageRank/" style="font-size:13.2px">PageRank</a> <a href="/tags/Random-Forest/" style="font-size:13px">Random Forest</a> <a href="/tags/SVM/" style="font-size:13.2px">SVM</a> <a href="/tags/docker/" style="font-size:13.2px">docker</a> <a href="/tags/flask/" style="font-size:13.2px">flask</a> <a href="/tags/hdfs/" style="font-size:13.4px">hdfs</a> <a href="/tags/hive/" style="font-size:13.2px">hive</a> <a href="/tags/kali/" style="font-size:13px">kali</a> <a href="/tags/mapreduce/" style="font-size:13px">mapreduce</a> <a href="/tags/python/" style="font-size:14px">python</a> <a href="/tags/scala/" style="font-size:13.4px">scala</a> <a href="/tags/shadowsock/" style="font-size:13px">shadowsock</a> <a href="/tags/sklearn/" style="font-size:13px">sklearn</a> <a href="/tags/spark/" style="font-size:14px">spark</a> <a href="/tags/yarn/" style="font-size:13px">yarn</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size:13.4px">排序</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" style="font-size:14px">数据仓库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" style="font-size:13px">数据清洗</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/" style="font-size:13px">数据采集</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size:13px">数组</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8/" style="font-size:13.2px">服务器安全</a> <a href="/tags/%E6%A0%88/" style="font-size:13px">栈</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:13px">爬虫</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size:13.6px">特征工程</a> <a href="/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" style="font-size:13.2px">用户画像</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E8%A1%A8/" style="font-size:13px">线性表</a> <a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" style="font-size:13px">词向量</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size:13px">逻辑回归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size:13.2px">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size:13px">队列</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">二月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">21</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">13</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/2020/04/06/def-neverGrowUp/" class="title">def neverGrowUp()</a></p><p class="item-date"><time datetime="2020-04-05T16:00:00.000Z" itemprop="datePublished">2020-04-06</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/2020/04/04/%E6%8A%97%E7%96%AB%E8%8B%B1%E9%9B%84/" class="title">抗疫英雄</a></p><p class="item-date"><time datetime="2020-04-04T14:45:15.000Z" itemprop="datePublished">2020-04-04</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/python/">python</a></p><p class="item-title"><a href="/2020/03/11/python-Flask-Ajax-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/" class="title">python Flask &amp; Ajax 数据传输</a></p><p class="item-date"><time datetime="2020-03-11T14:43:01.000Z" itemprop="datePublished">2020-03-11</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/python/">python</a></p><p class="item-title"><a href="/2020/03/10/Python-Flask%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1-%E7%A4%BA%E4%BE%8B/" class="title">Python Flask接口设计-示例</a></p><p class="item-date"><time datetime="2020-03-10T15:08:35.000Z" itemprop="datePublished">2020-03-10</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a></p><p class="item-title"><a href="/2020/01/22/IDEA-install-TabNine/" class="title">IDEA install TabNine</a></p><p class="item-date"><time datetime="2020-01-22T02:26:15.000Z" itemprop="datePublished">2020-01-22</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-Spark-RDD讲解" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Spark - RDD讲解</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/" class="article-date"><time datetime="2017-03-11T22:41:50.000Z" itemprop="datePublished">2017-03-12</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/" class="leancloud_visitors" data-flag-title="Spark - RDD讲解">0</span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 6.6k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 24(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p><a name="3cPKu"></a></p><h2 id="What-is-RDD"><a href="#What-is-RDD" class="headerlink" title="What is RDD"></a>What is RDD</h2><p><br>在讲RDD之前，先和大家说一下在Spark中，我们分析数据的过程，主要会碰到RDD，DataFrame，DataSet概念，这三种都是我们计算过程中的数据单元。在Spark 2.0之前，主要的数据操作都是操作RDD，而在Spark 2.0以后，官方开始呼吁大家迁移到基于DataSet，即使你是从Spark 2.0以后开始接触的spark，但是我也很负责人的告诉你，RDD你必须要懂～～</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586532563251-f0130d61-3860-45ce-9f05-6d1bc50120ba.png#align=left&display=inline&height=595&name=rdd-1024x595.png&originHeight=595&originWidth=1024&size=139751&status=done&style=none&width=1024" alt="rdd-1024x595.png"><br></p><p><a name="oDNmw"></a></p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><br>弹性分布式数据集（Resilient Distributed Datasets : RDD），表示已被分区、不可变的，并能够被并行操作，可同错的数据集合。<br><br><br>针对上面的定义，还说描述的很抽象，接下来根据每个RDD的属性，进行逐点说明<br><br><br><strong>分区</strong><br><strong><br>分区这个概念在分布式计算中我们经常会看到，例如MapReduce中，数据在Map端写入到环形缓冲区，数据进行分区，reduce端读取相应的分区文件，还有在Kafka中，topic中的分区概念。<br><br><br>在RDD中，本质上是一个只读的分区记录集合。也就是我们要处理的源数据的抽象。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同节点上，从而可以在集群中的不同节点上进行并行计算。这也是它可以被并行处理的前提。反向来思考的话，如果RDD不可分区，只是一个单独不可拆分的数据块，那么集群中的节点怎么对这个源数据进行分布式并行计算呢？<br><br><br>逻辑上，我们可以认为 RDD 是一个大的数组。数组中的每个元素可以代表一个分区（Partition）。在物理存储中，每个分区指向一个存放在堆内内存和堆外内存或者磁盘中的数据块（Block），而这些数据块是独立的，它们可以被存放在系统中的不同节点。所以，RDD 只是抽象意义的数据集合，分区内部并不会存储具体的数据，</strong>仅保存了元数据信息<strong>。下图很好地展示了 RDD 的分区逻辑结构：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586532448747-18dcac15-77c0-4b3a-a3ba-c0051f88621a.png#align=left&display=inline&height=408&name=1_gDz_AuuB-q0ux9Pl9CrvHA.png&originHeight=408&originWidth=648&size=26208&status=done&style=none&width=648" alt="1_gDz_AuuB-q0ux9Pl9CrvHA.png"><br><br><br><br><br>RDD 中的每个分区存有它在该 RDD 中的 index。通过 RDD 的 ID 和分区的 index 可以唯一确定对应数据块的编号，从而通过底层存储层的接口中提取到数据进行处理。<br><br><br>在集群中，各个节点上的数据块会尽可能地存放在内存中，只有当内存没有空间时才会存入硬盘。这样可以最大化地减少硬盘读写的开销。虽然 RDD 内部存储的数据是只读的，但是，我们可以去修改（例如通过 repartition 转换操作）并行计算单元的划分结构，也就是分区的数量。<br><br><br><br><br></strong>不可变性<strong><br></strong><br>不可变性代表每一个 RDD 都是只读的，它所包含的分区信息不可以被改变。既然已有的 RDD 不可以被改变，我们只可以对现有的 RDD 进行转换（Transformation）操作，得到新的 RDD 作为中间计算的结果。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586533275342-812154ce-5c32-4b4b-8d84-f8ce3214d419.png#align=left&display=inline&height=958&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-10%20%E4%B8%8B%E5%8D%8811.36.45.png&originHeight=958&originWidth=1502&size=184038&status=done&style=none&width=1502" alt="屏幕快照 2020-04-10 下午11.36.45.png"><br><br><br>上图也就是刚刚提到的针对RDD的Transformation操作中，包括的map算子，flatMap算子，filter算子，这些算子我们接下来的文章在一一讲解，这里我想给大家看的是，在举例的这三个算子实现方法中，我们可以看到都new MapPartitionsRDD（），也就是说，在对一个已有的RDD进行转换操作的过程中，并不是对这个RDD进行直接的修改，变换，而是读取父RDD，创建了一个新的RDD进行转换并且最后返回。<br><br><br><br><br>那么这样会带来什么好处呢？显然，对于代表中间结果的 RDD，我们需要记录它是通过哪个 RDD 进行哪些转换操作得来，即依赖关系，而不用立刻去具体存储计算出的数据本身。这样做有助于提升 Spark 的计算效率，并且使错误恢复更加容易。<br><br><br>试想，在一个有 N 步的计算模型中，如果记载第 N 步输出 RDD 的节点发生故障，数据丢失，我们可以从第 N-1 步的 RDD 出发，再次计算，而无需重复整个 N 步计算过程。这样的容错特性也是 RDD 为什么是一个“弹性”的数据集的原因之一。后边我们会提到 RDD 如何存储这样的依赖关系。<br><br><br><strong>并行操作</strong><br><strong><br>由于单个 RDD 的分区特性，使得它天然支持并行操作，即不同节点上的数据可以被分别处理，然后产生一个新的 RDD。<br><br><br></strong>容错性<strong><br></strong><br>为了保证RDD 中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。 相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的 特定数据转换（Transformation）操作（filter, map, join etc.)行为。当这个RDD的部分分区数据丢失时 ，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制 了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。</p><p>另外，在RDD计算中，也通过checkpoint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是 logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通 过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算 生成丢失的分区数据。<br></p><p><a name="GAzgp"></a></p><h3 id="RDD-五大特性"><a href="#RDD-五大特性" class="headerlink" title="RDD 五大特性"></a>RDD 五大特性</h3><ul><li>A list of partitions</li></ul><p>RDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。</p><ul><li>A function for computing each split</li></ul><p>一个函数计算每一个分片，RDD的每个partition上面都会有function，也就是函数应用</p><ul><li>A list of dependencies on other RDDs</li></ul><p>RDD会记录它的依赖 ，依赖还具体分为宽依赖和窄依赖，RDD在计算的过程中，不断的转换，在内存中，不落地磁盘，如果某一环节出错，可以根据依赖来找回上一状态的RDD，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。</p><ul><li><p>Optionally,a Partitioner for Key-value RDDs</p><p>可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面</p></li><li><p>Optionally, a list of preferred locations to compute each split on</p></li></ul><p>我们的原则是移动计算，不移动数据，默认的是，磁盘中的数据是作为RDD加载到本机的内存中，但是，Spark这里给出了一个可选项，可以选择加载到指定的机器内存中，就是可以选择将数据放在那几台性能好的节点上<br><br><br></p><p><a name="G3itb"></a></p><h3 id="RDD-结构"><a href="#RDD-结构" class="headerlink" title="RDD 结构"></a>RDD 结构</h3><p><br>通过上述讲解，我们了解了 RDD 的基本特性。而且，我们还提到每一个 RDD 里都会包括分区信息、所依赖的父 RDD 以及通过怎样的转换操作才能由父 RDD 得来等信息。实际上 RDD 的结构远比你想象的要复杂，让我们来看一个 RDD 的简易结构示意图：<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586535998022-c05a9f99-ec99-4918-8c80-518dc452f94c.png#align=left&display=inline&height=2263&name=image.png&originHeight=2263&originWidth=1776&size=821123&status=done&style=none&width=1776" alt="image.png"><br><br><br>SparkContext 是所有 Spark 功能的入口，它代表了与 Spark 节点的连接，可以用来创建 RDD 对象以及在节点中的广播变量等。一个线程只有一个 SparkContext。SparkConf 则是一些参数配置信息。感兴趣的同学可以去阅读官方的技术文档，一些相对不重要的概念我就不再赘述了。Partitions 前文中我已经提到过，它代表 RDD 中数据的逻辑结构，每个 Partition 会映射到某个节点内存或硬盘的一个数据块。Partitioner 决定了 RDD 的分区方式，目前有两种主流的分区方式：Hash partitioner 和 Range partitioner。Hash，顾名思义就是对数据的 Key 进行散列分区，Range 则是按照 Key 的排序进行均匀分区。此外我们还可以创建自定义的 Partitioner。<br></p><p><a name="ttfuZ"></a></p><h3 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h3><p><br>Dependencies 是 RDD 中最重要的组件之一。如前文所说，Spark 不需要将每个中间计算结果进行数据复制以防数据丢失，因为每一步产生的 RDD 里都会存储它的依赖关系，即它是通过哪个 RDD 经过哪个转换操作得到的。细心的读者会问这样一个问题，父 RDD 的分区和子 RDD 的分区之间是否是一对一的对应关系呢？Spark 支持两种依赖关系：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586536275036-3197bdda-1278-474a-a00a-d054cf9f5b75.jpeg#align=left&display=inline&height=1346&name=image.jpeg&originHeight=1346&originWidth=2554&size=358946&status=done&style=none&width=2554" alt="image.jpeg"><br><br><br>窄依赖就是父 RDD 的分区可以一一对应到子 RDD 的分区，宽依赖就是父 RDD 的每个分区可以被多个子 RDD 的分区使用。<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586536297023-1fcf7cd3-99bf-4509-933d-4c22c7b5a37d.jpeg#align=left&display=inline&height=1608&name=image.jpeg&originHeight=1608&originWidth=3313&size=608667&status=done&style=none&width=3313" alt="image.jpeg"><br><br><br>显然，窄依赖允许子 RDD 的每个分区可以被并行处理产生，而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理。<strong>宽依赖本质就是shuffle,计算代价大,经过大量shuffle生成的RDD，建议进行缓存。这样避免失败后重新计算带来的开销</strong><br><br><br>如上图所示，一些转换操作如 map、filter 会产生窄依赖关系，而 Join、groupBy 则会生成宽依赖关系。这很容易理解，因为 map 是将分区里的每一个元素通过计算转化为另一个元素，一个分区里的数据不会跑到两个不同的分区。而 groupBy 则要将拥有所有分区里有相同 Key 的元素放到同一个目标分区，而每一个父分区都可能包含各种 Key 的元素，所以它可能被任意一个子分区所依赖。<br><br><br>Spark 之所以要区分宽依赖和窄依赖是出于以下两点考虑：<br></p><ol><li>窄依赖可以支持在同一个节点上链式执行多条命令，例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。</li><li>从失败恢复的角度考虑，窄依赖的失败恢复更有效，因为它只需要重新计算丢失的父分区即可，而宽依赖牵涉到 RDD 各级的多个父分区。</li></ol><p><a name="fKFPg"></a></p><h3 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h3><p><br>结合上篇spark运行原理和这篇RDD的讲述，我们来讲一下关于任务运行中，Job，Stage，Task的划分<br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586537393419-89e9f68c-cd15-4fe4-9523-ed88c26d7f8a.jpeg#align=left&display=inline&height=359&name=map-reduce-vs-spark-16-638.jpg&originHeight=359&originWidth=638&size=26721&status=done&style=none&width=638" alt="map-reduce-vs-spark-16-638.jpg"><br><br><br>spark任务运行中，会存在一个或者多个Job，action算子的触发会生成一个Job, Job会提交给DAGScheduler,分解成Stage。<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1072113/1586536929586-31e35a09-fff8-46fa-93e4-48ae08c98429.jpeg#align=left&display=inline&height=308&name=image.jpeg&originHeight=308&originWidth=441&size=61707&status=done&style=none&width=441" alt="image.jpeg"><br>上图是一个job被切割成三个Stage。</p><p>job分割stage的规则是从G端向前开始分割，遇到宽依赖，就分割一个stage.。<br>F–&gt;G 切割 stage2 和 stage3<br>A–&gt;B stage1<br><br><br>上图，程序的运行最小单元是Task，就拿stage2来举例：<br>stage2有4个Task: C端有2个partition，E端有两个partition。每个partition为开始，最终到F端，作为一个Task。 其中B阶段呢，属于一个程序级别的优化操作。一般分布式程序中，为了让程序能平稳的执行，就要做一些优化操作。<br><br><br>在以后的Spark开发中，我们在Web UI中会经常看到类似于下图的工作流程，这里展示了一个Job的划分和对应操作的细节。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586537506644-95706e68-11e9-46a9-b365-37c5586ef8ad.png#align=left&display=inline&height=879&name=Screen-Shot-2015-06-19-at-2.04.05-PM-1024x879.png&originHeight=879&originWidth=1024&size=175040&status=done&style=none&width=1024" alt="Screen-Shot-2015-06-19-at-2.04.05-PM-1024x879.png"><br><br><br><br><br></p><p><a name="HT2Rw"></a></p><h2 id="How-use-RDD"><a href="#How-use-RDD" class="headerlink" title="How use RDD"></a>How use RDD</h2><p><br>上面讲了很长篇幅的RDD概念和属性，那么我们该如何开始实操RDD呢？我们的第一个RDD来自哪里？上面我说过了，其实RDD就是我们要进行处理的源数据集合。在实际的业务场景中，对于离线数据分析，大多数的场景下，源数据可以是Spark从Hive、HBase中读取，转化成RDD，再细小一点的场景，源数据可以是一个csv报表数据，读取目标CSV进行RDD的转换。那么在下面的讲解中，我们无需对接上游生产环境的数据源，我们可以在IDEA中直接进行RDD的创建，随后再进行各种转换算子和行动算子的操作演示<br></p><p><a name="6SATi"></a></p><h3 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h3><p><br>RDD的创建有三种方法</p><ul><li>利用内存中集合中创建</li><li>利用外部文件创建</li><li>由其他RDD创建新的RDD</li></ul><p><br>从集合中创建可以使用parallelize() 和 makeRDD()方法创建<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598425017-b170b3f9-8c83-413c-af4e-26910a40520a.png#align=left&display=inline&height=188&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.42.55.png&originHeight=188&originWidth=1032&size=74739&status=done&style=none&width=1032" alt="屏幕快照 2020-04-11 下午5.42.55.png"><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598440371-17cc06c4-0c80-435c-840a-f7fe4a9a9eb1.png#align=left&display=inline&height=120&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.43.10.png&originHeight=120&originWidth=1082&size=52437&status=done&style=none&width=1082" alt="屏幕快照 2020-04-11 下午5.43.10.png"><br><br><br>我们可以看到，这两个方法需要我们传入的参数是 Seq[T] 集合，返回的都是RDD[T]。注意，makeRDD给了两种方法，这里先记为第一种makeRDD和第二种makeRDD<br><br><br>这里，先把源码po出来<br><br><br><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598754505-bf688f34-4e86-4839-a469-2217c41f41c4.png#align=left&display=inline&height=898&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.47.53.png&originHeight=898&originWidth=1566&size=212047&status=done&style=none&width=1566" alt="屏幕快照 2020-04-11 下午5.47.53.png"><img src="https://cdn.nlark.com/yuque/0/2020/png/1072113/1586598769350-0c9e738d-8582-4306-ab4d-1a1c59360b8f.png#align=left&display=inline&height=634&name=%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-04-11%20%E4%B8%8B%E5%8D%885.48.07.png&originHeight=634&originWidth=1706&size=177034&status=done&style=none&width=1706" alt="屏幕快照 2020-04-11 下午5.48.07.png"><br><br><br>可以清晰的看到，第一种makeRDD的源码实现中，实际上是调用了parallelize（），并且都可以指定分区数量，而且在注释中清晰的写明了，第一种makeRDD和parallelize是identical（完全相同的）。再来看第二种makeRDD，第二种实现可以为数据提供位置信息，并且不能指定RDD的分区数量，除此之外的实现和parallelize函数也是一致的<br><br><br>创建示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> make_rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> make_rdd_1: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">"Brent"</span>,<span class="string">"HayLee"</span>,<span class="string">"Henry"</span>))</span><br><span class="line"><span class="keyword">val</span> make_rdd_2: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> make_rdd_3: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">"Brent"</span>,<span class="string">"HayLee"</span>,<span class="string">"Henry"</span>),<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">println(make_rdd.partitions.size)</span><br><span class="line">println(make_rdd_1.partitions.size)</span><br><span class="line">println(make_rdd_2.partitions.size)</span><br><span class="line">println(make_rdd_3.partitions.size)</span><br></pre></td></tr></table></figure><p><br>结果：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><br><br>**外部文件创建**<br>**<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"hdfs://hadoop000:9000/xxx/data.txt"</span>) <span class="comment">// 读取hdfs文件</span></span><br><span class="line">sc.textFile(<span class="string">"data.txt"</span>) <span class="comment">// 这里纯粹的本地文件是不推荐的，</span></span><br><span class="line">													 因为这个文件访问是针对每一个<span class="type">Worker</span>都要是能访问的</span><br><span class="line">													 换言之,如果是本地文件,则必须保证每一个<span class="type">Worker</span>的本地都有一份这个文件</span><br></pre></td></tr></table></figure><br><br><br><p><a name="BLLcC"></a></p><h3 id="RDD-的转换操作"><a href="#RDD-的转换操作" class="headerlink" title="RDD 的转换操作"></a>RDD 的转换操作</h3><p><br>RDD 的数据操作分为两种：转换（Transformation）和动作（Action）。顾名思义，转换是用来把一个 RDD 转换成另一个 RDD，而动作则是通过计算返回一个结果。<br><br><br>下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.rdd" target="_blank" rel="external nofollow noopener noreferrer">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaRDD.html" target="_blank" rel="external nofollow noopener noreferrer">Java</a>，<a href="http://spark.apachecn.org/#/api/python/pyspark.html?id=pyspark.rdd" target="_blank" rel="external nofollow noopener noreferrer">Python</a>，<a href="http://spark.apachecn.org/#/api/R/index.html" target="_blank" rel="external nofollow noopener noreferrer">R</a>）和 pair RDD 函数文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.pairrddfunctions" target="_blank" rel="external nofollow noopener noreferrer">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaPairRDD.html" target="_blank" rel="external nofollow noopener noreferrer">Java</a>）</p><table><thead><tr><th>Transformation（转换）</th><th>Meaning（含义）</th></tr></thead><tbody><tr><td><strong>map</strong>(<em>func</em>)</td><td>返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中的元素应用一个函数 <em>func</em> 来生成。</td></tr><tr><td><strong>filter</strong>(<em>func</em>)</td><td>返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中应用一个函数 <em>func</em> 且返回值为 true 的元素来生成。</td></tr><tr><td><strong>flatMap</strong>(<em>func</em>)</td><td>与 map 类似，但是每一个输入的 item 可以被映射成 0 个或多个输出的 items（所以 <em>func</em> 应该返回一个 Seq 而不是一个单独的 item）。</td></tr><tr><td><strong>mapPartitions</strong>(<em>func</em>)</td><td>与 map 类似，但是单独的运行在在每个 RDD 的 partition（分区，block）上，所以在一个类型为 T 的 RDD 上运行时 <em>func</em> 必须是 Iterator<t>=&gt; Iterator<u> 类型。</u></t></td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td><td>与 mapPartitions 类似，但是也需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 <em>func_，所以在一个类型为 T 的 RDD 上运行时 _func</em> 必须是 (Int, Iterator<t>) =&gt; Iterator<u> 类型。</u></t></td></tr><tr><td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td><td>样本数据，设置是否放回（withReplacement），采样的百分比（_fraction_）、使用指定的随机数生成器的种子（seed）。</td></tr><tr><td><strong>union</strong>(<em>otherDataset</em>)</td><td>返回一个新的 dataset，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的并集。</td></tr><tr><td><strong>intersection</strong>(<em>otherDataset</em>)</td><td>返回一个新的 RDD，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的交集。</td></tr><tr><td><strong>distinct</strong>([_numTasks_]))</td><td>返回一个新的 dataset，它包含了 source dataset（源数据集）中去重的元素。</td></tr><tr><td><strong>groupByKey</strong>([_numTasks_])</td><td>在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable<v>) .</v></td></tr><tr><td><strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 来计算性能会更好.</td><td></td></tr><tr><td><strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 <code>numTasks</code> 参数来设置不同的任务数。</td><td></td></tr><tr><td><strong>reduceByKey</strong>(<em>func</em>, [_numTasks_])</td><td>在 (K, V) pairs 的 dataset 上调用时，返回 dataset of (K, V) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的函数 <em>func</em> 来进行聚合的，它必须是 type (V,V) =&gt; V 的类型。像 <code>groupByKey</code> 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。</td></tr><tr><td><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [_numTasks_])</td><td>在 (K, V) pairs 的 dataset 上调用时，返回 (K, U) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的 combine 函数以及一个 neutral “0” 值来进行聚合的。允许聚合值的类型与输入值的类型不一样，同时避免不必要的配置。像 <code>groupByKey</code> 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。</td></tr><tr><td><strong>sortByKey</strong>([_ascending_], [_numTasks_])</td><td>在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset，由 boolean 类型的 <code>ascending</code> 参数来指定。</td></tr><tr><td><strong>join</strong>(<em>otherDataset</em>, [_numTasks_])</td><td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 来实现。</td></tr><tr><td><strong>cogroup</strong>(<em>otherDataset</em>, [_numTasks_])</td><td>在一个 (K, V) 和的 dataset 上调用时，返回一个 (K, (Iterable<v>, Iterable<w>)) tuples 的 dataset。这个操作也调用了 <code>groupWith</code>。</w></v></td></tr><tr><td><strong>cartesian</strong>(<em>otherDataset</em>)</td><td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) pairs 类型的 dataset（所有元素的 pairs，即笛卡尔积）。</td></tr><tr><td><strong>pipe</strong>(<em>command</em>, <em>[envVars]</em>)</td><td>通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回。</td></tr><tr><td><strong>coalesce</strong>(<em>numPartitions</em>)</td><td>Decrease（降低）RDD 中 partitions（分区）的数量为 numPartitions。对于执行过滤后一个大的 dataset 操作是更有效的。</td></tr><tr><td><strong>repartition</strong>(<em>numPartitions</em>)</td><td>Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀。该操作总是通过网络来 shuffles 所有的数据。</td></tr><tr><td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td><td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并在每个结果分区中，按照 key 值对记录排序。这比每一个分区中先调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行。</td></tr></tbody></table><p><br>下表列出了一些 Spark 常用的 actions 操作。详细请参考 RDD API 文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.rdd" target="_blank" rel="external nofollow noopener noreferrer">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaRDD.html" target="_blank" rel="external nofollow noopener noreferrer">Java</a>，<a href="http://spark.apachecn.org/#/api/python/pyspark.html?id=pyspark.rdd" target="_blank" rel="external nofollow noopener noreferrer">Python</a>，<a href="http://spark.apachecn.org/#/api/R/index.html" target="_blank" rel="external nofollow noopener noreferrer">R</a>）<br>和 pair RDD 函数文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.pairrddfunctions" target="_blank" rel="external nofollow noopener noreferrer">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaPairRDD.html" target="_blank" rel="external nofollow noopener noreferrer">Java</a>）。<br></p><table><thead><tr><th>Action（动作）</th><th>Meaning（含义）</th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>使用函数 <em>func</em> 聚合 dataset 中的元素，这个函数 <em>func</em> 输入为两个元素，返回为一个元素。这个函数应该是可交换（commutative）和关联（associative）的，这样才能保证它可以被并行地正确计算。</td></tr><tr><td><strong>collect</strong>()</td><td>在 driver 程序中，以一个 array 数组的形式返回 dataset 的所有元素。这在过滤器（filter）或其他操作（other operation）之后返回足够小（sufficiently small）的数据子集通常是有用的。</td></tr><tr><td><strong>count</strong>()</td><td>返回 dataset 中元素的个数。</td></tr><tr><td><strong>first</strong>()</td><td>返回 dataset 中的第一个元素（类似于 take(1)。</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td></tr><tr><td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [_seed_])</td><td>对一个 dataset 进行随机抽样，返回一个包含 <em>num</em> 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。</td></tr><tr><td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td>返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 <em>n</em> 个元素。</td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>将 dataset 中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录。</td></tr><tr><td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td><td></td></tr><tr><td>(Java and Scala)</td><td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int，Double，String 等等)。</td></tr><tr><td><strong>saveAsObjectFile</strong>(<em>path</em>)</td><td></td></tr><tr><td>(Java and Scala)</td><td>使用 Java 序列化（serialization）以简单的格式（simple format）编写数据集的元素，然后使用 <code>SparkContext.objectFile()</code> 进行加载。</td></tr><tr><td><strong>countByKey</strong>()</td><td>仅适用于（K,V）类型的 RDD。返回具有每个 key 的计数的（K , Int）pairs 的 hashmap。</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>对 dataset 中每个元素运行函数 _func_。这通常用于副作用（side effects），例如更新一个 <a href="http://spark.apachecn.org/#/docs/4?id=accumulators" target="_blank" rel="external nofollow noopener noreferrer">Accumulator</a>（累加器）或与外部存储系统（external storage systems）进行交互。<strong>Note</strong>：修改除 <code>foreach()</code>之外的累加器以外的变量（variables）可能会导致未定义的行为（undefined behavior）。详细介绍请阅读 <a href="http://spark.apachecn.org/#/docs/4?id=understanding-closures-a-nameclosureslinka" target="_blank" rel="external nofollow noopener noreferrer">Understanding closures（理解闭包）</a> 部分。</td></tr></tbody></table><p>该 Spark RDD API 还暴露了一些 actions（操作）的异步版本，例如针对 <code>foreach</code> 的 <code>foreachAsync</code>，它们会立即返回一个<code>FutureAction</code> 到调用者，而不是在完成 action 时阻塞。这可以用于管理或等待 action 的异步执行。</p><p>部分Transformation算子操作：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> rdd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">transformation_func</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"transformation_func"</span>)</span><br><span class="line">      .setMaster(<span class="string">"local"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> original_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize(<span class="type">Array</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">2</span>),(<span class="string">"c"</span>,<span class="number">4</span>),(<span class="string">"c"</span>,<span class="number">4</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> map_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.map(x =&gt;(x._1,x._2+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"map操作：对original_rdd每个数据的第二个元素+1"</span>)</span><br><span class="line">    map_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"filter操作：过滤掉original_rdd中，第一个元素不为a的数据"</span>)</span><br><span class="line">    <span class="keyword">var</span> filter_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.filter(x =&gt; x._1 == <span class="string">"a"</span>)</span><br><span class="line">    filter_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"flatmap操作：对original_rdd做映射扁平化操作"</span>)</span><br><span class="line">    <span class="keyword">val</span> flatmap_rdd: <span class="type">RDD</span>[<span class="type">Char</span>] = original_rdd.flatMap(x=&gt; x._1 + x._2)</span><br><span class="line">    flatmap_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"mapPartitions操作：对original_rdd每个分区做相应操作"</span>)</span><br><span class="line">    <span class="keyword">val</span> mapPartitions_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.mapPartitions(x=&gt;&#123;x.map(item=&gt;(item._1,item._2+<span class="number">1</span>))&#125;)</span><br><span class="line">    mapPartitions_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    println(<span class="string">"sample操作：提取样本"</span>)</span><br><span class="line">    <span class="keyword">val</span> sample_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.sample(<span class="literal">true</span>, <span class="number">0.25</span>)</span><br><span class="line">    sample_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    println(<span class="string">"distinct操作：去重"</span>)</span><br><span class="line">    <span class="keyword">val</span> distinct_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.distinct()</span><br><span class="line">    distinct_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    println(<span class="string">"groupbykey操作：分组聚合"</span>)</span><br><span class="line">    <span class="keyword">val</span> groupByKey_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = original_rdd.groupByKey()</span><br><span class="line">    groupByKey_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    println(<span class="string">"reduceByKey操作：聚合"</span>)</span><br><span class="line">    <span class="keyword">val</span> reduceByKey_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.reduceByKey(_+_)</span><br><span class="line">    reduceByKey_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"sortByKey操作：排序"</span>)</span><br><span class="line">    <span class="keyword">val</span> sortByKey_rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = original_rdd.sortByKey()</span><br><span class="line">    sortByKey_rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><p><a name="urGy2"></a></p><h3 id="RDD-的持久化（缓存）"><a href="#RDD-的持久化（缓存）" class="headerlink" title="RDD 的持久化（缓存）"></a>RDD 的持久化（缓存）</h3><p><br>每当我们对 RDD 调用一个新的 action 操作时，整个 RDD 都会从头开始运算。因此，如果某个 RDD 会被反复重用的话，每次都从头计算非常低效，我们应该对多次使用的 RDD 进行一个持久化操作。<br><br><br>Spark 的 <strong>persist() 和 cache()</strong> 方法支持将 RDD 的数据缓存至内存或硬盘中，这样当下次对同一 RDD 进行 Action 操作时，可以直接读取 RDD 的结果，大幅提高了 Spark 的计算效率。<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">rdd1 = rdd.map(lambda x: x+<span class="number">5</span>)</span><br><span class="line">rdd2 = rdd1.filter(lambda x: x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">rdd2.persist()</span><br><span class="line">count = rdd2.count() <span class="comment">// 3</span></span><br><span class="line">first = rdd2.first() <span class="comment">// 6</span></span><br><span class="line">rdd2.unpersist()</span><br></pre></td></tr></table></figure><p><br>在文中的代码例子中你可以看到，我们对 RDD2 进行了多个不同的 action 操作。由于在第四行我把 RDD2 的结果缓存在内存中，所以 Spark 无需从一开始的 rdd 开始算起了（持久化处理过的 RDD 只有第一次有 action 操作时才会从源头计算，之后就把结果存储下来，所以在这个例子中，count 需要从源头开始计算，而 first 不需要）。<br><br><br>在缓存 RDD 的时候，它所有的依赖关系也会被一并存下来。所以持久化的 RDD 有自动的容错机制。如果 RDD 的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算。持久化可以选择不同的存储级别。正如我们讲 RDD 的结构时提到的一样，有 MEMORY_ONLY，MEMORY_AND_DISK，DISK_ONLY 等。cache() 方法会默认取 MEMORY_ONLY 这一级别。<br></p><p><a name="RLBIS"></a></p><h3 id="RDD-Checkpoint"><a href="#RDD-Checkpoint" class="headerlink" title="RDD Checkpoint"></a>RDD Checkpoint</h3><p><br>Checkpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，但是在正常生产环境下放在 HDFS 上，这就天然的借助HDFS 高可靠的特征来完成最大化的<strong>可靠的持久化数据的方式</strong>。</p><p>在进行 RDD 的 Checkpoint 的时候，其所依赖的所有 RDD 都会清空掉；官方建议如果要进行 checkpoint 时，必需先缓存在内存中。但实际可以考虑缓存在本地磁盘上或者是第三方组件，e.g. Taychon 上。在进行 checkpoint 之前需要通过 SparkConetxt 设置 checkpoint 的文件夹<br><br><br>作为最佳实践，一般在进行 checkpoint 方法调用前都要进行 persists 来把当前 RDD 的数据持久化到内存或者是磁盘上，这是因为 checkpoint 是 lazy 级别，必需有 Job 的执行且在Job 执行完成后才会从后往前回溯哪个 RDD 进行了Checkpoint 标记，然后对该标记了要进行 Checkpoint 的 RDD 新启动一个Job 执行具体 Checkpoint 的过程<br><br><br></p><p><a name="PPjSG"></a></p><h2 id="Why-use-RDD"><a href="#Why-use-RDD" class="headerlink" title="Why use RDD"></a>Why use RDD</h2><p><br>首先，它的数据可以尽可能地存在内存中，从而大大提高的数据处理的效率；其次它是分区存储，所以天然支持并行处理；而且它还存储了每一步骤计算结果之间的依赖关系，从而大大提升了数据容错性和错误恢复的正确率，使 Spark 更加可靠。</p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/" title="Spark - RDD讲解" target="_blank" rel="external">cpeixin.cn/2017/03/12/Spark-RDD%E8%AE%B2%E8%A7%A3/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external nofollow noopener noreferrer">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/cpeixin" target="_blank" class="img-burn thumb-sm visible-lg" rel="external nofollow noopener noreferrer"><img src="/images/avatar.jpg" class="img-rounded w-full" alt></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/cpeixin" target="_blank" rel="external nofollow noopener noreferrer"><span class="text-dark">Brent</span><small class="ml-1x">大数据工程师 &amp; 机器学习</small></a></h3><div>一心九用的工程师</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2017/03/15/Spark-Core-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%93%8D%E4%BD%9C/" title="Spark Core 数据存储操作"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2017/03/10/Spark-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/" title="Spark 运行原理"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/cpeixin" target="_blank" title="Github" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/1970875963" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top" rel="external nofollow noopener noreferrer"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank" rel="external nofollow noopener noreferrer">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank" rel="external nofollow noopener noreferrer">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"SsxmBzBQ3R2S2zWTv0FrONel-gzGzoHsz",appKey:"w0K528Ye7NhOr07RHrzVzHbW",placeholder:"说点什么呢？",avatar:"mm",meta:meta,pageSize:"10",visitor:!0})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html><!-- rebuild by neat -->